{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classification model\n",
    "Here we use machine learning techniques to create and validate a model that can predict the probability of a relatively rare event (imbalanced classes problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# import generic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# pd.options.display.max_columns = None\n",
    "# pd.options.display.max_colwidth = 100\n",
    "from IPython.display import display\n",
    "\n",
    "# visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "%matplotlib inline\n",
    "\n",
    "# module loading settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load to data frame\n",
    "df = pd.read_csv('')\n",
    "\n",
    "# extract and remove timestamps from data frame\n",
    "timestamps = df['timestamp']\n",
    "df.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "# determine categoricals\n",
    "high_capacity = df.columns.values[~np.array(df.dtypes == np.number)].tolist()\n",
    "print \"high capacity categorical feature columns:\"\n",
    "print high_capacity\n",
    "\n",
    "# print some info\n",
    "print \"{:d} observations\".format(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model specification\n",
    "Here we set some specifications for the model: type, how it should be fitted, optimized and validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_type = 'rf'  # the classification algorithm\n",
    "tune_model = False  # optimize hyperparameters\n",
    "\n",
    "cross_val_method = 'temporal'  # cross-validation routine\n",
    "\n",
    "cost_fp = 1000  # preferably in euros!\n",
    "benefit_tp = 3000\n",
    "class_weights = {0: cost_fp, 1: benefit_tp}  # costs for fn and fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross-validation procedure\n",
    "To validate whether the model makes sensible predictions, we need to perform cross-validation. The exact procedure for this is specified below. Random cross-validation (set-aside a random sample for testing) is fast, but temporal cross-validation (set-aside a time period for testing) gives the most realistic results due to the resemblence of real-world model usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, train_test_split\n",
    "\n",
    "#source: https://github.com/BigDataRepublic/bdr-analytics-py\n",
    "#! pip install -e git+ssh://git@github.com/BigDataRepublic/bdr-analytics.git#egg=bdranalytics-0.1\n",
    "from bdranalytics.pipeline.encoders import WeightOfEvidenceEncoder\n",
    "from bdranalytics.model_selection.growingwindow import IntervalGrowingWindow\n",
    "\n",
    "from sklearn.metrics import average_precision_score, make_scorer, roc_auc_score\n",
    "\n",
    "if cross_val_method is 'random':\n",
    "  \n",
    "    # split train data into stratified random folds\n",
    "    cv_dev = StratifiedShuffleSplit(test_size=0.1, train_size=0.1, n_splits=5, random_state=1)\n",
    "    \n",
    "    cv_test = StratifiedShuffleSplit(test_size=0.33, n_splits=1, random_state=2)\n",
    "\n",
    "elif cross_val_method is 'temporal':\n",
    "    \n",
    "    train_size = pd.Timedelta(days=365 * 4 )\n",
    "    \n",
    "    # create a cross-validation routine for parameter tuning\n",
    "    cv_dev = IntervalGrowingWindow(timestamps=timestamps,\n",
    "                               test_start_date=pd.datetime(year=2015, month=1, day=1),\n",
    "                               test_end_date=pd.datetime(year=2015, month=12, day=31),\n",
    "                               test_size=pd.Timedelta(days=30), \n",
    "                               train_size=train_size)\n",
    "    \n",
    "    # create a cross-validation routine for model evaluation\n",
    "    cv_test = IntervalGrowingWindow(timestamps=timestamps,\n",
    "                               test_start_date=pd.datetime(year=2016, month=1, day=1),\n",
    "                               test_end_date=pd.datetime(year=2016, month=8, day=31),\n",
    "                               test_size=pd.Timedelta(days=2*30),\n",
    "                               train_size=train_size)    \n",
    "\n",
    "# number of parallel jobs for cross-validation\n",
    "n_jobs = 1\n",
    "\n",
    "# two functions for advanced performance evaluation metrics\n",
    "def roc_auc(y_true, y_pred):\n",
    "    return roc_auc_score(pd.get_dummies(y_true), y_pred)\n",
    "\n",
    "roc_auc_scorer = make_scorer(roc_auc, needs_proba=True)\n",
    "\n",
    "def pr_auc(y_true, y_pred):\n",
    "    return average_precision_score(pd.get_dummies(y_true), y_pred, average=\"micro\")\n",
    "\n",
    "pr_auc_scorer = make_scorer(pr_auc, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# convert date frame to bare X and y variables for the model pipeline\n",
    "y_col = 'target'\n",
    "X = df.copy().drop(y_col, axis=1)\n",
    "y = np.array(df[y_col])\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# define preprocessing steps\n",
    "preproc_steps = [('woe', WeightOfEvidenceEncoder(cols=high_capacity)),\n",
    "                 ('imputer', Imputer(missing_values='NaN', strategy='median', axis=0)),\n",
    "                 ('standardizer', StandardScaler(with_mean=True, with_std=True))]\n",
    "\n",
    "# specification of different model types and their defaults\n",
    "model_steps_dict = {'lr': [('lr', LogisticRegression(C=0.001, penalty='l2', tol=0.01,\n",
    "                                                     class_weight=class_weights))],\n",
    "                   'rf': [('rf', RandomForestClassifier(n_estimators=400, max_features='auto',\n",
    "                                                       class_weight=class_weights))],\n",
    "                   'gbc': [('gbc', GradientBoostingClassifier(n_estimators=400, max_depth=3))],\n",
    "                   'xgb': [('xgb', XGBClassifier(scale_pos_weight=class_weights[1],\n",
    "                                                 n_estimators=100, max_depth=4))],\n",
    "                   'dummy': [('dummy', DummyClassifier(strategy='prior'))]\n",
    "                   }\n",
    "\n",
    "# specification of the different model hyperparameters and tuning space\n",
    "model_params_grid = {'lr': {'lr__C': [1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "                    'rf': {'rf__max_features': [3, n_features, np.sqrt(n_features)],\n",
    "                           'rf__n_estimators': [10, 100, 1000]},\n",
    "                     'gbc': {'gbc__n_estimators': [100, 200]},\n",
    "                     'xgb': {'xgb__max_depth': [3,6,9],\n",
    "                             'xgb__reg_alpha': [0,5,15],\n",
    "                             'xgb__reg_lambda': [0,5,15],\n",
    "                             'xgb__gamma' : [0,10,50,100]},\n",
    "                     'dummy': {}}\n",
    "\n",
    "# store the model step\n",
    "model_steps = model_steps_dict[model_type]\n",
    "\n",
    "# combine everything in one pipeline\n",
    "estimator = Pipeline(steps=(preproc_steps + model_steps))\n",
    "print estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model parameter tuning\n",
    "If desired, we can optimize the model hyperparameters to get the best possible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# procedure depends on cross-validation type\n",
    "if cross_val_method is 'random': \n",
    "    train_index = next(cv_test.split(X, y))[0]\n",
    "    X_dev = X.iloc[train_index,:]\n",
    "    y_dev = y[train_index]\n",
    "elif cross_val_method is 'temporal':\n",
    "    X_dev = X\n",
    "    y_dev = y\n",
    "\n",
    "# setting to include class weights in the gradient boosting model\n",
    "if model_type is 'gbc':\n",
    "    sample_weights = np.array(map(lambda x: class_weights[x], y_dev))\n",
    "    fit_params = {'gbc__sample_weight': sample_weights}\n",
    "else: \n",
    "    fit_params = {}\n",
    "\n",
    "# tune model with a parameter grid search if desired\n",
    "if tune_model:\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator, cv=cv_dev, n_jobs=n_jobs, refit=False,\n",
    "                             param_grid=model_params_grid[model_type],\n",
    "                             scoring=pr_auc_scorer, fit_params=fit_params)\n",
    "\n",
    "    grid_search.fit(X_dev, y_dev)\n",
    "    \n",
    "    # show grid search results\n",
    "    display(pd.DataFrame(grid_search.cv_results_))\n",
    "    \n",
    "    # set best parameters for estimator\n",
    "    estimator.set_params(**grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model validation\n",
    "The final test on the holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_proba = []  # initialize empty predictions array\n",
    "y_true = []  # initialize empty ground-truth array\n",
    "\n",
    "# loop over the test folds\n",
    "for i_fold, (train_index, test_index) in enumerate(cv_test.split(X, y)):\n",
    "    \n",
    "    print \"validation fold {:d}\".format(i_fold)\n",
    "    \n",
    "    X_train = X.iloc[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.iloc[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    if model_type is 'gbc':\n",
    "        sample_weights = map(lambda x: class_weights[x], y_train)\n",
    "        fit_params = {'gbc__sample_weight': sample_weights}\n",
    "    else: \n",
    "        fit_params = {}\n",
    "    \n",
    "    # fit the model\n",
    "    estimator.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    # probability outputs for class 1\n",
    "    y_pred_proba.append(map(lambda x: x[1], estimator.predict_proba(X_test)))\n",
    "    \n",
    "    # store the true y labels for each fold\n",
    "    y_true.append(np.array(y_test))\n",
    "\n",
    "# postprocess the results\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred_proba = np.concatenate(y_pred_proba) \n",
    "y_pred_bin = (y_pred_proba > 0.5) * 1.\n",
    "\n",
    "# print some stats\n",
    "n_samples_test = len(y_true)\n",
    "n_pos_test = sum(y_true)\n",
    "n_neg_test = n_samples_test - n_pos_test\n",
    "print \"events: {}\".format(n_pos_test)\n",
    "print \"p_no_event: {}\".format(n_neg_test / n_samples_test)\n",
    "print \"test accuracy: {}\".format((np.equal(y_pred_bin, y_true) * 1.).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Receiver-operator characteristics\n",
    "Line is constructed by applying various threshold to the model output.   \n",
    "Y-axis: proportion of events correctly identified, hit-rate  \n",
    "X-axis: proportion of false positives, usually results in waste of resources \n",
    "Dotted line is guessing (no model). Blue line above the dotted line means there is information in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "     \n",
    "# plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"ROC curve (area = {:.2f})\".format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Receiver-operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Costs and benefits\n",
    "ROC optimization with cost matrix. Critical information: cost of FP and cost of FN (i.e. benefit of TP). Also used to train the model with `class_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def benefit(tpr, fpr):\n",
    "\n",
    "    n_tp = tpr * n_pos_test  # number of true positives (benefits)\n",
    "    n_fp = fpr * n_neg_test  # number of false positives (extra costs)\n",
    "    \n",
    "    fp_costs = n_fp * cost_fp\n",
    "    tp_benefits =  n_tp * benefit_tp\n",
    "    \n",
    "    return tp_benefits - fp_costs\n",
    "\n",
    "benefits = np.zeros_like(thresholds)\n",
    "for i, _ in enumerate(thresholds):\n",
    "    benefits[i] = benefit(tpr[i], fpr[i])\n",
    "\n",
    "i_max = np.argmax(benefits)\n",
    "print (\"max benefits: {:.0f}k euros, tpr: {:.3f}, fpr: {:.3f}, threshold: {:.3f}\"\n",
    "       .format(benefits[i_max]/ 1e3, benefits[i_max]/ 1e3 / 8, tpr[i_max], fpr[i_max], thresholds[i_max]))\n",
    "\n",
    "plt.plot(thresholds, benefits)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,np.max(benefits)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# recalibrate threshold based on benefits (optional, should still be around 0.5)\n",
    "y_pred_bin = (y_pred_proba > thresholds[i_max]) * 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Precision-recall curve\n",
    "Another way to look at it. Note that models which perform well in PR-space are necessarily also dominating ROC-space. The opposite is not the case! Line is constructed by applying various threshold to the model output.  \n",
    "Y-axis: proportion of events among all positives (precision)  \n",
    "X-axis: proportion of events correctly identified (recall, hit rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba, pos_label=1)\n",
    "\n",
    "average_precision = average_precision_score(y_true, y_pred_proba, average=\"micro\")\n",
    "\n",
    "baseline = n_pos_test / n_samples_test\n",
    "\n",
    "# plot PR curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=\"PR curve (area = {:.2f})\".format(average_precision))\n",
    "plt.plot([0, 1], [baseline, baseline], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-recall curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "if model_type is 'dummy':\n",
    "    print 'DummyClassifier only has endpoints in PR-curve'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['no event','event']\n",
    "\n",
    "print classification_report(y_true, y_pred_bin, target_names=target_names, digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = pd.DataFrame(confusion_matrix(y_true, y_pred_bin), index=target_names, columns=target_names)\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\")\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Accuracies at different classifier thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "thresholds = (np.arange(0,100,1) / 100.)\n",
    "acc = map(lambda thresh: accuracy_score(y_true, map(lambda prob: prob > thresh, y_pred_proba)), thresholds)\n",
    "plt.hist(acc, bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Thresholds versus accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, acc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Feature importance\n",
    "Note that these models are optimized to make accurate predictions, and **not** to make solid statistical inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_labels = filter(lambda k: y_col not in k, df.columns.values) \n",
    "\n",
    "if model_type is 'lr':\n",
    "    weights = estimator._final_estimator.coef_[0]\n",
    "elif model_type in ['rf','gbc']:\n",
    "    weights = estimator._final_estimator.feature_importances_\n",
    "elif model_type is 'dummy':\n",
    "    print 'DummyClassifier does not have weights'\n",
    "    weights = np.zeros(len(feature_labels))\n",
    "    \n",
    "feature_weights = pd.Series(weights, index=feature_labels)\n",
    "feature_weights.plot.barh(title='Feature importance', fontsize=8, figsize=(12,30), grid=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "if model_type is 'gbc':\n",
    "    preproc_pipe = Pipeline(steps=preproc_steps)\n",
    "    X_transformed = preproc_pipe.fit_transform(X_dev, y_dev)\n",
    "\n",
    "    plot_partial_dependence(estimator._final_estimator, X_transformed,\n",
    "                            features=range(n_features), feature_names=feature_labels,\n",
    "                            figsize=(12,180), n_cols=4, percentiles=(0.2,0.8));\n",
    "else:\n",
    "    print \"No partial dependence plots available for this model type.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
