{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries Feature Generation & Selection\n",
    "In this notebook we show how one can create and select features specifically for time series modelling.  \n",
    "There are some places in which you can explore alternative solutions, and try to find an even better solution yourself.  \n",
    "\n",
    "The notebook contains the following parts:\n",
    " 1. **Setup**: Here we setup and verify the environment (mainly data directory and imports)\n",
    " 1. **Data load**: Now we load the data, and show some statistics\n",
    " 1. **Feature generation**: Derive features to allow better models\n",
    " 1. **Defining evaluation**: Define our objective/cost function, and train/validate/test sets.\n",
    " 1. **Model tuning**: Determine a model which performs best given the features\n",
    " 1. **Selecting features**: Choose other features, such that the model generalizes better\n",
    " 1. **Final score on the test set**: After all our tuning, get the final score on the testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the directory holding the dataset, this folder should contain the `soi.dat` and `recruit.dat` datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafolder = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining wether the folder holds the expected data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "has_soi = sum([name.endswith(\"soi.dat\") for name in os.listdir(datafolder)])\n",
    "has_recruit = sum([name.endswith(\"recruit.dat\") for name in os.listdir(datafolder)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And telling you if the folder is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are ready to go\n"
     ]
    }
   ],
   "source": [
    "if (has_soi and has_recruit):\n",
    "    print 'You are ready to go'\n",
    "else:\n",
    "    print 'Your current directory is:'\n",
    "    print os.getcwd()\n",
    "    print 'And found the following files in the \"{}\" directory:'.format(datafolder)\n",
    "    print os.listdir(datafolder)\n",
    "    print ''\n",
    "if not has_soi:\n",
    "    print 'You are missing soi.dat'\n",
    "if not has_recruit:\n",
    "    print 'You are missing recruit.dat'\n",
    "assert (has_soi and has_recruit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We have placed all imports at the beginning to not clutter the other cells.\n",
    "Any failed import means you probably aren't running in the correct kernel. \n",
    "Please verify you are using 'Python [conda env:meetup_ds_timeseris]', which can be created using `conda env create -f environment.yml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerben/anaconda3/envs/bdranalytics/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPython version: 5.3.0\n",
      "statsmodels: 0.8.0\n",
      "numpy: 1.13.1\n",
      "scipy: 0.19.1\n",
      "sklearn: 0.18.2\n",
      "pandas: 0.20.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "#%load_ext autoreload\n",
    "#%autoreload 1\n",
    "#%aimport bdranalytics\n",
    "import bdranalytics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import seaborn as sns\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import sklearn\n",
    "from sklearn import linear_model, model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.metrics.scorer import r2_scorer, mean_squared_error_scorer\n",
    "import statsmodels\n",
    "import statsmodels.tsa.api as sm\n",
    "from bdranalytics.model_selection.growingwindow import GrowingWindow\n",
    "from bdranalytics.pandaspipeline.transformers import PdFeatureChain, PdFeatureUnion, PdWindowTransformer, PdLagTransformer\n",
    "from IPython.display import display\n",
    "import IPython\n",
    "print \"IPython version: {}\".format(IPython.__version__)\n",
    "print \"statsmodels: {}\".format(statsmodels.__version__)\n",
    "print \"numpy: {}\".format(np.__version__)\n",
    "print \"scipy: {}\".format(sc.__version__)\n",
    "print \"sklearn: {}\".format(sklearn.__version__)\n",
    "print \"pandas: {}\".format(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 - Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the two datasets, `soi.dat` and `recruit.dat`.\n",
    " - `soi.dat` holds the Southern Oscillation Index Data, which is is the difference in barometric pressure at sea level between Tahiti and Darwin. This is related to the El Nino / El Nina effect.\n",
    " - `recruit.dat` holds new fish recruitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_orig = pd.read_csv(os.path.join(datafolder, \"soi.dat\"), header=0, names=[\"soi\"])\n",
    "rng=pd.date_range('1/1/1866', periods=X_orig.size, freq='MS')\n",
    "X_orig = X_orig.set_index(rng)\n",
    "y_orig = pd.read_csv(os.path.join(datafolder, \"recruit.dat\"), header=0, names=[\"recruit\"]).set_index(rng).iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some information about the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To following cells show you some (basic) information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The soi dataset is used as features, and is 452 rows by 1 columns\n",
      "The recruit dataset holds the target value, and is a series of 452 rows\n",
      "The first few rows, combining the features with the target, looks as follows:\n",
      "                       soi  recruit\n",
      "1866-01-01 00:00:00  0.246    68.63\n",
      "1866-02-01 00:00:00  0.311    68.63\n",
      "1866-03-01 00:00:00  0.104    68.63\n",
      "1866-04-01 00:00:00 -0.016    68.63\n",
      "1866-05-01 00:00:00  0.235    68.63\n",
      "Some quantile statistics about the range of values:\n",
      "              soi     recruit\n",
      "count  452.000000  452.000000\n",
      "mean     0.079381   62.248695\n",
      "std      0.382915   28.006504\n",
      "min     -1.000000    1.720000\n",
      "25%     -0.182750   39.597500\n",
      "50%      0.115000   68.625000\n",
      "75%      0.366000   86.860000\n",
      "max      1.000000  100.000000\n"
     ]
    }
   ],
   "source": [
    "print \"The soi dataset is used as features, and is {} rows by {} columns\".format(X_orig.shape[0], X_orig.shape[1])\n",
    "print \"The recruit dataset holds the target value, and is a series of {} rows\".format(y_orig.shape[0])\n",
    "print \"The first few rows, combining the features with the target, looks as follows:\"\n",
    "print X_orig.join(y_orig).head()\n",
    "print \"Some quantile statistics about the range of values:\"\n",
    "print X_orig.join(y_orig).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now show how the timeseries look through time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerben/anaconda3/envs/bdranalytics/lib/python2.7/site-packages/seaborn/timeseries.py:183: UserWarning: The tsplot function is deprecated and will be removed or replaced (in a substantially altered version) in a future release.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+UAAAFACAYAAADajd3+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREPKKiXERERERERMQj/w9Ygvzybj6p+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f74e210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(17, 5), ncols=1, nrows=2)\n",
    "ax[0].set_title(\"soi (the feature)\")\n",
    "fig1 = sns.tsplot(X_orig.soi, ax=ax[0])\n",
    "ax[1].set_title(\"recruit (the target)\")\n",
    "fig2 = sns.tsplot(y_orig, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the target variable\n",
    "Instead of predicting the *current* recruit, we will attempt to predict the *next* recruit. Therefore we shift the `y_orig` by `-1` to get the value of 1 time ahead.\n",
    "\n",
    "Note that this will result in a NaN value on the last index, which we therefore remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = y_orig.shift(-1).dropna() # the next recruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the base features\n",
    "Currently we only have the `soi` feature as dataset. As we also know the *current* `soi` value, we add that to X.\n",
    "\n",
    "As we have one less value of `y` (we don't know the next value at the last row), we subselect only the `X` features for which we have a predictable future `recruit` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X_orig,  # the original features: the current soi\n",
    "               y_orig.to_frame() # the current recruit\n",
    "              ], axis=1, join_axes=[X_orig.index]).loc[y.index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 - Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a set of features `X`, and a target sequence `y`.\n",
    "We will now enrich the dataset by with additional features to allow better predictions on `y`. The final set of features will be assigned to the variable `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses a sklearn transformer `PdLagTransformer` which will be explained in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add derived features\n",
    "Given this first version of X, we can add new features. In general, you can add transformations of the colums, using for example :\n",
    "  * http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shift.html\n",
    "  * http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html\n",
    "  \n",
    "To simplify the application of such transformations, we have created a few helper transformers, which work nicely with sklearn's pipelines:\n",
    "  * `PdLagTransformer(lag)`: This transforms the columns by adding a 'lagged' version. The constructor argument defines the lag to apply. This uses `pandas.DataFrame.shift` to shift the columns. The returned transformation however, also has modified column names. A column `soi`, if shifted `1`, will be transformed into a column named `soi_lag1`.\n",
    "  * `PdWindowTransformer(function, window)`: This transforms the column using a `rolling`. All constructor arguments are passed through to `pandas.DataFrame.rolling`. In general, one usually provides a `function`, and a `window`. The `window` defines the number of previous values to take, and then applies the provided `function` to those values.\n",
    "\n",
    "The sklearn `FeatureUnion` and `Pipeline` unfortunately drop the column names when pandas dataframes are passed to them. Therefore we've also made pandas versions:\n",
    " * `PdFeatureUnion`: Same as sklearn's `FeatureUnion`, but propagates column names of the contained transformers.  \n",
    " The sklearn's `FeatureUnion` always returns a `numpy` `array`, without column labels.  \n",
    " Our `PdFeatureUnion` always returns a `pandas` `DataFrame`, propagating the column labels of the contained transformers.  \n",
    " If the applied transformer does not return a `pandas` `DataFrame` (thus no column labels), column labels are generated using the format `transformername-index`. For example, `PdFeatureUnion([(feat,MyTransformer())])` will generate column labels like `feat-1` and `feat-2` if the `MyTransformer`'s `transform()` function returns a numpy array.\n",
    " * `PdFeatureChain`: Similar to sklearn's `Pipeline`, applies subsequent transformers to each other result.  \n",
    " Thus `PdFeatureChain( [ ('f1', MyFirstTransformer()), ('f2', MySecondTransformer()) ] )` will apply `MySecondTransformer.transform` on the result of `MyFirstTransformer.transform`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended solution for feature generation: create lagged windows\n",
    "One way to create a lot of features, is to first create different windows, and then apply different lags to those windows. As we already have lags of single moments, and windows ending at the current time, we here only add lagged windows. To prevent too many overlapping windows, we use a stepsize of the lag which is equal to the window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_transformers = PdFeatureUnion([\n",
    "        ('window{}'.format(window), PdWindowTransformer(lambda x: x.mean(), window=window)) for window in range(1, 12)\n",
    "    ])\n",
    "lag_transformers = PdFeatureUnion([\n",
    "        ('lag{}'.format(lag), PdLagTransformer(lag)) for lag in range(20)])\n",
    "\n",
    "new_features = [\n",
    "    ('chain{}'.format(window),\n",
    "     PdFeatureChain([\n",
    "        ('window{}'.format(window), PdWindowTransformer(lambda x: x.mean(), window=window)),\n",
    "        ('lags', PdFeatureUnion([('lag{}'.format(lag), PdLagTransformer(lag)) for lag in range(window, 20, window)]))\n",
    "        ])\n",
    "     )\n",
    "     for window in range(2, 12, 2)]\n",
    "combined_features = PdFeatureUnion(\n",
    "    [('window{}'.format(window), PdWindowTransformer(lambda x: x.mean(), window=window)) for window in range(1,12)]\n",
    "    + \n",
    "    [('lag{}'.format(lag), PdLagTransformer(lag)) for lag in range(20)]\n",
    "    + new_features\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting usefull rows\n",
    "As the previous examples show, applying lags and windows result in `NaN` values for the first few rows. Let us remove those to only have complete rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = combined_features.fit_transform(X).dropna()\n",
    "y = y[X_dev.index] # because of dropped rows in X, need to also select corresponding remaining rows from y\n",
    "X = X.loc[X_dev.index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ps. You might think, are we allowed to do `fit_transform` on the **complete** dataset?! Yes we are, because we don't have information leakage, because: \n",
    " * the feature only takes into acount previous rows (rows with older timestamps)\n",
    " * and our cross validation, explained below, only tests on future rows (rows with more recent timestamps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 - Defining the evaluation\n",
    "### Defining the test method of the model\n",
    "With a dataset of features (X), and a target variable (y), let's see how well we can predict the recruit.  \n",
    "First we define the evaluation metric, and some helper functions to easily get results for cross validation:\n",
    " * `model_score` is the function we use to determine the cost\n",
    " * `model_scorer` is the same function, wrapped by sklearn to be able to be used by model selectors.\n",
    " * `cross_val` determines the cross validated test score, in which the folds are created specifically for timeseries. The data is divided into `cv_count+` splits.   \n",
    " The first fold uses split 1 as train set, and split 2 as validation set.  \n",
    " The second fold uses split 1 and 2 as train set, and split 3 as validation set.\n",
    " The `cv_count`th fold uses split 1 up till `cv_count` as train set, and split `cv_count+1` as validation set.\n",
    " * `cross_val_train` determines the cross validated train score. Thus similar to `cross_val`, but returns the train error instead of the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_score = mean_squared_error\n",
    "model_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "def cross_val(estimator, X, y, scorer = model_scorer, cv_count=10):\n",
    "    return model_selection.cross_val_score(estimator, X, y.to_frame(), \n",
    "                                           scoring = scorer,\n",
    "                                           cv=GrowingWindow(cv_count))\n",
    "\n",
    "## for different folds, trains the model, en returns the error on the **train** sets.\n",
    "def cross_val_train(estimator, X, y, scorer = model_scorer, cv_count=10):\n",
    "    return [scorer(estimator.fit(X.iloc[train,:], y.iloc[train]),\n",
    "                   X.iloc[train,:],\n",
    "                   y.iloc[train])\n",
    "             for train, test in GrowingWindow(cv_count).split(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a hold out set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we extract a train & test set from the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasize dimensions = (332, 2), Test datasets dimensions= (48, 2) \n"
     ]
    }
   ],
   "source": [
    "i_train, i_test = list(itertools.islice(GrowingWindow(8).split(X), 6, 7))[0]\n",
    "X_train = X.iloc[i_train,:]\n",
    "y_train = y[i_train]\n",
    "\n",
    "X_test = X.iloc[i_test,:]\n",
    "y_test = y[i_test]\n",
    "print \"Train datasize dimensions = {}, Test datasets dimensions= {} \".format(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of the available columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([u'soi', u'recruit'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"The names of the available columns:\"\n",
    "display(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Neural network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_series = 1\n",
    "timesteps = X_train.shape[0]\n",
    "data_dim = X_train.shape[1]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p = x_scaler.transform(X_train).reshape(num_series, timesteps, data_dim)\n",
    "y_train_p = y_scaler.transform(y_train.values.reshape(-1, 1)).reshape(num_series, timesteps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 332, 2)\n",
      "(1, 332, 1)\n"
     ]
    }
   ],
   "source": [
    "print X_train_p.shape\n",
    "print y_train_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 58.65421705]\n",
      "[ 29.50997955]\n"
     ]
    }
   ],
   "source": [
    "print y_scaler.mean_\n",
    "print y_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.66291068,  0.46302952],\n",
       "       [ 2.00151723,  0.31591731],\n",
       "       [ 1.26743891,  0.21535134],\n",
       "       [ 1.18214108, -0.16041367],\n",
       "       [-0.28860034, -0.7002781 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler.transform(X_train)[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.66291068,  0.46302952],\n",
       "        [ 2.00151723,  0.31591731],\n",
       "        [ 1.26743891,  0.21535134],\n",
       "        [ 1.18214108, -0.16041367],\n",
       "        [-0.28860034, -0.7002781 ]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_p[:,0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "val_size = 40\n",
    "i_fit = np.arange(0, timesteps - val_size)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    y_true = y_true * y_scaler.scale_ + y_scaler.mean_\n",
    "    y_pred = y_pred * y_scaler.scale_ + y_scaler.mean_\n",
    "    return K.mean(K.square(y_true[-val_size:] - y_pred[-val_size:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 128)         67072     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 1)           129       \n",
      "=================================================================\n",
      "Total params: 198,785\n",
      "Trainable params: 198,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.9083 - mse: 790.9547 - val_loss: 0.8972 - val_mse: 781.3371\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.8545 - mse: 744.1086 - val_loss: 0.8220 - val_mse: 715.8292\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.7201 - mse: 627.1072 - val_loss: 0.7449 - val_mse: 648.7037\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.6964 - mse: 606.4576 - val_loss: 0.6683 - val_mse: 582.0075\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.5992 - mse: 521.8114 - val_loss: 0.5940 - val_mse: 517.2427\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.5811 - mse: 506.0759 - val_loss: 0.5254 - val_mse: 457.5026\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.4938 - mse: 430.0151 - val_loss: 0.4597 - val_mse: 400.3083\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.4550 - mse: 396.2624 - val_loss: 0.3962 - val_mse: 344.9967\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.3530 - mse: 307.4027 - val_loss: 0.3350 - val_mse: 291.7655\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.2901 - mse: 252.6662 - val_loss: 0.2872 - val_mse: 250.1254\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.2736 - mse: 238.2263 - val_loss: 0.2682 - val_mse: 233.5919\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.2781 - mse: 242.1921 - val_loss: 0.2633 - val_mse: 229.3116\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.2814 - mse: 245.0701 - val_loss: 0.2577 - val_mse: 224.4009\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.2767 - mse: 240.9391 - val_loss: 0.2511 - val_mse: 218.6911\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.3114 - mse: 271.1451 - val_loss: 0.2364 - val_mse: 205.8622\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.2619 - mse: 228.0664 - val_loss: 0.2182 - val_mse: 190.0244\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.2818 - mse: 245.3675 - val_loss: 0.2062 - val_mse: 179.6104\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.2044 - mse: 177.9973 - val_loss: 0.2047 - val_mse: 178.2897\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.1942 - mse: 169.0754 - val_loss: 0.2073 - val_mse: 180.5076\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.3306 - mse: 287.9187 - val_loss: 0.2037 - val_mse: 177.3622\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.2130 - mse: 185.4852 - val_loss: 0.1985 - val_mse: 172.8472\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.2941 - mse: 256.1532 - val_loss: 0.1929 - val_mse: 168.0213\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.1936 - mse: 168.6019 - val_loss: 0.1911 - val_mse: 166.4178\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.2203 - mse: 191.8451 - val_loss: 0.1939 - val_mse: 168.8548\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.2065 - mse: 179.8041 - val_loss: 0.2105 - val_mse: 183.3094\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.2015 - mse: 175.5173 - val_loss: 0.2343 - val_mse: 204.0158\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.1837 - mse: 159.9419 - val_loss: 0.2422 - val_mse: 210.9469\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.1833 - mse: 159.6100 - val_loss: 0.2349 - val_mse: 204.5684\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.2445 - mse: 212.9469 - val_loss: 0.1979 - val_mse: 172.3583\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.2161 - mse: 188.2079 - val_loss: 0.1684 - val_mse: 146.6255\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.1626 - mse: 141.6063 - val_loss: 0.1639 - val_mse: 142.7368\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.1724 - mse: 150.1331 - val_loss: 0.1769 - val_mse: 154.0398\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.1604 - mse: 139.6449 - val_loss: 0.1985 - val_mse: 172.8857\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.1878 - mse: 163.5768 - val_loss: 0.2125 - val_mse: 185.0526\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.2004 - mse: 174.5559 - val_loss: 0.2125 - val_mse: 185.0214\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.2185 - mse: 190.2495 - val_loss: 0.1997 - val_mse: 173.9435\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.2271 - mse: 197.7722 - val_loss: 0.1810 - val_mse: 157.6171\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.2218 - mse: 193.1829 - val_loss: 0.1620 - val_mse: 141.0877\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.1919 - mse: 167.1468 - val_loss: 0.1515 - val_mse: 131.9361\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.1535 - mse: 133.7166 - val_loss: 0.1518 - val_mse: 132.1612\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.1539 - mse: 133.9853 - val_loss: 0.1576 - val_mse: 137.2102\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.1678 - mse: 146.1017 - val_loss: 0.1604 - val_mse: 139.7161\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.1471 - mse: 128.1036 - val_loss: 0.1624 - val_mse: 141.3821\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.2245 - mse: 195.5364 - val_loss: 0.1568 - val_mse: 136.5336\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.1524 - mse: 132.7142 - val_loss: 0.1542 - val_mse: 134.3004\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.1840 - mse: 160.2746 - val_loss: 0.1490 - val_mse: 129.7815\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.1563 - mse: 136.0831 - val_loss: 0.1461 - val_mse: 127.2540\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.1372 - mse: 119.4427 - val_loss: 0.1467 - val_mse: 127.7800\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.1346 - mse: 117.2456 - val_loss: 0.1510 - val_mse: 131.4627\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.1376 - mse: 119.8246 - val_loss: 0.1552 - val_mse: 135.1169\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.1362 - mse: 118.6021 - val_loss: 0.1570 - val_mse: 136.7486\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.1613 - mse: 140.4967 - val_loss: 0.1552 - val_mse: 135.1500\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.1307 - mse: 113.7794 - val_loss: 0.1513 - val_mse: 131.7612\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.2208 - mse: 192.3058 - val_loss: 0.1435 - val_mse: 124.9799\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.1257 - mse: 109.4633 - val_loss: 0.1385 - val_mse: 120.5724\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.1270 - mse: 110.6120 - val_loss: 0.1347 - val_mse: 117.3207\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.1805 - mse: 157.1558 - val_loss: 0.1330 - val_mse: 115.7831\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.1624 - mse: 141.4470 - val_loss: 0.1358 - val_mse: 118.2851\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.1632 - mse: 142.1065 - val_loss: 0.1374 - val_mse: 119.6755\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.1383 - mse: 120.4376 - val_loss: 0.1397 - val_mse: 121.6656\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.1555 - mse: 135.4362 - val_loss: 0.1385 - val_mse: 120.6030\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.1372 - mse: 119.5116 - val_loss: 0.1393 - val_mse: 121.3228\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.1297 - mse: 112.9816 - val_loss: 0.1389 - val_mse: 120.9676\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.1178 - mse: 102.5564 - val_loss: 0.1380 - val_mse: 120.1913\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.1434 - mse: 124.8869 - val_loss: 0.1374 - val_mse: 119.6929\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.1140 - mse: 99.2693 - val_loss: 0.1359 - val_mse: 118.3578\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.1201 - mse: 104.5711 - val_loss: 0.1325 - val_mse: 115.3851\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.1170 - mse: 101.8704 - val_loss: 0.1279 - val_mse: 111.3930\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.1135 - mse: 98.8300 - val_loss: 0.1247 - val_mse: 108.5933\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.1205 - mse: 104.9758 - val_loss: 0.1233 - val_mse: 107.3951\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.1111 - mse: 96.7330 - val_loss: 0.1218 - val_mse: 106.0637\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.1202 - mse: 104.6330 - val_loss: 0.1198 - val_mse: 104.2951\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.1128 - mse: 98.1955 - val_loss: 0.1185 - val_mse: 103.2204\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.1037 - mse: 90.3159 - val_loss: 0.1174 - val_mse: 102.2423\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.1033 - mse: 89.9682 - val_loss: 0.1168 - val_mse: 101.7077\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.1119 - mse: 97.4672 - val_loss: 0.1160 - val_mse: 100.9744\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.1065 - mse: 92.7217 - val_loss: 0.1147 - val_mse: 99.9124\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.1051 - mse: 91.5340 - val_loss: 0.1137 - val_mse: 99.0092\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.1063 - mse: 92.6031 - val_loss: 0.1121 - val_mse: 97.6027\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.1202 - mse: 104.6314 - val_loss: 0.1107 - val_mse: 96.3913\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.1204 - mse: 104.8131 - val_loss: 0.1090 - val_mse: 94.9133\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0980 - mse: 85.3662 - val_loss: 0.1081 - val_mse: 94.1610\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.1105 - mse: 96.2268 - val_loss: 0.1081 - val_mse: 94.1771\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0972 - mse: 84.6610 - val_loss: 0.1079 - val_mse: 93.9676\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0994 - mse: 86.5996 - val_loss: 0.1069 - val_mse: 93.0885\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.1257 - mse: 109.4343 - val_loss: 0.1049 - val_mse: 91.3109\n",
      "Epoch 87/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0951 - mse: 82.7767 - val_loss: 0.1048 - val_mse: 91.2785\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0948 - mse: 82.5715 - val_loss: 0.1036 - val_mse: 90.2457\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.1124 - mse: 97.9170 - val_loss: 0.1025 - val_mse: 89.2864\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.1045 - mse: 90.9808 - val_loss: 0.1004 - val_mse: 87.4332\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0907 - mse: 79.0079 - val_loss: 0.1005 - val_mse: 87.5444\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.1184 - mse: 103.1285 - val_loss: 0.0985 - val_mse: 85.7699\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.1157 - mse: 100.7416 - val_loss: 0.0955 - val_mse: 83.2083\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.1190 - mse: 103.6401 - val_loss: 0.0950 - val_mse: 82.7130\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.1210 - mse: 105.4023 - val_loss: 0.0933 - val_mse: 81.2413\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0848 - mse: 73.8514 - val_loss: 0.0930 - val_mse: 81.0007\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0905 - mse: 78.8280 - val_loss: 0.0918 - val_mse: 79.9519\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0947 - mse: 82.4417 - val_loss: 0.0896 - val_mse: 78.0054\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.1018 - mse: 88.6559 - val_loss: 0.0874 - val_mse: 76.1100\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0861 - mse: 74.9937 - val_loss: 0.0862 - val_mse: 75.1052\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0872 - mse: 75.9684 - val_loss: 0.0853 - val_mse: 74.2714\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0732 - mse: 63.7717 - val_loss: 0.0846 - val_mse: 73.6401\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0829 - mse: 72.2009 - val_loss: 0.0837 - val_mse: 72.8614\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0813 - mse: 70.8370 - val_loss: 0.0831 - val_mse: 72.4073\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0950 - mse: 82.7439 - val_loss: 0.0843 - val_mse: 73.3819\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0808 - mse: 70.4017 - val_loss: 0.0858 - val_mse: 74.7128\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0880 - mse: 76.6234 - val_loss: 0.0858 - val_mse: 74.7516\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0794 - mse: 69.1472 - val_loss: 0.0835 - val_mse: 72.7304\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0955 - mse: 83.1477 - val_loss: 0.0802 - val_mse: 69.8361\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0805 - mse: 70.0702 - val_loss: 0.0762 - val_mse: 66.3376\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0642 - mse: 55.9143 - val_loss: 0.0736 - val_mse: 64.0842\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0671 - mse: 58.4030 - val_loss: 0.0728 - val_mse: 63.4173\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0659 - mse: 57.4085 - val_loss: 0.0731 - val_mse: 63.6168\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.1013 - mse: 88.2473 - val_loss: 0.0723 - val_mse: 62.9779\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0821 - mse: 71.4999 - val_loss: 0.0734 - val_mse: 63.9336\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0696 - mse: 60.5828 - val_loss: 0.0764 - val_mse: 66.5572\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0800 - mse: 69.6441 - val_loss: 0.0777 - val_mse: 67.6460\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0723 - mse: 62.9801 - val_loss: 0.0762 - val_mse: 66.3920\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0599 - mse: 52.1769 - val_loss: 0.0742 - val_mse: 64.5909\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0723 - mse: 62.9356 - val_loss: 0.0703 - val_mse: 61.2229\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0730 - mse: 63.5731 - val_loss: 0.0664 - val_mse: 57.8112\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0819 - mse: 71.3524 - val_loss: 0.0655 - val_mse: 57.0145\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0592 - mse: 51.5968 - val_loss: 0.0681 - val_mse: 59.2802\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0714 - mse: 62.1368 - val_loss: 0.0739 - val_mse: 64.3187\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.1414 - mse: 123.1134 - val_loss: 0.0739 - val_mse: 64.3515\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0740 - mse: 64.4629 - val_loss: 0.0698 - val_mse: 60.7531\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0756 - mse: 65.8090 - val_loss: 0.0652 - val_mse: 56.8106\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0623 - mse: 54.2742 - val_loss: 0.0623 - val_mse: 54.2130\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0849 - mse: 73.9324 - val_loss: 0.0600 - val_mse: 52.2197\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0642 - mse: 55.9325 - val_loss: 0.0610 - val_mse: 53.1611\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0536 - mse: 46.6396 - val_loss: 0.0652 - val_mse: 56.7380\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0646 - mse: 56.2374 - val_loss: 0.0671 - val_mse: 58.4548\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0704 - mse: 61.2862 - val_loss: 0.0654 - val_mse: 56.9163\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0671 - mse: 58.4420 - val_loss: 0.0612 - val_mse: 53.2528\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0656 - mse: 57.1286 - val_loss: 0.0574 - val_mse: 49.9647\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0602 - mse: 52.4673 - val_loss: 0.0559 - val_mse: 48.6455\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0584 - mse: 50.8433 - val_loss: 0.0568 - val_mse: 49.4215\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0603 - mse: 52.4844 - val_loss: 0.0577 - val_mse: 50.2571\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0544 - mse: 47.3429 - val_loss: 0.0581 - val_mse: 50.5951\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0624 - mse: 54.3192 - val_loss: 0.0557 - val_mse: 48.5443\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0799 - mse: 69.6224 - val_loss: 0.0567 - val_mse: 49.3430\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0522 - mse: 45.4318 - val_loss: 0.0585 - val_mse: 50.9816\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0840 - mse: 73.1153 - val_loss: 0.0579 - val_mse: 50.4065\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0591 - mse: 51.4576 - val_loss: 0.0575 - val_mse: 50.1074\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.1035 - mse: 90.0985 - val_loss: 0.0519 - val_mse: 45.1880\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0516 - mse: 44.9362 - val_loss: 0.0497 - val_mse: 43.3079\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0547 - mse: 47.6384 - val_loss: 0.0506 - val_mse: 44.0872\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0573 - mse: 49.9043 - val_loss: 0.0524 - val_mse: 45.6420\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0591 - mse: 51.5081 - val_loss: 0.0537 - val_mse: 46.7469\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0503 - mse: 43.8238 - val_loss: 0.0549 - val_mse: 47.8310\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0539 - mse: 46.9762 - val_loss: 0.0551 - val_mse: 47.9897\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0526 - mse: 45.7710 - val_loss: 0.0542 - val_mse: 47.1580\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0773 - mse: 67.3304 - val_loss: 0.0531 - val_mse: 46.2105\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0691 - mse: 60.1510 - val_loss: 0.0492 - val_mse: 42.8825\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0492 - mse: 42.8325 - val_loss: 0.0463 - val_mse: 40.3593\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.1011 - mse: 88.0809 - val_loss: 0.0462 - val_mse: 40.2181\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0765 - mse: 66.5921 - val_loss: 0.0471 - val_mse: 41.0075\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0595 - mse: 51.7943 - val_loss: 0.0475 - val_mse: 41.3547\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0456 - mse: 39.7497 - val_loss: 0.0496 - val_mse: 43.1677\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0604 - mse: 52.5630 - val_loss: 0.0536 - val_mse: 46.6879\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0719 - mse: 62.5769 - val_loss: 0.0529 - val_mse: 46.0758\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0868 - mse: 75.5905 - val_loss: 0.0482 - val_mse: 41.9876\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0525 - mse: 45.7433 - val_loss: 0.0437 - val_mse: 38.0892\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0455 - mse: 39.6568 - val_loss: 0.0415 - val_mse: 36.1424\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0416 - mse: 36.2600 - val_loss: 0.0420 - val_mse: 36.6147\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0440 - mse: 38.2811 - val_loss: 0.0438 - val_mse: 38.1830\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0426 - mse: 37.1007 - val_loss: 0.0454 - val_mse: 39.5515\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0546 - mse: 47.5601 - val_loss: 0.0458 - val_mse: 39.8883\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0551 - mse: 47.9949 - val_loss: 0.0433 - val_mse: 37.6899\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0466 - mse: 40.6051 - val_loss: 0.0413 - val_mse: 35.9455\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0622 - mse: 54.1763 - val_loss: 0.0407 - val_mse: 35.4224\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0428 - mse: 37.3082 - val_loss: 0.0433 - val_mse: 37.6853\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0418 - mse: 36.3979 - val_loss: 0.0463 - val_mse: 40.2890\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0527 - mse: 45.9148 - val_loss: 0.0459 - val_mse: 40.0007\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0520 - mse: 45.2563 - val_loss: 0.0463 - val_mse: 40.3205\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0422 - mse: 36.7868 - val_loss: 0.0459 - val_mse: 39.9339\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0404 - mse: 35.1677 - val_loss: 0.0447 - val_mse: 38.9013\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0394 - mse: 34.2991 - val_loss: 0.0432 - val_mse: 37.6205\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0460 - mse: 40.0824 - val_loss: 0.0410 - val_mse: 35.6734\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0444 - mse: 38.6780 - val_loss: 0.0393 - val_mse: 34.2383\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0553 - mse: 48.1582 - val_loss: 0.0374 - val_mse: 32.5278\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0399 - mse: 34.7895 - val_loss: 0.0368 - val_mse: 32.0135\n",
      "Epoch 183/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0502 - mse: 43.7044 - val_loss: 0.0372 - val_mse: 32.4369\n",
      "Epoch 184/1000\n",
      "1s - loss: 0.0363 - mse: 31.5982 - val_loss: 0.0381 - val_mse: 33.2090\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0477 - mse: 41.5755 - val_loss: 0.0385 - val_mse: 33.5367\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0430 - mse: 37.4745 - val_loss: 0.0389 - val_mse: 33.9146\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0484 - mse: 42.1331 - val_loss: 0.0386 - val_mse: 33.6459\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0347 - mse: 30.2259 - val_loss: 0.0387 - val_mse: 33.7303\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0385 - mse: 33.4922 - val_loss: 0.0397 - val_mse: 34.5716\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0775 - mse: 67.5116 - val_loss: 0.0392 - val_mse: 34.1039\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0421 - mse: 36.6361 - val_loss: 0.0382 - val_mse: 33.2592\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0383 - mse: 33.3505 - val_loss: 0.0376 - val_mse: 32.7001\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0432 - mse: 37.6010 - val_loss: 0.0363 - val_mse: 31.6132\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0452 - mse: 39.3804 - val_loss: 0.0352 - val_mse: 30.6933\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0471 - mse: 41.0564 - val_loss: 0.0358 - val_mse: 31.1750\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0388 - mse: 33.8010 - val_loss: 0.0362 - val_mse: 31.5662\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0416 - mse: 36.2050 - val_loss: 0.0364 - val_mse: 31.6998\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0420 - mse: 36.5998 - val_loss: 0.0357 - val_mse: 31.1135\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0425 - mse: 37.0475 - val_loss: 0.0348 - val_mse: 30.3219\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0425 - mse: 37.0414 - val_loss: 0.0350 - val_mse: 30.4434\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0648 - mse: 56.4550 - val_loss: 0.0365 - val_mse: 31.7438\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0351 - mse: 30.5457 - val_loss: 0.0403 - val_mse: 35.1135\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0654 - mse: 56.9385 - val_loss: 0.0413 - val_mse: 35.9984\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0371 - mse: 32.3235 - val_loss: 0.0417 - val_mse: 36.3204\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0666 - mse: 58.0123 - val_loss: 0.0391 - val_mse: 34.0262\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0356 - mse: 30.9718 - val_loss: 0.0363 - val_mse: 31.5842\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0617 - mse: 53.7635 - val_loss: 0.0342 - val_mse: 29.8222\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0491 - mse: 42.7184 - val_loss: 0.0352 - val_mse: 30.6342\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0369 - mse: 32.1279 - val_loss: 0.0384 - val_mse: 33.4310\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0481 - mse: 41.9140 - val_loss: 0.0415 - val_mse: 36.1211\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0500 - mse: 43.5665 - val_loss: 0.0433 - val_mse: 37.7344\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0519 - mse: 45.2337 - val_loss: 0.0406 - val_mse: 35.3833\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0435 - mse: 37.8833 - val_loss: 0.0370 - val_mse: 32.2214\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0469 - mse: 40.8749 - val_loss: 0.0337 - val_mse: 29.3585\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0505 - mse: 44.0147 - val_loss: 0.0352 - val_mse: 30.6570\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0402 - mse: 35.0095 - val_loss: 0.0402 - val_mse: 34.9787\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0345 - mse: 30.0196 - val_loss: 0.0447 - val_mse: 38.9681\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0646 - mse: 56.2780 - val_loss: 0.0444 - val_mse: 38.6787\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0420 - mse: 36.5457 - val_loss: 0.0415 - val_mse: 36.1105\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0508 - mse: 44.2634 - val_loss: 0.0370 - val_mse: 32.2478\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0350 - mse: 30.4381 - val_loss: 0.0339 - val_mse: 29.5207\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0361 - mse: 31.4221 - val_loss: 0.0328 - val_mse: 28.5946\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0442 - mse: 38.5312 - val_loss: 0.0328 - val_mse: 28.5453\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0355 - mse: 30.8768 - val_loss: 0.0342 - val_mse: 29.7414\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0465 - mse: 40.5089 - val_loss: 0.0359 - val_mse: 31.2381\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0646 - mse: 56.2876 - val_loss: 0.0350 - val_mse: 30.4869\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0360 - mse: 31.3754 - val_loss: 0.0339 - val_mse: 29.5451\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0392 - mse: 34.1020 - val_loss: 0.0329 - val_mse: 28.6580\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0396 - mse: 34.5189 - val_loss: 0.0330 - val_mse: 28.7482\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0320 - mse: 27.8401 - val_loss: 0.0333 - val_mse: 29.0175\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0404 - mse: 35.2244 - val_loss: 0.0327 - val_mse: 28.4711\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0397 - mse: 34.5503 - val_loss: 0.0320 - val_mse: 27.8468\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0362 - mse: 31.5538 - val_loss: 0.0320 - val_mse: 27.9021\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0326 - mse: 28.4041 - val_loss: 0.0326 - val_mse: 28.3932\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0379 - mse: 33.0129 - val_loss: 0.0331 - val_mse: 28.8638\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0462 - mse: 40.2076 - val_loss: 0.0331 - val_mse: 28.8250\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0333 - mse: 28.9578 - val_loss: 0.0329 - val_mse: 28.6826\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0417 - mse: 36.3561 - val_loss: 0.0324 - val_mse: 28.2299\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0328 - mse: 28.5512 - val_loss: 0.0326 - val_mse: 28.3753\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0339 - mse: 29.5278 - val_loss: 0.0335 - val_mse: 29.1932\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0387 - mse: 33.7023 - val_loss: 0.0346 - val_mse: 30.1528\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0312 - mse: 27.2127 - val_loss: 0.0363 - val_mse: 31.5886\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.0369 - mse: 32.1000 - val_loss: 0.0371 - val_mse: 32.2843\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0380 - mse: 33.0910 - val_loss: 0.0360 - val_mse: 31.3113\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0355 - mse: 30.9226 - val_loss: 0.0345 - val_mse: 30.0697\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0353 - mse: 30.7592 - val_loss: 0.0339 - val_mse: 29.4826\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0369 - mse: 32.0942 - val_loss: 0.0350 - val_mse: 30.4509\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0389 - mse: 33.8972 - val_loss: 0.0358 - val_mse: 31.1962\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0448 - mse: 39.0287 - val_loss: 0.0360 - val_mse: 31.3933\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0702 - mse: 61.1286 - val_loss: 0.0345 - val_mse: 30.0231\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0448 - mse: 38.9710 - val_loss: 0.0325 - val_mse: 28.3129\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0357 - mse: 31.1127 - val_loss: 0.0330 - val_mse: 28.7171\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0342 - mse: 29.7554 - val_loss: 0.0352 - val_mse: 30.6663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122463110>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run1():\n",
    "    tensor_board = TensorBoard(log_dir=\"../tensorboardlog/baseline\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, activation='tanh', input_shape=(None, 2)))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='tanh', dropout=0.5))\n",
    "    model.add(TimeDistributed(Dense(1, activation='linear')))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[mse])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    fit = model.fit(X_train_p[:,i_fit,:], y_train_p[:,i_fit,:], epochs=1000,\n",
    "              validation_data=(X_train_p, y_train_p),\n",
    "              verbose=2, callbacks=[early_stopping, tensor_board])\n",
    "    return fit\n",
    "run1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \\\n",
    "#                        allow_soft_placement=True, device_count = {'CPU': 1})\n",
    "#session = tf.Session(config=config)\n",
    "#K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With adaptive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, None, 128)         67072     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 1)           129       \n",
      "=================================================================\n",
      "Total params: 198,785\n",
      "Trainable params: 198,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 1.1465 - mse: 998.4250 - val_loss: 5.1012 - val_mse: 4442.3613\n",
      "Epoch 2/1000\n",
      "1s - loss: 1.2659 - mse: 1102.3705 - val_loss: 0.7449 - val_mse: 648.6743\n",
      "Epoch 3/1000\n",
      "1s - loss: 1.2923 - mse: 1125.3623 - val_loss: 7.9018 - val_mse: 6881.2305\n",
      "Epoch 4/1000\n",
      "1s - loss: 6.3767 - mse: 5553.1060 - val_loss: 6.3168 - val_mse: 5500.9170\n",
      "Epoch 5/1000\n",
      "1s - loss: 7.1250 - mse: 6204.7271 - val_loss: 0.9911 - val_mse: 863.0881\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.9746 - mse: 848.7036 - val_loss: 2.1019 - val_mse: 1830.4066\n",
      "Epoch 7/1000\n",
      "1s - loss: 2.2955 - mse: 1998.9812 - val_loss: 1.4191 - val_mse: 1235.7671\n",
      "Epoch 8/1000\n",
      "1s - loss: 1.5638 - mse: 1361.8092 - val_loss: 0.9853 - val_mse: 858.0172\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.9327 - mse: 812.2507 - val_loss: 1.0880 - val_mse: 947.4519\n",
      "Epoch 10/1000\n",
      "1s - loss: 1.2820 - mse: 1116.4261 - val_loss: 1.0319 - val_mse: 898.6151\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.9334 - mse: 812.8337 - val_loss: 0.9842 - val_mse: 857.0486\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.9720 - mse: 846.4697 - val_loss: 0.9725 - val_mse: 846.9117\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.9328 - mse: 812.3026 - val_loss: 0.9795 - val_mse: 852.9534\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.9850 - mse: 857.7466 - val_loss: 0.9740 - val_mse: 848.1866\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.9891 - mse: 861.3672 - val_loss: 0.9595 - val_mse: 835.5325\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.9378 - mse: 816.6587 - val_loss: 0.9694 - val_mse: 844.1813\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.9170 - mse: 798.5861 - val_loss: 0.9791 - val_mse: 852.6243\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.9233 - mse: 804.0872 - val_loss: 0.9862 - val_mse: 858.8570\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.9146 - mse: 796.4493 - val_loss: 0.9898 - val_mse: 861.9689\n",
      "Epoch 20/1000\n",
      "1s - loss: 1.0133 - mse: 882.4155 - val_loss: 0.9815 - val_mse: 854.7602\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.9097 - mse: 792.1597 - val_loss: 0.9683 - val_mse: 843.2084\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.9330 - mse: 812.5115 - val_loss: 0.9553 - val_mse: 831.9518\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.9243 - mse: 804.8976 - val_loss: 0.9395 - val_mse: 818.1611\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.9126 - mse: 794.7335 - val_loss: 0.9357 - val_mse: 814.8511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124a59bd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run2():\n",
    "    tensor_board = TensorBoard(log_dir=\"../tensorboardlog/adaptivelearningrate\")\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=10, min_lr=1e-4)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=21, verbose=0, mode='auto')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, activation='tanh', input_shape=(None, 2)))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='tanh', dropout=0.5))\n",
    "    model.add(TimeDistributed(Dense(1, activation='linear')))\n",
    "    optimizer = keras.optimizers.Adam(lr=3e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[mse])\n",
    "    model.summary()\n",
    "\n",
    "    fit = model.fit(X_train_p[:,i_fit,:], y_train_p[:,i_fit,:], epochs=1000,\n",
    "              validation_data=(X_train_p, y_train_p),\n",
    "              verbose=2, callbacks=[reduce_lr, early_stopping, tensor_board])\n",
    "    return fit\n",
    "run2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thus in 40 epochs less a 10% improved `val_mse` by using adaptive lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if adding nodes did not improve for this nr of steps, we stop adding nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_weights(model, prev_weights):\n",
    "    for i in range(min(len(prev_weights), len(model.layers))): # over minimum nr of layers\n",
    "            old_weights = prev_weights[i]\n",
    "            curr_weights = model.layers[i].get_weights()\n",
    "            for j in range(len(curr_weights)): # within layer\n",
    "                if (curr_weights[j].shape == old_weights[j].shape): # only if shape is equal (else useless)\n",
    "                    curr_weights[j] = old_weights[j]\n",
    "            model.layers[i].set_weights(curr_weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note , the reduce_lr and early stopping should collaborate well, in case the  initial lr is way too high, it should have the possibility to be reduced enough before the early stopping kicks in\n",
    "\n",
    "so initial lr * factor^floor(patience_early_stopping / patience_lr_reduce) should be low enough to always converge\n",
    "in a few steps : patience_early_stopping % patience_lr_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_depth1(width, learning_params, initial_weights=None, label=''):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(width, return_sequences=True, activation='tanh', input_shape=(None, 2)))\n",
    "    model.add(TimeDistributed(Dense(1, activation='linear')))\n",
    "    optimizer = keras.optimizers.Adam(lr=learning_params['lr_intial'], \n",
    "                                      beta_1=0.9, \n",
    "                                      beta_2=0.999, \n",
    "                                      epsilon=1e-08, \n",
    "                                      decay=learning_params['lr_decay'])\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[mse])\n",
    "    if not initial_weights is None:\n",
    "        assign_weights(model, initial_weights)   \n",
    "    model.summary()\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  cooldown=learning_params['lr_plateau_cooldown'], \n",
    "                                  factor=learning_params['lr_plateau_factor'], \n",
    "                                  patience=learning_params['lr_plateau_patience'], \n",
    "                                  min_lr=learning_params['lr_minimum'],\n",
    "                                  verbose=1)\n",
    "    tensor_board = TensorBoard(log_dir=\"../tensorboardlog/{}depth1_width{}\".format(label, width))\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   min_delta=learning_params['stop_delta'],\n",
    "                                   patience=learning_params['stop_patience'], \n",
    "                                   verbose=0, \n",
    "                                   mode='auto')\n",
    "    \n",
    "    fit = model.fit(X_train_p[:,i_fit,:], \n",
    "                    y_train_p[:,i_fit,:], \n",
    "                    epochs=learning_params['epochs_max'], \n",
    "                    validation_data=(X_train_p, y_train_p),\n",
    "                    verbose=2, \n",
    "                    callbacks=[reduce_lr, early_stopping, tensor_board]\n",
    "                   )\n",
    "    return fit, {'width':width, \n",
    "                 'label':label,\n",
    "                 'dropout':0.0,\n",
    "                       'val_mse':fit.history['val_mse'][-1],\n",
    "                       'val_loss':fit.history['val_loss'][-1],\n",
    "                       'mse':fit.history['mse'][-1],\n",
    "                       'loss':fit.history['loss'][-1],\n",
    "                      }, [layer.get_weights() for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learningrates1 = {\n",
    "'lr_intial' : 1e-2, # higher than the default\n",
    "'lr_decay' : 0.01, # for adam it is 1 / (1 + decay * t) , thus with decay 0.001 and at t==1000, lr is divided by 2\n",
    "                # note that the effect of this decay is not visible in tensorboard\n",
    "'lr_plateau_factor' : 0.7, # if no convergence (possibly by too high lr), we boost the lr decay\n",
    "'lr_plateau_patience' : 4, # nr of consequetive epochs without improvement before we boost the lr decay\n",
    "'lr_plateau_cooldown' : 10, # first do this nr of iterations at new lr before detecting plateau\n",
    "'lr_minimum' : 1e-6, # the minimum lr too which we decay (for plateau detection)\n",
    "\n",
    "'stop_patience' : 30, # if no extra improvement after this nr of steps , we terminate learning\n",
    "'stop_delta' : 0.0001, # the epsilon, changes below this threshold are 'no improvement'\n",
    "'epochs_max' : 1000 # limit the total nr op epochs, very high, we will stop based on plateau\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_59 (LSTM)               (None, None, 10)          520       \n",
      "_________________________________________________________________\n",
      "time_distributed_56 (TimeDis (None, None, 1)           11        \n",
      "=================================================================\n",
      "Total params: 531\n",
      "Trainable params: 531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 1.3547 - mse: 1179.7335 - val_loss: 1.2747 - val_mse: 1110.0435\n",
      "Epoch 2/1000\n",
      "0s - loss: 1.2338 - mse: 1074.4353 - val_loss: 1.1850 - val_mse: 1031.9619\n",
      "Epoch 3/1000\n",
      "0s - loss: 1.1431 - mse: 995.4827 - val_loss: 1.0981 - val_mse: 956.3091\n",
      "Epoch 4/1000\n",
      "0s - loss: 1.0556 - mse: 919.2869 - val_loss: 1.0151 - val_mse: 884.0290\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.9723 - mse: 846.6887 - val_loss: 0.9360 - val_mse: 815.0782\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.8929 - mse: 777.6034 - val_loss: 0.8585 - val_mse: 747.6198\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.8155 - mse: 710.1999 - val_loss: 0.7881 - val_mse: 686.2993\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.7454 - mse: 649.1453 - val_loss: 0.7233 - val_mse: 629.8950\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.6812 - mse: 593.2500 - val_loss: 0.6652 - val_mse: 579.3065\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.6240 - mse: 543.4362 - val_loss: 0.6145 - val_mse: 535.1041\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.5745 - mse: 500.2588 - val_loss: 0.5709 - val_mse: 497.1838\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.5323 - mse: 463.5423 - val_loss: 0.5336 - val_mse: 464.7080\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.4965 - mse: 432.3303 - val_loss: 0.5012 - val_mse: 436.4428\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.4654 - mse: 405.2642 - val_loss: 0.4722 - val_mse: 411.2238\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.4376 - mse: 381.0974 - val_loss: 0.4457 - val_mse: 388.1028\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.4121 - mse: 358.8678 - val_loss: 0.4207 - val_mse: 366.3392\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.3880 - mse: 337.8780 - val_loss: 0.3966 - val_mse: 345.4022\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.3648 - mse: 317.6667 - val_loss: 0.3732 - val_mse: 325.0078\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.3422 - mse: 298.0150 - val_loss: 0.3504 - val_mse: 305.1399\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.3203 - mse: 278.9517 - val_loss: 0.3284 - val_mse: 285.9854\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.2993 - mse: 260.6832 - val_loss: 0.3075 - val_mse: 267.8020\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.2796 - mse: 243.4658 - val_loss: 0.2880 - val_mse: 250.7869\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.2612 - mse: 227.4813 - val_loss: 0.2699 - val_mse: 234.9961\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.2443 - mse: 212.7672 - val_loss: 0.2530 - val_mse: 220.3377\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.2288 - mse: 199.2192 - val_loss: 0.2373 - val_mse: 206.6260\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.2143 - mse: 186.6522 - val_loss: 0.2224 - val_mse: 193.6642\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.2008 - mse: 174.8815 - val_loss: 0.2082 - val_mse: 181.3066\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.1881 - mse: 163.7771 - val_loss: 0.1948 - val_mse: 169.6130\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.1761 - mse: 153.3901 - val_loss: 0.1818 - val_mse: 158.3001\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.1647 - mse: 143.4556 - val_loss: 0.1695 - val_mse: 147.5660\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.1540 - mse: 134.1203 - val_loss: 0.1579 - val_mse: 137.5199\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.1440 - mse: 125.4388 - val_loss: 0.1473 - val_mse: 128.3141\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.1349 - mse: 117.5014 - val_loss: 0.1379 - val_mse: 120.1214\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.1268 - mse: 110.4019 - val_loss: 0.1298 - val_mse: 113.0358\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.1196 - mse: 104.1749 - val_loss: 0.1230 - val_mse: 107.0760\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.1135 - mse: 98.8196 - val_loss: 0.1172 - val_mse: 102.0328\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.1081 - mse: 94.1371 - val_loss: 0.1124 - val_mse: 97.8990\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.1035 - mse: 90.1350 - val_loss: 0.1087 - val_mse: 94.6634\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0997 - mse: 86.8474 - val_loss: 0.1060 - val_mse: 92.3249\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0968 - mse: 84.3314 - val_loss: 0.1042 - val_mse: 90.7707\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0948 - mse: 82.5717 - val_loss: 0.1032 - val_mse: 89.8557\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0935 - mse: 81.4481 - val_loss: 0.1025 - val_mse: 89.2860\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0927 - mse: 80.7063 - val_loss: 0.1019 - val_mse: 88.6950\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0919 - mse: 80.0025 - val_loss: 0.1008 - val_mse: 87.7764\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0908 - mse: 79.0434 - val_loss: 0.0992 - val_mse: 86.4155\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0892 - mse: 77.7093 - val_loss: 0.0973 - val_mse: 84.6951\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0874 - mse: 76.0782 - val_loss: 0.0951 - val_mse: 82.8344\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0854 - mse: 74.3685 - val_loss: 0.0931 - val_mse: 81.0627\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0836 - mse: 72.8101 - val_loss: 0.0913 - val_mse: 79.5218\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0821 - mse: 71.5369 - val_loss: 0.0898 - val_mse: 78.2432\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0810 - mse: 70.5540 - val_loss: 0.0886 - val_mse: 77.1253\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0801 - mse: 69.7262 - val_loss: 0.0874 - val_mse: 76.0957\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0792 - mse: 68.9456 - val_loss: 0.0863 - val_mse: 75.1569\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0783 - mse: 68.2111 - val_loss: 0.0854 - val_mse: 74.3466\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0776 - mse: 67.5775 - val_loss: 0.0846 - val_mse: 73.6581\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0770 - mse: 67.0499 - val_loss: 0.0839 - val_mse: 73.0304\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0764 - mse: 66.5652 - val_loss: 0.0831 - val_mse: 72.3776\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0758 - mse: 66.0284 - val_loss: 0.0823 - val_mse: 71.6549\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0751 - mse: 65.3890 - val_loss: 0.0814 - val_mse: 70.9175\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0743 - mse: 64.7104 - val_loss: 0.0807 - val_mse: 70.2366\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0736 - mse: 64.0818 - val_loss: 0.0800 - val_mse: 69.6359\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0730 - mse: 63.5423 - val_loss: 0.0793 - val_mse: 69.1001\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0724 - mse: 63.0754 - val_loss: 0.0787 - val_mse: 68.5781\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0719 - mse: 62.6161 - val_loss: 0.0781 - val_mse: 68.0390\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0713 - mse: 62.1342 - val_loss: 0.0775 - val_mse: 67.4808\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0708 - mse: 61.6483 - val_loss: 0.0769 - val_mse: 66.9260\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0703 - mse: 61.1908 - val_loss: 0.0762 - val_mse: 66.3855\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0698 - mse: 60.7598 - val_loss: 0.0756 - val_mse: 65.8473\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0693 - mse: 60.3226 - val_loss: 0.0750 - val_mse: 65.2970\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0687 - mse: 59.8559 - val_loss: 0.0743 - val_mse: 64.7451\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0682 - mse: 59.3785 - val_loss: 0.0737 - val_mse: 64.2158\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0677 - mse: 58.9270 - val_loss: 0.0732 - val_mse: 63.7451\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0672 - mse: 58.5359 - val_loss: 0.0727 - val_mse: 63.3374\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0668 - mse: 58.1969 - val_loss: 0.0723 - val_mse: 62.9619\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0665 - mse: 57.8817 - val_loss: 0.0719 - val_mse: 62.5880\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0661 - mse: 57.5765 - val_loss: 0.0715 - val_mse: 62.2236\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0658 - mse: 57.2896 - val_loss: 0.0711 - val_mse: 61.8932\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0655 - mse: 57.0267 - val_loss: 0.0707 - val_mse: 61.5854\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0652 - mse: 56.7661 - val_loss: 0.0704 - val_mse: 61.2679\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0649 - mse: 56.4888 - val_loss: 0.0700 - val_mse: 60.9322\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0645 - mse: 56.1957 - val_loss: 0.0696 - val_mse: 60.6078\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0642 - mse: 55.9086 - val_loss: 0.0692 - val_mse: 60.3026\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0639 - mse: 55.6349 - val_loss: 0.0689 - val_mse: 59.9948\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0636 - mse: 55.3690 - val_loss: 0.0685 - val_mse: 59.6736\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0633 - mse: 55.1042 - val_loss: 0.0682 - val_mse: 59.3595\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0630 - mse: 54.8497 - val_loss: 0.0678 - val_mse: 59.0662\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0627 - mse: 54.6141 - val_loss: 0.0675 - val_mse: 58.7778\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0625 - mse: 54.3870 - val_loss: 0.0672 - val_mse: 58.4860\n",
      "Epoch 89/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0622 - mse: 54.1590 - val_loss: 0.0668 - val_mse: 58.1937\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0619 - mse: 53.9263 - val_loss: 0.0665 - val_mse: 57.9063\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0617 - mse: 53.6936 - val_loss: 0.0662 - val_mse: 57.6257\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0614 - mse: 53.4680 - val_loss: 0.0659 - val_mse: 57.3509\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0611 - mse: 53.2500 - val_loss: 0.0656 - val_mse: 57.0844\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0609 - mse: 53.0371 - val_loss: 0.0653 - val_mse: 56.8274\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0607 - mse: 52.8289 - val_loss: 0.0650 - val_mse: 56.5798\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0604 - mse: 52.6275 - val_loss: 0.0647 - val_mse: 56.3419\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0602 - mse: 52.4326 - val_loss: 0.0644 - val_mse: 56.1132\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0600 - mse: 52.2401 - val_loss: 0.0642 - val_mse: 55.8892\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0598 - mse: 52.0462 - val_loss: 0.0639 - val_mse: 55.6689\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0595 - mse: 51.8529 - val_loss: 0.0637 - val_mse: 55.4500\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0593 - mse: 51.6593 - val_loss: 0.0634 - val_mse: 55.2381\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0591 - mse: 51.4700 - val_loss: 0.0632 - val_mse: 55.0298\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0589 - mse: 51.2842 - val_loss: 0.0630 - val_mse: 54.8226\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0587 - mse: 51.1016 - val_loss: 0.0627 - val_mse: 54.6182\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0585 - mse: 50.9234 - val_loss: 0.0625 - val_mse: 54.4173\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0583 - mse: 50.7493 - val_loss: 0.0623 - val_mse: 54.2167\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0581 - mse: 50.5781 - val_loss: 0.0620 - val_mse: 54.0143\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0579 - mse: 50.4078 - val_loss: 0.0618 - val_mse: 53.8115\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0577 - mse: 50.2379 - val_loss: 0.0616 - val_mse: 53.6122\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0575 - mse: 50.0699 - val_loss: 0.0613 - val_mse: 53.4144\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0573 - mse: 49.9053 - val_loss: 0.0611 - val_mse: 53.2191\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0571 - mse: 49.7437 - val_loss: 0.0609 - val_mse: 53.0264\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0569 - mse: 49.5843 - val_loss: 0.0607 - val_mse: 52.8347\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0568 - mse: 49.4266 - val_loss: 0.0605 - val_mse: 52.6442\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0566 - mse: 49.2696 - val_loss: 0.0602 - val_mse: 52.4568\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0564 - mse: 49.1138 - val_loss: 0.0600 - val_mse: 52.2708\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0562 - mse: 48.9589 - val_loss: 0.0598 - val_mse: 52.0861\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0560 - mse: 48.8048 - val_loss: 0.0596 - val_mse: 51.9050\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0559 - mse: 48.6527 - val_loss: 0.0594 - val_mse: 51.7266\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0557 - mse: 48.5028 - val_loss: 0.0592 - val_mse: 51.5501\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0555 - mse: 48.3547 - val_loss: 0.0590 - val_mse: 51.3765\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0554 - mse: 48.2083 - val_loss: 0.0588 - val_mse: 51.2030\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0552 - mse: 48.0623 - val_loss: 0.0586 - val_mse: 51.0305\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0550 - mse: 47.9177 - val_loss: 0.0584 - val_mse: 50.8590\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0549 - mse: 47.7735 - val_loss: 0.0582 - val_mse: 50.6879\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0547 - mse: 47.6303 - val_loss: 0.0580 - val_mse: 50.5175\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0545 - mse: 47.4880 - val_loss: 0.0578 - val_mse: 50.3484\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0544 - mse: 47.3464 - val_loss: 0.0576 - val_mse: 50.1793\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0542 - mse: 47.2059 - val_loss: 0.0574 - val_mse: 50.0116\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0540 - mse: 47.0660 - val_loss: 0.0572 - val_mse: 49.8443\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0539 - mse: 46.9269 - val_loss: 0.0570 - val_mse: 49.6800\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0537 - mse: 46.7898 - val_loss: 0.0569 - val_mse: 49.5178\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0536 - mse: 46.6548 - val_loss: 0.0567 - val_mse: 49.3567\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0534 - mse: 46.5201 - val_loss: 0.0565 - val_mse: 49.1962\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0533 - mse: 46.3862 - val_loss: 0.0563 - val_mse: 49.0366\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0531 - mse: 46.2524 - val_loss: 0.0561 - val_mse: 48.8770\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0530 - mse: 46.1197 - val_loss: 0.0559 - val_mse: 48.7208\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0528 - mse: 45.9885 - val_loss: 0.0558 - val_mse: 48.5630\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0527 - mse: 45.8597 - val_loss: 0.0556 - val_mse: 48.4196\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0525 - mse: 45.7380 - val_loss: 0.0555 - val_mse: 48.2927\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0524 - mse: 45.6532 - val_loss: 0.0555 - val_mse: 48.3620\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0525 - mse: 45.7432 - val_loss: 0.0559 - val_mse: 48.6911\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0530 - mse: 46.1973 - val_loss: 0.0558 - val_mse: 48.6026\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0529 - mse: 46.0710 - val_loss: 0.0548 - val_mse: 47.6887\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0518 - mse: 45.1385 - val_loss: 0.0551 - val_mse: 48.0233\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0523 - mse: 45.5607 - val_loss: 0.0549 - val_mse: 47.7767\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0520 - mse: 45.2860 - val_loss: 0.0544 - val_mse: 47.3657\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0515 - mse: 44.8814 - val_loss: 0.0546 - val_mse: 47.5799\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0519 - mse: 45.1816 - val_loss: 0.0540 - val_mse: 47.0298\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0512 - mse: 44.5914 - val_loss: 0.0542 - val_mse: 47.2376\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0515 - mse: 44.8386 - val_loss: 0.0538 - val_mse: 46.8250\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0510 - mse: 44.4427 - val_loss: 0.0538 - val_mse: 46.8367\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0511 - mse: 44.4926 - val_loss: 0.0536 - val_mse: 46.6463\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0509 - mse: 44.2993 - val_loss: 0.0534 - val_mse: 46.5181\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0507 - mse: 44.1927 - val_loss: 0.0533 - val_mse: 46.4065\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0507 - mse: 44.1186 - val_loss: 0.0531 - val_mse: 46.2057\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0504 - mse: 43.9298 - val_loss: 0.0530 - val_mse: 46.1767\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0504 - mse: 43.9186 - val_loss: 0.0528 - val_mse: 45.9466\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0502 - mse: 43.7042 - val_loss: 0.0527 - val_mse: 45.9169\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0502 - mse: 43.7143 - val_loss: 0.0525 - val_mse: 45.6928\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0499 - mse: 43.4937 - val_loss: 0.0525 - val_mse: 45.6817\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0500 - mse: 43.5046 - val_loss: 0.0522 - val_mse: 45.4601\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0497 - mse: 43.2990 - val_loss: 0.0522 - val_mse: 45.4259\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0497 - mse: 43.2980 - val_loss: 0.0519 - val_mse: 45.2353\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0495 - mse: 43.1145 - val_loss: 0.0519 - val_mse: 45.1844\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0495 - mse: 43.0874 - val_loss: 0.0517 - val_mse: 45.0127\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0493 - mse: 42.9374 - val_loss: 0.0516 - val_mse: 44.9275\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0492 - mse: 42.8772 - val_loss: 0.0514 - val_mse: 44.8016\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0491 - mse: 42.7657 - val_loss: 0.0513 - val_mse: 44.6856\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0490 - mse: 42.6702 - val_loss: 0.0512 - val_mse: 44.5849\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0489 - mse: 42.5941 - val_loss: 0.0510 - val_mse: 44.4474\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0488 - mse: 42.4734 - val_loss: 0.0509 - val_mse: 44.3675\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0487 - mse: 42.4161 - val_loss: 0.0508 - val_mse: 44.2210\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0486 - mse: 42.2883 - val_loss: 0.0507 - val_mse: 44.1391\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0485 - mse: 42.2311 - val_loss: 0.0505 - val_mse: 44.0049\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0484 - mse: 42.1151 - val_loss: 0.0504 - val_mse: 43.9082\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0483 - mse: 42.0405 - val_loss: 0.0503 - val_mse: 43.7974\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0482 - mse: 41.9491 - val_loss: 0.0502 - val_mse: 43.6818\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0481 - mse: 41.8524 - val_loss: 0.0501 - val_mse: 43.5873\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0480 - mse: 41.7804 - val_loss: 0.0499 - val_mse: 43.4635\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0479 - mse: 41.6745 - val_loss: 0.0498 - val_mse: 43.3722\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0478 - mse: 41.6041 - val_loss: 0.0497 - val_mse: 43.2555\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0477 - mse: 41.5081 - val_loss: 0.0496 - val_mse: 43.1510\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0476 - mse: 41.4235 - val_loss: 0.0494 - val_mse: 43.0538\n",
      "Epoch 185/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0475 - mse: 41.3442 - val_loss: 0.0493 - val_mse: 42.9396\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0474 - mse: 41.2487 - val_loss: 0.0492 - val_mse: 42.8453\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0473 - mse: 41.1747 - val_loss: 0.0491 - val_mse: 42.7399\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0472 - mse: 41.0841 - val_loss: 0.0490 - val_mse: 42.6400\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0471 - mse: 41.0015 - val_loss: 0.0489 - val_mse: 42.5412\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0470 - mse: 40.9226 - val_loss: 0.0487 - val_mse: 42.4364\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0469 - mse: 40.8330 - val_loss: 0.0486 - val_mse: 42.3438\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0468 - mse: 40.7568 - val_loss: 0.0485 - val_mse: 42.2406\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0467 - mse: 40.6726 - val_loss: 0.0484 - val_mse: 42.1415\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0466 - mse: 40.5896 - val_loss: 0.0483 - val_mse: 42.0500\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0465 - mse: 40.5127 - val_loss: 0.0482 - val_mse: 41.9476\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0464 - mse: 40.4274 - val_loss: 0.0481 - val_mse: 41.8523\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0463 - mse: 40.3488 - val_loss: 0.0480 - val_mse: 41.7588\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0462 - mse: 40.2692 - val_loss: 0.0478 - val_mse: 41.6589\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0461 - mse: 40.1860 - val_loss: 0.0477 - val_mse: 41.5643\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0461 - mse: 40.1087 - val_loss: 0.0476 - val_mse: 41.4696\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0460 - mse: 40.0278 - val_loss: 0.0475 - val_mse: 41.3731\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0459 - mse: 39.9472 - val_loss: 0.0474 - val_mse: 41.2786\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0458 - mse: 39.8697 - val_loss: 0.0473 - val_mse: 41.1838\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0457 - mse: 39.7887 - val_loss: 0.0472 - val_mse: 41.0897\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0456 - mse: 39.7098 - val_loss: 0.0471 - val_mse: 40.9949\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0455 - mse: 39.6317 - val_loss: 0.0470 - val_mse: 40.9004\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0454 - mse: 39.5512 - val_loss: 0.0469 - val_mse: 40.8074\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0453 - mse: 39.4731 - val_loss: 0.0468 - val_mse: 40.7124\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0452 - mse: 39.3947 - val_loss: 0.0466 - val_mse: 40.6184\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0451 - mse: 39.3146 - val_loss: 0.0465 - val_mse: 40.5259\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0451 - mse: 39.2364 - val_loss: 0.0464 - val_mse: 40.4314\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0450 - mse: 39.1574 - val_loss: 0.0463 - val_mse: 40.3381\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0449 - mse: 39.0779 - val_loss: 0.0462 - val_mse: 40.2461\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0448 - mse: 38.9997 - val_loss: 0.0461 - val_mse: 40.1522\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0447 - mse: 38.9212 - val_loss: 0.0460 - val_mse: 40.0599\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0446 - mse: 38.8427 - val_loss: 0.0459 - val_mse: 39.9686\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0445 - mse: 38.7649 - val_loss: 0.0458 - val_mse: 39.8754\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0444 - mse: 38.6863 - val_loss: 0.0457 - val_mse: 39.7831\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0443 - mse: 38.6075 - val_loss: 0.0456 - val_mse: 39.6919\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0442 - mse: 38.5290 - val_loss: 0.0455 - val_mse: 39.5990\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0442 - mse: 38.4501 - val_loss: 0.0454 - val_mse: 39.5072\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0441 - mse: 38.3715 - val_loss: 0.0453 - val_mse: 39.4169\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0440 - mse: 38.2939 - val_loss: 0.0452 - val_mse: 39.3252\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0439 - mse: 38.2160 - val_loss: 0.0451 - val_mse: 39.2344\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0438 - mse: 38.1383 - val_loss: 0.0449 - val_mse: 39.1442\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0437 - mse: 38.0605 - val_loss: 0.0448 - val_mse: 39.0527\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0436 - mse: 37.9820 - val_loss: 0.0447 - val_mse: 38.9618\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0435 - mse: 37.9035 - val_loss: 0.0446 - val_mse: 38.8718\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0434 - mse: 37.8251 - val_loss: 0.0445 - val_mse: 38.7806\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0433 - mse: 37.7463 - val_loss: 0.0444 - val_mse: 38.6899\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0433 - mse: 37.6677 - val_loss: 0.0443 - val_mse: 38.5999\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0432 - mse: 37.5891 - val_loss: 0.0442 - val_mse: 38.5095\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0431 - mse: 37.5105 - val_loss: 0.0441 - val_mse: 38.4200\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0430 - mse: 37.4324 - val_loss: 0.0440 - val_mse: 38.3315\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0429 - mse: 37.3546 - val_loss: 0.0439 - val_mse: 38.2429\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0428 - mse: 37.2769 - val_loss: 0.0438 - val_mse: 38.1546\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0427 - mse: 37.1996 - val_loss: 0.0437 - val_mse: 38.0673\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0426 - mse: 37.1227 - val_loss: 0.0436 - val_mse: 37.9801\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0425 - mse: 37.0461 - val_loss: 0.0435 - val_mse: 37.8929\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0425 - mse: 36.9694 - val_loss: 0.0434 - val_mse: 37.8064\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0424 - mse: 36.8929 - val_loss: 0.0433 - val_mse: 37.7196\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0423 - mse: 36.8164 - val_loss: 0.0432 - val_mse: 37.6328\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0422 - mse: 36.7402 - val_loss: 0.0431 - val_mse: 37.5465\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0421 - mse: 36.6640 - val_loss: 0.0430 - val_mse: 37.4599\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0420 - mse: 36.5878 - val_loss: 0.0429 - val_mse: 37.3735\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0419 - mse: 36.5117 - val_loss: 0.0428 - val_mse: 37.2876\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0418 - mse: 36.4357 - val_loss: 0.0427 - val_mse: 37.2017\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0418 - mse: 36.3599 - val_loss: 0.0426 - val_mse: 37.1159\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0417 - mse: 36.2842 - val_loss: 0.0425 - val_mse: 37.0306\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0416 - mse: 36.2086 - val_loss: 0.0424 - val_mse: 36.9466\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0415 - mse: 36.1332 - val_loss: 0.0423 - val_mse: 36.8644\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0414 - mse: 36.0580 - val_loss: 0.0422 - val_mse: 36.7827\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0413 - mse: 35.9830 - val_loss: 0.0421 - val_mse: 36.7012\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0412 - mse: 35.9085 - val_loss: 0.0421 - val_mse: 36.6200\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0411 - mse: 35.8340 - val_loss: 0.0420 - val_mse: 36.5390\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0411 - mse: 35.7597 - val_loss: 0.0419 - val_mse: 36.4581\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0410 - mse: 35.6858 - val_loss: 0.0418 - val_mse: 36.3774\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0409 - mse: 35.6118 - val_loss: 0.0417 - val_mse: 36.2972\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0408 - mse: 35.5381 - val_loss: 0.0416 - val_mse: 36.2174\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0407 - mse: 35.4649 - val_loss: 0.0415 - val_mse: 36.1384\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0406 - mse: 35.3924 - val_loss: 0.0414 - val_mse: 36.0599\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0406 - mse: 35.3202 - val_loss: 0.0413 - val_mse: 35.9818\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0405 - mse: 35.2483 - val_loss: 0.0412 - val_mse: 35.9042\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0404 - mse: 35.1767 - val_loss: 0.0411 - val_mse: 35.8271\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0403 - mse: 35.1054 - val_loss: 0.0411 - val_mse: 35.7505\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0402 - mse: 35.0345 - val_loss: 0.0410 - val_mse: 35.6745\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0401 - mse: 34.9642 - val_loss: 0.0409 - val_mse: 35.5995\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.0401 - mse: 34.8946 - val_loss: 0.0408 - val_mse: 35.5249\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0400 - mse: 34.8253 - val_loss: 0.0407 - val_mse: 35.4509\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0399 - mse: 34.7564 - val_loss: 0.0406 - val_mse: 35.3777\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0398 - mse: 34.6880 - val_loss: 0.0405 - val_mse: 35.3049\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0398 - mse: 34.6199 - val_loss: 0.0405 - val_mse: 35.2326\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0397 - mse: 34.5521 - val_loss: 0.0404 - val_mse: 35.1607\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0396 - mse: 34.4847 - val_loss: 0.0403 - val_mse: 35.0887\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0395 - mse: 34.4176 - val_loss: 0.0402 - val_mse: 35.0171\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0394 - mse: 34.3508 - val_loss: 0.0401 - val_mse: 34.9460\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0394 - mse: 34.2843 - val_loss: 0.0400 - val_mse: 34.8757\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0393 - mse: 34.2180 - val_loss: 0.0400 - val_mse: 34.8057\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0392 - mse: 34.1520 - val_loss: 0.0399 - val_mse: 34.7362\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0391 - mse: 34.0862 - val_loss: 0.0398 - val_mse: 34.6670\n",
      "Epoch 281/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0391 - mse: 34.0207 - val_loss: 0.0397 - val_mse: 34.5983\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0390 - mse: 33.9555 - val_loss: 0.0397 - val_mse: 34.5303\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0389 - mse: 33.8905 - val_loss: 0.0396 - val_mse: 34.4627\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0388 - mse: 33.8257 - val_loss: 0.0395 - val_mse: 34.3955\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0388 - mse: 33.7612 - val_loss: 0.0394 - val_mse: 34.3282\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0387 - mse: 33.6966 - val_loss: 0.0393 - val_mse: 34.2610\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0386 - mse: 33.6321 - val_loss: 0.0393 - val_mse: 34.1940\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0385 - mse: 33.5678 - val_loss: 0.0392 - val_mse: 34.1272\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0385 - mse: 33.5036 - val_loss: 0.0391 - val_mse: 34.0607\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0384 - mse: 33.4396 - val_loss: 0.0390 - val_mse: 33.9944\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0383 - mse: 33.3759 - val_loss: 0.0390 - val_mse: 33.9284\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0383 - mse: 33.3123 - val_loss: 0.0389 - val_mse: 33.8627\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0382 - mse: 33.2489 - val_loss: 0.0388 - val_mse: 33.7974\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0381 - mse: 33.1857 - val_loss: 0.0387 - val_mse: 33.7324\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0380 - mse: 33.1227 - val_loss: 0.0387 - val_mse: 33.6678\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0380 - mse: 33.0599 - val_loss: 0.0386 - val_mse: 33.6035\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0379 - mse: 32.9973 - val_loss: 0.0385 - val_mse: 33.5397\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0378 - mse: 32.9349 - val_loss: 0.0384 - val_mse: 33.4761\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0377 - mse: 32.8727 - val_loss: 0.0384 - val_mse: 33.4130\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0377 - mse: 32.8108 - val_loss: 0.0383 - val_mse: 33.3504\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0376 - mse: 32.7492 - val_loss: 0.0382 - val_mse: 33.2885\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0375 - mse: 32.6879 - val_loss: 0.0382 - val_mse: 33.2270\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0375 - mse: 32.6269 - val_loss: 0.0381 - val_mse: 33.1657\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0374 - mse: 32.5661 - val_loss: 0.0380 - val_mse: 33.1046\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0373 - mse: 32.5055 - val_loss: 0.0379 - val_mse: 33.0439\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0373 - mse: 32.4452 - val_loss: 0.0379 - val_mse: 32.9837\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0372 - mse: 32.3850 - val_loss: 0.0378 - val_mse: 32.9240\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0371 - mse: 32.3252 - val_loss: 0.0377 - val_mse: 32.8649\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0371 - mse: 32.2656 - val_loss: 0.0377 - val_mse: 32.8056\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0370 - mse: 32.2062 - val_loss: 0.0376 - val_mse: 32.7461\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0369 - mse: 32.1470 - val_loss: 0.0375 - val_mse: 32.6863\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0368 - mse: 32.0880 - val_loss: 0.0375 - val_mse: 32.6267\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0368 - mse: 32.0292 - val_loss: 0.0374 - val_mse: 32.5677\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0367 - mse: 31.9707 - val_loss: 0.0373 - val_mse: 32.5094\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0366 - mse: 31.9124 - val_loss: 0.0373 - val_mse: 32.4510\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0366 - mse: 31.8543 - val_loss: 0.0372 - val_mse: 32.3930\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0365 - mse: 31.7964 - val_loss: 0.0371 - val_mse: 32.3353\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0364 - mse: 31.7387 - val_loss: 0.0371 - val_mse: 32.2777\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0364 - mse: 31.6813 - val_loss: 0.0370 - val_mse: 32.2210\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0363 - mse: 31.6240 - val_loss: 0.0369 - val_mse: 32.1645\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0362 - mse: 31.5669 - val_loss: 0.0369 - val_mse: 32.1083\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0362 - mse: 31.5100 - val_loss: 0.0368 - val_mse: 32.0527\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0361 - mse: 31.4534 - val_loss: 0.0367 - val_mse: 31.9972\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0361 - mse: 31.3972 - val_loss: 0.0367 - val_mse: 31.9420\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0360 - mse: 31.3412 - val_loss: 0.0366 - val_mse: 31.8879\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0359 - mse: 31.2857 - val_loss: 0.0366 - val_mse: 31.8338\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0359 - mse: 31.2305 - val_loss: 0.0365 - val_mse: 31.7799\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0358 - mse: 31.1753 - val_loss: 0.0364 - val_mse: 31.7264\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0357 - mse: 31.1204 - val_loss: 0.0364 - val_mse: 31.6731\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0357 - mse: 31.0657 - val_loss: 0.0363 - val_mse: 31.6205\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0356 - mse: 31.0113 - val_loss: 0.0362 - val_mse: 31.5675\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0355 - mse: 30.9570 - val_loss: 0.0362 - val_mse: 31.5154\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0355 - mse: 30.9029 - val_loss: 0.0361 - val_mse: 31.4634\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0354 - mse: 30.8490 - val_loss: 0.0361 - val_mse: 31.4120\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0354 - mse: 30.7954 - val_loss: 0.0360 - val_mse: 31.3615\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0353 - mse: 30.7420 - val_loss: 0.0360 - val_mse: 31.3107\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0352 - mse: 30.6887 - val_loss: 0.0359 - val_mse: 31.2609\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.0352 - mse: 30.6358 - val_loss: 0.0358 - val_mse: 31.2104\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0351 - mse: 30.5831 - val_loss: 0.0358 - val_mse: 31.1611\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0351 - mse: 30.5304 - val_loss: 0.0357 - val_mse: 31.1114\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0350 - mse: 30.4780 - val_loss: 0.0357 - val_mse: 31.0620\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0349 - mse: 30.4259 - val_loss: 0.0356 - val_mse: 31.0139\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.0349 - mse: 30.3739 - val_loss: 0.0356 - val_mse: 30.9650\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0348 - mse: 30.3221 - val_loss: 0.0355 - val_mse: 30.9179\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.0348 - mse: 30.2705 - val_loss: 0.0354 - val_mse: 30.8696\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.0347 - mse: 30.2189 - val_loss: 0.0354 - val_mse: 30.8223\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.0346 - mse: 30.1676 - val_loss: 0.0353 - val_mse: 30.7748\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0346 - mse: 30.1165 - val_loss: 0.0353 - val_mse: 30.7272\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.0345 - mse: 30.0656 - val_loss: 0.0352 - val_mse: 30.6816\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0345 - mse: 30.0150 - val_loss: 0.0352 - val_mse: 30.6341\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.0344 - mse: 29.9645 - val_loss: 0.0351 - val_mse: 30.5890\n",
      "Epoch 352/1000\n",
      "0s - loss: 0.0344 - mse: 29.9149 - val_loss: 0.0351 - val_mse: 30.5409\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.0343 - mse: 29.8652 - val_loss: 0.0350 - val_mse: 30.4948\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.0342 - mse: 29.8157 - val_loss: 0.0350 - val_mse: 30.4486\n",
      "Epoch 355/1000\n",
      "0s - loss: 0.0342 - mse: 29.7667 - val_loss: 0.0349 - val_mse: 30.4020\n",
      "Epoch 356/1000\n",
      "0s - loss: 0.0341 - mse: 29.7179 - val_loss: 0.0349 - val_mse: 30.3567\n",
      "Epoch 357/1000\n",
      "0s - loss: 0.0341 - mse: 29.6692 - val_loss: 0.0348 - val_mse: 30.3093\n",
      "Epoch 358/1000\n",
      "0s - loss: 0.0340 - mse: 29.6206 - val_loss: 0.0348 - val_mse: 30.2634\n",
      "Epoch 359/1000\n",
      "0s - loss: 0.0340 - mse: 29.5722 - val_loss: 0.0347 - val_mse: 30.2166\n",
      "Epoch 360/1000\n",
      "0s - loss: 0.0339 - mse: 29.5240 - val_loss: 0.0346 - val_mse: 30.1708\n",
      "Epoch 361/1000\n",
      "0s - loss: 0.0338 - mse: 29.4759 - val_loss: 0.0346 - val_mse: 30.1253\n",
      "Epoch 362/1000\n",
      "0s - loss: 0.0338 - mse: 29.4281 - val_loss: 0.0345 - val_mse: 30.0794\n",
      "Epoch 363/1000\n",
      "0s - loss: 0.0337 - mse: 29.3804 - val_loss: 0.0345 - val_mse: 30.0348\n",
      "Epoch 364/1000\n",
      "0s - loss: 0.0337 - mse: 29.3329 - val_loss: 0.0344 - val_mse: 29.9885\n",
      "Epoch 365/1000\n",
      "0s - loss: 0.0336 - mse: 29.2856 - val_loss: 0.0344 - val_mse: 29.9450\n",
      "Epoch 366/1000\n",
      "0s - loss: 0.0336 - mse: 29.2384 - val_loss: 0.0343 - val_mse: 29.8991\n",
      "Epoch 367/1000\n",
      "0s - loss: 0.0335 - mse: 29.1915 - val_loss: 0.0343 - val_mse: 29.8569\n",
      "Epoch 368/1000\n",
      "0s - loss: 0.0335 - mse: 29.1446 - val_loss: 0.0342 - val_mse: 29.8115\n",
      "Epoch 369/1000\n",
      "0s - loss: 0.0334 - mse: 29.0979 - val_loss: 0.0342 - val_mse: 29.7696\n",
      "Epoch 370/1000\n",
      "0s - loss: 0.0334 - mse: 29.0512 - val_loss: 0.0341 - val_mse: 29.7246\n",
      "Epoch 371/1000\n",
      "0s - loss: 0.0333 - mse: 29.0047 - val_loss: 0.0341 - val_mse: 29.6829\n",
      "Epoch 372/1000\n",
      "0s - loss: 0.0333 - mse: 28.9584 - val_loss: 0.0340 - val_mse: 29.6393\n",
      "Epoch 373/1000\n",
      "0s - loss: 0.0332 - mse: 28.9122 - val_loss: 0.0340 - val_mse: 29.5976\n",
      "Epoch 374/1000\n",
      "0s - loss: 0.0331 - mse: 28.8662 - val_loss: 0.0339 - val_mse: 29.5554\n",
      "Epoch 375/1000\n",
      "0s - loss: 0.0331 - mse: 28.8203 - val_loss: 0.0339 - val_mse: 29.5126\n",
      "Epoch 376/1000\n",
      "0s - loss: 0.0330 - mse: 28.7747 - val_loss: 0.0338 - val_mse: 29.4715\n",
      "Epoch 377/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0330 - mse: 28.7292 - val_loss: 0.0338 - val_mse: 29.4283\n",
      "Epoch 378/1000\n",
      "0s - loss: 0.0329 - mse: 28.6839 - val_loss: 0.0337 - val_mse: 29.3886\n",
      "Epoch 379/1000\n",
      "0s - loss: 0.0329 - mse: 28.6388 - val_loss: 0.0337 - val_mse: 29.3451\n",
      "Epoch 380/1000\n",
      "0s - loss: 0.0328 - mse: 28.5936 - val_loss: 0.0337 - val_mse: 29.3057\n",
      "Epoch 381/1000\n",
      "0s - loss: 0.0328 - mse: 28.5487 - val_loss: 0.0336 - val_mse: 29.2622\n",
      "Epoch 382/1000\n",
      "0s - loss: 0.0327 - mse: 28.5037 - val_loss: 0.0336 - val_mse: 29.2228\n",
      "Epoch 383/1000\n",
      "0s - loss: 0.0327 - mse: 28.4587 - val_loss: 0.0335 - val_mse: 29.1808\n",
      "Epoch 384/1000\n",
      "0s - loss: 0.0326 - mse: 28.4138 - val_loss: 0.0335 - val_mse: 29.1413\n",
      "Epoch 385/1000\n",
      "0s - loss: 0.0326 - mse: 28.3691 - val_loss: 0.0334 - val_mse: 29.1009\n",
      "Epoch 386/1000\n",
      "0s - loss: 0.0325 - mse: 28.3247 - val_loss: 0.0334 - val_mse: 29.0611\n",
      "Epoch 387/1000\n",
      "0s - loss: 0.0325 - mse: 28.2804 - val_loss: 0.0333 - val_mse: 29.0218\n",
      "Epoch 388/1000\n",
      "0s - loss: 0.0324 - mse: 28.2363 - val_loss: 0.0333 - val_mse: 28.9814\n",
      "Epoch 389/1000\n",
      "0s - loss: 0.0324 - mse: 28.1923 - val_loss: 0.0332 - val_mse: 28.9428\n",
      "Epoch 390/1000\n",
      "0s - loss: 0.0323 - mse: 28.1485 - val_loss: 0.0332 - val_mse: 28.9017\n",
      "Epoch 391/1000\n",
      "0s - loss: 0.0323 - mse: 28.1047 - val_loss: 0.0331 - val_mse: 28.8638\n",
      "Epoch 392/1000\n",
      "0s - loss: 0.0322 - mse: 28.0612 - val_loss: 0.0331 - val_mse: 28.8226\n",
      "Epoch 393/1000\n",
      "0s - loss: 0.0322 - mse: 28.0176 - val_loss: 0.0331 - val_mse: 28.7850\n",
      "Epoch 394/1000\n",
      "0s - loss: 0.0321 - mse: 27.9741 - val_loss: 0.0330 - val_mse: 28.7449\n",
      "Epoch 395/1000\n",
      "0s - loss: 0.0321 - mse: 27.9307 - val_loss: 0.0330 - val_mse: 28.7076\n",
      "Epoch 396/1000\n",
      "0s - loss: 0.0320 - mse: 27.8875 - val_loss: 0.0329 - val_mse: 28.6682\n",
      "Epoch 397/1000\n",
      "0s - loss: 0.0320 - mse: 27.8445 - val_loss: 0.0329 - val_mse: 28.6311\n",
      "Epoch 398/1000\n",
      "0s - loss: 0.0319 - mse: 27.8014 - val_loss: 0.0328 - val_mse: 28.5918\n",
      "Epoch 399/1000\n",
      "0s - loss: 0.0319 - mse: 27.7585 - val_loss: 0.0328 - val_mse: 28.5550\n",
      "Epoch 400/1000\n",
      "0s - loss: 0.0318 - mse: 27.7156 - val_loss: 0.0327 - val_mse: 28.5166\n",
      "Epoch 401/1000\n",
      "0s - loss: 0.0318 - mse: 27.6729 - val_loss: 0.0327 - val_mse: 28.4800\n",
      "Epoch 402/1000\n",
      "0s - loss: 0.0317 - mse: 27.6303 - val_loss: 0.0327 - val_mse: 28.4422\n",
      "Epoch 403/1000\n",
      "0s - loss: 0.0317 - mse: 27.5878 - val_loss: 0.0326 - val_mse: 28.4048\n",
      "Epoch 404/1000\n",
      "0s - loss: 0.0316 - mse: 27.5454 - val_loss: 0.0326 - val_mse: 28.3678\n",
      "Epoch 405/1000\n",
      "0s - loss: 0.0316 - mse: 27.5032 - val_loss: 0.0325 - val_mse: 28.3294\n",
      "Epoch 406/1000\n",
      "0s - loss: 0.0315 - mse: 27.4612 - val_loss: 0.0325 - val_mse: 28.2946\n",
      "Epoch 407/1000\n",
      "0s - loss: 0.0315 - mse: 27.4196 - val_loss: 0.0324 - val_mse: 28.2553\n",
      "Epoch 408/1000\n",
      "0s - loss: 0.0314 - mse: 27.3781 - val_loss: 0.0324 - val_mse: 28.2231\n",
      "Epoch 409/1000\n",
      "0s - loss: 0.0314 - mse: 27.3366 - val_loss: 0.0324 - val_mse: 28.1837\n",
      "Epoch 410/1000\n",
      "0s - loss: 0.0313 - mse: 27.2952 - val_loss: 0.0323 - val_mse: 28.1530\n",
      "Epoch 411/1000\n",
      "0s - loss: 0.0313 - mse: 27.2540 - val_loss: 0.0323 - val_mse: 28.1130\n",
      "Epoch 412/1000\n",
      "0s - loss: 0.0312 - mse: 27.2130 - val_loss: 0.0322 - val_mse: 28.0831\n",
      "Epoch 413/1000\n",
      "0s - loss: 0.0312 - mse: 27.1720 - val_loss: 0.0322 - val_mse: 28.0420\n",
      "Epoch 414/1000\n",
      "0s - loss: 0.0312 - mse: 27.1313 - val_loss: 0.0322 - val_mse: 28.0134\n",
      "Epoch 415/1000\n",
      "0s - loss: 0.0311 - mse: 27.0906 - val_loss: 0.0321 - val_mse: 27.9712\n",
      "Epoch 416/1000\n",
      "0s - loss: 0.0311 - mse: 27.0498 - val_loss: 0.0321 - val_mse: 27.9427\n",
      "Epoch 417/1000\n",
      "0s - loss: 0.0310 - mse: 27.0093 - val_loss: 0.0320 - val_mse: 27.8995\n",
      "Epoch 418/1000\n",
      "0s - loss: 0.0310 - mse: 26.9682 - val_loss: 0.0320 - val_mse: 27.8695\n",
      "Epoch 419/1000\n",
      "0s - loss: 0.0309 - mse: 26.9264 - val_loss: 0.0320 - val_mse: 27.8272\n",
      "Epoch 420/1000\n",
      "0s - loss: 0.0309 - mse: 26.8844 - val_loss: 0.0319 - val_mse: 27.7959\n",
      "Epoch 421/1000\n",
      "0s - loss: 0.0308 - mse: 26.8425 - val_loss: 0.0319 - val_mse: 27.7573\n",
      "Epoch 422/1000\n",
      "0s - loss: 0.0308 - mse: 26.8010 - val_loss: 0.0318 - val_mse: 27.7244\n",
      "Epoch 423/1000\n",
      "0s - loss: 0.0307 - mse: 26.7600 - val_loss: 0.0318 - val_mse: 27.6893\n",
      "Epoch 424/1000\n",
      "0s - loss: 0.0307 - mse: 26.7194 - val_loss: 0.0318 - val_mse: 27.6552\n",
      "Epoch 425/1000\n",
      "0s - loss: 0.0306 - mse: 26.6791 - val_loss: 0.0317 - val_mse: 27.6220\n",
      "Epoch 426/1000\n",
      "0s - loss: 0.0306 - mse: 26.6390 - val_loss: 0.0317 - val_mse: 27.5875\n",
      "Epoch 427/1000\n",
      "0s - loss: 0.0305 - mse: 26.5990 - val_loss: 0.0316 - val_mse: 27.5552\n",
      "Epoch 428/1000\n",
      "0s - loss: 0.0305 - mse: 26.5591 - val_loss: 0.0316 - val_mse: 27.5195\n",
      "Epoch 429/1000\n",
      "0s - loss: 0.0305 - mse: 26.5194 - val_loss: 0.0316 - val_mse: 27.4875\n",
      "Epoch 430/1000\n",
      "0s - loss: 0.0304 - mse: 26.4796 - val_loss: 0.0315 - val_mse: 27.4514\n",
      "Epoch 431/1000\n",
      "0s - loss: 0.0304 - mse: 26.4400 - val_loss: 0.0315 - val_mse: 27.4200\n",
      "Epoch 432/1000\n",
      "0s - loss: 0.0303 - mse: 26.4004 - val_loss: 0.0314 - val_mse: 27.3839\n",
      "Epoch 433/1000\n",
      "0s - loss: 0.0303 - mse: 26.3605 - val_loss: 0.0314 - val_mse: 27.3528\n",
      "Epoch 434/1000\n",
      "0s - loss: 0.0302 - mse: 26.3204 - val_loss: 0.0314 - val_mse: 27.3170\n",
      "Epoch 435/1000\n",
      "0s - loss: 0.0302 - mse: 26.2804 - val_loss: 0.0313 - val_mse: 27.2855\n",
      "Epoch 436/1000\n",
      "0s - loss: 0.0301 - mse: 26.2402 - val_loss: 0.0313 - val_mse: 27.2506\n",
      "Epoch 437/1000\n",
      "0s - loss: 0.0301 - mse: 26.2002 - val_loss: 0.0313 - val_mse: 27.2190\n",
      "Epoch 438/1000\n",
      "0s - loss: 0.0300 - mse: 26.1602 - val_loss: 0.0312 - val_mse: 27.1854\n",
      "Epoch 439/1000\n",
      "0s - loss: 0.0300 - mse: 26.1203 - val_loss: 0.0312 - val_mse: 27.1537\n",
      "Epoch 440/1000\n",
      "0s - loss: 0.0299 - mse: 26.0805 - val_loss: 0.0311 - val_mse: 27.1207\n",
      "Epoch 441/1000\n",
      "0s - loss: 0.0299 - mse: 26.0407 - val_loss: 0.0311 - val_mse: 27.0885\n",
      "Epoch 442/1000\n",
      "0s - loss: 0.0299 - mse: 26.0010 - val_loss: 0.0311 - val_mse: 27.0566\n",
      "Epoch 443/1000\n",
      "0s - loss: 0.0298 - mse: 25.9614 - val_loss: 0.0310 - val_mse: 27.0237\n",
      "Epoch 444/1000\n",
      "0s - loss: 0.0298 - mse: 25.9218 - val_loss: 0.0310 - val_mse: 26.9925\n",
      "Epoch 445/1000\n",
      "0s - loss: 0.0297 - mse: 25.8823 - val_loss: 0.0310 - val_mse: 26.9590\n",
      "Epoch 446/1000\n",
      "0s - loss: 0.0297 - mse: 25.8428 - val_loss: 0.0309 - val_mse: 26.9285\n",
      "Epoch 447/1000\n",
      "0s - loss: 0.0296 - mse: 25.8033 - val_loss: 0.0309 - val_mse: 26.8952\n",
      "Epoch 448/1000\n",
      "0s - loss: 0.0296 - mse: 25.7640 - val_loss: 0.0309 - val_mse: 26.8658\n",
      "Epoch 449/1000\n",
      "0s - loss: 0.0295 - mse: 25.7247 - val_loss: 0.0308 - val_mse: 26.8322\n",
      "Epoch 450/1000\n",
      "0s - loss: 0.0295 - mse: 25.6855 - val_loss: 0.0308 - val_mse: 26.8029\n",
      "Epoch 451/1000\n",
      "0s - loss: 0.0295 - mse: 25.6464 - val_loss: 0.0307 - val_mse: 26.7697\n",
      "Epoch 452/1000\n",
      "0s - loss: 0.0294 - mse: 25.6073 - val_loss: 0.0307 - val_mse: 26.7400\n",
      "Epoch 453/1000\n",
      "0s - loss: 0.0294 - mse: 25.5682 - val_loss: 0.0307 - val_mse: 26.7079\n",
      "Epoch 454/1000\n",
      "0s - loss: 0.0293 - mse: 25.5293 - val_loss: 0.0306 - val_mse: 26.6779\n",
      "Epoch 455/1000\n",
      "0s - loss: 0.0293 - mse: 25.4904 - val_loss: 0.0306 - val_mse: 26.6461\n",
      "Epoch 456/1000\n",
      "0s - loss: 0.0292 - mse: 25.4516 - val_loss: 0.0306 - val_mse: 26.6168\n",
      "Epoch 457/1000\n",
      "0s - loss: 0.0292 - mse: 25.4129 - val_loss: 0.0305 - val_mse: 26.5855\n",
      "Epoch 458/1000\n",
      "0s - loss: 0.0291 - mse: 25.3743 - val_loss: 0.0305 - val_mse: 26.5579\n",
      "Epoch 459/1000\n",
      "0s - loss: 0.0291 - mse: 25.3360 - val_loss: 0.0305 - val_mse: 26.5255\n",
      "Epoch 460/1000\n",
      "0s - loss: 0.0291 - mse: 25.2981 - val_loss: 0.0304 - val_mse: 26.4996\n",
      "Epoch 461/1000\n",
      "0s - loss: 0.0290 - mse: 25.2603 - val_loss: 0.0304 - val_mse: 26.4645\n",
      "Epoch 462/1000\n",
      "0s - loss: 0.0290 - mse: 25.2225 - val_loss: 0.0304 - val_mse: 26.4395\n",
      "Epoch 463/1000\n",
      "0s - loss: 0.0289 - mse: 25.1843 - val_loss: 0.0303 - val_mse: 26.4036\n",
      "Epoch 464/1000\n",
      "0s - loss: 0.0289 - mse: 25.1459 - val_loss: 0.0303 - val_mse: 26.3772\n",
      "Epoch 465/1000\n",
      "0s - loss: 0.0288 - mse: 25.1072 - val_loss: 0.0303 - val_mse: 26.3430\n",
      "Epoch 466/1000\n",
      "0s - loss: 0.0288 - mse: 25.0688 - val_loss: 0.0302 - val_mse: 26.3157\n",
      "Epoch 467/1000\n",
      "0s - loss: 0.0287 - mse: 25.0307 - val_loss: 0.0302 - val_mse: 26.2843\n",
      "Epoch 468/1000\n",
      "0s - loss: 0.0287 - mse: 24.9927 - val_loss: 0.0302 - val_mse: 26.2567\n",
      "Epoch 469/1000\n",
      "0s - loss: 0.0287 - mse: 24.9548 - val_loss: 0.0301 - val_mse: 26.2277\n",
      "Epoch 470/1000\n",
      "0s - loss: 0.0286 - mse: 24.9171 - val_loss: 0.0301 - val_mse: 26.1994\n",
      "Epoch 471/1000\n",
      "0s - loss: 0.0286 - mse: 24.8794 - val_loss: 0.0301 - val_mse: 26.1709\n",
      "Epoch 472/1000\n",
      "0s - loss: 0.0285 - mse: 24.8417 - val_loss: 0.0300 - val_mse: 26.1429\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0285 - mse: 24.8040 - val_loss: 0.0300 - val_mse: 26.1149\n",
      "Epoch 474/1000\n",
      "0s - loss: 0.0284 - mse: 24.7663 - val_loss: 0.0300 - val_mse: 26.0882\n",
      "Epoch 475/1000\n",
      "0s - loss: 0.0284 - mse: 24.7288 - val_loss: 0.0299 - val_mse: 26.0595\n",
      "Epoch 476/1000\n",
      "0s - loss: 0.0284 - mse: 24.6914 - val_loss: 0.0299 - val_mse: 26.0340\n",
      "Epoch 477/1000\n",
      "0s - loss: 0.0283 - mse: 24.6541 - val_loss: 0.0299 - val_mse: 26.0042\n",
      "Epoch 478/1000\n",
      "0s - loss: 0.0283 - mse: 24.6169 - val_loss: 0.0298 - val_mse: 25.9801\n",
      "Epoch 479/1000\n",
      "0s - loss: 0.0282 - mse: 24.5797 - val_loss: 0.0298 - val_mse: 25.9500\n",
      "Epoch 480/1000\n",
      "0s - loss: 0.0282 - mse: 24.5425 - val_loss: 0.0298 - val_mse: 25.9259\n",
      "Epoch 481/1000\n",
      "0s - loss: 0.0281 - mse: 24.5052 - val_loss: 0.0297 - val_mse: 25.8957\n",
      "Epoch 482/1000\n",
      "0s - loss: 0.0281 - mse: 24.4680 - val_loss: 0.0297 - val_mse: 25.8702\n",
      "Epoch 483/1000\n",
      "0s - loss: 0.0281 - mse: 24.4308 - val_loss: 0.0297 - val_mse: 25.8416\n",
      "Epoch 484/1000\n",
      "0s - loss: 0.0280 - mse: 24.3937 - val_loss: 0.0296 - val_mse: 25.8150\n",
      "Epoch 485/1000\n",
      "0s - loss: 0.0280 - mse: 24.3570 - val_loss: 0.0296 - val_mse: 25.7877\n",
      "Epoch 486/1000\n",
      "0s - loss: 0.0279 - mse: 24.3205 - val_loss: 0.0296 - val_mse: 25.7618\n",
      "Epoch 487/1000\n",
      "0s - loss: 0.0279 - mse: 24.2842 - val_loss: 0.0296 - val_mse: 25.7367\n",
      "Epoch 488/1000\n",
      "0s - loss: 0.0278 - mse: 24.2482 - val_loss: 0.0295 - val_mse: 25.7099\n",
      "Epoch 489/1000\n",
      "0s - loss: 0.0278 - mse: 24.2123 - val_loss: 0.0295 - val_mse: 25.6860\n",
      "Epoch 490/1000\n",
      "0s - loss: 0.0278 - mse: 24.1766 - val_loss: 0.0295 - val_mse: 25.6584\n",
      "Epoch 491/1000\n",
      "0s - loss: 0.0277 - mse: 24.1409 - val_loss: 0.0294 - val_mse: 25.6360\n",
      "Epoch 492/1000\n",
      "0s - loss: 0.0277 - mse: 24.1053 - val_loss: 0.0294 - val_mse: 25.6084\n",
      "Epoch 493/1000\n",
      "0s - loss: 0.0276 - mse: 24.0699 - val_loss: 0.0294 - val_mse: 25.5874\n",
      "Epoch 494/1000\n",
      "0s - loss: 0.0276 - mse: 24.0347 - val_loss: 0.0293 - val_mse: 25.5589\n",
      "Epoch 495/1000\n",
      "0s - loss: 0.0276 - mse: 23.9995 - val_loss: 0.0293 - val_mse: 25.5383\n",
      "Epoch 496/1000\n",
      "0s - loss: 0.0275 - mse: 23.9642 - val_loss: 0.0293 - val_mse: 25.5095\n",
      "Epoch 497/1000\n",
      "0s - loss: 0.0275 - mse: 23.9288 - val_loss: 0.0293 - val_mse: 25.4885\n",
      "Epoch 498/1000\n",
      "0s - loss: 0.0274 - mse: 23.8935 - val_loss: 0.0292 - val_mse: 25.4613\n",
      "Epoch 499/1000\n",
      "0s - loss: 0.0274 - mse: 23.8584 - val_loss: 0.0292 - val_mse: 25.4411\n",
      "Epoch 500/1000\n",
      "0s - loss: 0.0274 - mse: 23.8236 - val_loss: 0.0292 - val_mse: 25.4142\n",
      "Epoch 501/1000\n",
      "0s - loss: 0.0273 - mse: 23.7889 - val_loss: 0.0292 - val_mse: 25.3940\n",
      "Epoch 502/1000\n",
      "0s - loss: 0.0273 - mse: 23.7540 - val_loss: 0.0291 - val_mse: 25.3672\n",
      "Epoch 503/1000\n",
      "0s - loss: 0.0272 - mse: 23.7191 - val_loss: 0.0291 - val_mse: 25.3437\n",
      "Epoch 504/1000\n",
      "0s - loss: 0.0272 - mse: 23.6845 - val_loss: 0.0291 - val_mse: 25.3197\n",
      "Epoch 505/1000\n",
      "0s - loss: 0.0272 - mse: 23.6501 - val_loss: 0.0290 - val_mse: 25.2943\n",
      "Epoch 506/1000\n",
      "0s - loss: 0.0271 - mse: 23.6160 - val_loss: 0.0290 - val_mse: 25.2743\n",
      "Epoch 507/1000\n",
      "0s - loss: 0.0271 - mse: 23.5819 - val_loss: 0.0290 - val_mse: 25.2490\n",
      "Epoch 508/1000\n",
      "0s - loss: 0.0270 - mse: 23.5477 - val_loss: 0.0290 - val_mse: 25.2296\n",
      "Epoch 509/1000\n",
      "0s - loss: 0.0270 - mse: 23.5135 - val_loss: 0.0289 - val_mse: 25.2066\n",
      "Epoch 510/1000\n",
      "0s - loss: 0.0270 - mse: 23.4794 - val_loss: 0.0289 - val_mse: 25.1848\n",
      "Epoch 511/1000\n",
      "0s - loss: 0.0269 - mse: 23.4455 - val_loss: 0.0289 - val_mse: 25.1635\n",
      "Epoch 512/1000\n",
      "0s - loss: 0.0269 - mse: 23.4118 - val_loss: 0.0289 - val_mse: 25.1398\n",
      "Epoch 513/1000\n",
      "0s - loss: 0.0268 - mse: 23.3782 - val_loss: 0.0288 - val_mse: 25.1195\n",
      "Epoch 514/1000\n",
      "0s - loss: 0.0268 - mse: 23.3446 - val_loss: 0.0288 - val_mse: 25.0963\n",
      "Epoch 515/1000\n",
      "0s - loss: 0.0268 - mse: 23.3111 - val_loss: 0.0288 - val_mse: 25.0784\n",
      "Epoch 516/1000\n",
      "0s - loss: 0.0267 - mse: 23.2778 - val_loss: 0.0288 - val_mse: 25.0547\n",
      "Epoch 517/1000\n",
      "0s - loss: 0.0267 - mse: 23.2446 - val_loss: 0.0288 - val_mse: 25.0382\n",
      "Epoch 518/1000\n",
      "0s - loss: 0.0267 - mse: 23.2112 - val_loss: 0.0287 - val_mse: 25.0149\n",
      "Epoch 519/1000\n",
      "0s - loss: 0.0266 - mse: 23.1778 - val_loss: 0.0287 - val_mse: 24.9959\n",
      "Epoch 520/1000\n",
      "0s - loss: 0.0266 - mse: 23.1447 - val_loss: 0.0287 - val_mse: 24.9752\n",
      "Epoch 521/1000\n",
      "0s - loss: 0.0265 - mse: 23.1121 - val_loss: 0.0287 - val_mse: 24.9540\n",
      "Epoch 522/1000\n",
      "0s - loss: 0.0265 - mse: 23.0796 - val_loss: 0.0286 - val_mse: 24.9375\n",
      "Epoch 523/1000\n",
      "0s - loss: 0.0265 - mse: 23.0478 - val_loss: 0.0286 - val_mse: 24.9121\n",
      "Epoch 524/1000\n",
      "0s - loss: 0.0264 - mse: 23.0159 - val_loss: 0.0286 - val_mse: 24.8962\n",
      "Epoch 525/1000\n",
      "0s - loss: 0.0264 - mse: 22.9837 - val_loss: 0.0286 - val_mse: 24.8700\n",
      "Epoch 526/1000\n",
      "0s - loss: 0.0264 - mse: 22.9512 - val_loss: 0.0285 - val_mse: 24.8512\n",
      "Epoch 527/1000\n",
      "0s - loss: 0.0263 - mse: 22.9190 - val_loss: 0.0285 - val_mse: 24.8286\n",
      "Epoch 528/1000\n",
      "0s - loss: 0.0263 - mse: 22.8870 - val_loss: 0.0285 - val_mse: 24.8101\n",
      "Epoch 529/1000\n",
      "0s - loss: 0.0262 - mse: 22.8552 - val_loss: 0.0285 - val_mse: 24.7904\n",
      "Epoch 530/1000\n",
      "0s - loss: 0.0262 - mse: 22.8236 - val_loss: 0.0284 - val_mse: 24.7719\n",
      "Epoch 531/1000\n",
      "0s - loss: 0.0262 - mse: 22.7920 - val_loss: 0.0284 - val_mse: 24.7502\n",
      "Epoch 532/1000\n",
      "0s - loss: 0.0261 - mse: 22.7606 - val_loss: 0.0284 - val_mse: 24.7307\n",
      "Epoch 533/1000\n",
      "0s - loss: 0.0261 - mse: 22.7292 - val_loss: 0.0284 - val_mse: 24.7085\n",
      "Epoch 534/1000\n",
      "0s - loss: 0.0261 - mse: 22.6982 - val_loss: 0.0284 - val_mse: 24.6935\n",
      "Epoch 535/1000\n",
      "0s - loss: 0.0260 - mse: 22.6674 - val_loss: 0.0283 - val_mse: 24.6695\n",
      "Epoch 536/1000\n",
      "0s - loss: 0.0260 - mse: 22.6371 - val_loss: 0.0283 - val_mse: 24.6585\n",
      "Epoch 537/1000\n",
      "0s - loss: 0.0260 - mse: 22.6064 - val_loss: 0.0283 - val_mse: 24.6315\n",
      "Epoch 538/1000\n",
      "0s - loss: 0.0259 - mse: 22.5754 - val_loss: 0.0283 - val_mse: 24.6194\n",
      "Epoch 539/1000\n",
      "0s - loss: 0.0259 - mse: 22.5445 - val_loss: 0.0282 - val_mse: 24.5949\n",
      "Epoch 540/1000\n",
      "0s - loss: 0.0259 - mse: 22.5133 - val_loss: 0.0282 - val_mse: 24.5775\n",
      "Epoch 541/1000\n",
      "0s - loss: 0.0258 - mse: 22.4827 - val_loss: 0.0282 - val_mse: 24.5603\n",
      "Epoch 542/1000\n",
      "0s - loss: 0.0258 - mse: 22.4528 - val_loss: 0.0282 - val_mse: 24.5371\n",
      "Epoch 543/1000\n",
      "0s - loss: 0.0257 - mse: 22.4231 - val_loss: 0.0282 - val_mse: 24.5244\n",
      "Epoch 544/1000\n",
      "0s - loss: 0.0257 - mse: 22.3932 - val_loss: 0.0281 - val_mse: 24.5002\n",
      "Epoch 545/1000\n",
      "0s - loss: 0.0257 - mse: 22.3631 - val_loss: 0.0281 - val_mse: 24.4860\n",
      "Epoch 546/1000\n",
      "0s - loss: 0.0256 - mse: 22.3329 - val_loss: 0.0281 - val_mse: 24.4642\n",
      "Epoch 547/1000\n",
      "0s - loss: 0.0256 - mse: 22.3028 - val_loss: 0.0281 - val_mse: 24.4454\n",
      "Epoch 548/1000\n",
      "0s - loss: 0.0256 - mse: 22.2731 - val_loss: 0.0281 - val_mse: 24.4310\n",
      "Epoch 549/1000\n",
      "0s - loss: 0.0255 - mse: 22.2444 - val_loss: 0.0280 - val_mse: 24.4070\n",
      "Epoch 550/1000\n",
      "0s - loss: 0.0255 - mse: 22.2161 - val_loss: 0.0280 - val_mse: 24.4010\n",
      "Epoch 551/1000\n",
      "0s - loss: 0.0255 - mse: 22.1879 - val_loss: 0.0280 - val_mse: 24.3721\n",
      "Epoch 552/1000\n",
      "0s - loss: 0.0254 - mse: 22.1579 - val_loss: 0.0280 - val_mse: 24.3599\n",
      "Epoch 553/1000\n",
      "0s - loss: 0.0254 - mse: 22.1274 - val_loss: 0.0279 - val_mse: 24.3378\n",
      "Epoch 554/1000\n",
      "0s - loss: 0.0254 - mse: 22.0978 - val_loss: 0.0279 - val_mse: 24.3175\n",
      "Epoch 555/1000\n",
      "0s - loss: 0.0253 - mse: 22.0698 - val_loss: 0.0279 - val_mse: 24.3088\n",
      "Epoch 556/1000\n",
      "0s - loss: 0.0253 - mse: 22.0424 - val_loss: 0.0279 - val_mse: 24.2830\n",
      "Epoch 557/1000\n",
      "0s - loss: 0.0253 - mse: 22.0139 - val_loss: 0.0279 - val_mse: 24.2753\n",
      "Epoch 558/1000\n",
      "0s - loss: 0.0252 - mse: 21.9851 - val_loss: 0.0278 - val_mse: 24.2497\n",
      "Epoch 559/1000\n",
      "\n",
      "Epoch 00558: reducing learning rate to 0.00699999984354.\n",
      "5s - loss: 0.0252 - mse: 21.9556 - val_loss: 0.0278 - val_mse: 24.2346\n",
      "Epoch 560/1000\n",
      "0s - loss: 0.0252 - mse: 21.9265 - val_loss: 0.0278 - val_mse: 24.2230\n",
      "Epoch 561/1000\n",
      "0s - loss: 0.0252 - mse: 21.9068 - val_loss: 0.0278 - val_mse: 24.2075\n",
      "Epoch 562/1000\n",
      "0s - loss: 0.0251 - mse: 21.8870 - val_loss: 0.0278 - val_mse: 24.1957\n",
      "Epoch 563/1000\n",
      "0s - loss: 0.0251 - mse: 21.8672 - val_loss: 0.0278 - val_mse: 24.1857\n",
      "Epoch 564/1000\n",
      "0s - loss: 0.0251 - mse: 21.8477 - val_loss: 0.0278 - val_mse: 24.1724\n",
      "Epoch 565/1000\n",
      "0s - loss: 0.0251 - mse: 21.8279 - val_loss: 0.0277 - val_mse: 24.1611\n",
      "Epoch 566/1000\n",
      "0s - loss: 0.0250 - mse: 21.8084 - val_loss: 0.0277 - val_mse: 24.1521\n",
      "Epoch 567/1000\n",
      "0s - loss: 0.0250 - mse: 21.7889 - val_loss: 0.0277 - val_mse: 24.1400\n",
      "Epoch 568/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0250 - mse: 21.7693 - val_loss: 0.0277 - val_mse: 24.1275\n",
      "Epoch 569/1000\n",
      "0s - loss: 0.0250 - mse: 21.7499 - val_loss: 0.0277 - val_mse: 24.1164\n",
      "Epoch 570/1000\n",
      "0s - loss: 0.0250 - mse: 21.7304 - val_loss: 0.0277 - val_mse: 24.1042\n",
      "Epoch 571/1000\n",
      "0s - loss: 0.0249 - mse: 21.7111 - val_loss: 0.0277 - val_mse: 24.0910\n",
      "Epoch 572/1000\n",
      "0s - loss: 0.0249 - mse: 21.6919 - val_loss: 0.0277 - val_mse: 24.0804\n",
      "Epoch 573/1000\n",
      "\n",
      "Epoch 00572: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0249 - mse: 21.6726 - val_loss: 0.0276 - val_mse: 24.0701\n",
      "Epoch 574/1000\n",
      "0s - loss: 0.0249 - mse: 21.6535 - val_loss: 0.0276 - val_mse: 24.0610\n",
      "Epoch 575/1000\n",
      "0s - loss: 0.0248 - mse: 21.6400 - val_loss: 0.0276 - val_mse: 24.0524\n",
      "Epoch 576/1000\n",
      "0s - loss: 0.0248 - mse: 21.6267 - val_loss: 0.0276 - val_mse: 24.0452\n",
      "Epoch 577/1000\n",
      "0s - loss: 0.0248 - mse: 21.6133 - val_loss: 0.0276 - val_mse: 24.0378\n",
      "Epoch 578/1000\n",
      "0s - loss: 0.0248 - mse: 21.5999 - val_loss: 0.0276 - val_mse: 24.0288\n",
      "Epoch 579/1000\n",
      "0s - loss: 0.0248 - mse: 21.5864 - val_loss: 0.0276 - val_mse: 24.0204\n",
      "Epoch 580/1000\n",
      "0s - loss: 0.0248 - mse: 21.5731 - val_loss: 0.0276 - val_mse: 24.0130\n",
      "Epoch 581/1000\n",
      "0s - loss: 0.0248 - mse: 21.5596 - val_loss: 0.0276 - val_mse: 24.0056\n",
      "Epoch 582/1000\n",
      "0s - loss: 0.0247 - mse: 21.5462 - val_loss: 0.0276 - val_mse: 23.9968\n",
      "Epoch 583/1000\n",
      "0s - loss: 0.0247 - mse: 21.5328 - val_loss: 0.0275 - val_mse: 23.9881\n",
      "Epoch 584/1000\n",
      "0s - loss: 0.0247 - mse: 21.5195 - val_loss: 0.0275 - val_mse: 23.9809\n",
      "Epoch 585/1000\n",
      "0s - loss: 0.0247 - mse: 21.5061 - val_loss: 0.0275 - val_mse: 23.9743\n",
      "Epoch 586/1000\n",
      "0s - loss: 0.0247 - mse: 21.4928 - val_loss: 0.0275 - val_mse: 23.9665\n",
      "Epoch 587/1000\n",
      "\n",
      "Epoch 00586: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0247 - mse: 21.4794 - val_loss: 0.0275 - val_mse: 23.9588\n",
      "Epoch 588/1000\n",
      "0s - loss: 0.0246 - mse: 21.4661 - val_loss: 0.0275 - val_mse: 23.9542\n",
      "Epoch 589/1000\n",
      "0s - loss: 0.0246 - mse: 21.4568 - val_loss: 0.0275 - val_mse: 23.9499\n",
      "Epoch 590/1000\n",
      "0s - loss: 0.0246 - mse: 21.4475 - val_loss: 0.0275 - val_mse: 23.9446\n",
      "Epoch 591/1000\n",
      "0s - loss: 0.0246 - mse: 21.4382 - val_loss: 0.0275 - val_mse: 23.9386\n",
      "Epoch 592/1000\n",
      "0s - loss: 0.0246 - mse: 21.4289 - val_loss: 0.0275 - val_mse: 23.9330\n",
      "Epoch 593/1000\n",
      "0s - loss: 0.0246 - mse: 21.4197 - val_loss: 0.0275 - val_mse: 23.9280\n",
      "Epoch 594/1000\n",
      "0s - loss: 0.0246 - mse: 21.4104 - val_loss: 0.0275 - val_mse: 23.9236\n",
      "Epoch 595/1000\n",
      "0s - loss: 0.0246 - mse: 21.4012 - val_loss: 0.0275 - val_mse: 23.9185\n",
      "Epoch 596/1000\n",
      "0s - loss: 0.0246 - mse: 21.3920 - val_loss: 0.0275 - val_mse: 23.9129\n",
      "Epoch 597/1000\n",
      "0s - loss: 0.0246 - mse: 21.3827 - val_loss: 0.0275 - val_mse: 23.9077\n",
      "Epoch 598/1000\n",
      "0s - loss: 0.0245 - mse: 21.3736 - val_loss: 0.0274 - val_mse: 23.9030\n",
      "Epoch 599/1000\n",
      "0s - loss: 0.0245 - mse: 21.3644 - val_loss: 0.0274 - val_mse: 23.8984\n",
      "Epoch 600/1000\n",
      "0s - loss: 0.0245 - mse: 21.3552 - val_loss: 0.0274 - val_mse: 23.8934\n",
      "Epoch 601/1000\n",
      "\n",
      "Epoch 00600: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0245 - mse: 21.3460 - val_loss: 0.0274 - val_mse: 23.8880\n",
      "Epoch 602/1000\n",
      "0s - loss: 0.0245 - mse: 21.3368 - val_loss: 0.0274 - val_mse: 23.8843\n",
      "Epoch 603/1000\n",
      "0s - loss: 0.0245 - mse: 21.3304 - val_loss: 0.0274 - val_mse: 23.8807\n",
      "Epoch 604/1000\n",
      "0s - loss: 0.0245 - mse: 21.3239 - val_loss: 0.0274 - val_mse: 23.8774\n",
      "Epoch 605/1000\n",
      "0s - loss: 0.0245 - mse: 21.3175 - val_loss: 0.0274 - val_mse: 23.8742\n",
      "Epoch 606/1000\n",
      "0s - loss: 0.0245 - mse: 21.3111 - val_loss: 0.0274 - val_mse: 23.8707\n",
      "Epoch 607/1000\n",
      "0s - loss: 0.0245 - mse: 21.3047 - val_loss: 0.0274 - val_mse: 23.8668\n",
      "Epoch 608/1000\n",
      "0s - loss: 0.0245 - mse: 21.2982 - val_loss: 0.0274 - val_mse: 23.8629\n",
      "Epoch 609/1000\n",
      "0s - loss: 0.0244 - mse: 21.2918 - val_loss: 0.0274 - val_mse: 23.8594\n",
      "Epoch 610/1000\n",
      "0s - loss: 0.0244 - mse: 21.2855 - val_loss: 0.0274 - val_mse: 23.8561\n",
      "Epoch 611/1000\n",
      "0s - loss: 0.0244 - mse: 21.2791 - val_loss: 0.0274 - val_mse: 23.8528\n",
      "Epoch 612/1000\n",
      "0s - loss: 0.0244 - mse: 21.2727 - val_loss: 0.0274 - val_mse: 23.8494\n",
      "Epoch 613/1000\n",
      "0s - loss: 0.0244 - mse: 21.2663 - val_loss: 0.0274 - val_mse: 23.8461\n",
      "Epoch 614/1000\n",
      "0s - loss: 0.0244 - mse: 21.2599 - val_loss: 0.0274 - val_mse: 23.8428\n",
      "Epoch 615/1000\n",
      "\n",
      "Epoch 00614: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0244 - mse: 21.2536 - val_loss: 0.0274 - val_mse: 23.8394\n",
      "Epoch 616/1000\n",
      "0s - loss: 0.0244 - mse: 21.2472 - val_loss: 0.0274 - val_mse: 23.8369\n",
      "Epoch 617/1000\n",
      "0s - loss: 0.0244 - mse: 21.2428 - val_loss: 0.0274 - val_mse: 23.8342\n",
      "Epoch 618/1000\n",
      "0s - loss: 0.0244 - mse: 21.2383 - val_loss: 0.0274 - val_mse: 23.8317\n",
      "Epoch 619/1000\n",
      "0s - loss: 0.0244 - mse: 21.2339 - val_loss: 0.0274 - val_mse: 23.8293\n",
      "Epoch 620/1000\n",
      "0s - loss: 0.0244 - mse: 21.2295 - val_loss: 0.0274 - val_mse: 23.8271\n",
      "Epoch 621/1000\n",
      "0s - loss: 0.0244 - mse: 21.2250 - val_loss: 0.0274 - val_mse: 23.8249\n",
      "Epoch 622/1000\n",
      "0s - loss: 0.0244 - mse: 21.2207 - val_loss: 0.0274 - val_mse: 23.8224\n",
      "Epoch 623/1000\n",
      "0s - loss: 0.0244 - mse: 21.2162 - val_loss: 0.0274 - val_mse: 23.8198\n",
      "Epoch 624/1000\n",
      "0s - loss: 0.0244 - mse: 21.2118 - val_loss: 0.0273 - val_mse: 23.8172\n",
      "Epoch 625/1000\n",
      "0s - loss: 0.0244 - mse: 21.2075 - val_loss: 0.0273 - val_mse: 23.8149\n",
      "Epoch 626/1000\n",
      "0s - loss: 0.0243 - mse: 21.2031 - val_loss: 0.0273 - val_mse: 23.8128\n",
      "Epoch 627/1000\n",
      "0s - loss: 0.0243 - mse: 21.1987 - val_loss: 0.0273 - val_mse: 23.8107\n",
      "Epoch 628/1000\n",
      "0s - loss: 0.0243 - mse: 21.1943 - val_loss: 0.0273 - val_mse: 23.8081\n",
      "Epoch 629/1000\n",
      "\n",
      "Epoch 00628: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0243 - mse: 21.1900 - val_loss: 0.0273 - val_mse: 23.8054\n",
      "Epoch 630/1000\n",
      "0s - loss: 0.0243 - mse: 21.1857 - val_loss: 0.0273 - val_mse: 23.8035\n",
      "Epoch 631/1000\n",
      "0s - loss: 0.0243 - mse: 21.1826 - val_loss: 0.0273 - val_mse: 23.8016\n",
      "Epoch 632/1000\n",
      "0s - loss: 0.0243 - mse: 21.1796 - val_loss: 0.0273 - val_mse: 23.7999\n",
      "Epoch 633/1000\n",
      "0s - loss: 0.0243 - mse: 21.1766 - val_loss: 0.0273 - val_mse: 23.7982\n",
      "Epoch 634/1000\n",
      "0s - loss: 0.0243 - mse: 21.1736 - val_loss: 0.0273 - val_mse: 23.7965\n",
      "Epoch 635/1000\n",
      "0s - loss: 0.0243 - mse: 21.1705 - val_loss: 0.0273 - val_mse: 23.7948\n",
      "Epoch 636/1000\n",
      "0s - loss: 0.0243 - mse: 21.1675 - val_loss: 0.0273 - val_mse: 23.7928\n",
      "Epoch 637/1000\n",
      "0s - loss: 0.0243 - mse: 21.1645 - val_loss: 0.0273 - val_mse: 23.7908\n",
      "Epoch 638/1000\n",
      "0s - loss: 0.0243 - mse: 21.1615 - val_loss: 0.0273 - val_mse: 23.7887\n",
      "Epoch 639/1000\n",
      "0s - loss: 0.0243 - mse: 21.1585 - val_loss: 0.0273 - val_mse: 23.7867\n",
      "Epoch 640/1000\n",
      "0s - loss: 0.0243 - mse: 21.1555 - val_loss: 0.0273 - val_mse: 23.7847\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_60 (LSTM)               (None, None, 110)         49720     \n",
      "_________________________________________________________________\n",
      "time_distributed_57 (TimeDis (None, None, 1)           111       \n",
      "=================================================================\n",
      "Total params: 49,831\n",
      "Trainable params: 49,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 0.7365 - mse: 641.3654 - val_loss: 0.5433 - val_mse: 473.1311\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.5076 - mse: 442.0287 - val_loss: 0.3253 - val_mse: 283.3094\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.3100 - mse: 269.9839 - val_loss: 0.3372 - val_mse: 293.6357\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.3307 - mse: 287.9990 - val_loss: 0.2563 - val_mse: 223.2028\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2490 - mse: 216.8209 - val_loss: 0.1950 - val_mse: 169.8398\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.1880 - mse: 163.6879 - val_loss: 0.1962 - val_mse: 170.8252\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.1822 - mse: 158.6581 - val_loss: 0.1334 - val_mse: 116.1705\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1234 - mse: 107.5031 - val_loss: 0.1279 - val_mse: 111.4091\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1186 - mse: 103.2709 - val_loss: 0.1259 - val_mse: 109.6017\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1161 - mse: 101.0995 - val_loss: 0.1218 - val_mse: 106.0683\n",
      "Epoch 11/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.1114 - mse: 97.0205 - val_loss: 0.1206 - val_mse: 105.0086\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1086 - mse: 94.5747 - val_loss: 0.1240 - val_mse: 107.9594\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1084 - mse: 94.4353 - val_loss: 0.1293 - val_mse: 112.6279\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1089 - mse: 94.8657 - val_loss: 0.1305 - val_mse: 113.6810\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1072 - mse: 93.3968 - val_loss: 0.1231 - val_mse: 107.1900\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.1011 - mse: 88.0818 - val_loss: 0.1120 - val_mse: 97.5286\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0940 - mse: 81.8917 - val_loss: 0.1037 - val_mse: 90.3205\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0898 - mse: 78.2228 - val_loss: 0.0988 - val_mse: 86.0671\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0880 - mse: 76.6174 - val_loss: 0.0949 - val_mse: 82.6390\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0861 - mse: 74.9839 - val_loss: 0.0903 - val_mse: 78.6506\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0829 - mse: 72.2169 - val_loss: 0.0856 - val_mse: 74.5602\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0792 - mse: 68.9555 - val_loss: 0.0825 - val_mse: 71.8667\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0767 - mse: 66.7866 - val_loss: 0.0813 - val_mse: 70.8222\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0759 - mse: 66.0786 - val_loss: 0.0794 - val_mse: 69.1466\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0743 - mse: 64.6916 - val_loss: 0.0752 - val_mse: 65.4767\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0703 - mse: 61.2402 - val_loss: 0.0711 - val_mse: 61.9030\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0662 - mse: 57.6762 - val_loss: 0.0691 - val_mse: 60.1905\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0641 - mse: 55.7917 - val_loss: 0.0678 - val_mse: 59.0485\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0626 - mse: 54.4860 - val_loss: 0.0650 - val_mse: 56.6461\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0598 - mse: 52.0894 - val_loss: 0.0617 - val_mse: 53.7389\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0567 - mse: 49.3956 - val_loss: 0.0598 - val_mse: 52.0420\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0551 - mse: 48.0220 - val_loss: 0.0589 - val_mse: 51.3275\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0546 - mse: 47.5853 - val_loss: 0.0580 - val_mse: 50.4715\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0538 - mse: 46.8308 - val_loss: 0.0564 - val_mse: 49.1349\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0520 - mse: 45.3101 - val_loss: 0.0555 - val_mse: 48.3306\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0508 - mse: 44.2279 - val_loss: 0.0550 - val_mse: 47.9048\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0499 - mse: 43.4897 - val_loss: 0.0540 - val_mse: 46.9914\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0484 - mse: 42.1874 - val_loss: 0.0528 - val_mse: 45.9668\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0469 - mse: 40.8260 - val_loss: 0.0519 - val_mse: 45.1971\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0458 - mse: 39.9137 - val_loss: 0.0510 - val_mse: 44.4149\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0450 - mse: 39.1533 - val_loss: 0.0499 - val_mse: 43.4321\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0439 - mse: 38.2560 - val_loss: 0.0487 - val_mse: 42.4127\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0428 - mse: 37.2568 - val_loss: 0.0480 - val_mse: 41.8263\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0421 - mse: 36.6301 - val_loss: 0.0472 - val_mse: 41.0743\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0411 - mse: 35.8268 - val_loss: 0.0461 - val_mse: 40.1102\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0400 - mse: 34.8181 - val_loss: 0.0450 - val_mse: 39.2305\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0390 - mse: 33.9583 - val_loss: 0.0439 - val_mse: 38.2322\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0381 - mse: 33.1657 - val_loss: 0.0424 - val_mse: 36.9566\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0370 - mse: 32.2604 - val_loss: 0.0410 - val_mse: 35.7440\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0361 - mse: 31.4664 - val_loss: 0.0400 - val_mse: 34.8749\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0356 - mse: 31.0273 - val_loss: 0.0389 - val_mse: 33.8808\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0350 - mse: 30.4437 - val_loss: 0.0378 - val_mse: 32.8784\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0342 - mse: 29.7405 - val_loss: 0.0370 - val_mse: 32.1870\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0335 - mse: 29.1440 - val_loss: 0.0363 - val_mse: 31.6354\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0328 - mse: 28.5638 - val_loss: 0.0356 - val_mse: 31.0248\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0321 - mse: 27.9945 - val_loss: 0.0349 - val_mse: 30.3883\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0316 - mse: 27.5032 - val_loss: 0.0342 - val_mse: 29.7685\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0311 - mse: 27.0527 - val_loss: 0.0335 - val_mse: 29.1712\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0305 - mse: 26.5461 - val_loss: 0.0330 - val_mse: 28.7483\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0300 - mse: 26.1310 - val_loss: 0.0324 - val_mse: 28.1859\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0294 - mse: 25.5753 - val_loss: 0.0318 - val_mse: 27.6567\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0287 - mse: 25.0081 - val_loss: 0.0313 - val_mse: 27.2769\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0282 - mse: 24.5711 - val_loss: 0.0307 - val_mse: 26.7406\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0277 - mse: 24.1176 - val_loss: 0.0300 - val_mse: 26.1021\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0272 - mse: 23.6764 - val_loss: 0.0294 - val_mse: 25.6054\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0268 - mse: 23.3132 - val_loss: 0.0289 - val_mse: 25.1763\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0263 - mse: 22.9108 - val_loss: 0.0285 - val_mse: 24.8253\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0259 - mse: 22.5391 - val_loss: 0.0282 - val_mse: 24.5369\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0255 - mse: 22.1889 - val_loss: 0.0279 - val_mse: 24.2848\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0251 - mse: 21.8324 - val_loss: 0.0277 - val_mse: 24.0966\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0248 - mse: 21.5723 - val_loss: 0.0274 - val_mse: 23.8369\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0244 - mse: 21.2812 - val_loss: 0.0271 - val_mse: 23.5572\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0241 - mse: 20.9686 - val_loss: 0.0266 - val_mse: 23.1994\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0237 - mse: 20.6677 - val_loss: 0.0262 - val_mse: 22.7954\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0234 - mse: 20.3479 - val_loss: 0.0257 - val_mse: 22.4179\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0230 - mse: 20.0659 - val_loss: 0.0254 - val_mse: 22.0775\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0227 - mse: 19.7711 - val_loss: 0.0250 - val_mse: 21.7585\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0224 - mse: 19.4797 - val_loss: 0.0245 - val_mse: 21.3785\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0220 - mse: 19.1829 - val_loss: 0.0241 - val_mse: 20.9994\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0217 - mse: 18.8851 - val_loss: 0.0237 - val_mse: 20.6710\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0214 - mse: 18.6010 - val_loss: 0.0234 - val_mse: 20.4104\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0210 - mse: 18.3093 - val_loss: 0.0232 - val_mse: 20.1723\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0207 - mse: 18.0418 - val_loss: 0.0229 - val_mse: 19.8996\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0204 - mse: 17.7677 - val_loss: 0.0225 - val_mse: 19.6029\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0201 - mse: 17.4860 - val_loss: 0.0222 - val_mse: 19.3422\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0198 - mse: 17.2067 - val_loss: 0.0220 - val_mse: 19.1258\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0195 - mse: 16.9587 - val_loss: 0.0218 - val_mse: 18.9657\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0193 - mse: 16.8198 - val_loss: 0.0222 - val_mse: 19.3374\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0199 - mse: 17.3064 - val_loss: 0.0260 - val_mse: 22.6555\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0241 - mse: 20.9555 - val_loss: 0.0233 - val_mse: 20.2515\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0212 - mse: 18.4631 - val_loss: 0.0210 - val_mse: 18.2627\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0186 - mse: 16.2009 - val_loss: 0.0218 - val_mse: 19.0085\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0195 - mse: 16.9511 - val_loss: 0.0209 - val_mse: 18.1994\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0184 - mse: 16.0202 - val_loss: 0.0208 - val_mse: 18.1529\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0181 - mse: 15.7509 - val_loss: 0.0214 - val_mse: 18.6240\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0184 - mse: 15.9989 - val_loss: 0.0202 - val_mse: 17.6017\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0173 - mse: 15.0260 - val_loss: 0.0205 - val_mse: 17.8475\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0179 - mse: 15.6262 - val_loss: 0.0196 - val_mse: 17.0812\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0172 - mse: 14.9350 - val_loss: 0.0191 - val_mse: 16.6677\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0168 - mse: 14.6084 - val_loss: 0.0194 - val_mse: 16.9163\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0172 - mse: 14.9420 - val_loss: 0.0185 - val_mse: 16.0987\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0160 - mse: 13.9491 - val_loss: 0.0188 - val_mse: 16.3833\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0162 - mse: 14.1353 - val_loss: 0.0186 - val_mse: 16.2201\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0159 - mse: 13.8416 - val_loss: 0.0182 - val_mse: 15.8200\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0153 - mse: 13.2938 - val_loss: 0.0183 - val_mse: 15.8939\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0153 - mse: 13.3532 - val_loss: 0.0178 - val_mse: 15.4781\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0149 - mse: 12.9795 - val_loss: 0.0174 - val_mse: 15.1873\n",
      "Epoch 108/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0146 - mse: 12.7301 - val_loss: 0.0175 - val_mse: 15.2133\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0146 - mse: 12.7158 - val_loss: 0.0171 - val_mse: 14.9243\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0142 - mse: 12.3565 - val_loss: 0.0169 - val_mse: 14.7035\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0139 - mse: 12.1089 - val_loss: 0.0167 - val_mse: 14.5662\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0138 - mse: 12.0523 - val_loss: 0.0163 - val_mse: 14.2321\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0136 - mse: 11.8452 - val_loss: 0.0161 - val_mse: 13.9820\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0133 - mse: 11.5560 - val_loss: 0.0158 - val_mse: 13.7706\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0130 - mse: 11.3283 - val_loss: 0.0156 - val_mse: 13.6077\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0129 - mse: 11.2063 - val_loss: 0.0157 - val_mse: 13.6795\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0129 - mse: 11.1911 - val_loss: 0.0157 - val_mse: 13.6498\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0127 - mse: 11.0917 - val_loss: 0.0156 - val_mse: 13.6027\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0127 - mse: 11.0301 - val_loss: 0.0152 - val_mse: 13.2769\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0124 - mse: 10.8003 - val_loss: 0.0152 - val_mse: 13.1986\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0121 - mse: 10.5594 - val_loss: 0.0150 - val_mse: 13.0468\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0118 - mse: 10.2495 - val_loss: 0.0147 - val_mse: 12.8411\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0115 - mse: 10.0305 - val_loss: 0.0146 - val_mse: 12.6915\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0113 - mse: 9.8264 - val_loss: 0.0145 - val_mse: 12.6111\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0111 - mse: 9.6916 - val_loss: 0.0145 - val_mse: 12.5920\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0111 - mse: 9.6475 - val_loss: 0.0146 - val_mse: 12.7441\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0113 - mse: 9.8220 - val_loss: 0.0154 - val_mse: 13.3963\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0120 - mse: 10.4435 - val_loss: 0.0182 - val_mse: 15.8225\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0151 - mse: 13.1617 - val_loss: 0.0173 - val_mse: 15.0691\n",
      "Epoch 130/1000\n",
      "\n",
      "Epoch 00129: reducing learning rate to 0.00699999984354.\n",
      "4s - loss: 0.0139 - mse: 12.1237 - val_loss: 0.0148 - val_mse: 12.8634\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0115 - mse: 9.9824 - val_loss: 0.0140 - val_mse: 12.1651\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0107 - mse: 9.3494 - val_loss: 0.0143 - val_mse: 12.4962\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0110 - mse: 9.5951 - val_loss: 0.0140 - val_mse: 12.2213\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0102 - mse: 8.9160 - val_loss: 0.0144 - val_mse: 12.5268\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0106 - mse: 9.2471 - val_loss: 0.0136 - val_mse: 11.8413\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0100 - mse: 8.6966 - val_loss: 0.0138 - val_mse: 12.0307\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0103 - mse: 8.9633 - val_loss: 0.0135 - val_mse: 11.7276\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0097 - mse: 8.4136 - val_loss: 0.0138 - val_mse: 11.9848\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0100 - mse: 8.6920 - val_loss: 0.0130 - val_mse: 11.3191\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0094 - mse: 8.1902 - val_loss: 0.0131 - val_mse: 11.3899\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0097 - mse: 8.4064 - val_loss: 0.0128 - val_mse: 11.1504\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0092 - mse: 7.9772 - val_loss: 0.0132 - val_mse: 11.4601\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0094 - mse: 8.1667 - val_loss: 0.0126 - val_mse: 10.9900\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0090 - mse: 7.7954 - val_loss: 0.0126 - val_mse: 10.9458\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0091 - mse: 7.9604 - val_loss: 0.0123 - val_mse: 10.7397\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0088 - mse: 7.6266 - val_loss: 0.0127 - val_mse: 11.0168\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0089 - mse: 7.7408 - val_loss: 0.0123 - val_mse: 10.7041\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0085 - mse: 7.4341 - val_loss: 0.0122 - val_mse: 10.6509\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0087 - mse: 7.5558 - val_loss: 0.0120 - val_mse: 10.4137\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0083 - mse: 7.2610 - val_loss: 0.0121 - val_mse: 10.5730\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0084 - mse: 7.3489 - val_loss: 0.0119 - val_mse: 10.3215\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0082 - mse: 7.1218 - val_loss: 0.0118 - val_mse: 10.2560\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0082 - mse: 7.1410 - val_loss: 0.0117 - val_mse: 10.1862\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0080 - mse: 6.9881 - val_loss: 0.0117 - val_mse: 10.2125\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0080 - mse: 6.9281 - val_loss: 0.0116 - val_mse: 10.1280\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0079 - mse: 6.8734 - val_loss: 0.0115 - val_mse: 9.9844\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0077 - mse: 6.7393 - val_loss: 0.0115 - val_mse: 10.0384\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0077 - mse: 6.7407 - val_loss: 0.0114 - val_mse: 9.9313\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0076 - mse: 6.5891 - val_loss: 0.0114 - val_mse: 9.9022\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0076 - mse: 6.5848 - val_loss: 0.0113 - val_mse: 9.7975\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0074 - mse: 6.4820 - val_loss: 0.0112 - val_mse: 9.7274\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0074 - mse: 6.4084 - val_loss: 0.0112 - val_mse: 9.7271\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0073 - mse: 6.3886 - val_loss: 0.0111 - val_mse: 9.6474\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0072 - mse: 6.2669 - val_loss: 0.0110 - val_mse: 9.5518\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0072 - mse: 6.2301 - val_loss: 0.0109 - val_mse: 9.4591\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0071 - mse: 6.1712 - val_loss: 0.0109 - val_mse: 9.4519\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0070 - mse: 6.0772 - val_loss: 0.0108 - val_mse: 9.3979\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0069 - mse: 6.0468 - val_loss: 0.0106 - val_mse: 9.2473\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0069 - mse: 5.9811 - val_loss: 0.0106 - val_mse: 9.2279\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0068 - mse: 5.8993 - val_loss: 0.0106 - val_mse: 9.1980\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0067 - mse: 5.8667 - val_loss: 0.0104 - val_mse: 9.0472\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0067 - mse: 5.8044 - val_loss: 0.0104 - val_mse: 9.0594\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0066 - mse: 5.7301 - val_loss: 0.0104 - val_mse: 9.0761\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0065 - mse: 5.6928 - val_loss: 0.0102 - val_mse: 8.9236\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0065 - mse: 5.6395 - val_loss: 0.0102 - val_mse: 8.8974\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0064 - mse: 5.5704 - val_loss: 0.0102 - val_mse: 8.8839\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0063 - mse: 5.5238 - val_loss: 0.0101 - val_mse: 8.7632\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0063 - mse: 5.4801 - val_loss: 0.0100 - val_mse: 8.7485\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0062 - mse: 5.4229 - val_loss: 0.0100 - val_mse: 8.7278\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0062 - mse: 5.3674 - val_loss: 0.0099 - val_mse: 8.6566\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0061 - mse: 5.3222 - val_loss: 0.0099 - val_mse: 8.6627\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0061 - mse: 5.2792 - val_loss: 0.0099 - val_mse: 8.6197\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0060 - mse: 5.2256 - val_loss: 0.0098 - val_mse: 8.5595\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0059 - mse: 5.1738 - val_loss: 0.0098 - val_mse: 8.5182\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0059 - mse: 5.1306 - val_loss: 0.0097 - val_mse: 8.4782\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0058 - mse: 5.0873 - val_loss: 0.0097 - val_mse: 8.4447\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0058 - mse: 5.0380 - val_loss: 0.0096 - val_mse: 8.3766\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0057 - mse: 4.9894 - val_loss: 0.0096 - val_mse: 8.3692\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0057 - mse: 4.9442 - val_loss: 0.0096 - val_mse: 8.3492\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0056 - mse: 4.9003 - val_loss: 0.0095 - val_mse: 8.2858\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0056 - mse: 4.8574 - val_loss: 0.0095 - val_mse: 8.3012\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0055 - mse: 4.8126 - val_loss: 0.0095 - val_mse: 8.2499\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0055 - mse: 4.7693 - val_loss: 0.0095 - val_mse: 8.2384\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0054 - mse: 4.7257 - val_loss: 0.0094 - val_mse: 8.2175\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0054 - mse: 4.6846 - val_loss: 0.0094 - val_mse: 8.1571\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0053 - mse: 4.6434 - val_loss: 0.0093 - val_mse: 8.1397\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0053 - mse: 4.6038 - val_loss: 0.0093 - val_mse: 8.1147\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0052 - mse: 4.5643 - val_loss: 0.0093 - val_mse: 8.1122\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0052 - mse: 4.5254 - val_loss: 0.0093 - val_mse: 8.0903\n",
      "Epoch 200/1000\n",
      "\n",
      "Epoch 00199: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0052 - mse: 4.4870 - val_loss: 0.0093 - val_mse: 8.0899\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0051 - mse: 4.4484 - val_loss: 0.0093 - val_mse: 8.0564\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0051 - mse: 4.4210 - val_loss: 0.0092 - val_mse: 8.0351\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0050 - mse: 4.3948 - val_loss: 0.0092 - val_mse: 8.0440\n",
      "Epoch 204/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0050 - mse: 4.3697 - val_loss: 0.0092 - val_mse: 8.0341\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0050 - mse: 4.3430 - val_loss: 0.0092 - val_mse: 8.0140\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0050 - mse: 4.3177 - val_loss: 0.0092 - val_mse: 8.0377\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0049 - mse: 4.2923 - val_loss: 0.0092 - val_mse: 8.0199\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0049 - mse: 4.2663 - val_loss: 0.0092 - val_mse: 7.9955\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0049 - mse: 4.2416 - val_loss: 0.0092 - val_mse: 7.9940\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0048 - mse: 4.2164 - val_loss: 0.0092 - val_mse: 7.9685\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0048 - mse: 4.1912 - val_loss: 0.0091 - val_mse: 7.9531\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0048 - mse: 4.1672 - val_loss: 0.0092 - val_mse: 7.9687\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0048 - mse: 4.1431 - val_loss: 0.0091 - val_mse: 7.9550\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0047 - mse: 4.1189 - val_loss: 0.0091 - val_mse: 7.9476\n",
      "Epoch 215/1000\n",
      "\n",
      "Epoch 00214: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0047 - mse: 4.0958 - val_loss: 0.0091 - val_mse: 7.9458\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0047 - mse: 4.0729 - val_loss: 0.0091 - val_mse: 7.9240\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0047 - mse: 4.0561 - val_loss: 0.0091 - val_mse: 7.9106\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0046 - mse: 4.0407 - val_loss: 0.0091 - val_mse: 7.9186\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0046 - mse: 4.0245 - val_loss: 0.0091 - val_mse: 7.9190\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0046 - mse: 4.0092 - val_loss: 0.0091 - val_mse: 7.9056\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0046 - mse: 3.9934 - val_loss: 0.0091 - val_mse: 7.8973\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0046 - mse: 3.9783 - val_loss: 0.0091 - val_mse: 7.9039\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0046 - mse: 3.9629 - val_loss: 0.0091 - val_mse: 7.8987\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0045 - mse: 3.9473 - val_loss: 0.0091 - val_mse: 7.8880\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0045 - mse: 3.9324 - val_loss: 0.0091 - val_mse: 7.8869\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0045 - mse: 3.9172 - val_loss: 0.0091 - val_mse: 7.8972\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0045 - mse: 3.9022 - val_loss: 0.0091 - val_mse: 7.8902\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0045 - mse: 3.8872 - val_loss: 0.0090 - val_mse: 7.8752\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0044 - mse: 3.8725 - val_loss: 0.0090 - val_mse: 7.8779\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0044 - mse: 3.8572 - val_loss: 0.0091 - val_mse: 7.8883\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0044 - mse: 3.8428 - val_loss: 0.0090 - val_mse: 7.8800\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0044 - mse: 3.8278 - val_loss: 0.0090 - val_mse: 7.8731\n",
      "Epoch 233/1000\n",
      "\n",
      "Epoch 00232: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0044 - mse: 3.8132 - val_loss: 0.0091 - val_mse: 7.8874\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0044 - mse: 3.7987 - val_loss: 0.0091 - val_mse: 7.8827\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0044 - mse: 3.7882 - val_loss: 0.0090 - val_mse: 7.8712\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0043 - mse: 3.7781 - val_loss: 0.0090 - val_mse: 7.8739\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0043 - mse: 3.7679 - val_loss: 0.0091 - val_mse: 7.8835\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0043 - mse: 3.7578 - val_loss: 0.0091 - val_mse: 7.8839\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0043 - mse: 3.7477 - val_loss: 0.0090 - val_mse: 7.8807\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0043 - mse: 3.7377 - val_loss: 0.0090 - val_mse: 7.8757\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0043 - mse: 3.7277 - val_loss: 0.0090 - val_mse: 7.8727\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0043 - mse: 3.7177 - val_loss: 0.0090 - val_mse: 7.8759\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0043 - mse: 3.7077 - val_loss: 0.0090 - val_mse: 7.8799\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0042 - mse: 3.6978 - val_loss: 0.0090 - val_mse: 7.8791\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0042 - mse: 3.6879 - val_loss: 0.0090 - val_mse: 7.8772\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0042 - mse: 3.6778 - val_loss: 0.0090 - val_mse: 7.8783\n",
      "Epoch 247/1000\n",
      "\n",
      "Epoch 00246: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0042 - mse: 3.6680 - val_loss: 0.0090 - val_mse: 7.8717\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0042 - mse: 3.6581 - val_loss: 0.0090 - val_mse: 7.8665\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0042 - mse: 3.6512 - val_loss: 0.0090 - val_mse: 7.8680\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0042 - mse: 3.6443 - val_loss: 0.0090 - val_mse: 7.8746\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0042 - mse: 3.6374 - val_loss: 0.0090 - val_mse: 7.8776\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0042 - mse: 3.6306 - val_loss: 0.0090 - val_mse: 7.8752\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0042 - mse: 3.6238 - val_loss: 0.0090 - val_mse: 7.8740\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0042 - mse: 3.6170 - val_loss: 0.0090 - val_mse: 7.8721\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0041 - mse: 3.6102 - val_loss: 0.0090 - val_mse: 7.8712\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0041 - mse: 3.6034 - val_loss: 0.0090 - val_mse: 7.8750\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0041 - mse: 3.5966 - val_loss: 0.0090 - val_mse: 7.8764\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0041 - mse: 3.5899 - val_loss: 0.0090 - val_mse: 7.8711\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0041 - mse: 3.5830 - val_loss: 0.0090 - val_mse: 7.8668\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_61 (LSTM)               (None, None, 210)         178920    \n",
      "_________________________________________________________________\n",
      "time_distributed_58 (TimeDis (None, None, 1)           211       \n",
      "=================================================================\n",
      "Total params: 179,131\n",
      "Trainable params: 179,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 0.9337 - mse: 813.0980 - val_loss: 0.8469 - val_mse: 737.4858\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.8268 - mse: 720.0391 - val_loss: 0.6121 - val_mse: 533.0340\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.5848 - mse: 509.2330 - val_loss: 0.6801 - val_mse: 592.2913\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.6511 - mse: 567.0449 - val_loss: 0.5522 - val_mse: 480.8908\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.5313 - mse: 462.6993 - val_loss: 0.3531 - val_mse: 307.4778\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.3346 - mse: 291.3590 - val_loss: 0.3336 - val_mse: 290.5438\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.3164 - mse: 275.5431 - val_loss: 0.2267 - val_mse: 197.3759\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.2220 - mse: 193.2933 - val_loss: 0.1826 - val_mse: 159.0040\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1846 - mse: 160.7847 - val_loss: 0.1583 - val_mse: 137.8739\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1560 - mse: 135.8562 - val_loss: 0.1416 - val_mse: 123.2750\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1349 - mse: 117.4843 - val_loss: 0.1526 - val_mse: 132.8505\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1429 - mse: 124.4127 - val_loss: 0.1244 - val_mse: 108.3210\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1142 - mse: 99.4490 - val_loss: 0.1044 - val_mse: 90.9488\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.0884 - mse: 76.9802 - val_loss: 0.1286 - val_mse: 111.9713\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1046 - mse: 91.0547 - val_loss: 0.1293 - val_mse: 112.5734\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.1059 - mse: 92.1819 - val_loss: 0.1082 - val_mse: 94.2266\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0928 - mse: 80.8057 - val_loss: 0.0953 - val_mse: 83.0281\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0844 - mse: 73.4843 - val_loss: 0.0927 - val_mse: 80.7114\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0823 - mse: 71.6658 - val_loss: 0.0914 - val_mse: 79.5876\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0814 - mse: 70.8868 - val_loss: 0.0842 - val_mse: 73.3409\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0752 - mse: 65.5042 - val_loss: 0.0758 - val_mse: 65.9810\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0680 - mse: 59.2196 - val_loss: 0.0703 - val_mse: 61.2272\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0637 - mse: 55.4662 - val_loss: 0.0687 - val_mse: 59.8581\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0632 - mse: 55.0423 - val_loss: 0.0671 - val_mse: 58.3998\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0626 - mse: 54.5022 - val_loss: 0.0608 - val_mse: 52.9450\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0570 - mse: 49.6084 - val_loss: 0.0543 - val_mse: 47.3018\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0506 - mse: 44.0853 - val_loss: 0.0509 - val_mse: 44.2847\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0470 - mse: 40.9158 - val_loss: 0.0486 - val_mse: 42.2990\n",
      "Epoch 29/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0444 - mse: 38.6319 - val_loss: 0.0465 - val_mse: 40.4524\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0419 - mse: 36.4758 - val_loss: 0.0451 - val_mse: 39.3164\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0404 - mse: 35.2213 - val_loss: 0.0436 - val_mse: 37.9768\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0393 - mse: 34.1936 - val_loss: 0.0407 - val_mse: 35.4840\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0372 - mse: 32.3909 - val_loss: 0.0383 - val_mse: 33.3239\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0354 - mse: 30.8088 - val_loss: 0.0372 - val_mse: 32.4383\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0345 - mse: 30.0498 - val_loss: 0.0368 - val_mse: 32.0146\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0339 - mse: 29.4845 - val_loss: 0.0356 - val_mse: 30.9890\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0326 - mse: 28.3702 - val_loss: 0.0340 - val_mse: 29.5898\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0309 - mse: 26.9066 - val_loss: 0.0331 - val_mse: 28.8265\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0298 - mse: 25.9105 - val_loss: 0.0328 - val_mse: 28.5431\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0291 - mse: 25.3097 - val_loss: 0.0319 - val_mse: 27.7750\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0280 - mse: 24.4096 - val_loss: 0.0308 - val_mse: 26.8165\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0270 - mse: 23.5320 - val_loss: 0.0305 - val_mse: 26.5251\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0268 - mse: 23.3426 - val_loss: 0.0306 - val_mse: 26.6908\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0270 - mse: 23.4915 - val_loss: 0.0304 - val_mse: 26.4876\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0266 - mse: 23.1338 - val_loss: 0.0296 - val_mse: 25.7511\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0254 - mse: 22.1274 - val_loss: 0.0289 - val_mse: 25.1479\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0243 - mse: 21.1710 - val_loss: 0.0289 - val_mse: 25.1269\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0239 - mse: 20.8135 - val_loss: 0.0289 - val_mse: 25.1817\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0238 - mse: 20.7134 - val_loss: 0.0283 - val_mse: 24.6141\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0233 - mse: 20.2607 - val_loss: 0.0271 - val_mse: 23.5971\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0224 - mse: 19.5373 - val_loss: 0.0262 - val_mse: 22.7798\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0218 - mse: 19.0149 - val_loss: 0.0256 - val_mse: 22.3357\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0215 - mse: 18.7598 - val_loss: 0.0252 - val_mse: 21.9843\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0212 - mse: 18.4750 - val_loss: 0.0248 - val_mse: 21.6005\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0207 - mse: 18.0336 - val_loss: 0.0245 - val_mse: 21.3385\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0202 - mse: 17.5957 - val_loss: 0.0244 - val_mse: 21.2445\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0198 - mse: 17.2784 - val_loss: 0.0242 - val_mse: 21.1087\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0195 - mse: 17.0009 - val_loss: 0.0238 - val_mse: 20.7433\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0191 - mse: 16.6695 - val_loss: 0.0232 - val_mse: 20.2015\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0187 - mse: 16.3228 - val_loss: 0.0226 - val_mse: 19.6487\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0184 - mse: 16.0233 - val_loss: 0.0220 - val_mse: 19.1815\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0181 - mse: 15.7552 - val_loss: 0.0216 - val_mse: 18.8303\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0178 - mse: 15.4800 - val_loss: 0.0214 - val_mse: 18.6079\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0175 - mse: 15.1978 - val_loss: 0.0212 - val_mse: 18.4829\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0171 - mse: 14.9194 - val_loss: 0.0211 - val_mse: 18.3735\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0168 - mse: 14.6456 - val_loss: 0.0209 - val_mse: 18.2090\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0165 - mse: 14.3861 - val_loss: 0.0206 - val_mse: 17.9697\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0162 - mse: 14.1508 - val_loss: 0.0203 - val_mse: 17.6703\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0160 - mse: 13.9276 - val_loss: 0.0199 - val_mse: 17.3439\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0157 - mse: 13.7024 - val_loss: 0.0196 - val_mse: 17.0301\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0155 - mse: 13.4823 - val_loss: 0.0192 - val_mse: 16.7363\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0152 - mse: 13.2666 - val_loss: 0.0189 - val_mse: 16.4346\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0150 - mse: 13.0420 - val_loss: 0.0185 - val_mse: 16.1233\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0147 - mse: 12.8244 - val_loss: 0.0182 - val_mse: 15.8289\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0145 - mse: 12.6454 - val_loss: 0.0179 - val_mse: 15.5606\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0143 - mse: 12.4936 - val_loss: 0.0176 - val_mse: 15.2981\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0141 - mse: 12.3184 - val_loss: 0.0173 - val_mse: 15.0569\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0139 - mse: 12.1171 - val_loss: 0.0171 - val_mse: 14.8808\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0137 - mse: 11.9412 - val_loss: 0.0169 - val_mse: 14.7494\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0135 - mse: 11.7993 - val_loss: 0.0167 - val_mse: 14.5848\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0134 - mse: 11.6465 - val_loss: 0.0165 - val_mse: 14.3603\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0132 - mse: 11.4731 - val_loss: 0.0162 - val_mse: 14.1279\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0130 - mse: 11.3207 - val_loss: 0.0160 - val_mse: 13.9238\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0129 - mse: 11.1998 - val_loss: 0.0158 - val_mse: 13.7340\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0127 - mse: 11.0755 - val_loss: 0.0156 - val_mse: 13.5582\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0126 - mse: 10.9439 - val_loss: 0.0154 - val_mse: 13.4079\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0124 - mse: 10.8266 - val_loss: 0.0152 - val_mse: 13.2642\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0123 - mse: 10.7154 - val_loss: 0.0151 - val_mse: 13.1073\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0122 - mse: 10.5948 - val_loss: 0.0149 - val_mse: 12.9504\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0120 - mse: 10.4769 - val_loss: 0.0147 - val_mse: 12.8028\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0119 - mse: 10.3712 - val_loss: 0.0145 - val_mse: 12.6520\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0118 - mse: 10.2661 - val_loss: 0.0143 - val_mse: 12.4949\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0117 - mse: 10.1590 - val_loss: 0.0142 - val_mse: 12.3407\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0115 - mse: 10.0572 - val_loss: 0.0140 - val_mse: 12.1900\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0114 - mse: 9.9577 - val_loss: 0.0138 - val_mse: 12.0460\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0113 - mse: 9.8578 - val_loss: 0.0137 - val_mse: 11.9227\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0112 - mse: 9.7598 - val_loss: 0.0136 - val_mse: 11.8287\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0111 - mse: 9.6630 - val_loss: 0.0135 - val_mse: 11.7588\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0110 - mse: 9.5673 - val_loss: 0.0134 - val_mse: 11.6956\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0109 - mse: 9.4753 - val_loss: 0.0133 - val_mse: 11.6191\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0108 - mse: 9.3857 - val_loss: 0.0132 - val_mse: 11.5326\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0107 - mse: 9.2978 - val_loss: 0.0132 - val_mse: 11.4531\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0106 - mse: 9.2115 - val_loss: 0.0131 - val_mse: 11.3909\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0105 - mse: 9.1263 - val_loss: 0.0130 - val_mse: 11.3415\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0104 - mse: 9.0436 - val_loss: 0.0130 - val_mse: 11.2894\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0103 - mse: 8.9632 - val_loss: 0.0129 - val_mse: 11.2174\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0102 - mse: 8.8811 - val_loss: 0.0128 - val_mse: 11.1262\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0101 - mse: 8.7996 - val_loss: 0.0127 - val_mse: 11.0311\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0100 - mse: 8.7218 - val_loss: 0.0126 - val_mse: 10.9463\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0099 - mse: 8.6441 - val_loss: 0.0125 - val_mse: 10.8756\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0098 - mse: 8.5660 - val_loss: 0.0124 - val_mse: 10.8081\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0098 - mse: 8.4907 - val_loss: 0.0123 - val_mse: 10.7304\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0097 - mse: 8.4155 - val_loss: 0.0122 - val_mse: 10.6499\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0096 - mse: 8.3396 - val_loss: 0.0122 - val_mse: 10.5857\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0095 - mse: 8.2651 - val_loss: 0.0121 - val_mse: 10.5419\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0094 - mse: 8.1906 - val_loss: 0.0121 - val_mse: 10.5114\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0093 - mse: 8.1162 - val_loss: 0.0120 - val_mse: 10.4732\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0092 - mse: 8.0427 - val_loss: 0.0120 - val_mse: 10.4083\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0092 - mse: 7.9694 - val_loss: 0.0119 - val_mse: 10.3247\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0091 - mse: 7.8960 - val_loss: 0.0118 - val_mse: 10.2413\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0090 - mse: 7.8224 - val_loss: 0.0117 - val_mse: 10.1651\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0089 - mse: 7.7484 - val_loss: 0.0116 - val_mse: 10.0985\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0088 - mse: 7.6740 - val_loss: 0.0115 - val_mse: 10.0249\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0087 - mse: 7.5993 - val_loss: 0.0114 - val_mse: 9.9369\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0086 - mse: 7.5245 - val_loss: 0.0113 - val_mse: 9.8479\n",
      "Epoch 126/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0086 - mse: 7.4488 - val_loss: 0.0112 - val_mse: 9.7580\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0085 - mse: 7.3719 - val_loss: 0.0111 - val_mse: 9.6762\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0084 - mse: 7.2948 - val_loss: 0.0110 - val_mse: 9.5908\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0083 - mse: 7.2163 - val_loss: 0.0109 - val_mse: 9.5099\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0082 - mse: 7.1368 - val_loss: 0.0108 - val_mse: 9.4250\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0081 - mse: 7.0575 - val_loss: 0.0107 - val_mse: 9.3504\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0080 - mse: 6.9773 - val_loss: 0.0106 - val_mse: 9.2217\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0079 - mse: 6.9031 - val_loss: 0.0107 - val_mse: 9.2808\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0079 - mse: 6.8966 - val_loss: 0.0110 - val_mse: 9.5878\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0088 - mse: 7.6652 - val_loss: 0.0198 - val_mse: 17.2342\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0173 - mse: 15.0680 - val_loss: 0.0164 - val_mse: 14.3223\n",
      "Epoch 137/1000\n",
      "\n",
      "Epoch 00136: reducing learning rate to 0.00699999984354.\n",
      "5s - loss: 0.0162 - mse: 14.0918 - val_loss: 0.0113 - val_mse: 9.8571\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0095 - mse: 8.2812 - val_loss: 0.0131 - val_mse: 11.3907\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0110 - mse: 9.5420 - val_loss: 0.0115 - val_mse: 9.9924\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0092 - mse: 8.0420 - val_loss: 0.0113 - val_mse: 9.8569\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0095 - mse: 8.2654 - val_loss: 0.0118 - val_mse: 10.3032\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0103 - mse: 9.0045 - val_loss: 0.0108 - val_mse: 9.4052\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0091 - mse: 7.9010 - val_loss: 0.0109 - val_mse: 9.4824\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0088 - mse: 7.6281 - val_loss: 0.0120 - val_mse: 10.4141\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0096 - mse: 8.3434 - val_loss: 0.0118 - val_mse: 10.2380\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0093 - mse: 8.1232 - val_loss: 0.0108 - val_mse: 9.4295\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0086 - mse: 7.4464 - val_loss: 0.0108 - val_mse: 9.4479\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0088 - mse: 7.6805 - val_loss: 0.0111 - val_mse: 9.7035\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0092 - mse: 7.9838 - val_loss: 0.0108 - val_mse: 9.4370\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0086 - mse: 7.4963 - val_loss: 0.0108 - val_mse: 9.4351\n",
      "Epoch 151/1000\n",
      "\n",
      "Epoch 00150: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0083 - mse: 7.2242 - val_loss: 0.0112 - val_mse: 9.7732\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0086 - mse: 7.4669 - val_loss: 0.0111 - val_mse: 9.6462\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0085 - mse: 7.4187 - val_loss: 0.0105 - val_mse: 9.1713\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0082 - mse: 7.1213 - val_loss: 0.0101 - val_mse: 8.7954\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0080 - mse: 6.9537 - val_loss: 0.0100 - val_mse: 8.7127\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0081 - mse: 7.0295 - val_loss: 0.0100 - val_mse: 8.7227\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0081 - mse: 7.0797 - val_loss: 0.0099 - val_mse: 8.6537\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0080 - mse: 6.9384 - val_loss: 0.0099 - val_mse: 8.6027\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0078 - mse: 6.7855 - val_loss: 0.0099 - val_mse: 8.6446\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0078 - mse: 6.7834 - val_loss: 0.0099 - val_mse: 8.6476\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0078 - mse: 6.8240 - val_loss: 0.0098 - val_mse: 8.4976\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0078 - mse: 6.7621 - val_loss: 0.0095 - val_mse: 8.2874\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0076 - mse: 6.6416 - val_loss: 0.0094 - val_mse: 8.1823\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0076 - mse: 6.5827 - val_loss: 0.0094 - val_mse: 8.2114\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0076 - mse: 6.5815 - val_loss: 0.0095 - val_mse: 8.2741\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0075 - mse: 6.5413 - val_loss: 0.0095 - val_mse: 8.3034\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0074 - mse: 6.4530 - val_loss: 0.0095 - val_mse: 8.3057\n",
      "Epoch 168/1000\n",
      "\n",
      "Epoch 00167: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0073 - mse: 6.3952 - val_loss: 0.0095 - val_mse: 8.2706\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0073 - mse: 6.3827 - val_loss: 0.0094 - val_mse: 8.1992\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0073 - mse: 6.3622 - val_loss: 0.0093 - val_mse: 8.0964\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0073 - mse: 6.3171 - val_loss: 0.0092 - val_mse: 8.0112\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0072 - mse: 6.2716 - val_loss: 0.0092 - val_mse: 7.9762\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0072 - mse: 6.2440 - val_loss: 0.0092 - val_mse: 7.9831\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0071 - mse: 6.2262 - val_loss: 0.0092 - val_mse: 7.9958\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0071 - mse: 6.1988 - val_loss: 0.0092 - val_mse: 7.9833\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0071 - mse: 6.1582 - val_loss: 0.0091 - val_mse: 7.9376\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0070 - mse: 6.1188 - val_loss: 0.0090 - val_mse: 7.8658\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0070 - mse: 6.0909 - val_loss: 0.0089 - val_mse: 7.7776\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0070 - mse: 6.0685 - val_loss: 0.0088 - val_mse: 7.6858\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0069 - mse: 6.0402 - val_loss: 0.0087 - val_mse: 7.6090\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0069 - mse: 6.0045 - val_loss: 0.0087 - val_mse: 7.5647\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0069 - mse: 5.9708 - val_loss: 0.0087 - val_mse: 7.5555\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0068 - mse: 5.9448 - val_loss: 0.0087 - val_mse: 7.5611\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0068 - mse: 5.9215 - val_loss: 0.0087 - val_mse: 7.5533\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0068 - mse: 5.8926 - val_loss: 0.0086 - val_mse: 7.5186\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0067 - mse: 5.8592 - val_loss: 0.0086 - val_mse: 7.4648\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0067 - mse: 5.8292 - val_loss: 0.0085 - val_mse: 7.4064\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0067 - mse: 5.8048 - val_loss: 0.0084 - val_mse: 7.3532\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0066 - mse: 5.7795 - val_loss: 0.0084 - val_mse: 7.3130\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0066 - mse: 5.7493 - val_loss: 0.0084 - val_mse: 7.2920\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0066 - mse: 5.7182 - val_loss: 0.0084 - val_mse: 7.2878\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0065 - mse: 5.6915 - val_loss: 0.0084 - val_mse: 7.2847\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0065 - mse: 5.6670 - val_loss: 0.0083 - val_mse: 7.2648\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0065 - mse: 5.6396 - val_loss: 0.0083 - val_mse: 7.2235\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0064 - mse: 5.6097 - val_loss: 0.0082 - val_mse: 7.1715\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0064 - mse: 5.5819 - val_loss: 0.0082 - val_mse: 7.1222\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0064 - mse: 5.5568 - val_loss: 0.0081 - val_mse: 7.0827\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0064 - mse: 5.5306 - val_loss: 0.0081 - val_mse: 7.0556\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0063 - mse: 5.5017 - val_loss: 0.0081 - val_mse: 7.0397\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0063 - mse: 5.4735 - val_loss: 0.0081 - val_mse: 7.0263\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0063 - mse: 5.4473 - val_loss: 0.0080 - val_mse: 7.0022\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0062 - mse: 5.4208 - val_loss: 0.0080 - val_mse: 6.9607\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0062 - mse: 5.3920 - val_loss: 0.0079 - val_mse: 6.9081\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0062 - mse: 5.3632 - val_loss: 0.0079 - val_mse: 6.8566\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0061 - mse: 5.3361 - val_loss: 0.0078 - val_mse: 6.8142\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0061 - mse: 5.3093 - val_loss: 0.0078 - val_mse: 6.7831\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0061 - mse: 5.2807 - val_loss: 0.0078 - val_mse: 6.7616\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0060 - mse: 5.2516 - val_loss: 0.0077 - val_mse: 6.7431\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0060 - mse: 5.2237 - val_loss: 0.0077 - val_mse: 6.7185\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0060 - mse: 5.1960 - val_loss: 0.0077 - val_mse: 6.6831\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0059 - mse: 5.1672 - val_loss: 0.0076 - val_mse: 6.6407\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0059 - mse: 5.1377 - val_loss: 0.0076 - val_mse: 6.5992\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0059 - mse: 5.1090 - val_loss: 0.0075 - val_mse: 6.5642\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0058 - mse: 5.0801 - val_loss: 0.0075 - val_mse: 6.5365\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0058 - mse: 5.0502 - val_loss: 0.0075 - val_mse: 6.5132\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0058 - mse: 5.0200 - val_loss: 0.0075 - val_mse: 6.4894\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0057 - mse: 4.9900 - val_loss: 0.0074 - val_mse: 6.4596\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0057 - mse: 4.9598 - val_loss: 0.0074 - val_mse: 6.4223\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0057 - mse: 4.9288 - val_loss: 0.0073 - val_mse: 6.3811\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0056 - mse: 4.8974 - val_loss: 0.0073 - val_mse: 6.3417\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0056 - mse: 4.8660 - val_loss: 0.0072 - val_mse: 6.3071\n",
      "Epoch 222/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0056 - mse: 4.8343 - val_loss: 0.0072 - val_mse: 6.2765\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0055 - mse: 4.8019 - val_loss: 0.0072 - val_mse: 6.2475\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0055 - mse: 4.7692 - val_loss: 0.0071 - val_mse: 6.2164\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0054 - mse: 4.7365 - val_loss: 0.0071 - val_mse: 6.1812\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0054 - mse: 4.7031 - val_loss: 0.0071 - val_mse: 6.1429\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0054 - mse: 4.6690 - val_loss: 0.0070 - val_mse: 6.1047\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0053 - mse: 4.6345 - val_loss: 0.0070 - val_mse: 6.0690\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0053 - mse: 4.5996 - val_loss: 0.0069 - val_mse: 6.0358\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0052 - mse: 4.5641 - val_loss: 0.0069 - val_mse: 6.0039\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0052 - mse: 4.5281 - val_loss: 0.0069 - val_mse: 5.9718\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0052 - mse: 4.4916 - val_loss: 0.0068 - val_mse: 5.9378\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0051 - mse: 4.4543 - val_loss: 0.0068 - val_mse: 5.9018\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0051 - mse: 4.4162 - val_loss: 0.0067 - val_mse: 5.8649\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0050 - mse: 4.3778 - val_loss: 0.0067 - val_mse: 5.8278\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0050 - mse: 4.3388 - val_loss: 0.0067 - val_mse: 5.7916\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0049 - mse: 4.2990 - val_loss: 0.0066 - val_mse: 5.7562\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0049 - mse: 4.2584 - val_loss: 0.0066 - val_mse: 5.7215\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0048 - mse: 4.2171 - val_loss: 0.0065 - val_mse: 5.6865\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0048 - mse: 4.1751 - val_loss: 0.0065 - val_mse: 5.6496\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0047 - mse: 4.1323 - val_loss: 0.0064 - val_mse: 5.6105\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0047 - mse: 4.0889 - val_loss: 0.0064 - val_mse: 5.5708\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0046 - mse: 4.0450 - val_loss: 0.0064 - val_mse: 5.5321\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0046 - mse: 4.0002 - val_loss: 0.0063 - val_mse: 5.4954\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0045 - mse: 3.9547 - val_loss: 0.0063 - val_mse: 5.4596\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0045 - mse: 3.9085 - val_loss: 0.0062 - val_mse: 5.4227\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0044 - mse: 3.8617 - val_loss: 0.0062 - val_mse: 5.3844\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0044 - mse: 3.8145 - val_loss: 0.0061 - val_mse: 5.3453\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0043 - mse: 3.7669 - val_loss: 0.0061 - val_mse: 5.3077\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0043 - mse: 3.7190 - val_loss: 0.0061 - val_mse: 5.2715\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0042 - mse: 3.6707 - val_loss: 0.0060 - val_mse: 5.2347\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0042 - mse: 3.6212 - val_loss: 0.0060 - val_mse: 5.1963\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0041 - mse: 3.5703 - val_loss: 0.0059 - val_mse: 5.1571\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0040 - mse: 3.5186 - val_loss: 0.0059 - val_mse: 5.1188\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0040 - mse: 3.4661 - val_loss: 0.0058 - val_mse: 5.0809\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0039 - mse: 3.4133 - val_loss: 0.0058 - val_mse: 5.0422\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0039 - mse: 3.3602 - val_loss: 0.0057 - val_mse: 5.0018\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0038 - mse: 3.3065 - val_loss: 0.0057 - val_mse: 4.9620\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0037 - mse: 3.2527 - val_loss: 0.0057 - val_mse: 4.9236\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0037 - mse: 3.1987 - val_loss: 0.0056 - val_mse: 4.8848\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0036 - mse: 3.1442 - val_loss: 0.0056 - val_mse: 4.8446\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0035 - mse: 3.0896 - val_loss: 0.0055 - val_mse: 4.8054\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0035 - mse: 3.0349 - val_loss: 0.0055 - val_mse: 4.7683\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0034 - mse: 2.9800 - val_loss: 0.0054 - val_mse: 4.7306\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0034 - mse: 2.9243 - val_loss: 0.0054 - val_mse: 4.6908\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0033 - mse: 2.8694 - val_loss: 0.0053 - val_mse: 4.6500\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0032 - mse: 2.8146 - val_loss: 0.0053 - val_mse: 4.6094\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.0032 - mse: 2.7597 - val_loss: 0.0052 - val_mse: 4.5686\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0031 - mse: 2.7048 - val_loss: 0.0052 - val_mse: 4.5270\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0030 - mse: 2.6502 - val_loss: 0.0052 - val_mse: 4.4864\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0030 - mse: 2.5966 - val_loss: 0.0051 - val_mse: 4.4455\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0029 - mse: 2.5436 - val_loss: 0.0051 - val_mse: 4.4025\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0029 - mse: 2.4912 - val_loss: 0.0050 - val_mse: 4.3597\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0028 - mse: 2.4395 - val_loss: 0.0050 - val_mse: 4.3207\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0027 - mse: 2.3892 - val_loss: 0.0049 - val_mse: 4.2811\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0027 - mse: 2.3401 - val_loss: 0.0049 - val_mse: 4.2374\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0026 - mse: 2.2919 - val_loss: 0.0048 - val_mse: 4.1959\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0026 - mse: 2.2453 - val_loss: 0.0048 - val_mse: 4.1581\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0025 - mse: 2.1996 - val_loss: 0.0047 - val_mse: 4.1195\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0025 - mse: 2.1553 - val_loss: 0.0047 - val_mse: 4.0808\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0024 - mse: 2.1117 - val_loss: 0.0046 - val_mse: 4.0451\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0024 - mse: 2.0690 - val_loss: 0.0046 - val_mse: 4.0104\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0023 - mse: 2.0282 - val_loss: 0.0046 - val_mse: 3.9754\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0023 - mse: 1.9888 - val_loss: 0.0045 - val_mse: 3.9425\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0022 - mse: 1.9505 - val_loss: 0.0045 - val_mse: 3.9118\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0022 - mse: 1.9132 - val_loss: 0.0045 - val_mse: 3.8785\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0022 - mse: 1.8770 - val_loss: 0.0044 - val_mse: 3.8464\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0021 - mse: 1.8419 - val_loss: 0.0044 - val_mse: 3.8159\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0021 - mse: 1.8079 - val_loss: 0.0043 - val_mse: 3.7864\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0020 - mse: 1.7749 - val_loss: 0.0043 - val_mse: 3.7592\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0020 - mse: 1.7430 - val_loss: 0.0043 - val_mse: 3.7344\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0020 - mse: 1.7123 - val_loss: 0.0043 - val_mse: 3.7103\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0019 - mse: 1.6828 - val_loss: 0.0042 - val_mse: 3.6885\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0019 - mse: 1.6543 - val_loss: 0.0042 - val_mse: 3.6681\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0019 - mse: 1.6271 - val_loss: 0.0042 - val_mse: 3.6488\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0018 - mse: 1.6012 - val_loss: 0.0042 - val_mse: 3.6313\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0018 - mse: 1.5764 - val_loss: 0.0042 - val_mse: 3.6158\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0018 - mse: 1.5526 - val_loss: 0.0041 - val_mse: 3.6009\n",
      "Epoch 299/1000\n",
      "\n",
      "Epoch 00298: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0018 - mse: 1.5296 - val_loss: 0.0041 - val_mse: 3.5866\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0017 - mse: 1.5074 - val_loss: 0.0041 - val_mse: 3.5774\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0017 - mse: 1.4925 - val_loss: 0.0041 - val_mse: 3.5683\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0017 - mse: 1.4779 - val_loss: 0.0041 - val_mse: 3.5585\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0017 - mse: 1.4638 - val_loss: 0.0041 - val_mse: 3.5495\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0017 - mse: 1.4501 - val_loss: 0.0041 - val_mse: 3.5416\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0016 - mse: 1.4368 - val_loss: 0.0041 - val_mse: 3.5333\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0016 - mse: 1.4239 - val_loss: 0.0040 - val_mse: 3.5245\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0016 - mse: 1.4114 - val_loss: 0.0040 - val_mse: 3.5161\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0016 - mse: 1.3992 - val_loss: 0.0040 - val_mse: 3.5085\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0016 - mse: 1.3874 - val_loss: 0.0040 - val_mse: 3.5013\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0016 - mse: 1.3759 - val_loss: 0.0040 - val_mse: 3.4948\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0016 - mse: 1.3647 - val_loss: 0.0040 - val_mse: 3.4891\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0016 - mse: 1.3538 - val_loss: 0.0040 - val_mse: 3.4834\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0015 - mse: 1.3432 - val_loss: 0.0040 - val_mse: 3.4786\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0015 - mse: 1.3329 - val_loss: 0.0040 - val_mse: 3.4743\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0015 - mse: 1.3228 - val_loss: 0.0040 - val_mse: 3.4695\n",
      "Epoch 316/1000\n",
      "\n",
      "Epoch 00315: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0015 - mse: 1.3131 - val_loss: 0.0040 - val_mse: 3.4640\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0015 - mse: 1.3036 - val_loss: 0.0040 - val_mse: 3.4604\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0015 - mse: 1.2971 - val_loss: 0.0040 - val_mse: 3.4569\n",
      "Epoch 319/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0015 - mse: 1.2908 - val_loss: 0.0040 - val_mse: 3.4537\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0015 - mse: 1.2846 - val_loss: 0.0040 - val_mse: 3.4507\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0015 - mse: 1.2785 - val_loss: 0.0040 - val_mse: 3.4477\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0015 - mse: 1.2725 - val_loss: 0.0040 - val_mse: 3.4446\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0015 - mse: 1.2666 - val_loss: 0.0040 - val_mse: 3.4415\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0014 - mse: 1.2608 - val_loss: 0.0039 - val_mse: 3.4387\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0014 - mse: 1.2551 - val_loss: 0.0039 - val_mse: 3.4364\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0014 - mse: 1.2495 - val_loss: 0.0039 - val_mse: 3.4343\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0014 - mse: 1.2439 - val_loss: 0.0039 - val_mse: 3.4322\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0014 - mse: 1.2384 - val_loss: 0.0039 - val_mse: 3.4300\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0014 - mse: 1.2331 - val_loss: 0.0039 - val_mse: 3.4279\n",
      "Epoch 330/1000\n",
      "\n",
      "Epoch 00329: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0014 - mse: 1.2278 - val_loss: 0.0039 - val_mse: 3.4257\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0014 - mse: 1.2225 - val_loss: 0.0039 - val_mse: 3.4241\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0014 - mse: 1.2189 - val_loss: 0.0039 - val_mse: 3.4227\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0014 - mse: 1.2153 - val_loss: 0.0039 - val_mse: 3.4214\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0014 - mse: 1.2118 - val_loss: 0.0039 - val_mse: 3.4199\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0014 - mse: 1.2084 - val_loss: 0.0039 - val_mse: 3.4187\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0014 - mse: 1.2049 - val_loss: 0.0039 - val_mse: 3.4173\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0014 - mse: 1.2015 - val_loss: 0.0039 - val_mse: 3.4160\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.0014 - mse: 1.1981 - val_loss: 0.0039 - val_mse: 3.4146\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0014 - mse: 1.1947 - val_loss: 0.0039 - val_mse: 3.4131\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0014 - mse: 1.1914 - val_loss: 0.0039 - val_mse: 3.4116\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0014 - mse: 1.1880 - val_loss: 0.0039 - val_mse: 3.4103\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0014 - mse: 1.1847 - val_loss: 0.0039 - val_mse: 3.4095\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}, {'loss': 0.0013604337582364678, 'dropout': 0.0, 'val_mse': 3.409489631652832, 'width': 210, 'mse': 1.1847187280654907, 'val_loss': 0.003915177658200264}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_62 (LSTM)               (None, None, 310)         388120    \n",
      "_________________________________________________________________\n",
      "time_distributed_59 (TimeDis (None, None, 1)           311       \n",
      "=================================================================\n",
      "Total params: 388,431\n",
      "Trainable params: 388,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 1.0260 - mse: 893.4940 - val_loss: 0.7142 - val_mse: 621.9908\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.6783 - mse: 590.7087 - val_loss: 0.6959 - val_mse: 606.0475\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.6680 - mse: 581.7374 - val_loss: 0.5772 - val_mse: 502.6556\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.5572 - mse: 485.2192 - val_loss: 0.3772 - val_mse: 328.5114\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.3442 - mse: 299.7696 - val_loss: 0.3075 - val_mse: 267.7470\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.3163 - mse: 275.4109 - val_loss: 0.3049 - val_mse: 265.4926\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.2990 - mse: 260.4160 - val_loss: 0.2543 - val_mse: 221.4547\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.2466 - mse: 214.7823 - val_loss: 0.2375 - val_mse: 206.8568\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.2268 - mse: 197.5102 - val_loss: 0.1712 - val_mse: 149.0692\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1660 - mse: 144.5650 - val_loss: 0.2468 - val_mse: 214.9185\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.2348 - mse: 204.4991 - val_loss: 0.1438 - val_mse: 125.2340\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1311 - mse: 114.1534 - val_loss: 0.1830 - val_mse: 159.3876\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1684 - mse: 146.6598 - val_loss: 0.1580 - val_mse: 137.5986\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1444 - mse: 125.7562 - val_loss: 0.1258 - val_mse: 109.5865\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1137 - mse: 99.0236 - val_loss: 0.1121 - val_mse: 97.6092\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.0994 - mse: 86.5487 - val_loss: 0.1118 - val_mse: 97.3708\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0966 - mse: 84.1660 - val_loss: 0.1063 - val_mse: 92.5805\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0899 - mse: 78.3062 - val_loss: 0.0927 - val_mse: 80.7402\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0772 - mse: 67.2719 - val_loss: 0.0829 - val_mse: 72.1493\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0684 - mse: 59.6067 - val_loss: 0.0790 - val_mse: 68.8222\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0658 - mse: 57.2752 - val_loss: 0.0758 - val_mse: 66.0353\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0647 - mse: 56.3005 - val_loss: 0.0709 - val_mse: 61.7702\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0622 - mse: 54.1942 - val_loss: 0.0653 - val_mse: 56.8666\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0587 - mse: 51.0935 - val_loss: 0.0600 - val_mse: 52.2078\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0548 - mse: 47.6912 - val_loss: 0.0552 - val_mse: 48.0820\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0508 - mse: 44.2714 - val_loss: 0.0514 - val_mse: 44.8039\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0472 - mse: 41.1130 - val_loss: 0.0492 - val_mse: 42.8488\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0445 - mse: 38.7906 - val_loss: 0.0480 - val_mse: 41.7825\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0428 - mse: 37.2476 - val_loss: 0.0464 - val_mse: 40.4146\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0410 - mse: 35.6710 - val_loss: 0.0443 - val_mse: 38.5381\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0389 - mse: 33.8556 - val_loss: 0.0420 - val_mse: 36.5418\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0367 - mse: 31.9607 - val_loss: 0.0395 - val_mse: 34.3623\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0343 - mse: 29.8375 - val_loss: 0.0368 - val_mse: 32.0491\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0318 - mse: 27.6660 - val_loss: 0.0343 - val_mse: 29.9063\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0298 - mse: 25.9412 - val_loss: 0.0320 - val_mse: 27.8347\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0283 - mse: 24.6092 - val_loss: 0.0296 - val_mse: 25.7762\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0270 - mse: 23.4734 - val_loss: 0.0274 - val_mse: 23.8799\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0257 - mse: 22.4070 - val_loss: 0.0256 - val_mse: 22.3278\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0246 - mse: 21.3980 - val_loss: 0.0247 - val_mse: 21.5020\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0239 - mse: 20.7977 - val_loss: 0.0243 - val_mse: 21.1784\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0234 - mse: 20.4095 - val_loss: 0.0240 - val_mse: 20.8902\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0228 - mse: 19.8339 - val_loss: 0.0236 - val_mse: 20.5670\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0219 - mse: 19.1146 - val_loss: 0.0231 - val_mse: 20.0901\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0209 - mse: 18.2231 - val_loss: 0.0224 - val_mse: 19.5419\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0199 - mse: 17.3004 - val_loss: 0.0220 - val_mse: 19.1323\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0191 - mse: 16.6081 - val_loss: 0.0216 - val_mse: 18.7975\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0185 - mse: 16.0881 - val_loss: 0.0213 - val_mse: 18.5712\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0181 - mse: 15.7251 - val_loss: 0.0212 - val_mse: 18.4472\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0178 - mse: 15.4618 - val_loss: 0.0210 - val_mse: 18.2600\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0174 - mse: 15.1176 - val_loss: 0.0208 - val_mse: 18.0886\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0170 - mse: 14.7776 - val_loss: 0.0206 - val_mse: 17.9307\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0166 - mse: 14.4609 - val_loss: 0.0203 - val_mse: 17.6898\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0162 - mse: 14.1212 - val_loss: 0.0199 - val_mse: 17.3042\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0157 - mse: 13.7102 - val_loss: 0.0194 - val_mse: 16.8574\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0153 - mse: 13.2886 - val_loss: 0.0189 - val_mse: 16.4511\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0148 - mse: 12.9223 - val_loss: 0.0185 - val_mse: 16.1517\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0145 - mse: 12.6532 - val_loss: 0.0183 - val_mse: 15.9359\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0143 - mse: 12.4453 - val_loss: 0.0181 - val_mse: 15.7533\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0140 - mse: 12.2341 - val_loss: 0.0179 - val_mse: 15.6150\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0138 - mse: 12.0438 - val_loss: 0.0178 - val_mse: 15.4585\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0136 - mse: 11.8619 - val_loss: 0.0175 - val_mse: 15.2397\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0134 - mse: 11.6976 - val_loss: 0.0171 - val_mse: 14.9178\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0132 - mse: 11.5211 - val_loss: 0.0167 - val_mse: 14.5167\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0130 - mse: 11.3116 - val_loss: 0.0162 - val_mse: 14.1210\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0127 - mse: 11.0878 - val_loss: 0.0159 - val_mse: 13.8238\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0125 - mse: 10.9184 - val_loss: 0.0156 - val_mse: 13.6259\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0124 - mse: 10.7945 - val_loss: 0.0155 - val_mse: 13.5010\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0123 - mse: 10.6878 - val_loss: 0.0154 - val_mse: 13.3844\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0121 - mse: 10.5424 - val_loss: 0.0152 - val_mse: 13.2685\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0119 - mse: 10.3629 - val_loss: 0.0151 - val_mse: 13.1834\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0117 - mse: 10.1887 - val_loss: 0.0151 - val_mse: 13.1457\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0115 - mse: 10.0458 - val_loss: 0.0151 - val_mse: 13.1187\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0114 - mse: 9.9191 - val_loss: 0.0150 - val_mse: 13.0564\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0113 - mse: 9.7989 - val_loss: 0.0149 - val_mse: 12.9351\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0111 - mse: 9.6794 - val_loss: 0.0147 - val_mse: 12.7680\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0110 - mse: 9.5620 - val_loss: 0.0144 - val_mse: 12.5750\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0108 - mse: 9.4481 - val_loss: 0.0142 - val_mse: 12.3762\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0107 - mse: 9.3283 - val_loss: 0.0140 - val_mse: 12.2151\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0106 - mse: 9.2118 - val_loss: 0.0139 - val_mse: 12.0989\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0104 - mse: 9.0972 - val_loss: 0.0138 - val_mse: 12.0102\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0103 - mse: 8.9844 - val_loss: 0.0137 - val_mse: 11.9305\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0102 - mse: 8.8689 - val_loss: 0.0136 - val_mse: 11.8508\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0100 - mse: 8.7502 - val_loss: 0.0135 - val_mse: 11.7726\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0099 - mse: 8.6433 - val_loss: 0.0134 - val_mse: 11.6959\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0098 - mse: 8.5493 - val_loss: 0.0134 - val_mse: 11.6316\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0097 - mse: 8.4569 - val_loss: 0.0133 - val_mse: 11.5692\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0096 - mse: 8.3559 - val_loss: 0.0132 - val_mse: 11.4993\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0095 - mse: 8.2547 - val_loss: 0.0131 - val_mse: 11.4230\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0094 - mse: 8.1616 - val_loss: 0.0130 - val_mse: 11.3184\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0093 - mse: 8.0742 - val_loss: 0.0129 - val_mse: 11.1934\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0092 - mse: 7.9866 - val_loss: 0.0127 - val_mse: 11.0772\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0091 - mse: 7.8979 - val_loss: 0.0126 - val_mse: 10.9758\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0090 - mse: 7.8111 - val_loss: 0.0125 - val_mse: 10.9158\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0089 - mse: 7.7251 - val_loss: 0.0125 - val_mse: 10.8777\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0088 - mse: 7.6383 - val_loss: 0.0125 - val_mse: 10.8554\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0087 - mse: 7.5518 - val_loss: 0.0124 - val_mse: 10.8345\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0086 - mse: 7.4691 - val_loss: 0.0124 - val_mse: 10.7876\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0085 - mse: 7.3884 - val_loss: 0.0123 - val_mse: 10.7348\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0084 - mse: 7.3094 - val_loss: 0.0122 - val_mse: 10.6437\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0083 - mse: 7.2356 - val_loss: 0.0122 - val_mse: 10.6315\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0083 - mse: 7.1925 - val_loss: 0.0122 - val_mse: 10.6396\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0084 - mse: 7.2851 - val_loss: 0.0134 - val_mse: 11.6567\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0095 - mse: 8.2360 - val_loss: 0.0160 - val_mse: 13.9187\n",
      "Epoch 104/1000\n",
      "\n",
      "Epoch 00103: reducing learning rate to 0.00699999984354.\n",
      "4s - loss: 0.0128 - mse: 11.1487 - val_loss: 0.0290 - val_mse: 25.2714\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0265 - mse: 23.1163 - val_loss: 0.0156 - val_mse: 13.5469\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0121 - mse: 10.5673 - val_loss: 0.0172 - val_mse: 14.9656\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0144 - mse: 12.5004 - val_loss: 0.0191 - val_mse: 16.6003\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0167 - mse: 14.5560 - val_loss: 0.0136 - val_mse: 11.8656\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0103 - mse: 8.9641 - val_loss: 0.0170 - val_mse: 14.8060\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0130 - mse: 11.3017 - val_loss: 0.0177 - val_mse: 15.4418\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0134 - mse: 11.6994 - val_loss: 0.0148 - val_mse: 12.8450\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0107 - mse: 9.2866 - val_loss: 0.0153 - val_mse: 13.3344\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0117 - mse: 10.1641 - val_loss: 0.0156 - val_mse: 13.6098\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0124 - mse: 10.7774 - val_loss: 0.0133 - val_mse: 11.5740\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0101 - mse: 8.8251 - val_loss: 0.0133 - val_mse: 11.5649\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0099 - mse: 8.6451 - val_loss: 0.0149 - val_mse: 13.0123\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0113 - mse: 9.8618 - val_loss: 0.0146 - val_mse: 12.7510\n",
      "Epoch 118/1000\n",
      "\n",
      "Epoch 00117: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0110 - mse: 9.5455 - val_loss: 0.0131 - val_mse: 11.4137\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0096 - mse: 8.3443 - val_loss: 0.0128 - val_mse: 11.1371\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0094 - mse: 8.1876 - val_loss: 0.0130 - val_mse: 11.2926\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0097 - mse: 8.4208 - val_loss: 0.0129 - val_mse: 11.2235\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0096 - mse: 8.3614 - val_loss: 0.0126 - val_mse: 10.9336\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0092 - mse: 8.0287 - val_loss: 0.0125 - val_mse: 10.8813\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0091 - mse: 7.9115 - val_loss: 0.0127 - val_mse: 11.0790\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0092 - mse: 8.0424 - val_loss: 0.0128 - val_mse: 11.1387\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0092 - mse: 8.0431 - val_loss: 0.0125 - val_mse: 10.9135\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0089 - mse: 7.7778 - val_loss: 0.0122 - val_mse: 10.6634\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0086 - mse: 7.5242 - val_loss: 0.0122 - val_mse: 10.6141\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0086 - mse: 7.5240 - val_loss: 0.0122 - val_mse: 10.6205\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0088 - mse: 7.6282 - val_loss: 0.0120 - val_mse: 10.4491\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0087 - mse: 7.5690 - val_loss: 0.0117 - val_mse: 10.1771\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0085 - mse: 7.3795 - val_loss: 0.0115 - val_mse: 10.0500\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0084 - mse: 7.2785 - val_loss: 0.0116 - val_mse: 10.1009\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0084 - mse: 7.3005 - val_loss: 0.0117 - val_mse: 10.1630\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0084 - mse: 7.2892 - val_loss: 0.0117 - val_mse: 10.1515\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0082 - mse: 7.1733 - val_loss: 0.0117 - val_mse: 10.1579\n",
      "Epoch 137/1000\n",
      "\n",
      "Epoch 00136: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0081 - mse: 7.0639 - val_loss: 0.0118 - val_mse: 10.2496\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0081 - mse: 7.0583 - val_loss: 0.0118 - val_mse: 10.2945\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0081 - mse: 7.0702 - val_loss: 0.0118 - val_mse: 10.2518\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0081 - mse: 7.0332 - val_loss: 0.0116 - val_mse: 10.1396\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0080 - mse: 6.9573 - val_loss: 0.0115 - val_mse: 10.0259\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0079 - mse: 6.8939 - val_loss: 0.0114 - val_mse: 9.9562\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0079 - mse: 6.8732 - val_loss: 0.0114 - val_mse: 9.9216\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0079 - mse: 6.8764 - val_loss: 0.0114 - val_mse: 9.8883\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0079 - mse: 6.8656 - val_loss: 0.0113 - val_mse: 9.8428\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0078 - mse: 6.8262 - val_loss: 0.0113 - val_mse: 9.8033\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0078 - mse: 6.7773 - val_loss: 0.0112 - val_mse: 9.7922\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0077 - mse: 6.7447 - val_loss: 0.0113 - val_mse: 9.8072\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0077 - mse: 6.7320 - val_loss: 0.0113 - val_mse: 9.8221\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0077 - mse: 6.7199 - val_loss: 0.0113 - val_mse: 9.8164\n",
      "Epoch 151/1000\n",
      "\n",
      "Epoch 00150: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0077 - mse: 6.6925 - val_loss: 0.0112 - val_mse: 9.7935\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0076 - mse: 6.6548 - val_loss: 0.0112 - val_mse: 9.7766\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0076 - mse: 6.6315 - val_loss: 0.0112 - val_mse: 9.7649\n",
      "Epoch 154/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0076 - mse: 6.6160 - val_loss: 0.0112 - val_mse: 9.7585\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0076 - mse: 6.6069 - val_loss: 0.0112 - val_mse: 9.7525\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0076 - mse: 6.5982 - val_loss: 0.0112 - val_mse: 9.7418\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0076 - mse: 6.5848 - val_loss: 0.0112 - val_mse: 9.7260\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0075 - mse: 6.5662 - val_loss: 0.0111 - val_mse: 9.7080\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0075 - mse: 6.5462 - val_loss: 0.0111 - val_mse: 9.6913\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0075 - mse: 6.5292 - val_loss: 0.0111 - val_mse: 9.6766\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0075 - mse: 6.5165 - val_loss: 0.0111 - val_mse: 9.6616\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0075 - mse: 6.5062 - val_loss: 0.0111 - val_mse: 9.6433\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0075 - mse: 6.4951 - val_loss: 0.0110 - val_mse: 9.6211\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0074 - mse: 6.4814 - val_loss: 0.0110 - val_mse: 9.5971\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0074 - mse: 6.4659 - val_loss: 0.0110 - val_mse: 9.5751\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0074 - mse: 6.4507 - val_loss: 0.0110 - val_mse: 9.5582\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0074 - mse: 6.4375 - val_loss: 0.0110 - val_mse: 9.5472\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0074 - mse: 6.4258 - val_loss: 0.0110 - val_mse: 9.5408\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0074 - mse: 6.4143 - val_loss: 0.0110 - val_mse: 9.5374\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0074 - mse: 6.4018 - val_loss: 0.0110 - val_mse: 9.5361\n",
      "Epoch 171/1000\n",
      "\n",
      "Epoch 00170: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0073 - mse: 6.3882 - val_loss: 0.0110 - val_mse: 9.5361\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0073 - mse: 6.3748 - val_loss: 0.0110 - val_mse: 9.5363\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0073 - mse: 6.3659 - val_loss: 0.0110 - val_mse: 9.5360\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0073 - mse: 6.3576 - val_loss: 0.0109 - val_mse: 9.5343\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0073 - mse: 6.3496 - val_loss: 0.0109 - val_mse: 9.5305\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0073 - mse: 6.3416 - val_loss: 0.0109 - val_mse: 9.5242\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0073 - mse: 6.3334 - val_loss: 0.0109 - val_mse: 9.5155\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0073 - mse: 6.3250 - val_loss: 0.0109 - val_mse: 9.5047\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0073 - mse: 6.3163 - val_loss: 0.0109 - val_mse: 9.4927\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0072 - mse: 6.3078 - val_loss: 0.0109 - val_mse: 9.4803\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0072 - mse: 6.2996 - val_loss: 0.0109 - val_mse: 9.4682\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0072 - mse: 6.2917 - val_loss: 0.0109 - val_mse: 9.4570\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0072 - mse: 6.2840 - val_loss: 0.0108 - val_mse: 9.4468\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0072 - mse: 6.2763 - val_loss: 0.0108 - val_mse: 9.4378\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0072 - mse: 6.2684 - val_loss: 0.0108 - val_mse: 9.4301\n",
      "Epoch 186/1000\n",
      "\n",
      "Epoch 00185: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0072 - mse: 6.2605 - val_loss: 0.0108 - val_mse: 9.4236\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0072 - mse: 6.2525 - val_loss: 0.0108 - val_mse: 9.4198\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0072 - mse: 6.2469 - val_loss: 0.0108 - val_mse: 9.4166\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0072 - mse: 6.2414 - val_loss: 0.0108 - val_mse: 9.4136\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0072 - mse: 6.2360 - val_loss: 0.0108 - val_mse: 9.4109\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0072 - mse: 6.2307 - val_loss: 0.0108 - val_mse: 9.4080\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0071 - mse: 6.2254 - val_loss: 0.0108 - val_mse: 9.4050\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0071 - mse: 6.2201 - val_loss: 0.0108 - val_mse: 9.4016\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0071 - mse: 6.2148 - val_loss: 0.0108 - val_mse: 9.3979\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0071 - mse: 6.2095 - val_loss: 0.0108 - val_mse: 9.3938\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0071 - mse: 6.2042 - val_loss: 0.0108 - val_mse: 9.3892\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0071 - mse: 6.1989 - val_loss: 0.0108 - val_mse: 9.3844\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0071 - mse: 6.1936 - val_loss: 0.0108 - val_mse: 9.3793\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0071 - mse: 6.1884 - val_loss: 0.0108 - val_mse: 9.3741\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0071 - mse: 6.1832 - val_loss: 0.0108 - val_mse: 9.3688\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0071 - mse: 6.1780 - val_loss: 0.0108 - val_mse: 9.3635\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0071 - mse: 6.1729 - val_loss: 0.0107 - val_mse: 9.3582\n",
      "Epoch 203/1000\n",
      "\n",
      "Epoch 00202: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0071 - mse: 6.1677 - val_loss: 0.0107 - val_mse: 9.3531\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0071 - mse: 6.1626 - val_loss: 0.0107 - val_mse: 9.3495\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0071 - mse: 6.1590 - val_loss: 0.0107 - val_mse: 9.3461\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0071 - mse: 6.1554 - val_loss: 0.0107 - val_mse: 9.3427\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0071 - mse: 6.1519 - val_loss: 0.0107 - val_mse: 9.3395\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0071 - mse: 6.1483 - val_loss: 0.0107 - val_mse: 9.3363\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0071 - mse: 6.1448 - val_loss: 0.0107 - val_mse: 9.3332\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0071 - mse: 6.1413 - val_loss: 0.0107 - val_mse: 9.3301\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0070 - mse: 6.1378 - val_loss: 0.0107 - val_mse: 9.3270\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0070 - mse: 6.1343 - val_loss: 0.0107 - val_mse: 9.3239\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0070 - mse: 6.1308 - val_loss: 0.0107 - val_mse: 9.3208\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0070 - mse: 6.1273 - val_loss: 0.0107 - val_mse: 9.3178\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0070 - mse: 6.1238 - val_loss: 0.0107 - val_mse: 9.3148\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0070 - mse: 6.1203 - val_loss: 0.0107 - val_mse: 9.3118\n",
      "Epoch 217/1000\n",
      "\n",
      "Epoch 00216: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0070 - mse: 6.1169 - val_loss: 0.0107 - val_mse: 9.3088\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0070 - mse: 6.1134 - val_loss: 0.0107 - val_mse: 9.3067\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0070 - mse: 6.1110 - val_loss: 0.0107 - val_mse: 9.3045\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0070 - mse: 6.1086 - val_loss: 0.0107 - val_mse: 9.3022\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0070 - mse: 6.1062 - val_loss: 0.0107 - val_mse: 9.2999\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0070 - mse: 6.1038 - val_loss: 0.0107 - val_mse: 9.2976\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0070 - mse: 6.1014 - val_loss: 0.0107 - val_mse: 9.2952\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0070 - mse: 6.0990 - val_loss: 0.0107 - val_mse: 9.2929\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0070 - mse: 6.0966 - val_loss: 0.0107 - val_mse: 9.2906\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0070 - mse: 6.0942 - val_loss: 0.0107 - val_mse: 9.2883\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0070 - mse: 6.0918 - val_loss: 0.0107 - val_mse: 9.2859\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0070 - mse: 6.0894 - val_loss: 0.0107 - val_mse: 9.2835\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0070 - mse: 6.0870 - val_loss: 0.0107 - val_mse: 9.2811\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0070 - mse: 6.0847 - val_loss: 0.0107 - val_mse: 9.2788\n",
      "Epoch 231/1000\n",
      "\n",
      "Epoch 00230: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0070 - mse: 6.0823 - val_loss: 0.0107 - val_mse: 9.2764\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0070 - mse: 6.0799 - val_loss: 0.0107 - val_mse: 9.2747\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0070 - mse: 6.0783 - val_loss: 0.0106 - val_mse: 9.2731\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0070 - mse: 6.0766 - val_loss: 0.0106 - val_mse: 9.2714\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0070 - mse: 6.0750 - val_loss: 0.0106 - val_mse: 9.2697\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0070 - mse: 6.0733 - val_loss: 0.0106 - val_mse: 9.2681\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0070 - mse: 6.0717 - val_loss: 0.0106 - val_mse: 9.2664\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0070 - mse: 6.0700 - val_loss: 0.0106 - val_mse: 9.2648\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0070 - mse: 6.0684 - val_loss: 0.0106 - val_mse: 9.2632\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0070 - mse: 6.0668 - val_loss: 0.0106 - val_mse: 9.2616\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0070 - mse: 6.0651 - val_loss: 0.0106 - val_mse: 9.2599\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0070 - mse: 6.0635 - val_loss: 0.0106 - val_mse: 9.2583\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0070 - mse: 6.0619 - val_loss: 0.0106 - val_mse: 9.2567\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0070 - mse: 6.0602 - val_loss: 0.0106 - val_mse: 9.2551\n",
      "Epoch 245/1000\n",
      "\n",
      "Epoch 00244: reducing learning rate to 0.000282475253334.\n",
      "0s - loss: 0.0070 - mse: 6.0586 - val_loss: 0.0106 - val_mse: 9.2535\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0070 - mse: 6.0570 - val_loss: 0.0106 - val_mse: 9.2524\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0070 - mse: 6.0559 - val_loss: 0.0106 - val_mse: 9.2512\n",
      "Epoch 248/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0070 - mse: 6.0547 - val_loss: 0.0106 - val_mse: 9.2501\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0070 - mse: 6.0536 - val_loss: 0.0106 - val_mse: 9.2489\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0070 - mse: 6.0525 - val_loss: 0.0106 - val_mse: 9.2478\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0069 - mse: 6.0513 - val_loss: 0.0106 - val_mse: 9.2466\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0069 - mse: 6.0502 - val_loss: 0.0106 - val_mse: 9.2454\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0069 - mse: 6.0491 - val_loss: 0.0106 - val_mse: 9.2442\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0069 - mse: 6.0479 - val_loss: 0.0106 - val_mse: 9.2430\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0069 - mse: 6.0468 - val_loss: 0.0106 - val_mse: 9.2418\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0069 - mse: 6.0457 - val_loss: 0.0106 - val_mse: 9.2406\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}, {'loss': 0.0013604337582364678, 'dropout': 0.0, 'val_mse': 3.409489631652832, 'width': 210, 'mse': 1.1847187280654907, 'val_loss': 0.003915177658200264}, {'loss': 0.0069423709064722061, 'dropout': 0.0, 'val_mse': 9.2405805587768555, 'width': 310, 'mse': 6.0456881523132324, 'val_loss': 0.010611124336719513}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_63 (LSTM)               (None, None, 410)         677320    \n",
      "_________________________________________________________________\n",
      "time_distributed_60 (TimeDis (None, None, 1)           411       \n",
      "=================================================================\n",
      "Total params: 677,731\n",
      "Trainable params: 677,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 1.1006 - mse: 958.4122 - val_loss: 2.3375 - val_mse: 2035.6252\n",
      "Epoch 2/1000\n",
      "0s - loss: 2.4132 - mse: 2101.4675 - val_loss: 0.8951 - val_mse: 779.5086\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.8461 - mse: 736.8291 - val_loss: 4.0227 - val_mse: 3503.1533\n",
      "Epoch 4/1000\n",
      "0s - loss: 4.0086 - mse: 3490.8865 - val_loss: 36.8235 - val_mse: 32067.3438\n",
      "Epoch 5/1000\n",
      "0s - loss: 36.5487 - mse: 31828.0000 - val_loss: 16.9065 - val_mse: 14722.8115\n",
      "Epoch 6/1000\n",
      "0s - loss: 16.8125 - mse: 14641.0000 - val_loss: 3.6342 - val_mse: 3164.7866\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.00699999984354.\n",
      "4s - loss: 3.6041 - mse: 3138.6069 - val_loss: 1.3419 - val_mse: 1168.6183\n",
      "Epoch 8/1000\n",
      "0s - loss: 1.3018 - mse: 1133.6401 - val_loss: 2.2022 - val_mse: 1917.7869\n",
      "Epoch 9/1000\n",
      "0s - loss: 2.1627 - mse: 1883.3838 - val_loss: 3.0337 - val_mse: 2641.8533\n",
      "Epoch 10/1000\n",
      "0s - loss: 2.9914 - mse: 2605.0496 - val_loss: 3.0475 - val_mse: 2653.9065\n",
      "Epoch 11/1000\n",
      "0s - loss: 3.0072 - mse: 2618.7988 - val_loss: 2.4855 - val_mse: 2164.4668\n",
      "Epoch 12/1000\n",
      "0s - loss: 2.4471 - mse: 2131.0718 - val_loss: 1.7448 - val_mse: 1519.4222\n",
      "Epoch 13/1000\n",
      "0s - loss: 1.7084 - mse: 1487.7103 - val_loss: 1.1877 - val_mse: 1034.3383\n",
      "Epoch 14/1000\n",
      "0s - loss: 1.1519 - mse: 1003.1459 - val_loss: 0.9550 - val_mse: 831.6653\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.9189 - mse: 800.2324 - val_loss: 1.0340 - val_mse: 900.4411\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.9963 - mse: 867.6051 - val_loss: 1.1701 - val_mse: 1018.9464\n",
      "Epoch 17/1000\n",
      "0s - loss: 1.1310 - mse: 984.9281 - val_loss: 1.2208 - val_mse: 1063.1036\n",
      "Epoch 18/1000\n",
      "0s - loss: 1.1811 - mse: 1028.5531 - val_loss: 1.1691 - val_mse: 1018.1335\n",
      "Epoch 19/1000\n",
      "0s - loss: 1.1295 - mse: 983.6422 - val_loss: 1.0646 - val_mse: 927.1246\n",
      "Epoch 20/1000\n",
      "0s - loss: 1.0253 - mse: 892.9117 - val_loss: 0.9582 - val_mse: 834.4328\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.9191 - mse: 800.3691 - val_loss: 0.8977 - val_mse: 781.7936\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.8593 - mse: 748.3133 - val_loss: 0.8923 - val_mse: 777.0812\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.8541 - mse: 743.7648 - val_loss: 0.9052 - val_mse: 788.2512\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.8666 - mse: 754.6543 - val_loss: 0.9187 - val_mse: 800.0731\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.8814 - mse: 767.5675 - val_loss: 0.9224 - val_mse: 803.2388\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.8847 - mse: 770.4394 - val_loss: 0.9138 - val_mse: 795.8054\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.8761 - mse: 762.9713 - val_loss: 0.8974 - val_mse: 781.5170\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.8602 - mse: 749.1096 - val_loss: 0.8741 - val_mse: 761.2253\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.8368 - mse: 728.7306 - val_loss: 0.8560 - val_mse: 745.4622\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.8186 - mse: 712.8535 - val_loss: 0.8509 - val_mse: 741.0015\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.8130 - mse: 708.0076 - val_loss: 0.8515 - val_mse: 741.4895\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.8138 - mse: 708.7024 - val_loss: 0.8470 - val_mse: 737.5798\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.8092 - mse: 704.7196 - val_loss: 0.8373 - val_mse: 729.1829\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.7995 - mse: 696.2567 - val_loss: 0.8253 - val_mse: 718.6602\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.7878 - mse: 686.0153 - val_loss: 0.8097 - val_mse: 705.1255\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.7725 - mse: 672.7638 - val_loss: 0.7942 - val_mse: 691.6073\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.7573 - mse: 659.4495 - val_loss: 0.7832 - val_mse: 682.0322\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.7463 - mse: 649.9368 - val_loss: 0.7773 - val_mse: 676.8800\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.7405 - mse: 644.8490 - val_loss: 0.7665 - val_mse: 667.4739\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.7299 - mse: 635.6636 - val_loss: 0.7442 - val_mse: 648.0978\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.7079 - mse: 616.5047 - val_loss: 0.7175 - val_mse: 624.8403\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.6811 - mse: 593.1638 - val_loss: 0.7047 - val_mse: 613.6486\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.6677 - mse: 581.4871 - val_loss: 0.6886 - val_mse: 599.6600\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.6512 - mse: 567.0562 - val_loss: 0.6681 - val_mse: 581.8247\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.6307 - mse: 549.1946 - val_loss: 0.6520 - val_mse: 567.8051\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.6139 - mse: 534.5876 - val_loss: 0.6290 - val_mse: 547.7825\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.5912 - mse: 514.8054 - val_loss: 0.6044 - val_mse: 526.3656\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.5678 - mse: 494.4255 - val_loss: 0.5927 - val_mse: 516.1829\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.5566 - mse: 484.6828 - val_loss: 0.5665 - val_mse: 493.3150\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.5302 - mse: 461.6938 - val_loss: 0.5546 - val_mse: 482.9259\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.5178 - mse: 450.9179 - val_loss: 0.5341 - val_mse: 465.1078\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.4977 - mse: 433.4292 - val_loss: 0.5175 - val_mse: 450.6293\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.4823 - mse: 419.9793 - val_loss: 0.5022 - val_mse: 437.3299\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.4675 - mse: 407.0965 - val_loss: 0.4870 - val_mse: 424.0975\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.4515 - mse: 393.1881 - val_loss: 0.4741 - val_mse: 412.8338\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.4389 - mse: 382.2155 - val_loss: 0.4561 - val_mse: 397.2016\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.4228 - mse: 368.1754 - val_loss: 0.4442 - val_mse: 386.8565\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.4119 - mse: 358.6962 - val_loss: 0.4300 - val_mse: 374.4708\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.3974 - mse: 346.0506 - val_loss: 0.4193 - val_mse: 365.0998\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.3871 - mse: 337.1168 - val_loss: 0.4033 - val_mse: 351.2529\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.3728 - mse: 324.6758 - val_loss: 0.3940 - val_mse: 343.0948\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.3646 - mse: 317.5195 - val_loss: 0.3808 - val_mse: 331.6472\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.3514 - mse: 305.9784 - val_loss: 0.3735 - val_mse: 325.2219\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.3440 - mse: 299.6058 - val_loss: 0.3606 - val_mse: 314.0447\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.3324 - mse: 289.4518 - val_loss: 0.3527 - val_mse: 307.1436\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.3258 - mse: 283.6809 - val_loss: 0.3427 - val_mse: 298.4740\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.3160 - mse: 275.1782 - val_loss: 0.3367 - val_mse: 293.1721\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.3098 - mse: 269.7773 - val_loss: 0.3277 - val_mse: 285.4164\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.3017 - mse: 262.7401 - val_loss: 0.3203 - val_mse: 278.8973\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.2955 - mse: 257.3346 - val_loss: 0.3129 - val_mse: 272.4930\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.2886 - mse: 251.3228 - val_loss: 0.3068 - val_mse: 267.1794\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.2823 - mse: 245.8759 - val_loss: 0.3004 - val_mse: 261.6218\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.2764 - mse: 240.7211 - val_loss: 0.2932 - val_mse: 255.3226\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.2703 - mse: 235.3714 - val_loss: 0.2874 - val_mse: 250.3213\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.2652 - mse: 230.9266 - val_loss: 0.2815 - val_mse: 245.1229\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.2591 - mse: 225.6732 - val_loss: 0.2766 - val_mse: 240.8372\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.2544 - mse: 221.5563 - val_loss: 0.2698 - val_mse: 234.9689\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.2485 - mse: 216.4300 - val_loss: 0.2644 - val_mse: 230.2860\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.2439 - mse: 212.4123 - val_loss: 0.2586 - val_mse: 225.2319\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.2382 - mse: 207.4505 - val_loss: 0.2539 - val_mse: 221.0763\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.2335 - mse: 203.3528 - val_loss: 0.2477 - val_mse: 215.6824\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.2280 - mse: 198.5198 - val_loss: 0.2421 - val_mse: 210.8149\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.2231 - mse: 194.3210 - val_loss: 0.2364 - val_mse: 205.8597\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.2177 - mse: 189.5389 - val_loss: 0.2314 - val_mse: 201.5273\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.2127 - mse: 185.2216 - val_loss: 0.2254 - val_mse: 196.3206\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.2072 - mse: 180.4391 - val_loss: 0.2198 - val_mse: 191.3919\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.2022 - mse: 176.0497 - val_loss: 0.2142 - val_mse: 186.5120\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.1967 - mse: 171.2797 - val_loss: 0.2091 - val_mse: 182.0999\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.1917 - mse: 166.9059 - val_loss: 0.2032 - val_mse: 176.9374\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.1862 - mse: 162.1788 - val_loss: 0.1977 - val_mse: 172.1427\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.1812 - mse: 157.7954 - val_loss: 0.1923 - val_mse: 167.4554\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.1758 - mse: 153.0898 - val_loss: 0.1868 - val_mse: 162.7028\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.1705 - mse: 148.4806 - val_loss: 0.1810 - val_mse: 157.6251\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.1652 - mse: 143.8332 - val_loss: 0.1755 - val_mse: 152.8575\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.1595 - mse: 138.8856 - val_loss: 0.1700 - val_mse: 148.0784\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.1539 - mse: 134.0498 - val_loss: 0.1650 - val_mse: 143.6826\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.1493 - mse: 130.0057 - val_loss: 0.1676 - val_mse: 145.9535\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.1507 - mse: 131.2679 - val_loss: 0.1981 - val_mse: 172.5337\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.1868 - mse: 162.6363 - val_loss: 0.2570 - val_mse: 223.7771\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.2390 - mse: 208.0996 - val_loss: 0.1609 - val_mse: 140.1088\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.1445 - mse: 125.8042 - val_loss: 0.2559 - val_mse: 222.8724\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.2470 - mse: 215.1170 - val_loss: 0.1533 - val_mse: 133.5398\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.1381 - mse: 120.2908 - val_loss: 0.2197 - val_mse: 191.3598\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.2032 - mse: 176.9440 - val_loss: 0.1700 - val_mse: 148.0649\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.1545 - mse: 134.5783 - val_loss: 0.1659 - val_mse: 144.5102\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.1541 - mse: 134.1719 - val_loss: 0.1860 - val_mse: 161.9886\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.1753 - mse: 152.6478 - val_loss: 0.1441 - val_mse: 125.5191\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.1307 - mse: 113.7958 - val_loss: 0.1736 - val_mse: 151.1822\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.1577 - mse: 137.3672 - val_loss: 0.1630 - val_mse: 141.9503\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.1472 - mse: 128.1824 - val_loss: 0.1368 - val_mse: 119.1123\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.1234 - mse: 107.4867 - val_loss: 0.1580 - val_mse: 137.5871\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.1468 - mse: 127.8512 - val_loss: 0.1374 - val_mse: 119.6718\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.1244 - mse: 108.3681 - val_loss: 0.1398 - val_mse: 121.7315\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.1232 - mse: 107.2633 - val_loss: 0.1524 - val_mse: 132.6823\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.1344 - mse: 117.0469 - val_loss: 0.1314 - val_mse: 114.4588\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.1151 - mse: 100.1929 - val_loss: 0.1364 - val_mse: 118.7691\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.1231 - mse: 107.2116 - val_loss: 0.1361 - val_mse: 118.5579\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.1230 - mse: 107.0989 - val_loss: 0.1273 - val_mse: 110.8145\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.1111 - mse: 96.7145 - val_loss: 0.1391 - val_mse: 121.1702\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.1211 - mse: 105.4697 - val_loss: 0.1300 - val_mse: 113.1983\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.1129 - mse: 98.3437 - val_loss: 0.1236 - val_mse: 107.6360\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.1095 - mse: 95.3588 - val_loss: 0.1278 - val_mse: 111.2735\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.1152 - mse: 100.2851 - val_loss: 0.1199 - val_mse: 104.4302\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.1056 - mse: 91.9731 - val_loss: 0.1247 - val_mse: 108.6123\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.1084 - mse: 94.4236 - val_loss: 0.1245 - val_mse: 108.4060\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.1082 - mse: 94.2296 - val_loss: 0.1165 - val_mse: 101.4705\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.1021 - mse: 88.9226 - val_loss: 0.1187 - val_mse: 103.3527\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.1060 - mse: 92.3049 - val_loss: 0.1156 - val_mse: 100.7054\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.1024 - mse: 89.2048 - val_loss: 0.1152 - val_mse: 100.3582\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.1003 - mse: 87.3874 - val_loss: 0.1180 - val_mse: 102.7997\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.1025 - mse: 89.2828 - val_loss: 0.1127 - val_mse: 98.1423\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0982 - mse: 85.5226 - val_loss: 0.1117 - val_mse: 97.2801\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0989 - mse: 86.0865 - val_loss: 0.1110 - val_mse: 96.6816\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0984 - mse: 85.7010 - val_loss: 0.1093 - val_mse: 95.1529\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0954 - mse: 83.0963 - val_loss: 0.1116 - val_mse: 97.1922\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0970 - mse: 84.4378 - val_loss: 0.1084 - val_mse: 94.4373\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0945 - mse: 82.2548 - val_loss: 0.1067 - val_mse: 92.9349\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0941 - mse: 81.9695 - val_loss: 0.1065 - val_mse: 92.7298\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0942 - mse: 82.0410 - val_loss: 0.1055 - val_mse: 91.8735\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0921 - mse: 80.2418 - val_loss: 0.1072 - val_mse: 93.3946\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0931 - mse: 81.0810 - val_loss: 0.1051 - val_mse: 91.5283\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0915 - mse: 79.6422 - val_loss: 0.1039 - val_mse: 90.5109\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0914 - mse: 79.5696 - val_loss: 0.1036 - val_mse: 90.2079\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0911 - mse: 79.3171 - val_loss: 0.1034 - val_mse: 90.0710\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0899 - mse: 78.2958 - val_loss: 0.1043 - val_mse: 90.8606\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0903 - mse: 78.6662 - val_loss: 0.1023 - val_mse: 89.0796\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0889 - mse: 77.4329 - val_loss: 0.1018 - val_mse: 88.6114\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0892 - mse: 77.6482 - val_loss: 0.1011 - val_mse: 88.0009\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0882 - mse: 76.7703 - val_loss: 0.1016 - val_mse: 88.4378\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0879 - mse: 76.5696 - val_loss: 0.1010 - val_mse: 87.9347\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0874 - mse: 76.1353 - val_loss: 0.0996 - val_mse: 86.7013\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0868 - mse: 75.5657 - val_loss: 0.0991 - val_mse: 86.3436\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0866 - mse: 75.4313 - val_loss: 0.0989 - val_mse: 86.1123\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0858 - mse: 74.7301 - val_loss: 0.0990 - val_mse: 86.2507\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0858 - mse: 74.7079 - val_loss: 0.0978 - val_mse: 85.1289\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0850 - mse: 74.0182 - val_loss: 0.0973 - val_mse: 84.7242\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0850 - mse: 74.0094 - val_loss: 0.0969 - val_mse: 84.3429\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0843 - mse: 73.4051 - val_loss: 0.0970 - val_mse: 84.5093\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0842 - mse: 73.3317 - val_loss: 0.0962 - val_mse: 83.7665\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0836 - mse: 72.8277 - val_loss: 0.0956 - val_mse: 83.2192\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0835 - mse: 72.6757 - val_loss: 0.0951 - val_mse: 82.8598\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0830 - mse: 72.2444 - val_loss: 0.0952 - val_mse: 82.8981\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0827 - mse: 72.0360 - val_loss: 0.0946 - val_mse: 82.3953\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0823 - mse: 71.6612 - val_loss: 0.0939 - val_mse: 81.8075\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0820 - mse: 71.4194 - val_loss: 0.0936 - val_mse: 81.4763\n",
      "Epoch 166/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0816 - mse: 71.0700 - val_loss: 0.0935 - val_mse: 81.4425\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0813 - mse: 70.8178 - val_loss: 0.0930 - val_mse: 81.0132\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0809 - mse: 70.4865 - val_loss: 0.0924 - val_mse: 80.4694\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0807 - mse: 70.2379 - val_loss: 0.0920 - val_mse: 80.1596\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0803 - mse: 69.9145 - val_loss: 0.0920 - val_mse: 80.0931\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0800 - mse: 69.6770 - val_loss: 0.0915 - val_mse: 79.6534\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0796 - mse: 69.3583 - val_loss: 0.0909 - val_mse: 79.1688\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0794 - mse: 69.1303 - val_loss: 0.0906 - val_mse: 78.8871\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0790 - mse: 68.8137 - val_loss: 0.0905 - val_mse: 78.7805\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0788 - mse: 68.5915 - val_loss: 0.0899 - val_mse: 78.3134\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0784 - mse: 68.2771 - val_loss: 0.0894 - val_mse: 77.8917\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0782 - mse: 68.0580 - val_loss: 0.0892 - val_mse: 77.6581\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0778 - mse: 67.7488 - val_loss: 0.0890 - val_mse: 77.4849\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0775 - mse: 67.5273 - val_loss: 0.0884 - val_mse: 77.0051\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0772 - mse: 67.2371 - val_loss: 0.0880 - val_mse: 76.6635\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0769 - mse: 67.0066 - val_loss: 0.0878 - val_mse: 76.4891\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0766 - mse: 66.7402 - val_loss: 0.0875 - val_mse: 76.2182\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0764 - mse: 66.5004 - val_loss: 0.0870 - val_mse: 75.7910\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0761 - mse: 66.2548 - val_loss: 0.0867 - val_mse: 75.5233\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0758 - mse: 66.0031 - val_loss: 0.0865 - val_mse: 75.3677\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0755 - mse: 65.7802 - val_loss: 0.0861 - val_mse: 74.9978\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0752 - mse: 65.5283 - val_loss: 0.0858 - val_mse: 74.6747\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0750 - mse: 65.3132 - val_loss: 0.0855 - val_mse: 74.4883\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0747 - mse: 65.0750 - val_loss: 0.0853 - val_mse: 74.2446\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0745 - mse: 64.8583 - val_loss: 0.0849 - val_mse: 73.8940\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0742 - mse: 64.6424 - val_loss: 0.0846 - val_mse: 73.6683\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0740 - mse: 64.4183 - val_loss: 0.0844 - val_mse: 73.5039\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0737 - mse: 64.2131 - val_loss: 0.0840 - val_mse: 73.1874\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0735 - mse: 63.9902 - val_loss: 0.0837 - val_mse: 72.9164\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0732 - mse: 63.7857 - val_loss: 0.0835 - val_mse: 72.7549\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0730 - mse: 63.5734 - val_loss: 0.0833 - val_mse: 72.5086\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0728 - mse: 63.3657 - val_loss: 0.0829 - val_mse: 72.2227\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0725 - mse: 63.1686 - val_loss: 0.0827 - val_mse: 72.0437\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0723 - mse: 62.9610 - val_loss: 0.0825 - val_mse: 71.8507\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0721 - mse: 62.7655 - val_loss: 0.0822 - val_mse: 71.5684\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0718 - mse: 62.5683 - val_loss: 0.0820 - val_mse: 71.3763\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0716 - mse: 62.3712 - val_loss: 0.0818 - val_mse: 71.2054\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0714 - mse: 62.1846 - val_loss: 0.0815 - val_mse: 70.9404\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0712 - mse: 61.9944 - val_loss: 0.0812 - val_mse: 70.7325\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0710 - mse: 61.8085 - val_loss: 0.0810 - val_mse: 70.5712\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0708 - mse: 61.6278 - val_loss: 0.0807 - val_mse: 70.3185\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0706 - mse: 61.4438 - val_loss: 0.0805 - val_mse: 70.1063\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0704 - mse: 61.2644 - val_loss: 0.0803 - val_mse: 69.9423\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0701 - mse: 61.0879 - val_loss: 0.0800 - val_mse: 69.7053\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0699 - mse: 60.9121 - val_loss: 0.0798 - val_mse: 69.5042\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0698 - mse: 60.7420 - val_loss: 0.0796 - val_mse: 69.3543\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0696 - mse: 60.5737 - val_loss: 0.0794 - val_mse: 69.1229\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0694 - mse: 60.4030 - val_loss: 0.0792 - val_mse: 68.9374\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0692 - mse: 60.2370 - val_loss: 0.0790 - val_mse: 68.7867\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0690 - mse: 60.0752 - val_loss: 0.0787 - val_mse: 68.5723\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0688 - mse: 59.9119 - val_loss: 0.0785 - val_mse: 68.4021\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0686 - mse: 59.7478 - val_loss: 0.0784 - val_mse: 68.2371\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0684 - mse: 59.5820 - val_loss: 0.0781 - val_mse: 68.0250\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0682 - mse: 59.4141 - val_loss: 0.0779 - val_mse: 67.8656\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0680 - mse: 59.2443 - val_loss: 0.0777 - val_mse: 67.6903\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0678 - mse: 59.0760 - val_loss: 0.0775 - val_mse: 67.4999\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0677 - mse: 58.9129 - val_loss: 0.0774 - val_mse: 67.3616\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0675 - mse: 58.7565 - val_loss: 0.0771 - val_mse: 67.1814\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0673 - mse: 58.6091 - val_loss: 0.0770 - val_mse: 67.0197\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0671 - mse: 58.4622 - val_loss: 0.0768 - val_mse: 66.8737\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0670 - mse: 58.3107 - val_loss: 0.0766 - val_mse: 66.6988\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0668 - mse: 58.1552 - val_loss: 0.0764 - val_mse: 66.5608\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0666 - mse: 58.0004 - val_loss: 0.0762 - val_mse: 66.3809\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0664 - mse: 57.8511 - val_loss: 0.0761 - val_mse: 66.2478\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0663 - mse: 57.7089 - val_loss: 0.0759 - val_mse: 66.0823\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0661 - mse: 57.5672 - val_loss: 0.0757 - val_mse: 65.9383\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0659 - mse: 57.4286 - val_loss: 0.0756 - val_mse: 65.8073\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0658 - mse: 57.2938 - val_loss: 0.0754 - val_mse: 65.6340\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0656 - mse: 57.1627 - val_loss: 0.0753 - val_mse: 65.5376\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0655 - mse: 57.0345 - val_loss: 0.0750 - val_mse: 65.3556\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0653 - mse: 56.9093 - val_loss: 0.0749 - val_mse: 65.2433\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0652 - mse: 56.7836 - val_loss: 0.0748 - val_mse: 65.0987\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0651 - mse: 56.6582 - val_loss: 0.0746 - val_mse: 64.9480\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0649 - mse: 56.5347 - val_loss: 0.0745 - val_mse: 64.8509\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0648 - mse: 56.4137 - val_loss: 0.0743 - val_mse: 64.6602\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0646 - mse: 56.2945 - val_loss: 0.0742 - val_mse: 64.5853\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0645 - mse: 56.1765 - val_loss: 0.0739 - val_mse: 64.3956\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0644 - mse: 56.0581 - val_loss: 0.0738 - val_mse: 64.2994\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0642 - mse: 55.9397 - val_loss: 0.0737 - val_mse: 64.1511\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0641 - mse: 55.8225 - val_loss: 0.0735 - val_mse: 64.0166\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0640 - mse: 55.7073 - val_loss: 0.0734 - val_mse: 63.8994\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0638 - mse: 55.5936 - val_loss: 0.0732 - val_mse: 63.7438\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0637 - mse: 55.4812 - val_loss: 0.0731 - val_mse: 63.6476\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0636 - mse: 55.3689 - val_loss: 0.0729 - val_mse: 63.4852\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0635 - mse: 55.2572 - val_loss: 0.0728 - val_mse: 63.3979\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0633 - mse: 55.1460 - val_loss: 0.0726 - val_mse: 63.2201\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0632 - mse: 55.0361 - val_loss: 0.0725 - val_mse: 63.1496\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0631 - mse: 54.9274 - val_loss: 0.0723 - val_mse: 62.9630\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0630 - mse: 54.8199 - val_loss: 0.0722 - val_mse: 62.9141\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0628 - mse: 54.7152 - val_loss: 0.0720 - val_mse: 62.6916\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0627 - mse: 54.6146 - val_loss: 0.0720 - val_mse: 62.7169\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0626 - mse: 54.5233 - val_loss: 0.0717 - val_mse: 62.4139\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0625 - mse: 54.4344 - val_loss: 0.0718 - val_mse: 62.5498\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0624 - mse: 54.3552 - val_loss: 0.0714 - val_mse: 62.1587\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0623 - mse: 54.2743 - val_loss: 0.0717 - val_mse: 62.4006\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0623 - mse: 54.2113 - val_loss: 0.0711 - val_mse: 61.9078\n",
      "Epoch 262/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0621 - mse: 54.1124 - val_loss: 0.0713 - val_mse: 62.1051\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0620 - mse: 53.9814 - val_loss: 0.0708 - val_mse: 61.6623\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0618 - mse: 53.8236 - val_loss: 0.0708 - val_mse: 61.6798\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0616 - mse: 53.6866 - val_loss: 0.0706 - val_mse: 61.4934\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0615 - mse: 53.5786 - val_loss: 0.0705 - val_mse: 61.3538\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0614 - mse: 53.4730 - val_loss: 0.0703 - val_mse: 61.2620\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0613 - mse: 53.3779 - val_loss: 0.0702 - val_mse: 61.1028\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0612 - mse: 53.2787 - val_loss: 0.0701 - val_mse: 61.0495\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0611 - mse: 53.1780 - val_loss: 0.0699 - val_mse: 60.8759\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0610 - mse: 53.0968 - val_loss: 0.0700 - val_mse: 60.9405\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0609 - mse: 53.0323 - val_loss: 0.0696 - val_mse: 60.6038\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0609 - mse: 53.0087 - val_loss: 0.0700 - val_mse: 60.9158\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0608 - mse: 52.9626 - val_loss: 0.0693 - val_mse: 60.3806\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0607 - mse: 52.8432 - val_loss: 0.0696 - val_mse: 60.5862\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0605 - mse: 52.7005 - val_loss: 0.0691 - val_mse: 60.1580\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0604 - mse: 52.5870 - val_loss: 0.0693 - val_mse: 60.3142\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0603 - mse: 52.5007 - val_loss: 0.0688 - val_mse: 59.8892\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0602 - mse: 52.4375 - val_loss: 0.0690 - val_mse: 60.0653\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0601 - mse: 52.3239 - val_loss: 0.0685 - val_mse: 59.6482\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0599 - mse: 52.1726 - val_loss: 0.0684 - val_mse: 59.6064\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0598 - mse: 52.0419 - val_loss: 0.0684 - val_mse: 59.5232\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0597 - mse: 51.9558 - val_loss: 0.0681 - val_mse: 59.2876\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0596 - mse: 51.8953 - val_loss: 0.0682 - val_mse: 59.4093\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0595 - mse: 51.8262 - val_loss: 0.0678 - val_mse: 59.0436\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0594 - mse: 51.7191 - val_loss: 0.0678 - val_mse: 59.0716\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0592 - mse: 51.5897 - val_loss: 0.0676 - val_mse: 58.8367\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0591 - mse: 51.4728 - val_loss: 0.0674 - val_mse: 58.7123\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0590 - mse: 51.3810 - val_loss: 0.0674 - val_mse: 58.6968\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0589 - mse: 51.3024 - val_loss: 0.0671 - val_mse: 58.4610\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0588 - mse: 51.2188 - val_loss: 0.0672 - val_mse: 58.4867\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0587 - mse: 51.1209 - val_loss: 0.0669 - val_mse: 58.2551\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0586 - mse: 51.0166 - val_loss: 0.0668 - val_mse: 58.2042\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0585 - mse: 50.9150 - val_loss: 0.0667 - val_mse: 58.0873\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0584 - mse: 50.8218 - val_loss: 0.0665 - val_mse: 57.9374\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0583 - mse: 50.7355 - val_loss: 0.0665 - val_mse: 57.9133\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0582 - mse: 50.6496 - val_loss: 0.0663 - val_mse: 57.7044\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0581 - mse: 50.5594 - val_loss: 0.0662 - val_mse: 57.6805\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0579 - mse: 50.4612 - val_loss: 0.0660 - val_mse: 57.4928\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0578 - mse: 50.3608 - val_loss: 0.0659 - val_mse: 57.4044\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0577 - mse: 50.2637 - val_loss: 0.0658 - val_mse: 57.2998\n",
      "Epoch 302/1000\n",
      "1s - loss: 0.0576 - mse: 50.1705 - val_loss: 0.0656 - val_mse: 57.1520\n",
      "Epoch 303/1000\n",
      "1s - loss: 0.0575 - mse: 50.0787 - val_loss: 0.0656 - val_mse: 57.1005\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0574 - mse: 49.9898 - val_loss: 0.0654 - val_mse: 56.9223\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0573 - mse: 49.9031 - val_loss: 0.0653 - val_mse: 56.8887\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0572 - mse: 49.8130 - val_loss: 0.0651 - val_mse: 56.7171\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0571 - mse: 49.7187 - val_loss: 0.0650 - val_mse: 56.6467\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0570 - mse: 49.6269 - val_loss: 0.0649 - val_mse: 56.5154\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0569 - mse: 49.5360 - val_loss: 0.0648 - val_mse: 56.4111\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0568 - mse: 49.4463 - val_loss: 0.0647 - val_mse: 56.3236\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0567 - mse: 49.3583 - val_loss: 0.0645 - val_mse: 56.1821\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0566 - mse: 49.2713 - val_loss: 0.0645 - val_mse: 56.1277\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0565 - mse: 49.1843 - val_loss: 0.0643 - val_mse: 55.9623\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0564 - mse: 49.0944 - val_loss: 0.0642 - val_mse: 55.8959\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0563 - mse: 49.0026 - val_loss: 0.0640 - val_mse: 55.7619\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0562 - mse: 48.9107 - val_loss: 0.0639 - val_mse: 55.6474\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0561 - mse: 48.8210 - val_loss: 0.0638 - val_mse: 55.5663\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0560 - mse: 48.7323 - val_loss: 0.0636 - val_mse: 55.4160\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0559 - mse: 48.6440 - val_loss: 0.0636 - val_mse: 55.3592\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0558 - mse: 48.5540 - val_loss: 0.0634 - val_mse: 55.2072\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0556 - mse: 48.4617 - val_loss: 0.0633 - val_mse: 55.1281\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0555 - mse: 48.3699 - val_loss: 0.0632 - val_mse: 55.0115\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0554 - mse: 48.2792 - val_loss: 0.0630 - val_mse: 54.9024\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0553 - mse: 48.1893 - val_loss: 0.0629 - val_mse: 54.8192\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0552 - mse: 48.1004 - val_loss: 0.0628 - val_mse: 54.6883\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0551 - mse: 48.0117 - val_loss: 0.0627 - val_mse: 54.6161\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0550 - mse: 47.9226 - val_loss: 0.0626 - val_mse: 54.4806\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0549 - mse: 47.8331 - val_loss: 0.0625 - val_mse: 54.4085\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0548 - mse: 47.7447 - val_loss: 0.0623 - val_mse: 54.2726\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0547 - mse: 47.6557 - val_loss: 0.0622 - val_mse: 54.1985\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0546 - mse: 47.5661 - val_loss: 0.0621 - val_mse: 54.0752\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0545 - mse: 47.4775 - val_loss: 0.0620 - val_mse: 53.9822\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0544 - mse: 47.3910 - val_loss: 0.0619 - val_mse: 53.8796\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0543 - mse: 47.3060 - val_loss: 0.0617 - val_mse: 53.7645\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0542 - mse: 47.2220 - val_loss: 0.0616 - val_mse: 53.6831\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0541 - mse: 47.1392 - val_loss: 0.0615 - val_mse: 53.5424\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0540 - mse: 47.0572 - val_loss: 0.0614 - val_mse: 53.4900\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.0539 - mse: 46.9744 - val_loss: 0.0612 - val_mse: 53.3262\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0538 - mse: 46.8894 - val_loss: 0.0612 - val_mse: 53.2876\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0537 - mse: 46.8038 - val_loss: 0.0610 - val_mse: 53.1178\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0536 - mse: 46.7163 - val_loss: 0.0609 - val_mse: 53.0726\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0535 - mse: 46.6272 - val_loss: 0.0608 - val_mse: 52.9231\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.0534 - mse: 46.5372 - val_loss: 0.0607 - val_mse: 52.8413\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0533 - mse: 46.4483 - val_loss: 0.0606 - val_mse: 52.7373\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.0532 - mse: 46.3617 - val_loss: 0.0604 - val_mse: 52.6155\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.0531 - mse: 46.2769 - val_loss: 0.0603 - val_mse: 52.5476\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.0530 - mse: 46.1928 - val_loss: 0.0602 - val_mse: 52.3968\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0529 - mse: 46.1085 - val_loss: 0.0601 - val_mse: 52.3430\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.0528 - mse: 46.0229 - val_loss: 0.0599 - val_mse: 52.1923\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0527 - mse: 45.9357 - val_loss: 0.0599 - val_mse: 52.1261\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.0526 - mse: 45.8487 - val_loss: 0.0597 - val_mse: 51.9980\n",
      "Epoch 352/1000\n",
      "0s - loss: 0.0525 - mse: 45.7617 - val_loss: 0.0596 - val_mse: 51.9104\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.0524 - mse: 45.6753 - val_loss: 0.0595 - val_mse: 51.8171\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.0524 - mse: 45.5908 - val_loss: 0.0594 - val_mse: 51.7004\n",
      "Epoch 355/1000\n",
      "0s - loss: 0.0523 - mse: 45.5079 - val_loss: 0.0593 - val_mse: 51.6396\n",
      "Epoch 356/1000\n",
      "0s - loss: 0.0522 - mse: 45.4252 - val_loss: 0.0591 - val_mse: 51.4992\n",
      "Epoch 357/1000\n",
      "0s - loss: 0.0521 - mse: 45.3411 - val_loss: 0.0591 - val_mse: 51.4491\n",
      "Epoch 358/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0520 - mse: 45.2561 - val_loss: 0.0589 - val_mse: 51.3073\n",
      "Epoch 359/1000\n",
      "0s - loss: 0.0519 - mse: 45.1688 - val_loss: 0.0588 - val_mse: 51.2457\n",
      "Epoch 360/1000\n",
      "0s - loss: 0.0518 - mse: 45.0820 - val_loss: 0.0587 - val_mse: 51.1207\n",
      "Epoch 361/1000\n",
      "0s - loss: 0.0517 - mse: 44.9947 - val_loss: 0.0586 - val_mse: 51.0428\n",
      "Epoch 362/1000\n",
      "0s - loss: 0.0516 - mse: 44.9071 - val_loss: 0.0585 - val_mse: 50.9358\n",
      "Epoch 363/1000\n",
      "0s - loss: 0.0515 - mse: 44.8206 - val_loss: 0.0584 - val_mse: 50.8357\n",
      "Epoch 364/1000\n",
      "0s - loss: 0.0514 - mse: 44.7347 - val_loss: 0.0583 - val_mse: 50.7502\n",
      "Epoch 365/1000\n",
      "0s - loss: 0.0513 - mse: 44.6502 - val_loss: 0.0581 - val_mse: 50.6352\n",
      "Epoch 366/1000\n",
      "0s - loss: 0.0512 - mse: 44.5667 - val_loss: 0.0581 - val_mse: 50.5701\n",
      "Epoch 367/1000\n",
      "0s - loss: 0.0511 - mse: 44.4833 - val_loss: 0.0579 - val_mse: 50.4441\n",
      "Epoch 368/1000\n",
      "0s - loss: 0.0510 - mse: 44.4005 - val_loss: 0.0579 - val_mse: 50.3927\n",
      "Epoch 369/1000\n",
      "0s - loss: 0.0509 - mse: 44.3170 - val_loss: 0.0577 - val_mse: 50.2600\n",
      "Epoch 370/1000\n",
      "0s - loss: 0.0508 - mse: 44.2326 - val_loss: 0.0576 - val_mse: 50.1990\n",
      "Epoch 371/1000\n",
      "0s - loss: 0.0507 - mse: 44.1472 - val_loss: 0.0575 - val_mse: 50.0801\n",
      "Epoch 372/1000\n",
      "0s - loss: 0.0506 - mse: 44.0619 - val_loss: 0.0574 - val_mse: 49.9953\n",
      "Epoch 373/1000\n",
      "0s - loss: 0.0505 - mse: 43.9772 - val_loss: 0.0573 - val_mse: 49.8954\n",
      "Epoch 374/1000\n",
      "0s - loss: 0.0504 - mse: 43.8931 - val_loss: 0.0572 - val_mse: 49.7897\n",
      "Epoch 375/1000\n",
      "0s - loss: 0.0503 - mse: 43.8097 - val_loss: 0.0571 - val_mse: 49.7099\n",
      "Epoch 376/1000\n",
      "0s - loss: 0.0502 - mse: 43.7267 - val_loss: 0.0569 - val_mse: 49.5900\n",
      "Epoch 377/1000\n",
      "0s - loss: 0.0501 - mse: 43.6431 - val_loss: 0.0569 - val_mse: 49.5164\n",
      "Epoch 378/1000\n",
      "0s - loss: 0.0500 - mse: 43.5593 - val_loss: 0.0567 - val_mse: 49.3990\n",
      "Epoch 379/1000\n",
      "0s - loss: 0.0499 - mse: 43.4746 - val_loss: 0.0566 - val_mse: 49.3113\n",
      "Epoch 380/1000\n",
      "0s - loss: 0.0498 - mse: 43.3894 - val_loss: 0.0565 - val_mse: 49.2106\n",
      "Epoch 381/1000\n",
      "0s - loss: 0.0497 - mse: 43.3043 - val_loss: 0.0564 - val_mse: 49.0926\n",
      "Epoch 382/1000\n",
      "0s - loss: 0.0496 - mse: 43.2185 - val_loss: 0.0563 - val_mse: 49.0165\n",
      "Epoch 383/1000\n",
      "0s - loss: 0.0495 - mse: 43.1362 - val_loss: 0.0561 - val_mse: 48.8801\n",
      "Epoch 384/1000\n",
      "0s - loss: 0.0494 - mse: 43.0557 - val_loss: 0.0561 - val_mse: 48.8176\n",
      "Epoch 385/1000\n",
      "0s - loss: 0.0493 - mse: 42.9752 - val_loss: 0.0559 - val_mse: 48.6838\n",
      "Epoch 386/1000\n",
      "0s - loss: 0.0493 - mse: 42.8917 - val_loss: 0.0558 - val_mse: 48.6088\n",
      "Epoch 387/1000\n",
      "0s - loss: 0.0492 - mse: 42.8074 - val_loss: 0.0557 - val_mse: 48.5021\n",
      "Epoch 388/1000\n",
      "0s - loss: 0.0491 - mse: 42.7230 - val_loss: 0.0556 - val_mse: 48.4061\n",
      "Epoch 389/1000\n",
      "0s - loss: 0.0490 - mse: 42.6397 - val_loss: 0.0555 - val_mse: 48.3261\n",
      "Epoch 390/1000\n",
      "0s - loss: 0.0489 - mse: 42.5571 - val_loss: 0.0554 - val_mse: 48.2133\n",
      "Epoch 391/1000\n",
      "0s - loss: 0.0488 - mse: 42.4759 - val_loss: 0.0553 - val_mse: 48.1474\n",
      "Epoch 392/1000\n",
      "0s - loss: 0.0487 - mse: 42.3957 - val_loss: 0.0551 - val_mse: 48.0216\n",
      "Epoch 393/1000\n",
      "0s - loss: 0.0486 - mse: 42.3169 - val_loss: 0.0551 - val_mse: 47.9655\n",
      "Epoch 394/1000\n",
      "0s - loss: 0.0485 - mse: 42.2379 - val_loss: 0.0549 - val_mse: 47.8314\n",
      "Epoch 395/1000\n",
      "0s - loss: 0.0484 - mse: 42.1558 - val_loss: 0.0548 - val_mse: 47.7593\n",
      "Epoch 396/1000\n",
      "0s - loss: 0.0483 - mse: 42.0709 - val_loss: 0.0547 - val_mse: 47.6448\n",
      "Epoch 397/1000\n",
      "0s - loss: 0.0482 - mse: 41.9875 - val_loss: 0.0546 - val_mse: 47.5515\n",
      "Epoch 398/1000\n",
      "0s - loss: 0.0481 - mse: 41.9060 - val_loss: 0.0545 - val_mse: 47.4656\n",
      "Epoch 399/1000\n",
      "0s - loss: 0.0480 - mse: 41.8256 - val_loss: 0.0544 - val_mse: 47.3559\n",
      "Epoch 400/1000\n",
      "0s - loss: 0.0479 - mse: 41.7457 - val_loss: 0.0543 - val_mse: 47.2846\n",
      "Epoch 401/1000\n",
      "0s - loss: 0.0478 - mse: 41.6664 - val_loss: 0.0542 - val_mse: 47.1672\n",
      "Epoch 402/1000\n",
      "0s - loss: 0.0478 - mse: 41.5861 - val_loss: 0.0541 - val_mse: 47.0940\n",
      "Epoch 403/1000\n",
      "0s - loss: 0.0477 - mse: 41.5054 - val_loss: 0.0540 - val_mse: 46.9899\n",
      "Epoch 404/1000\n",
      "0s - loss: 0.0476 - mse: 41.4246 - val_loss: 0.0539 - val_mse: 46.8988\n",
      "Epoch 405/1000\n",
      "0s - loss: 0.0475 - mse: 41.3445 - val_loss: 0.0538 - val_mse: 46.8217\n",
      "Epoch 406/1000\n",
      "0s - loss: 0.0474 - mse: 41.2652 - val_loss: 0.0536 - val_mse: 46.7121\n",
      "Epoch 407/1000\n",
      "0s - loss: 0.0473 - mse: 41.1863 - val_loss: 0.0536 - val_mse: 46.6473\n",
      "Epoch 408/1000\n",
      "0s - loss: 0.0472 - mse: 41.1065 - val_loss: 0.0534 - val_mse: 46.5311\n",
      "Epoch 409/1000\n",
      "0s - loss: 0.0471 - mse: 41.0263 - val_loss: 0.0534 - val_mse: 46.4625\n",
      "Epoch 410/1000\n",
      "0s - loss: 0.0470 - mse: 40.9450 - val_loss: 0.0532 - val_mse: 46.3539\n",
      "Epoch 411/1000\n",
      "0s - loss: 0.0469 - mse: 40.8640 - val_loss: 0.0531 - val_mse: 46.2708\n",
      "Epoch 412/1000\n",
      "0s - loss: 0.0468 - mse: 40.7838 - val_loss: 0.0530 - val_mse: 46.1818\n",
      "Epoch 413/1000\n",
      "0s - loss: 0.0467 - mse: 40.7046 - val_loss: 0.0529 - val_mse: 46.0831\n",
      "Epoch 414/1000\n",
      "0s - loss: 0.0467 - mse: 40.6261 - val_loss: 0.0528 - val_mse: 46.0122\n",
      "Epoch 415/1000\n",
      "0s - loss: 0.0466 - mse: 40.5484 - val_loss: 0.0527 - val_mse: 45.8991\n",
      "Epoch 416/1000\n",
      "0s - loss: 0.0465 - mse: 40.4713 - val_loss: 0.0526 - val_mse: 45.8372\n",
      "Epoch 417/1000\n",
      "0s - loss: 0.0464 - mse: 40.3932 - val_loss: 0.0525 - val_mse: 45.7204\n",
      "Epoch 418/1000\n",
      "0s - loss: 0.0463 - mse: 40.3142 - val_loss: 0.0524 - val_mse: 45.6422\n",
      "Epoch 419/1000\n",
      "0s - loss: 0.0462 - mse: 40.2352 - val_loss: 0.0523 - val_mse: 45.5465\n",
      "Epoch 420/1000\n",
      "0s - loss: 0.0461 - mse: 40.1581 - val_loss: 0.0522 - val_mse: 45.4455\n",
      "Epoch 421/1000\n",
      "0s - loss: 0.0460 - mse: 40.0823 - val_loss: 0.0521 - val_mse: 45.3712\n",
      "Epoch 422/1000\n",
      "0s - loss: 0.0459 - mse: 40.0065 - val_loss: 0.0520 - val_mse: 45.2573\n",
      "Epoch 423/1000\n",
      "0s - loss: 0.0459 - mse: 39.9302 - val_loss: 0.0519 - val_mse: 45.1854\n",
      "Epoch 424/1000\n",
      "0s - loss: 0.0458 - mse: 39.8536 - val_loss: 0.0518 - val_mse: 45.0733\n",
      "Epoch 425/1000\n",
      "0s - loss: 0.0457 - mse: 39.7772 - val_loss: 0.0517 - val_mse: 44.9882\n",
      "Epoch 426/1000\n",
      "0s - loss: 0.0456 - mse: 39.7012 - val_loss: 0.0516 - val_mse: 44.8949\n",
      "Epoch 427/1000\n",
      "0s - loss: 0.0455 - mse: 39.6259 - val_loss: 0.0514 - val_mse: 44.7942\n",
      "Epoch 428/1000\n",
      "0s - loss: 0.0454 - mse: 39.5514 - val_loss: 0.0513 - val_mse: 44.7152\n",
      "Epoch 429/1000\n",
      "0s - loss: 0.0453 - mse: 39.4774 - val_loss: 0.0512 - val_mse: 44.6054\n",
      "Epoch 430/1000\n",
      "0s - loss: 0.0452 - mse: 39.4044 - val_loss: 0.0511 - val_mse: 44.5382\n",
      "Epoch 431/1000\n",
      "0s - loss: 0.0452 - mse: 39.3306 - val_loss: 0.0510 - val_mse: 44.4241\n",
      "Epoch 432/1000\n",
      "0s - loss: 0.0451 - mse: 39.2561 - val_loss: 0.0509 - val_mse: 44.3558\n",
      "Epoch 433/1000\n",
      "0s - loss: 0.0450 - mse: 39.1809 - val_loss: 0.0508 - val_mse: 44.2534\n",
      "Epoch 434/1000\n",
      "0s - loss: 0.0449 - mse: 39.1050 - val_loss: 0.0507 - val_mse: 44.1672\n",
      "Epoch 435/1000\n",
      "0s - loss: 0.0448 - mse: 39.0302 - val_loss: 0.0506 - val_mse: 44.0878\n",
      "Epoch 436/1000\n",
      "0s - loss: 0.0447 - mse: 38.9559 - val_loss: 0.0505 - val_mse: 43.9849\n",
      "Epoch 437/1000\n",
      "0s - loss: 0.0446 - mse: 38.8820 - val_loss: 0.0504 - val_mse: 43.9177\n",
      "Epoch 438/1000\n",
      "0s - loss: 0.0446 - mse: 38.8077 - val_loss: 0.0503 - val_mse: 43.8087\n",
      "Epoch 439/1000\n",
      "0s - loss: 0.0445 - mse: 38.7343 - val_loss: 0.0502 - val_mse: 43.7431\n",
      "Epoch 440/1000\n",
      "0s - loss: 0.0444 - mse: 38.6587 - val_loss: 0.0501 - val_mse: 43.6349\n",
      "Epoch 441/1000\n",
      "0s - loss: 0.0443 - mse: 38.5835 - val_loss: 0.0500 - val_mse: 43.5606\n",
      "Epoch 442/1000\n",
      "0s - loss: 0.0442 - mse: 38.5078 - val_loss: 0.0499 - val_mse: 43.4687\n",
      "Epoch 443/1000\n",
      "0s - loss: 0.0441 - mse: 38.4327 - val_loss: 0.0498 - val_mse: 43.3746\n",
      "Epoch 444/1000\n",
      "0s - loss: 0.0440 - mse: 38.3583 - val_loss: 0.0497 - val_mse: 43.3045\n",
      "Epoch 445/1000\n",
      "0s - loss: 0.0440 - mse: 38.2868 - val_loss: 0.0496 - val_mse: 43.1973\n",
      "Epoch 446/1000\n",
      "0s - loss: 0.0439 - mse: 38.2158 - val_loss: 0.0495 - val_mse: 43.1369\n",
      "Epoch 447/1000\n",
      "0s - loss: 0.0438 - mse: 38.1409 - val_loss: 0.0494 - val_mse: 43.0199\n",
      "Epoch 448/1000\n",
      "0s - loss: 0.0437 - mse: 38.0651 - val_loss: 0.0493 - val_mse: 42.9529\n",
      "Epoch 449/1000\n",
      "0s - loss: 0.0436 - mse: 37.9896 - val_loss: 0.0492 - val_mse: 42.8568\n",
      "Epoch 450/1000\n",
      "0s - loss: 0.0435 - mse: 37.9150 - val_loss: 0.0491 - val_mse: 42.7628\n",
      "Epoch 451/1000\n",
      "0s - loss: 0.0435 - mse: 37.8422 - val_loss: 0.0490 - val_mse: 42.6944\n",
      "Epoch 452/1000\n",
      "0s - loss: 0.0434 - mse: 37.7702 - val_loss: 0.0489 - val_mse: 42.5805\n",
      "Epoch 453/1000\n",
      "0s - loss: 0.0433 - mse: 37.6974 - val_loss: 0.0488 - val_mse: 42.5155\n",
      "Epoch 454/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0432 - mse: 37.6210 - val_loss: 0.0487 - val_mse: 42.4026\n",
      "Epoch 455/1000\n",
      "0s - loss: 0.0431 - mse: 37.5431 - val_loss: 0.0486 - val_mse: 42.3182\n",
      "Epoch 456/1000\n",
      "0s - loss: 0.0430 - mse: 37.4658 - val_loss: 0.0485 - val_mse: 42.2317\n",
      "Epoch 457/1000\n",
      "0s - loss: 0.0429 - mse: 37.3907 - val_loss: 0.0484 - val_mse: 42.1315\n",
      "Epoch 458/1000\n",
      "0s - loss: 0.0429 - mse: 37.3182 - val_loss: 0.0483 - val_mse: 42.0615\n",
      "Epoch 459/1000\n",
      "0s - loss: 0.0428 - mse: 37.2456 - val_loss: 0.0482 - val_mse: 41.9523\n",
      "Epoch 460/1000\n",
      "0s - loss: 0.0427 - mse: 37.1717 - val_loss: 0.0481 - val_mse: 41.8714\n",
      "Epoch 461/1000\n",
      "0s - loss: 0.0426 - mse: 37.0970 - val_loss: 0.0480 - val_mse: 41.7759\n",
      "Epoch 462/1000\n",
      "0s - loss: 0.0425 - mse: 37.0233 - val_loss: 0.0479 - val_mse: 41.6805\n",
      "Epoch 463/1000\n",
      "0s - loss: 0.0424 - mse: 36.9511 - val_loss: 0.0478 - val_mse: 41.5990\n",
      "Epoch 464/1000\n",
      "0s - loss: 0.0423 - mse: 36.8793 - val_loss: 0.0477 - val_mse: 41.4974\n",
      "Epoch 465/1000\n",
      "0s - loss: 0.0423 - mse: 36.8068 - val_loss: 0.0476 - val_mse: 41.4174\n",
      "Epoch 466/1000\n",
      "0s - loss: 0.0422 - mse: 36.7334 - val_loss: 0.0475 - val_mse: 41.3226\n",
      "Epoch 467/1000\n",
      "0s - loss: 0.0421 - mse: 36.6603 - val_loss: 0.0474 - val_mse: 41.2346\n",
      "Epoch 468/1000\n",
      "0s - loss: 0.0420 - mse: 36.5889 - val_loss: 0.0473 - val_mse: 41.1564\n",
      "Epoch 469/1000\n",
      "0s - loss: 0.0419 - mse: 36.5182 - val_loss: 0.0471 - val_mse: 41.0567\n",
      "Epoch 470/1000\n",
      "0s - loss: 0.0419 - mse: 36.4465 - val_loss: 0.0471 - val_mse: 40.9809\n",
      "Epoch 471/1000\n",
      "0s - loss: 0.0418 - mse: 36.3741 - val_loss: 0.0470 - val_mse: 40.8910\n",
      "Epoch 472/1000\n",
      "0s - loss: 0.0417 - mse: 36.3023 - val_loss: 0.0469 - val_mse: 40.8051\n",
      "Epoch 473/1000\n",
      "0s - loss: 0.0416 - mse: 36.2313 - val_loss: 0.0468 - val_mse: 40.7339\n",
      "Epoch 474/1000\n",
      "0s - loss: 0.0415 - mse: 36.1613 - val_loss: 0.0467 - val_mse: 40.6385\n",
      "Epoch 475/1000\n",
      "0s - loss: 0.0414 - mse: 36.0904 - val_loss: 0.0466 - val_mse: 40.5663\n",
      "Epoch 476/1000\n",
      "0s - loss: 0.0414 - mse: 36.0194 - val_loss: 0.0465 - val_mse: 40.4763\n",
      "Epoch 477/1000\n",
      "0s - loss: 0.0413 - mse: 35.9486 - val_loss: 0.0464 - val_mse: 40.3913\n",
      "Epoch 478/1000\n",
      "0s - loss: 0.0412 - mse: 35.8787 - val_loss: 0.0463 - val_mse: 40.3127\n",
      "Epoch 479/1000\n",
      "0s - loss: 0.0411 - mse: 35.8093 - val_loss: 0.0462 - val_mse: 40.2211\n",
      "Epoch 480/1000\n",
      "0s - loss: 0.0410 - mse: 35.7400 - val_loss: 0.0461 - val_mse: 40.1439\n",
      "Epoch 481/1000\n",
      "0s - loss: 0.0410 - mse: 35.6705 - val_loss: 0.0460 - val_mse: 40.0539\n",
      "Epoch 482/1000\n",
      "0s - loss: 0.0409 - mse: 35.6009 - val_loss: 0.0459 - val_mse: 39.9706\n",
      "Epoch 483/1000\n",
      "0s - loss: 0.0408 - mse: 35.5317 - val_loss: 0.0458 - val_mse: 39.8871\n",
      "Epoch 484/1000\n",
      "0s - loss: 0.0407 - mse: 35.4623 - val_loss: 0.0457 - val_mse: 39.8018\n",
      "Epoch 485/1000\n",
      "0s - loss: 0.0406 - mse: 35.3931 - val_loss: 0.0456 - val_mse: 39.7220\n",
      "Epoch 486/1000\n",
      "0s - loss: 0.0406 - mse: 35.3239 - val_loss: 0.0455 - val_mse: 39.6372\n",
      "Epoch 487/1000\n",
      "0s - loss: 0.0405 - mse: 35.2549 - val_loss: 0.0454 - val_mse: 39.5544\n",
      "Epoch 488/1000\n",
      "0s - loss: 0.0404 - mse: 35.1860 - val_loss: 0.0453 - val_mse: 39.4676\n",
      "Epoch 489/1000\n",
      "0s - loss: 0.0403 - mse: 35.1170 - val_loss: 0.0452 - val_mse: 39.3864\n",
      "Epoch 490/1000\n",
      "0s - loss: 0.0402 - mse: 35.0481 - val_loss: 0.0451 - val_mse: 39.2963\n",
      "Epoch 491/1000\n",
      "0s - loss: 0.0402 - mse: 34.9801 - val_loss: 0.0450 - val_mse: 39.2192\n",
      "Epoch 492/1000\n",
      "0s - loss: 0.0401 - mse: 34.9116 - val_loss: 0.0449 - val_mse: 39.1311\n",
      "Epoch 493/1000\n",
      "0s - loss: 0.0400 - mse: 34.8431 - val_loss: 0.0448 - val_mse: 39.0502\n",
      "Epoch 494/1000\n",
      "0s - loss: 0.0399 - mse: 34.7745 - val_loss: 0.0447 - val_mse: 38.9674\n",
      "Epoch 495/1000\n",
      "0s - loss: 0.0399 - mse: 34.7063 - val_loss: 0.0447 - val_mse: 38.8842\n",
      "Epoch 496/1000\n",
      "0s - loss: 0.0398 - mse: 34.6383 - val_loss: 0.0446 - val_mse: 38.8050\n",
      "Epoch 497/1000\n",
      "0s - loss: 0.0397 - mse: 34.5703 - val_loss: 0.0445 - val_mse: 38.7206\n",
      "Epoch 498/1000\n",
      "0s - loss: 0.0396 - mse: 34.5025 - val_loss: 0.0444 - val_mse: 38.6406\n",
      "Epoch 499/1000\n",
      "0s - loss: 0.0395 - mse: 34.4353 - val_loss: 0.0443 - val_mse: 38.5575\n",
      "Epoch 500/1000\n",
      "0s - loss: 0.0395 - mse: 34.3679 - val_loss: 0.0442 - val_mse: 38.4789\n",
      "Epoch 501/1000\n",
      "0s - loss: 0.0394 - mse: 34.3009 - val_loss: 0.0441 - val_mse: 38.3962\n",
      "Epoch 502/1000\n",
      "0s - loss: 0.0393 - mse: 34.2342 - val_loss: 0.0440 - val_mse: 38.3174\n",
      "Epoch 503/1000\n",
      "0s - loss: 0.0392 - mse: 34.1678 - val_loss: 0.0439 - val_mse: 38.2401\n",
      "Epoch 504/1000\n",
      "0s - loss: 0.0392 - mse: 34.1017 - val_loss: 0.0438 - val_mse: 38.1592\n",
      "Epoch 505/1000\n",
      "0s - loss: 0.0391 - mse: 34.0364 - val_loss: 0.0437 - val_mse: 38.0882\n",
      "Epoch 506/1000\n",
      "0s - loss: 0.0390 - mse: 33.9716 - val_loss: 0.0436 - val_mse: 38.0040\n",
      "Epoch 507/1000\n",
      "0s - loss: 0.0389 - mse: 33.9071 - val_loss: 0.0436 - val_mse: 37.9344\n",
      "Epoch 508/1000\n",
      "0s - loss: 0.0389 - mse: 33.8425 - val_loss: 0.0435 - val_mse: 37.8520\n",
      "Epoch 509/1000\n",
      "0s - loss: 0.0388 - mse: 33.7781 - val_loss: 0.0434 - val_mse: 37.7756\n",
      "Epoch 510/1000\n",
      "0s - loss: 0.0387 - mse: 33.7141 - val_loss: 0.0433 - val_mse: 37.7038\n",
      "Epoch 511/1000\n",
      "0s - loss: 0.0386 - mse: 33.6502 - val_loss: 0.0432 - val_mse: 37.6238\n",
      "Epoch 512/1000\n",
      "0s - loss: 0.0386 - mse: 33.5865 - val_loss: 0.0431 - val_mse: 37.5547\n",
      "Epoch 513/1000\n",
      "0s - loss: 0.0385 - mse: 33.5230 - val_loss: 0.0430 - val_mse: 37.4753\n",
      "Epoch 514/1000\n",
      "0s - loss: 0.0384 - mse: 33.4596 - val_loss: 0.0430 - val_mse: 37.4054\n",
      "Epoch 515/1000\n",
      "0s - loss: 0.0383 - mse: 33.3963 - val_loss: 0.0429 - val_mse: 37.3265\n",
      "Epoch 516/1000\n",
      "0s - loss: 0.0383 - mse: 33.3332 - val_loss: 0.0428 - val_mse: 37.2540\n",
      "Epoch 517/1000\n",
      "0s - loss: 0.0382 - mse: 33.2706 - val_loss: 0.0427 - val_mse: 37.1781\n",
      "Epoch 518/1000\n",
      "0s - loss: 0.0381 - mse: 33.2085 - val_loss: 0.0426 - val_mse: 37.1034\n",
      "Epoch 519/1000\n",
      "0s - loss: 0.0381 - mse: 33.1467 - val_loss: 0.0425 - val_mse: 37.0300\n",
      "Epoch 520/1000\n",
      "0s - loss: 0.0380 - mse: 33.0849 - val_loss: 0.0424 - val_mse: 36.9540\n",
      "Epoch 521/1000\n",
      "0s - loss: 0.0379 - mse: 33.0236 - val_loss: 0.0424 - val_mse: 36.8858\n",
      "Epoch 522/1000\n",
      "0s - loss: 0.0379 - mse: 32.9627 - val_loss: 0.0423 - val_mse: 36.8089\n",
      "Epoch 523/1000\n",
      "0s - loss: 0.0378 - mse: 32.9021 - val_loss: 0.0422 - val_mse: 36.7412\n",
      "Epoch 524/1000\n",
      "0s - loss: 0.0377 - mse: 32.8417 - val_loss: 0.0421 - val_mse: 36.6685\n",
      "Epoch 525/1000\n",
      "0s - loss: 0.0376 - mse: 32.7817 - val_loss: 0.0420 - val_mse: 36.5945\n",
      "Epoch 526/1000\n",
      "0s - loss: 0.0376 - mse: 32.7225 - val_loss: 0.0419 - val_mse: 36.5293\n",
      "Epoch 527/1000\n",
      "0s - loss: 0.0375 - mse: 32.6639 - val_loss: 0.0419 - val_mse: 36.4517\n",
      "Epoch 528/1000\n",
      "0s - loss: 0.0374 - mse: 32.6050 - val_loss: 0.0418 - val_mse: 36.3863\n",
      "Epoch 529/1000\n",
      "0s - loss: 0.0374 - mse: 32.5460 - val_loss: 0.0417 - val_mse: 36.3170\n",
      "Epoch 530/1000\n",
      "0s - loss: 0.0373 - mse: 32.4878 - val_loss: 0.0416 - val_mse: 36.2450\n",
      "Epoch 531/1000\n",
      "0s - loss: 0.0372 - mse: 32.4301 - val_loss: 0.0415 - val_mse: 36.1826\n",
      "Epoch 532/1000\n",
      "0s - loss: 0.0372 - mse: 32.3722 - val_loss: 0.0415 - val_mse: 36.1101\n",
      "Epoch 533/1000\n",
      "0s - loss: 0.0371 - mse: 32.3138 - val_loss: 0.0414 - val_mse: 36.0468\n",
      "Epoch 534/1000\n",
      "0s - loss: 0.0370 - mse: 32.2556 - val_loss: 0.0413 - val_mse: 35.9822\n",
      "Epoch 535/1000\n",
      "0s - loss: 0.0370 - mse: 32.1981 - val_loss: 0.0412 - val_mse: 35.9098\n",
      "Epoch 536/1000\n",
      "0s - loss: 0.0369 - mse: 32.1405 - val_loss: 0.0412 - val_mse: 35.8481\n",
      "Epoch 537/1000\n",
      "0s - loss: 0.0368 - mse: 32.0822 - val_loss: 0.0411 - val_mse: 35.7785\n",
      "Epoch 538/1000\n",
      "0s - loss: 0.0368 - mse: 32.0229 - val_loss: 0.0410 - val_mse: 35.7058\n",
      "Epoch 539/1000\n",
      "0s - loss: 0.0367 - mse: 31.9617 - val_loss: 0.0409 - val_mse: 35.6396\n",
      "Epoch 540/1000\n",
      "0s - loss: 0.0366 - mse: 31.8987 - val_loss: 0.0408 - val_mse: 35.5683\n",
      "Epoch 541/1000\n",
      "0s - loss: 0.0366 - mse: 31.8370 - val_loss: 0.0408 - val_mse: 35.5042\n",
      "Epoch 542/1000\n",
      "0s - loss: 0.0365 - mse: 31.7762 - val_loss: 0.0407 - val_mse: 35.4364\n",
      "Epoch 543/1000\n",
      "0s - loss: 0.0364 - mse: 31.7140 - val_loss: 0.0406 - val_mse: 35.3671\n",
      "Epoch 544/1000\n",
      "0s - loss: 0.0363 - mse: 31.6530 - val_loss: 0.0405 - val_mse: 35.3059\n",
      "Epoch 545/1000\n",
      "0s - loss: 0.0363 - mse: 31.5928 - val_loss: 0.0405 - val_mse: 35.2382\n",
      "Epoch 546/1000\n",
      "0s - loss: 0.0362 - mse: 31.5329 - val_loss: 0.0404 - val_mse: 35.1751\n",
      "Epoch 547/1000\n",
      "0s - loss: 0.0361 - mse: 31.4747 - val_loss: 0.0403 - val_mse: 35.1136\n",
      "Epoch 548/1000\n",
      "0s - loss: 0.0361 - mse: 31.4171 - val_loss: 0.0403 - val_mse: 35.0513\n",
      "Epoch 549/1000\n",
      "0s - loss: 0.0360 - mse: 31.3611 - val_loss: 0.0402 - val_mse: 34.9923\n",
      "Epoch 550/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0359 - mse: 31.3047 - val_loss: 0.0401 - val_mse: 34.9322\n",
      "Epoch 551/1000\n",
      "0s - loss: 0.0359 - mse: 31.2492 - val_loss: 0.0400 - val_mse: 34.8733\n",
      "Epoch 552/1000\n",
      "0s - loss: 0.0358 - mse: 31.1957 - val_loss: 0.0400 - val_mse: 34.8193\n",
      "Epoch 553/1000\n",
      "0s - loss: 0.0358 - mse: 31.1431 - val_loss: 0.0399 - val_mse: 34.7564\n",
      "Epoch 554/1000\n",
      "0s - loss: 0.0357 - mse: 31.0912 - val_loss: 0.0398 - val_mse: 34.7007\n",
      "Epoch 555/1000\n",
      "0s - loss: 0.0356 - mse: 31.0395 - val_loss: 0.0398 - val_mse: 34.6381\n",
      "Epoch 556/1000\n",
      "0s - loss: 0.0356 - mse: 30.9879 - val_loss: 0.0397 - val_mse: 34.5771\n",
      "Epoch 557/1000\n",
      "0s - loss: 0.0355 - mse: 30.9364 - val_loss: 0.0396 - val_mse: 34.5194\n",
      "Epoch 558/1000\n",
      "0s - loss: 0.0355 - mse: 30.8855 - val_loss: 0.0396 - val_mse: 34.4559\n",
      "Epoch 559/1000\n",
      "0s - loss: 0.0354 - mse: 30.8349 - val_loss: 0.0395 - val_mse: 34.3998\n",
      "Epoch 560/1000\n",
      "0s - loss: 0.0354 - mse: 30.7848 - val_loss: 0.0394 - val_mse: 34.3432\n",
      "Epoch 561/1000\n",
      "0s - loss: 0.0353 - mse: 30.7352 - val_loss: 0.0394 - val_mse: 34.2830\n",
      "Epoch 562/1000\n",
      "0s - loss: 0.0352 - mse: 30.6864 - val_loss: 0.0393 - val_mse: 34.2314\n",
      "Epoch 563/1000\n",
      "0s - loss: 0.0352 - mse: 30.6386 - val_loss: 0.0392 - val_mse: 34.1718\n",
      "Epoch 564/1000\n",
      "0s - loss: 0.0351 - mse: 30.5910 - val_loss: 0.0392 - val_mse: 34.1158\n",
      "Epoch 565/1000\n",
      "0s - loss: 0.0351 - mse: 30.5435 - val_loss: 0.0391 - val_mse: 34.0624\n",
      "Epoch 566/1000\n",
      "0s - loss: 0.0350 - mse: 30.4969 - val_loss: 0.0390 - val_mse: 34.0017\n",
      "Epoch 567/1000\n",
      "0s - loss: 0.0350 - mse: 30.4504 - val_loss: 0.0390 - val_mse: 33.9492\n",
      "Epoch 568/1000\n",
      "0s - loss: 0.0349 - mse: 30.4040 - val_loss: 0.0389 - val_mse: 33.8912\n",
      "Epoch 569/1000\n",
      "0s - loss: 0.0349 - mse: 30.3579 - val_loss: 0.0389 - val_mse: 33.8350\n",
      "Epoch 570/1000\n",
      "0s - loss: 0.0348 - mse: 30.3124 - val_loss: 0.0388 - val_mse: 33.7836\n",
      "Epoch 571/1000\n",
      "0s - loss: 0.0348 - mse: 30.2676 - val_loss: 0.0387 - val_mse: 33.7283\n",
      "Epoch 572/1000\n",
      "0s - loss: 0.0347 - mse: 30.2233 - val_loss: 0.0387 - val_mse: 33.6785\n",
      "Epoch 573/1000\n",
      "0s - loss: 0.0347 - mse: 30.1791 - val_loss: 0.0386 - val_mse: 33.6290\n",
      "Epoch 574/1000\n",
      "0s - loss: 0.0346 - mse: 30.1351 - val_loss: 0.0386 - val_mse: 33.5779\n",
      "Epoch 575/1000\n",
      "0s - loss: 0.0346 - mse: 30.0915 - val_loss: 0.0385 - val_mse: 33.5319\n",
      "Epoch 576/1000\n",
      "0s - loss: 0.0345 - mse: 30.0481 - val_loss: 0.0384 - val_mse: 33.4833\n",
      "Epoch 577/1000\n",
      "0s - loss: 0.0345 - mse: 30.0050 - val_loss: 0.0384 - val_mse: 33.4350\n",
      "Epoch 578/1000\n",
      "0s - loss: 0.0344 - mse: 29.9622 - val_loss: 0.0383 - val_mse: 33.3888\n",
      "Epoch 579/1000\n",
      "0s - loss: 0.0344 - mse: 29.9194 - val_loss: 0.0383 - val_mse: 33.3407\n",
      "Epoch 580/1000\n",
      "0s - loss: 0.0343 - mse: 29.8767 - val_loss: 0.0382 - val_mse: 33.2935\n",
      "Epoch 581/1000\n",
      "0s - loss: 0.0343 - mse: 29.8340 - val_loss: 0.0382 - val_mse: 33.2465\n",
      "Epoch 582/1000\n",
      "0s - loss: 0.0342 - mse: 29.7920 - val_loss: 0.0381 - val_mse: 33.2006\n",
      "Epoch 583/1000\n",
      "0s - loss: 0.0342 - mse: 29.7507 - val_loss: 0.0381 - val_mse: 33.1522\n",
      "Epoch 584/1000\n",
      "0s - loss: 0.0341 - mse: 29.7095 - val_loss: 0.0380 - val_mse: 33.1080\n",
      "Epoch 585/1000\n",
      "0s - loss: 0.0341 - mse: 29.6686 - val_loss: 0.0380 - val_mse: 33.0618\n",
      "Epoch 586/1000\n",
      "0s - loss: 0.0340 - mse: 29.6282 - val_loss: 0.0379 - val_mse: 33.0186\n",
      "Epoch 587/1000\n",
      "0s - loss: 0.0340 - mse: 29.5879 - val_loss: 0.0379 - val_mse: 32.9752\n",
      "Epoch 588/1000\n",
      "0s - loss: 0.0339 - mse: 29.5477 - val_loss: 0.0378 - val_mse: 32.9303\n",
      "Epoch 589/1000\n",
      "0s - loss: 0.0339 - mse: 29.5076 - val_loss: 0.0378 - val_mse: 32.8871\n",
      "Epoch 590/1000\n",
      "0s - loss: 0.0338 - mse: 29.4678 - val_loss: 0.0377 - val_mse: 32.8426\n",
      "Epoch 591/1000\n",
      "0s - loss: 0.0338 - mse: 29.4280 - val_loss: 0.0377 - val_mse: 32.7980\n",
      "Epoch 592/1000\n",
      "0s - loss: 0.0337 - mse: 29.3883 - val_loss: 0.0376 - val_mse: 32.7570\n",
      "Epoch 593/1000\n",
      "0s - loss: 0.0337 - mse: 29.3486 - val_loss: 0.0376 - val_mse: 32.7119\n",
      "Epoch 594/1000\n",
      "0s - loss: 0.0337 - mse: 29.3092 - val_loss: 0.0375 - val_mse: 32.6690\n",
      "Epoch 595/1000\n",
      "0s - loss: 0.0336 - mse: 29.2699 - val_loss: 0.0375 - val_mse: 32.6277\n",
      "Epoch 596/1000\n",
      "0s - loss: 0.0336 - mse: 29.2308 - val_loss: 0.0374 - val_mse: 32.5824\n",
      "Epoch 597/1000\n",
      "0s - loss: 0.0335 - mse: 29.1918 - val_loss: 0.0374 - val_mse: 32.5404\n",
      "Epoch 598/1000\n",
      "0s - loss: 0.0335 - mse: 29.1527 - val_loss: 0.0373 - val_mse: 32.4969\n",
      "Epoch 599/1000\n",
      "0s - loss: 0.0334 - mse: 29.1137 - val_loss: 0.0373 - val_mse: 32.4523\n",
      "Epoch 600/1000\n",
      "0s - loss: 0.0334 - mse: 29.0744 - val_loss: 0.0372 - val_mse: 32.4081\n",
      "Epoch 601/1000\n",
      "0s - loss: 0.0333 - mse: 29.0349 - val_loss: 0.0372 - val_mse: 32.3628\n",
      "Epoch 602/1000\n",
      "0s - loss: 0.0333 - mse: 28.9953 - val_loss: 0.0371 - val_mse: 32.3180\n",
      "Epoch 603/1000\n",
      "0s - loss: 0.0333 - mse: 28.9564 - val_loss: 0.0371 - val_mse: 32.2763\n",
      "Epoch 604/1000\n",
      "0s - loss: 0.0332 - mse: 28.9183 - val_loss: 0.0370 - val_mse: 32.2336\n",
      "Epoch 605/1000\n",
      "0s - loss: 0.0332 - mse: 28.8806 - val_loss: 0.0370 - val_mse: 32.1934\n",
      "Epoch 606/1000\n",
      "0s - loss: 0.0331 - mse: 28.8439 - val_loss: 0.0369 - val_mse: 32.1545\n",
      "Epoch 607/1000\n",
      "0s - loss: 0.0331 - mse: 28.8078 - val_loss: 0.0369 - val_mse: 32.1128\n",
      "Epoch 608/1000\n",
      "0s - loss: 0.0330 - mse: 28.7716 - val_loss: 0.0368 - val_mse: 32.0724\n",
      "Epoch 609/1000\n",
      "0s - loss: 0.0330 - mse: 28.7354 - val_loss: 0.0368 - val_mse: 32.0331\n",
      "Epoch 610/1000\n",
      "0s - loss: 0.0330 - mse: 28.6992 - val_loss: 0.0367 - val_mse: 31.9912\n",
      "Epoch 611/1000\n",
      "0s - loss: 0.0329 - mse: 28.6628 - val_loss: 0.0367 - val_mse: 31.9511\n",
      "Epoch 612/1000\n",
      "0s - loss: 0.0329 - mse: 28.6264 - val_loss: 0.0366 - val_mse: 31.9128\n",
      "Epoch 613/1000\n",
      "0s - loss: 0.0328 - mse: 28.5900 - val_loss: 0.0366 - val_mse: 31.8718\n",
      "Epoch 614/1000\n",
      "0s - loss: 0.0328 - mse: 28.5536 - val_loss: 0.0366 - val_mse: 31.8330\n",
      "Epoch 615/1000\n",
      "0s - loss: 0.0327 - mse: 28.5174 - val_loss: 0.0365 - val_mse: 31.7955\n",
      "Epoch 616/1000\n",
      "0s - loss: 0.0327 - mse: 28.4815 - val_loss: 0.0365 - val_mse: 31.7551\n",
      "Epoch 617/1000\n",
      "0s - loss: 0.0327 - mse: 28.4458 - val_loss: 0.0364 - val_mse: 31.7191\n",
      "Epoch 618/1000\n",
      "0s - loss: 0.0326 - mse: 28.4101 - val_loss: 0.0364 - val_mse: 31.6814\n",
      "Epoch 619/1000\n",
      "0s - loss: 0.0326 - mse: 28.3745 - val_loss: 0.0363 - val_mse: 31.6420\n",
      "Epoch 620/1000\n",
      "0s - loss: 0.0325 - mse: 28.3394 - val_loss: 0.0363 - val_mse: 31.6076\n",
      "Epoch 621/1000\n",
      "0s - loss: 0.0325 - mse: 28.3045 - val_loss: 0.0363 - val_mse: 31.5718\n",
      "Epoch 622/1000\n",
      "0s - loss: 0.0325 - mse: 28.2698 - val_loss: 0.0362 - val_mse: 31.5331\n",
      "Epoch 623/1000\n",
      "0s - loss: 0.0324 - mse: 28.2351 - val_loss: 0.0362 - val_mse: 31.5011\n",
      "Epoch 624/1000\n",
      "0s - loss: 0.0324 - mse: 28.2009 - val_loss: 0.0361 - val_mse: 31.4603\n",
      "Epoch 625/1000\n",
      "0s - loss: 0.0323 - mse: 28.1661 - val_loss: 0.0361 - val_mse: 31.4225\n",
      "Epoch 626/1000\n",
      "0s - loss: 0.0323 - mse: 28.1316 - val_loss: 0.0360 - val_mse: 31.3876\n",
      "Epoch 627/1000\n",
      "0s - loss: 0.0323 - mse: 28.0972 - val_loss: 0.0360 - val_mse: 31.3469\n",
      "Epoch 628/1000\n",
      "0s - loss: 0.0322 - mse: 28.0625 - val_loss: 0.0360 - val_mse: 31.3107\n",
      "Epoch 629/1000\n",
      "0s - loss: 0.0322 - mse: 28.0282 - val_loss: 0.0359 - val_mse: 31.2765\n",
      "Epoch 630/1000\n",
      "0s - loss: 0.0321 - mse: 27.9946 - val_loss: 0.0359 - val_mse: 31.2361\n",
      "Epoch 631/1000\n",
      "0s - loss: 0.0321 - mse: 27.9606 - val_loss: 0.0358 - val_mse: 31.2011\n",
      "Epoch 632/1000\n",
      "0s - loss: 0.0321 - mse: 27.9261 - val_loss: 0.0358 - val_mse: 31.1647\n",
      "Epoch 633/1000\n",
      "0s - loss: 0.0320 - mse: 27.8918 - val_loss: 0.0357 - val_mse: 31.1259\n",
      "Epoch 634/1000\n",
      "0s - loss: 0.0320 - mse: 27.8576 - val_loss: 0.0357 - val_mse: 31.0927\n",
      "Epoch 635/1000\n",
      "0s - loss: 0.0319 - mse: 27.8229 - val_loss: 0.0357 - val_mse: 31.0576\n",
      "Epoch 636/1000\n",
      "0s - loss: 0.0319 - mse: 27.7884 - val_loss: 0.0356 - val_mse: 31.0232\n",
      "Epoch 637/1000\n",
      "0s - loss: 0.0319 - mse: 27.7545 - val_loss: 0.0356 - val_mse: 30.9914\n",
      "Epoch 638/1000\n",
      "0s - loss: 0.0318 - mse: 27.7206 - val_loss: 0.0355 - val_mse: 30.9528\n",
      "Epoch 639/1000\n",
      "0s - loss: 0.0318 - mse: 27.6863 - val_loss: 0.0355 - val_mse: 30.9163\n",
      "Epoch 640/1000\n",
      "0s - loss: 0.0318 - mse: 27.6522 - val_loss: 0.0355 - val_mse: 30.8808\n",
      "Epoch 641/1000\n",
      "0s - loss: 0.0317 - mse: 27.6183 - val_loss: 0.0354 - val_mse: 30.8421\n",
      "Epoch 642/1000\n",
      "0s - loss: 0.0317 - mse: 27.5845 - val_loss: 0.0354 - val_mse: 30.8076\n",
      "Epoch 643/1000\n",
      "0s - loss: 0.0316 - mse: 27.5502 - val_loss: 0.0353 - val_mse: 30.7714\n",
      "Epoch 644/1000\n",
      "0s - loss: 0.0316 - mse: 27.5163 - val_loss: 0.0353 - val_mse: 30.7326\n",
      "Epoch 645/1000\n",
      "0s - loss: 0.0316 - mse: 27.4828 - val_loss: 0.0353 - val_mse: 30.6977\n",
      "Epoch 646/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0315 - mse: 27.4489 - val_loss: 0.0352 - val_mse: 30.6600\n",
      "Epoch 647/1000\n",
      "0s - loss: 0.0315 - mse: 27.4149 - val_loss: 0.0352 - val_mse: 30.6212\n",
      "Epoch 648/1000\n",
      "0s - loss: 0.0314 - mse: 27.3810 - val_loss: 0.0351 - val_mse: 30.5871\n",
      "Epoch 649/1000\n",
      "0s - loss: 0.0314 - mse: 27.3467 - val_loss: 0.0351 - val_mse: 30.5498\n",
      "Epoch 650/1000\n",
      "0s - loss: 0.0314 - mse: 27.3103 - val_loss: 0.0350 - val_mse: 30.5124\n",
      "Epoch 651/1000\n",
      "0s - loss: 0.0313 - mse: 27.2718 - val_loss: 0.0350 - val_mse: 30.4765\n",
      "Epoch 652/1000\n",
      "0s - loss: 0.0313 - mse: 27.2322 - val_loss: 0.0350 - val_mse: 30.4374\n",
      "Epoch 653/1000\n",
      "0s - loss: 0.0312 - mse: 27.1923 - val_loss: 0.0349 - val_mse: 30.3972\n",
      "Epoch 654/1000\n",
      "0s - loss: 0.0312 - mse: 27.1539 - val_loss: 0.0349 - val_mse: 30.3582\n",
      "Epoch 655/1000\n",
      "0s - loss: 0.0311 - mse: 27.1169 - val_loss: 0.0348 - val_mse: 30.3165\n",
      "Epoch 656/1000\n",
      "0s - loss: 0.0311 - mse: 27.0796 - val_loss: 0.0348 - val_mse: 30.2750\n",
      "Epoch 657/1000\n",
      "0s - loss: 0.0311 - mse: 27.0435 - val_loss: 0.0347 - val_mse: 30.2364\n",
      "Epoch 658/1000\n",
      "0s - loss: 0.0310 - mse: 27.0079 - val_loss: 0.0347 - val_mse: 30.1958\n",
      "Epoch 659/1000\n",
      "0s - loss: 0.0310 - mse: 26.9724 - val_loss: 0.0346 - val_mse: 30.1564\n",
      "Epoch 660/1000\n",
      "0s - loss: 0.0309 - mse: 26.9371 - val_loss: 0.0346 - val_mse: 30.1201\n",
      "Epoch 661/1000\n",
      "0s - loss: 0.0309 - mse: 26.9012 - val_loss: 0.0345 - val_mse: 30.0831\n",
      "Epoch 662/1000\n",
      "0s - loss: 0.0309 - mse: 26.8656 - val_loss: 0.0345 - val_mse: 30.0452\n",
      "Epoch 663/1000\n",
      "0s - loss: 0.0308 - mse: 26.8301 - val_loss: 0.0345 - val_mse: 30.0075\n",
      "Epoch 664/1000\n",
      "0s - loss: 0.0308 - mse: 26.7940 - val_loss: 0.0344 - val_mse: 29.9683\n",
      "Epoch 665/1000\n",
      "0s - loss: 0.0307 - mse: 26.7571 - val_loss: 0.0344 - val_mse: 29.9304\n",
      "Epoch 666/1000\n",
      "0s - loss: 0.0307 - mse: 26.7202 - val_loss: 0.0343 - val_mse: 29.8947\n",
      "Epoch 667/1000\n",
      "0s - loss: 0.0306 - mse: 26.6831 - val_loss: 0.0343 - val_mse: 29.8575\n",
      "Epoch 668/1000\n",
      "0s - loss: 0.0306 - mse: 26.6459 - val_loss: 0.0342 - val_mse: 29.8209\n",
      "Epoch 669/1000\n",
      "0s - loss: 0.0306 - mse: 26.6091 - val_loss: 0.0342 - val_mse: 29.7813\n",
      "Epoch 670/1000\n",
      "0s - loss: 0.0305 - mse: 26.5723 - val_loss: 0.0341 - val_mse: 29.7359\n",
      "Epoch 671/1000\n",
      "0s - loss: 0.0305 - mse: 26.5357 - val_loss: 0.0341 - val_mse: 29.6892\n",
      "Epoch 672/1000\n",
      "0s - loss: 0.0304 - mse: 26.4990 - val_loss: 0.0340 - val_mse: 29.6441\n",
      "Epoch 673/1000\n",
      "0s - loss: 0.0304 - mse: 26.4623 - val_loss: 0.0340 - val_mse: 29.5997\n",
      "Epoch 674/1000\n",
      "0s - loss: 0.0303 - mse: 26.4259 - val_loss: 0.0339 - val_mse: 29.5611\n",
      "Epoch 675/1000\n",
      "0s - loss: 0.0303 - mse: 26.3895 - val_loss: 0.0339 - val_mse: 29.5270\n",
      "Epoch 676/1000\n",
      "0s - loss: 0.0303 - mse: 26.3534 - val_loss: 0.0339 - val_mse: 29.4917\n",
      "Epoch 677/1000\n",
      "0s - loss: 0.0302 - mse: 26.3177 - val_loss: 0.0338 - val_mse: 29.4580\n",
      "Epoch 678/1000\n",
      "0s - loss: 0.0302 - mse: 26.2818 - val_loss: 0.0338 - val_mse: 29.4219\n",
      "Epoch 679/1000\n",
      "0s - loss: 0.0301 - mse: 26.2459 - val_loss: 0.0337 - val_mse: 29.3826\n",
      "Epoch 680/1000\n",
      "0s - loss: 0.0301 - mse: 26.2104 - val_loss: 0.0337 - val_mse: 29.3464\n",
      "Epoch 681/1000\n",
      "0s - loss: 0.0301 - mse: 26.1746 - val_loss: 0.0337 - val_mse: 29.3113\n",
      "Epoch 682/1000\n",
      "0s - loss: 0.0300 - mse: 26.1389 - val_loss: 0.0336 - val_mse: 29.2729\n",
      "Epoch 683/1000\n",
      "0s - loss: 0.0300 - mse: 26.1031 - val_loss: 0.0336 - val_mse: 29.2364\n",
      "Epoch 684/1000\n",
      "0s - loss: 0.0299 - mse: 26.0672 - val_loss: 0.0335 - val_mse: 29.1985\n",
      "Epoch 685/1000\n",
      "0s - loss: 0.0299 - mse: 26.0317 - val_loss: 0.0335 - val_mse: 29.1570\n",
      "Epoch 686/1000\n",
      "0s - loss: 0.0299 - mse: 25.9958 - val_loss: 0.0334 - val_mse: 29.1189\n",
      "Epoch 687/1000\n",
      "1s - loss: 0.0298 - mse: 25.9601 - val_loss: 0.0334 - val_mse: 29.0814\n",
      "Epoch 688/1000\n",
      "0s - loss: 0.0298 - mse: 25.9248 - val_loss: 0.0334 - val_mse: 29.0444\n",
      "Epoch 689/1000\n",
      "0s - loss: 0.0297 - mse: 25.8899 - val_loss: 0.0333 - val_mse: 29.0112\n",
      "Epoch 690/1000\n",
      "0s - loss: 0.0297 - mse: 25.8547 - val_loss: 0.0333 - val_mse: 28.9785\n",
      "Epoch 691/1000\n",
      "0s - loss: 0.0296 - mse: 25.8196 - val_loss: 0.0332 - val_mse: 28.9440\n",
      "Epoch 692/1000\n",
      "0s - loss: 0.0296 - mse: 25.7850 - val_loss: 0.0332 - val_mse: 28.9109\n",
      "Epoch 693/1000\n",
      "0s - loss: 0.0296 - mse: 25.7516 - val_loss: 0.0332 - val_mse: 28.8785\n",
      "Epoch 694/1000\n",
      "0s - loss: 0.0295 - mse: 25.7188 - val_loss: 0.0331 - val_mse: 28.8454\n",
      "Epoch 695/1000\n",
      "0s - loss: 0.0295 - mse: 25.6860 - val_loss: 0.0331 - val_mse: 28.8097\n",
      "Epoch 696/1000\n",
      "0s - loss: 0.0295 - mse: 25.6532 - val_loss: 0.0330 - val_mse: 28.7733\n",
      "Epoch 697/1000\n",
      "0s - loss: 0.0294 - mse: 25.6203 - val_loss: 0.0330 - val_mse: 28.7355\n",
      "Epoch 698/1000\n",
      "0s - loss: 0.0294 - mse: 25.5874 - val_loss: 0.0330 - val_mse: 28.6992\n",
      "Epoch 699/1000\n",
      "0s - loss: 0.0293 - mse: 25.5545 - val_loss: 0.0329 - val_mse: 28.6644\n",
      "Epoch 700/1000\n",
      "0s - loss: 0.0293 - mse: 25.5216 - val_loss: 0.0329 - val_mse: 28.6328\n",
      "Epoch 701/1000\n",
      "0s - loss: 0.0293 - mse: 25.4888 - val_loss: 0.0328 - val_mse: 28.6025\n",
      "Epoch 702/1000\n",
      "0s - loss: 0.0292 - mse: 25.4560 - val_loss: 0.0328 - val_mse: 28.5715\n",
      "Epoch 703/1000\n",
      "0s - loss: 0.0292 - mse: 25.4230 - val_loss: 0.0328 - val_mse: 28.5399\n",
      "Epoch 704/1000\n",
      "0s - loss: 0.0292 - mse: 25.3906 - val_loss: 0.0327 - val_mse: 28.5083\n",
      "Epoch 705/1000\n",
      "0s - loss: 0.0291 - mse: 25.3584 - val_loss: 0.0327 - val_mse: 28.4774\n",
      "Epoch 706/1000\n",
      "0s - loss: 0.0291 - mse: 25.3262 - val_loss: 0.0327 - val_mse: 28.4467\n",
      "Epoch 707/1000\n",
      "0s - loss: 0.0290 - mse: 25.2939 - val_loss: 0.0326 - val_mse: 28.4160\n",
      "Epoch 708/1000\n",
      "0s - loss: 0.0290 - mse: 25.2618 - val_loss: 0.0326 - val_mse: 28.3814\n",
      "Epoch 709/1000\n",
      "0s - loss: 0.0290 - mse: 25.2298 - val_loss: 0.0326 - val_mse: 28.3470\n",
      "Epoch 710/1000\n",
      "0s - loss: 0.0289 - mse: 25.1982 - val_loss: 0.0325 - val_mse: 28.3152\n",
      "Epoch 711/1000\n",
      "0s - loss: 0.0289 - mse: 25.1670 - val_loss: 0.0325 - val_mse: 28.2863\n",
      "Epoch 712/1000\n",
      "0s - loss: 0.0289 - mse: 25.1357 - val_loss: 0.0324 - val_mse: 28.2548\n",
      "Epoch 713/1000\n",
      "0s - loss: 0.0288 - mse: 25.1045 - val_loss: 0.0324 - val_mse: 28.2226\n",
      "Epoch 714/1000\n",
      "0s - loss: 0.0288 - mse: 25.0733 - val_loss: 0.0324 - val_mse: 28.1921\n",
      "Epoch 715/1000\n",
      "0s - loss: 0.0288 - mse: 25.0417 - val_loss: 0.0323 - val_mse: 28.1608\n",
      "Epoch 716/1000\n",
      "0s - loss: 0.0287 - mse: 25.0099 - val_loss: 0.0323 - val_mse: 28.1296\n",
      "Epoch 717/1000\n",
      "0s - loss: 0.0287 - mse: 24.9780 - val_loss: 0.0323 - val_mse: 28.0992\n",
      "Epoch 718/1000\n",
      "0s - loss: 0.0286 - mse: 24.9462 - val_loss: 0.0322 - val_mse: 28.0585\n",
      "Epoch 719/1000\n",
      "0s - loss: 0.0286 - mse: 24.9145 - val_loss: 0.0322 - val_mse: 28.0225\n",
      "Epoch 720/1000\n",
      "0s - loss: 0.0286 - mse: 24.8826 - val_loss: 0.0321 - val_mse: 27.9917\n",
      "Epoch 721/1000\n",
      "0s - loss: 0.0285 - mse: 24.8509 - val_loss: 0.0321 - val_mse: 27.9522\n",
      "Epoch 722/1000\n",
      "0s - loss: 0.0285 - mse: 24.8187 - val_loss: 0.0321 - val_mse: 27.9162\n",
      "Epoch 723/1000\n",
      "0s - loss: 0.0285 - mse: 24.7868 - val_loss: 0.0320 - val_mse: 27.8839\n",
      "Epoch 724/1000\n",
      "0s - loss: 0.0284 - mse: 24.7552 - val_loss: 0.0320 - val_mse: 27.8446\n",
      "Epoch 725/1000\n",
      "0s - loss: 0.0284 - mse: 24.7230 - val_loss: 0.0319 - val_mse: 27.8132\n",
      "Epoch 726/1000\n",
      "0s - loss: 0.0284 - mse: 24.6912 - val_loss: 0.0319 - val_mse: 27.7778\n",
      "Epoch 727/1000\n",
      "0s - loss: 0.0283 - mse: 24.6589 - val_loss: 0.0319 - val_mse: 27.7473\n",
      "Epoch 728/1000\n",
      "0s - loss: 0.0283 - mse: 24.6272 - val_loss: 0.0318 - val_mse: 27.7124\n",
      "Epoch 729/1000\n",
      "0s - loss: 0.0282 - mse: 24.5956 - val_loss: 0.0318 - val_mse: 27.6820\n",
      "Epoch 730/1000\n",
      "0s - loss: 0.0282 - mse: 24.5645 - val_loss: 0.0318 - val_mse: 27.6537\n",
      "Epoch 731/1000\n",
      "0s - loss: 0.0282 - mse: 24.5343 - val_loss: 0.0317 - val_mse: 27.6184\n",
      "Epoch 732/1000\n",
      "0s - loss: 0.0281 - mse: 24.5033 - val_loss: 0.0317 - val_mse: 27.5858\n",
      "Epoch 733/1000\n",
      "0s - loss: 0.0281 - mse: 24.4741 - val_loss: 0.0316 - val_mse: 27.5605\n",
      "Epoch 734/1000\n",
      "0s - loss: 0.0281 - mse: 24.4439 - val_loss: 0.0316 - val_mse: 27.5342\n",
      "Epoch 735/1000\n",
      "0s - loss: 0.0280 - mse: 24.4152 - val_loss: 0.0316 - val_mse: 27.5028\n",
      "Epoch 736/1000\n",
      "0s - loss: 0.0280 - mse: 24.3862 - val_loss: 0.0316 - val_mse: 27.4753\n",
      "Epoch 737/1000\n",
      "0s - loss: 0.0280 - mse: 24.3581 - val_loss: 0.0315 - val_mse: 27.4436\n",
      "Epoch 738/1000\n",
      "0s - loss: 0.0279 - mse: 24.3302 - val_loss: 0.0315 - val_mse: 27.4122\n",
      "Epoch 739/1000\n",
      "0s - loss: 0.0279 - mse: 24.3033 - val_loss: 0.0314 - val_mse: 27.3861\n",
      "Epoch 740/1000\n",
      "0s - loss: 0.0279 - mse: 24.2757 - val_loss: 0.0314 - val_mse: 27.3568\n",
      "Epoch 741/1000\n",
      "0s - loss: 0.0278 - mse: 24.2490 - val_loss: 0.0314 - val_mse: 27.3252\n",
      "Epoch 742/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0278 - mse: 24.2227 - val_loss: 0.0313 - val_mse: 27.2977\n",
      "Epoch 743/1000\n",
      "0s - loss: 0.0278 - mse: 24.1962 - val_loss: 0.0313 - val_mse: 27.2732\n",
      "Epoch 744/1000\n",
      "0s - loss: 0.0278 - mse: 24.1708 - val_loss: 0.0313 - val_mse: 27.2409\n",
      "Epoch 745/1000\n",
      "0s - loss: 0.0277 - mse: 24.1442 - val_loss: 0.0312 - val_mse: 27.2067\n",
      "Epoch 746/1000\n",
      "0s - loss: 0.0277 - mse: 24.1187 - val_loss: 0.0312 - val_mse: 27.1808\n",
      "Epoch 747/1000\n",
      "0s - loss: 0.0277 - mse: 24.0920 - val_loss: 0.0312 - val_mse: 27.1630\n",
      "Epoch 748/1000\n",
      "0s - loss: 0.0276 - mse: 24.0665 - val_loss: 0.0312 - val_mse: 27.1358\n",
      "Epoch 749/1000\n",
      "0s - loss: 0.0276 - mse: 24.0404 - val_loss: 0.0311 - val_mse: 27.1000\n",
      "Epoch 750/1000\n",
      "0s - loss: 0.0276 - mse: 24.0134 - val_loss: 0.0311 - val_mse: 27.0640\n",
      "Epoch 751/1000\n",
      "0s - loss: 0.0275 - mse: 23.9878 - val_loss: 0.0310 - val_mse: 27.0368\n",
      "Epoch 752/1000\n",
      "0s - loss: 0.0275 - mse: 23.9606 - val_loss: 0.0310 - val_mse: 27.0203\n",
      "Epoch 753/1000\n",
      "0s - loss: 0.0275 - mse: 23.9357 - val_loss: 0.0310 - val_mse: 26.9937\n",
      "Epoch 754/1000\n",
      "0s - loss: 0.0275 - mse: 23.9099 - val_loss: 0.0310 - val_mse: 26.9551\n",
      "Epoch 755/1000\n",
      "0s - loss: 0.0274 - mse: 23.8828 - val_loss: 0.0309 - val_mse: 26.9183\n",
      "Epoch 756/1000\n",
      "0s - loss: 0.0274 - mse: 23.8573 - val_loss: 0.0309 - val_mse: 26.8960\n",
      "Epoch 757/1000\n",
      "0s - loss: 0.0274 - mse: 23.8304 - val_loss: 0.0309 - val_mse: 26.8765\n",
      "Epoch 758/1000\n",
      "0s - loss: 0.0273 - mse: 23.8046 - val_loss: 0.0308 - val_mse: 26.8508\n",
      "Epoch 759/1000\n",
      "0s - loss: 0.0273 - mse: 23.7786 - val_loss: 0.0308 - val_mse: 26.8189\n",
      "Epoch 760/1000\n",
      "0s - loss: 0.0273 - mse: 23.7530 - val_loss: 0.0308 - val_mse: 26.7937\n",
      "Epoch 761/1000\n",
      "0s - loss: 0.0272 - mse: 23.7270 - val_loss: 0.0307 - val_mse: 26.7727\n",
      "Epoch 762/1000\n",
      "0s - loss: 0.0272 - mse: 23.7014 - val_loss: 0.0307 - val_mse: 26.7416\n",
      "Epoch 763/1000\n",
      "0s - loss: 0.0272 - mse: 23.6755 - val_loss: 0.0307 - val_mse: 26.7146\n",
      "Epoch 764/1000\n",
      "0s - loss: 0.0272 - mse: 23.6501 - val_loss: 0.0306 - val_mse: 26.6807\n",
      "Epoch 765/1000\n",
      "0s - loss: 0.0271 - mse: 23.6249 - val_loss: 0.0306 - val_mse: 26.6529\n",
      "Epoch 766/1000\n",
      "0s - loss: 0.0271 - mse: 23.5992 - val_loss: 0.0306 - val_mse: 26.6313\n",
      "Epoch 767/1000\n",
      "0s - loss: 0.0271 - mse: 23.5739 - val_loss: 0.0305 - val_mse: 26.5970\n",
      "Epoch 768/1000\n",
      "0s - loss: 0.0270 - mse: 23.5486 - val_loss: 0.0305 - val_mse: 26.5673\n",
      "Epoch 769/1000\n",
      "0s - loss: 0.0270 - mse: 23.5233 - val_loss: 0.0305 - val_mse: 26.5438\n",
      "Epoch 770/1000\n",
      "0s - loss: 0.0270 - mse: 23.4980 - val_loss: 0.0304 - val_mse: 26.5137\n",
      "Epoch 771/1000\n",
      "0s - loss: 0.0270 - mse: 23.4730 - val_loss: 0.0304 - val_mse: 26.4890\n",
      "Epoch 772/1000\n",
      "0s - loss: 0.0269 - mse: 23.4478 - val_loss: 0.0304 - val_mse: 26.4560\n",
      "Epoch 773/1000\n",
      "0s - loss: 0.0269 - mse: 23.4229 - val_loss: 0.0304 - val_mse: 26.4323\n",
      "Epoch 774/1000\n",
      "0s - loss: 0.0269 - mse: 23.3978 - val_loss: 0.0303 - val_mse: 26.4106\n",
      "Epoch 775/1000\n",
      "0s - loss: 0.0268 - mse: 23.3730 - val_loss: 0.0303 - val_mse: 26.3838\n",
      "Epoch 776/1000\n",
      "0s - loss: 0.0268 - mse: 23.3484 - val_loss: 0.0303 - val_mse: 26.3467\n",
      "Epoch 777/1000\n",
      "0s - loss: 0.0268 - mse: 23.3241 - val_loss: 0.0302 - val_mse: 26.3209\n",
      "Epoch 778/1000\n",
      "0s - loss: 0.0268 - mse: 23.2991 - val_loss: 0.0302 - val_mse: 26.3037\n",
      "Epoch 779/1000\n",
      "0s - loss: 0.0267 - mse: 23.2739 - val_loss: 0.0302 - val_mse: 26.2751\n",
      "Epoch 780/1000\n",
      "0s - loss: 0.0267 - mse: 23.2491 - val_loss: 0.0301 - val_mse: 26.2464\n",
      "Epoch 781/1000\n",
      "0s - loss: 0.0267 - mse: 23.2247 - val_loss: 0.0301 - val_mse: 26.2155\n",
      "Epoch 782/1000\n",
      "0s - loss: 0.0266 - mse: 23.2003 - val_loss: 0.0301 - val_mse: 26.1893\n",
      "Epoch 783/1000\n",
      "0s - loss: 0.0266 - mse: 23.1763 - val_loss: 0.0300 - val_mse: 26.1563\n",
      "Epoch 784/1000\n",
      "0s - loss: 0.0266 - mse: 23.1520 - val_loss: 0.0300 - val_mse: 26.1294\n",
      "Epoch 785/1000\n",
      "0s - loss: 0.0266 - mse: 23.1276 - val_loss: 0.0300 - val_mse: 26.1107\n",
      "Epoch 786/1000\n",
      "0s - loss: 0.0265 - mse: 23.1041 - val_loss: 0.0299 - val_mse: 26.0775\n",
      "Epoch 787/1000\n",
      "0s - loss: 0.0265 - mse: 23.0794 - val_loss: 0.0299 - val_mse: 26.0498\n",
      "Epoch 788/1000\n",
      "0s - loss: 0.0265 - mse: 23.0555 - val_loss: 0.0299 - val_mse: 26.0271\n",
      "Epoch 789/1000\n",
      "0s - loss: 0.0264 - mse: 23.0314 - val_loss: 0.0299 - val_mse: 25.9956\n",
      "Epoch 790/1000\n",
      "0s - loss: 0.0264 - mse: 23.0079 - val_loss: 0.0298 - val_mse: 25.9756\n",
      "Epoch 791/1000\n",
      "0s - loss: 0.0264 - mse: 22.9839 - val_loss: 0.0298 - val_mse: 25.9431\n",
      "Epoch 792/1000\n",
      "0s - loss: 0.0264 - mse: 22.9604 - val_loss: 0.0298 - val_mse: 25.9207\n",
      "Epoch 793/1000\n",
      "0s - loss: 0.0263 - mse: 22.9362 - val_loss: 0.0297 - val_mse: 25.9028\n",
      "Epoch 794/1000\n",
      "0s - loss: 0.0263 - mse: 22.9137 - val_loss: 0.0297 - val_mse: 25.8629\n",
      "Epoch 795/1000\n",
      "0s - loss: 0.0263 - mse: 22.8892 - val_loss: 0.0297 - val_mse: 25.8400\n",
      "Epoch 796/1000\n",
      "0s - loss: 0.0263 - mse: 22.8651 - val_loss: 0.0296 - val_mse: 25.8108\n",
      "Epoch 797/1000\n",
      "0s - loss: 0.0262 - mse: 22.8414 - val_loss: 0.0296 - val_mse: 25.7912\n",
      "Epoch 798/1000\n",
      "0s - loss: 0.0262 - mse: 22.8186 - val_loss: 0.0296 - val_mse: 25.7523\n",
      "Epoch 799/1000\n",
      "0s - loss: 0.0262 - mse: 22.7950 - val_loss: 0.0295 - val_mse: 25.7258\n",
      "Epoch 800/1000\n",
      "0s - loss: 0.0261 - mse: 22.7716 - val_loss: 0.0295 - val_mse: 25.7072\n",
      "Epoch 801/1000\n",
      "0s - loss: 0.0261 - mse: 22.7478 - val_loss: 0.0295 - val_mse: 25.6760\n",
      "Epoch 802/1000\n",
      "0s - loss: 0.0261 - mse: 22.7245 - val_loss: 0.0295 - val_mse: 25.6495\n",
      "Epoch 803/1000\n",
      "0s - loss: 0.0261 - mse: 22.7016 - val_loss: 0.0294 - val_mse: 25.6247\n",
      "Epoch 804/1000\n",
      "0s - loss: 0.0260 - mse: 22.6783 - val_loss: 0.0294 - val_mse: 25.6041\n",
      "Epoch 805/1000\n",
      "0s - loss: 0.0260 - mse: 22.6556 - val_loss: 0.0294 - val_mse: 25.5686\n",
      "Epoch 806/1000\n",
      "0s - loss: 0.0260 - mse: 22.6321 - val_loss: 0.0293 - val_mse: 25.5439\n",
      "Epoch 807/1000\n",
      "0s - loss: 0.0260 - mse: 22.6086 - val_loss: 0.0293 - val_mse: 25.5285\n",
      "Epoch 808/1000\n",
      "0s - loss: 0.0259 - mse: 22.5867 - val_loss: 0.0293 - val_mse: 25.4927\n",
      "Epoch 809/1000\n",
      "0s - loss: 0.0259 - mse: 22.5621 - val_loss: 0.0292 - val_mse: 25.4683\n",
      "Epoch 810/1000\n",
      "0s - loss: 0.0259 - mse: 22.5389 - val_loss: 0.0292 - val_mse: 25.4469\n",
      "Epoch 811/1000\n",
      "0s - loss: 0.0259 - mse: 22.5169 - val_loss: 0.0292 - val_mse: 25.4101\n",
      "Epoch 812/1000\n",
      "0s - loss: 0.0258 - mse: 22.4943 - val_loss: 0.0292 - val_mse: 25.3867\n",
      "Epoch 813/1000\n",
      "0s - loss: 0.0258 - mse: 22.4714 - val_loss: 0.0291 - val_mse: 25.3704\n",
      "Epoch 814/1000\n",
      "0s - loss: 0.0258 - mse: 22.4494 - val_loss: 0.0291 - val_mse: 25.3359\n",
      "Epoch 815/1000\n",
      "0s - loss: 0.0258 - mse: 22.4261 - val_loss: 0.0291 - val_mse: 25.3071\n",
      "Epoch 816/1000\n",
      "0s - loss: 0.0257 - mse: 22.4037 - val_loss: 0.0290 - val_mse: 25.2790\n",
      "Epoch 817/1000\n",
      "0s - loss: 0.0257 - mse: 22.3806 - val_loss: 0.0290 - val_mse: 25.2595\n",
      "Epoch 818/1000\n",
      "0s - loss: 0.0257 - mse: 22.3576 - val_loss: 0.0290 - val_mse: 25.2396\n",
      "Epoch 819/1000\n",
      "0s - loss: 0.0256 - mse: 22.3352 - val_loss: 0.0289 - val_mse: 25.2084\n",
      "Epoch 820/1000\n",
      "0s - loss: 0.0256 - mse: 22.3125 - val_loss: 0.0289 - val_mse: 25.1827\n",
      "Epoch 821/1000\n",
      "0s - loss: 0.0256 - mse: 22.2903 - val_loss: 0.0289 - val_mse: 25.1602\n",
      "Epoch 822/1000\n",
      "0s - loss: 0.0256 - mse: 22.2687 - val_loss: 0.0289 - val_mse: 25.1249\n",
      "Epoch 823/1000\n",
      "0s - loss: 0.0255 - mse: 22.2465 - val_loss: 0.0288 - val_mse: 25.1033\n",
      "Epoch 824/1000\n",
      "0s - loss: 0.0255 - mse: 22.2242 - val_loss: 0.0288 - val_mse: 25.0793\n",
      "Epoch 825/1000\n",
      "0s - loss: 0.0255 - mse: 22.2024 - val_loss: 0.0288 - val_mse: 25.0503\n",
      "Epoch 826/1000\n",
      "0s - loss: 0.0255 - mse: 22.1807 - val_loss: 0.0287 - val_mse: 25.0252\n",
      "Epoch 827/1000\n",
      "0s - loss: 0.0254 - mse: 22.1588 - val_loss: 0.0287 - val_mse: 24.9975\n",
      "Epoch 828/1000\n",
      "0s - loss: 0.0254 - mse: 22.1373 - val_loss: 0.0287 - val_mse: 24.9760\n",
      "Epoch 829/1000\n",
      "0s - loss: 0.0254 - mse: 22.1157 - val_loss: 0.0287 - val_mse: 24.9597\n",
      "Epoch 830/1000\n",
      "0s - loss: 0.0254 - mse: 22.0948 - val_loss: 0.0286 - val_mse: 24.9230\n",
      "Epoch 831/1000\n",
      "0s - loss: 0.0253 - mse: 22.0737 - val_loss: 0.0286 - val_mse: 24.9022\n",
      "Epoch 832/1000\n",
      "0s - loss: 0.0253 - mse: 22.0524 - val_loss: 0.0286 - val_mse: 24.8876\n",
      "Epoch 833/1000\n",
      "0s - loss: 0.0253 - mse: 22.0315 - val_loss: 0.0285 - val_mse: 24.8624\n",
      "Epoch 834/1000\n",
      "0s - loss: 0.0253 - mse: 22.0107 - val_loss: 0.0285 - val_mse: 24.8329\n",
      "Epoch 835/1000\n",
      "0s - loss: 0.0253 - mse: 21.9896 - val_loss: 0.0285 - val_mse: 24.8054\n",
      "Epoch 836/1000\n",
      "0s - loss: 0.0252 - mse: 21.9693 - val_loss: 0.0285 - val_mse: 24.7849\n",
      "Epoch 837/1000\n",
      "0s - loss: 0.0252 - mse: 21.9486 - val_loss: 0.0284 - val_mse: 24.7728\n",
      "Epoch 838/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0252 - mse: 21.9286 - val_loss: 0.0284 - val_mse: 24.7424\n",
      "Epoch 839/1000\n",
      "0s - loss: 0.0252 - mse: 21.9077 - val_loss: 0.0284 - val_mse: 24.7162\n",
      "Epoch 840/1000\n",
      "0s - loss: 0.0251 - mse: 21.8877 - val_loss: 0.0284 - val_mse: 24.6993\n",
      "Epoch 841/1000\n",
      "0s - loss: 0.0251 - mse: 21.8674 - val_loss: 0.0283 - val_mse: 24.6836\n",
      "Epoch 842/1000\n",
      "0s - loss: 0.0251 - mse: 21.8474 - val_loss: 0.0283 - val_mse: 24.6555\n",
      "Epoch 843/1000\n",
      "0s - loss: 0.0251 - mse: 21.8266 - val_loss: 0.0283 - val_mse: 24.6341\n",
      "Epoch 844/1000\n",
      "0s - loss: 0.0250 - mse: 21.8069 - val_loss: 0.0283 - val_mse: 24.6122\n",
      "Epoch 845/1000\n",
      "0s - loss: 0.0250 - mse: 21.7869 - val_loss: 0.0282 - val_mse: 24.5876\n",
      "Epoch 846/1000\n",
      "0s - loss: 0.0250 - mse: 21.7663 - val_loss: 0.0282 - val_mse: 24.5667\n",
      "Epoch 847/1000\n",
      "0s - loss: 0.0250 - mse: 21.7467 - val_loss: 0.0282 - val_mse: 24.5423\n",
      "Epoch 848/1000\n",
      "0s - loss: 0.0249 - mse: 21.7272 - val_loss: 0.0282 - val_mse: 24.5178\n",
      "Epoch 849/1000\n",
      "0s - loss: 0.0249 - mse: 21.7073 - val_loss: 0.0281 - val_mse: 24.4983\n",
      "Epoch 850/1000\n",
      "0s - loss: 0.0249 - mse: 21.6878 - val_loss: 0.0281 - val_mse: 24.4744\n",
      "Epoch 851/1000\n",
      "0s - loss: 0.0249 - mse: 21.6683 - val_loss: 0.0281 - val_mse: 24.4648\n",
      "Epoch 852/1000\n",
      "0s - loss: 0.0249 - mse: 21.6487 - val_loss: 0.0281 - val_mse: 24.4337\n",
      "Epoch 853/1000\n",
      "0s - loss: 0.0248 - mse: 21.6289 - val_loss: 0.0280 - val_mse: 24.4118\n",
      "Epoch 854/1000\n",
      "0s - loss: 0.0248 - mse: 21.6097 - val_loss: 0.0280 - val_mse: 24.3966\n",
      "Epoch 855/1000\n",
      "0s - loss: 0.0248 - mse: 21.5910 - val_loss: 0.0280 - val_mse: 24.3616\n",
      "Epoch 856/1000\n",
      "0s - loss: 0.0248 - mse: 21.5725 - val_loss: 0.0280 - val_mse: 24.3504\n",
      "Epoch 857/1000\n",
      "0s - loss: 0.0247 - mse: 21.5531 - val_loss: 0.0279 - val_mse: 24.3302\n",
      "Epoch 858/1000\n",
      "0s - loss: 0.0247 - mse: 21.5346 - val_loss: 0.0279 - val_mse: 24.3065\n",
      "Epoch 859/1000\n",
      "0s - loss: 0.0247 - mse: 21.5156 - val_loss: 0.0279 - val_mse: 24.2809\n",
      "Epoch 860/1000\n",
      "0s - loss: 0.0247 - mse: 21.4970 - val_loss: 0.0278 - val_mse: 24.2522\n",
      "Epoch 861/1000\n",
      "0s - loss: 0.0247 - mse: 21.4786 - val_loss: 0.0278 - val_mse: 24.2442\n",
      "Epoch 862/1000\n",
      "0s - loss: 0.0246 - mse: 21.4600 - val_loss: 0.0278 - val_mse: 24.2158\n",
      "Epoch 863/1000\n",
      "0s - loss: 0.0246 - mse: 21.4414 - val_loss: 0.0278 - val_mse: 24.1921\n",
      "Epoch 864/1000\n",
      "0s - loss: 0.0246 - mse: 21.4232 - val_loss: 0.0278 - val_mse: 24.1713\n",
      "Epoch 865/1000\n",
      "0s - loss: 0.0246 - mse: 21.4051 - val_loss: 0.0277 - val_mse: 24.1495\n",
      "Epoch 866/1000\n",
      "0s - loss: 0.0246 - mse: 21.3871 - val_loss: 0.0277 - val_mse: 24.1343\n",
      "Epoch 867/1000\n",
      "0s - loss: 0.0245 - mse: 21.3692 - val_loss: 0.0277 - val_mse: 24.1059\n",
      "Epoch 868/1000\n",
      "0s - loss: 0.0245 - mse: 21.3516 - val_loss: 0.0277 - val_mse: 24.0907\n",
      "Epoch 869/1000\n",
      "0s - loss: 0.0245 - mse: 21.3337 - val_loss: 0.0276 - val_mse: 24.0734\n",
      "Epoch 870/1000\n",
      "0s - loss: 0.0245 - mse: 21.3162 - val_loss: 0.0276 - val_mse: 24.0507\n",
      "Epoch 871/1000\n",
      "0s - loss: 0.0245 - mse: 21.2985 - val_loss: 0.0276 - val_mse: 24.0284\n",
      "Epoch 872/1000\n",
      "0s - loss: 0.0244 - mse: 21.2811 - val_loss: 0.0276 - val_mse: 24.0059\n",
      "Epoch 873/1000\n",
      "0s - loss: 0.0244 - mse: 21.2636 - val_loss: 0.0276 - val_mse: 23.9918\n",
      "Epoch 874/1000\n",
      "0s - loss: 0.0244 - mse: 21.2459 - val_loss: 0.0275 - val_mse: 23.9725\n",
      "Epoch 875/1000\n",
      "0s - loss: 0.0244 - mse: 21.2285 - val_loss: 0.0275 - val_mse: 23.9506\n",
      "Epoch 876/1000\n",
      "0s - loss: 0.0244 - mse: 21.2108 - val_loss: 0.0275 - val_mse: 23.9269\n",
      "Epoch 877/1000\n",
      "0s - loss: 0.0243 - mse: 21.1932 - val_loss: 0.0275 - val_mse: 23.9058\n",
      "Epoch 878/1000\n",
      "0s - loss: 0.0243 - mse: 21.1755 - val_loss: 0.0274 - val_mse: 23.8909\n",
      "Epoch 879/1000\n",
      "0s - loss: 0.0243 - mse: 21.1580 - val_loss: 0.0274 - val_mse: 23.8674\n",
      "Epoch 880/1000\n",
      "0s - loss: 0.0243 - mse: 21.1406 - val_loss: 0.0274 - val_mse: 23.8487\n",
      "Epoch 881/1000\n",
      "0s - loss: 0.0243 - mse: 21.1233 - val_loss: 0.0274 - val_mse: 23.8234\n",
      "Epoch 882/1000\n",
      "0s - loss: 0.0242 - mse: 21.1059 - val_loss: 0.0273 - val_mse: 23.8038\n",
      "Epoch 883/1000\n",
      "0s - loss: 0.0242 - mse: 21.0885 - val_loss: 0.0273 - val_mse: 23.7841\n",
      "Epoch 884/1000\n",
      "0s - loss: 0.0242 - mse: 21.0712 - val_loss: 0.0273 - val_mse: 23.7590\n",
      "Epoch 885/1000\n",
      "0s - loss: 0.0242 - mse: 21.0541 - val_loss: 0.0273 - val_mse: 23.7447\n",
      "Epoch 886/1000\n",
      "0s - loss: 0.0242 - mse: 21.0370 - val_loss: 0.0272 - val_mse: 23.7179\n",
      "Epoch 887/1000\n",
      "0s - loss: 0.0241 - mse: 21.0198 - val_loss: 0.0272 - val_mse: 23.7069\n",
      "Epoch 888/1000\n",
      "0s - loss: 0.0241 - mse: 21.0026 - val_loss: 0.0272 - val_mse: 23.6807\n",
      "Epoch 889/1000\n",
      "0s - loss: 0.0241 - mse: 20.9854 - val_loss: 0.0272 - val_mse: 23.6617\n",
      "Epoch 890/1000\n",
      "0s - loss: 0.0241 - mse: 20.9681 - val_loss: 0.0271 - val_mse: 23.6403\n",
      "Epoch 891/1000\n",
      "0s - loss: 0.0241 - mse: 20.9508 - val_loss: 0.0271 - val_mse: 23.6156\n",
      "Epoch 892/1000\n",
      "0s - loss: 0.0240 - mse: 20.9336 - val_loss: 0.0271 - val_mse: 23.6055\n",
      "Epoch 893/1000\n",
      "0s - loss: 0.0240 - mse: 20.9167 - val_loss: 0.0271 - val_mse: 23.5719\n",
      "Epoch 894/1000\n",
      "0s - loss: 0.0240 - mse: 20.8999 - val_loss: 0.0271 - val_mse: 23.5670\n",
      "Epoch 895/1000\n",
      "0s - loss: 0.0240 - mse: 20.8827 - val_loss: 0.0270 - val_mse: 23.5306\n",
      "Epoch 896/1000\n",
      "0s - loss: 0.0240 - mse: 20.8655 - val_loss: 0.0270 - val_mse: 23.5263\n",
      "Epoch 897/1000\n",
      "0s - loss: 0.0239 - mse: 20.8480 - val_loss: 0.0270 - val_mse: 23.4946\n",
      "Epoch 898/1000\n",
      "0s - loss: 0.0239 - mse: 20.8306 - val_loss: 0.0270 - val_mse: 23.4824\n",
      "Epoch 899/1000\n",
      "0s - loss: 0.0239 - mse: 20.8133 - val_loss: 0.0269 - val_mse: 23.4601\n",
      "Epoch 900/1000\n",
      "0s - loss: 0.0239 - mse: 20.7961 - val_loss: 0.0269 - val_mse: 23.4409\n",
      "Epoch 901/1000\n",
      "0s - loss: 0.0239 - mse: 20.7791 - val_loss: 0.0269 - val_mse: 23.4276\n",
      "Epoch 902/1000\n",
      "0s - loss: 0.0238 - mse: 20.7621 - val_loss: 0.0269 - val_mse: 23.3948\n",
      "Epoch 903/1000\n",
      "0s - loss: 0.0238 - mse: 20.7449 - val_loss: 0.0269 - val_mse: 23.3918\n",
      "Epoch 904/1000\n",
      "0s - loss: 0.0238 - mse: 20.7282 - val_loss: 0.0268 - val_mse: 23.3542\n",
      "Epoch 905/1000\n",
      "0s - loss: 0.0238 - mse: 20.7112 - val_loss: 0.0268 - val_mse: 23.3603\n",
      "Epoch 906/1000\n",
      "0s - loss: 0.0238 - mse: 20.6941 - val_loss: 0.0268 - val_mse: 23.3116\n",
      "Epoch 907/1000\n",
      "0s - loss: 0.0237 - mse: 20.6768 - val_loss: 0.0268 - val_mse: 23.3110\n",
      "Epoch 908/1000\n",
      "0s - loss: 0.0237 - mse: 20.6593 - val_loss: 0.0267 - val_mse: 23.2771\n",
      "Epoch 909/1000\n",
      "0s - loss: 0.0237 - mse: 20.6415 - val_loss: 0.0267 - val_mse: 23.2688\n",
      "Epoch 910/1000\n",
      "0s - loss: 0.0237 - mse: 20.6242 - val_loss: 0.0267 - val_mse: 23.2449\n",
      "Epoch 911/1000\n",
      "0s - loss: 0.0237 - mse: 20.6072 - val_loss: 0.0267 - val_mse: 23.2217\n",
      "Epoch 912/1000\n",
      "0s - loss: 0.0236 - mse: 20.5906 - val_loss: 0.0267 - val_mse: 23.2128\n",
      "Epoch 913/1000\n",
      "0s - loss: 0.0236 - mse: 20.5740 - val_loss: 0.0266 - val_mse: 23.1879\n",
      "Epoch 914/1000\n",
      "0s - loss: 0.0236 - mse: 20.5580 - val_loss: 0.0266 - val_mse: 23.1900\n",
      "Epoch 915/1000\n",
      "0s - loss: 0.0236 - mse: 20.5423 - val_loss: 0.0266 - val_mse: 23.1474\n",
      "Epoch 916/1000\n",
      "\n",
      "Epoch 00915: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0236 - mse: 20.5271 - val_loss: 0.0266 - val_mse: 23.1668\n",
      "Epoch 917/1000\n",
      "0s - loss: 0.0236 - mse: 20.5123 - val_loss: 0.0266 - val_mse: 23.1222\n",
      "Epoch 918/1000\n",
      "0s - loss: 0.0235 - mse: 20.4990 - val_loss: 0.0265 - val_mse: 23.1172\n",
      "Epoch 919/1000\n",
      "0s - loss: 0.0235 - mse: 20.4870 - val_loss: 0.0266 - val_mse: 23.1335\n",
      "Epoch 920/1000\n",
      "0s - loss: 0.0235 - mse: 20.4773 - val_loss: 0.0265 - val_mse: 23.0937\n",
      "Epoch 921/1000\n",
      "0s - loss: 0.0235 - mse: 20.4642 - val_loss: 0.0265 - val_mse: 23.0780\n",
      "Epoch 922/1000\n",
      "0s - loss: 0.0235 - mse: 20.4532 - val_loss: 0.0265 - val_mse: 23.0944\n",
      "Epoch 923/1000\n",
      "0s - loss: 0.0235 - mse: 20.4427 - val_loss: 0.0265 - val_mse: 23.0666\n",
      "Epoch 924/1000\n",
      "0s - loss: 0.0235 - mse: 20.4302 - val_loss: 0.0265 - val_mse: 23.0515\n",
      "Epoch 925/1000\n",
      "0s - loss: 0.0234 - mse: 20.4200 - val_loss: 0.0265 - val_mse: 23.0679\n",
      "Epoch 926/1000\n",
      "0s - loss: 0.0234 - mse: 20.4092 - val_loss: 0.0265 - val_mse: 23.0405\n",
      "Epoch 927/1000\n",
      "0s - loss: 0.0234 - mse: 20.3973 - val_loss: 0.0264 - val_mse: 23.0202\n",
      "Epoch 928/1000\n",
      "0s - loss: 0.0234 - mse: 20.3872 - val_loss: 0.0265 - val_mse: 23.0355\n",
      "Epoch 929/1000\n",
      "0s - loss: 0.0234 - mse: 20.3763 - val_loss: 0.0264 - val_mse: 23.0176\n",
      "Epoch 930/1000\n",
      "0s - loss: 0.0234 - mse: 20.3649 - val_loss: 0.0264 - val_mse: 22.9969\n",
      "Epoch 931/1000\n",
      "0s - loss: 0.0234 - mse: 20.3547 - val_loss: 0.0264 - val_mse: 23.0061\n",
      "Epoch 932/1000\n",
      "\n",
      "Epoch 00931: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0234 - mse: 20.3438 - val_loss: 0.0264 - val_mse: 22.9837\n",
      "Epoch 933/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0233 - mse: 20.3325 - val_loss: 0.0264 - val_mse: 22.9699\n",
      "Epoch 934/1000\n",
      "0s - loss: 0.0233 - mse: 20.3252 - val_loss: 0.0264 - val_mse: 22.9706\n",
      "Epoch 935/1000\n",
      "0s - loss: 0.0233 - mse: 20.3175 - val_loss: 0.0264 - val_mse: 22.9724\n",
      "Epoch 936/1000\n",
      "0s - loss: 0.0233 - mse: 20.3101 - val_loss: 0.0264 - val_mse: 22.9586\n",
      "Epoch 937/1000\n",
      "0s - loss: 0.0233 - mse: 20.3025 - val_loss: 0.0263 - val_mse: 22.9462\n",
      "Epoch 938/1000\n",
      "0s - loss: 0.0233 - mse: 20.2951 - val_loss: 0.0263 - val_mse: 22.9444\n",
      "Epoch 939/1000\n",
      "0s - loss: 0.0233 - mse: 20.2876 - val_loss: 0.0263 - val_mse: 22.9415\n",
      "Epoch 940/1000\n",
      "0s - loss: 0.0233 - mse: 20.2802 - val_loss: 0.0263 - val_mse: 22.9318\n",
      "Epoch 941/1000\n",
      "0s - loss: 0.0233 - mse: 20.2728 - val_loss: 0.0263 - val_mse: 22.9268\n",
      "Epoch 942/1000\n",
      "0s - loss: 0.0233 - mse: 20.2654 - val_loss: 0.0263 - val_mse: 22.9251\n",
      "Epoch 943/1000\n",
      "0s - loss: 0.0233 - mse: 20.2580 - val_loss: 0.0263 - val_mse: 22.9161\n",
      "Epoch 944/1000\n",
      "0s - loss: 0.0233 - mse: 20.2506 - val_loss: 0.0263 - val_mse: 22.9056\n",
      "Epoch 945/1000\n",
      "0s - loss: 0.0232 - mse: 20.2433 - val_loss: 0.0263 - val_mse: 22.9030\n",
      "Epoch 946/1000\n",
      "\n",
      "Epoch 00945: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0232 - mse: 20.2359 - val_loss: 0.0263 - val_mse: 22.9016\n",
      "Epoch 947/1000\n",
      "0s - loss: 0.0232 - mse: 20.2286 - val_loss: 0.0263 - val_mse: 22.8941\n",
      "Epoch 948/1000\n",
      "0s - loss: 0.0232 - mse: 20.2234 - val_loss: 0.0263 - val_mse: 22.8870\n",
      "Epoch 949/1000\n",
      "0s - loss: 0.0232 - mse: 20.2183 - val_loss: 0.0263 - val_mse: 22.8840\n",
      "Epoch 950/1000\n",
      "0s - loss: 0.0232 - mse: 20.2131 - val_loss: 0.0263 - val_mse: 22.8829\n",
      "Epoch 951/1000\n",
      "0s - loss: 0.0232 - mse: 20.2080 - val_loss: 0.0263 - val_mse: 22.8800\n",
      "Epoch 952/1000\n",
      "0s - loss: 0.0232 - mse: 20.2029 - val_loss: 0.0263 - val_mse: 22.8745\n",
      "Epoch 953/1000\n",
      "0s - loss: 0.0232 - mse: 20.1978 - val_loss: 0.0263 - val_mse: 22.8699\n",
      "Epoch 954/1000\n",
      "0s - loss: 0.0232 - mse: 20.1927 - val_loss: 0.0263 - val_mse: 22.8673\n",
      "Epoch 955/1000\n",
      "0s - loss: 0.0232 - mse: 20.1876 - val_loss: 0.0263 - val_mse: 22.8634\n",
      "Epoch 956/1000\n",
      "0s - loss: 0.0232 - mse: 20.1825 - val_loss: 0.0262 - val_mse: 22.8579\n",
      "Epoch 957/1000\n",
      "0s - loss: 0.0232 - mse: 20.1774 - val_loss: 0.0262 - val_mse: 22.8541\n",
      "Epoch 958/1000\n",
      "0s - loss: 0.0232 - mse: 20.1723 - val_loss: 0.0262 - val_mse: 22.8515\n",
      "Epoch 959/1000\n",
      "0s - loss: 0.0232 - mse: 20.1672 - val_loss: 0.0262 - val_mse: 22.8477\n",
      "Epoch 960/1000\n",
      "0s - loss: 0.0232 - mse: 20.1621 - val_loss: 0.0262 - val_mse: 22.8419\n",
      "Epoch 961/1000\n",
      "0s - loss: 0.0231 - mse: 20.1570 - val_loss: 0.0262 - val_mse: 22.8368\n",
      "Epoch 962/1000\n",
      "0s - loss: 0.0231 - mse: 20.1519 - val_loss: 0.0262 - val_mse: 22.8336\n",
      "Epoch 963/1000\n",
      "0s - loss: 0.0231 - mse: 20.1467 - val_loss: 0.0262 - val_mse: 22.8301\n",
      "Epoch 964/1000\n",
      "0s - loss: 0.0231 - mse: 20.1416 - val_loss: 0.0262 - val_mse: 22.8244\n",
      "Epoch 965/1000\n",
      "\n",
      "Epoch 00964: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0231 - mse: 20.1365 - val_loss: 0.0262 - val_mse: 22.8185\n",
      "Epoch 966/1000\n",
      "0s - loss: 0.0231 - mse: 20.1315 - val_loss: 0.0262 - val_mse: 22.8159\n",
      "Epoch 967/1000\n",
      "0s - loss: 0.0231 - mse: 20.1279 - val_loss: 0.0262 - val_mse: 22.8143\n",
      "Epoch 968/1000\n",
      "0s - loss: 0.0231 - mse: 20.1243 - val_loss: 0.0262 - val_mse: 22.8125\n",
      "Epoch 969/1000\n",
      "0s - loss: 0.0231 - mse: 20.1208 - val_loss: 0.0262 - val_mse: 22.8098\n",
      "Epoch 970/1000\n",
      "0s - loss: 0.0231 - mse: 20.1172 - val_loss: 0.0262 - val_mse: 22.8078\n",
      "Epoch 971/1000\n",
      "0s - loss: 0.0231 - mse: 20.1137 - val_loss: 0.0262 - val_mse: 22.8064\n",
      "Epoch 972/1000\n",
      "0s - loss: 0.0231 - mse: 20.1102 - val_loss: 0.0262 - val_mse: 22.8038\n",
      "Epoch 973/1000\n",
      "0s - loss: 0.0231 - mse: 20.1066 - val_loss: 0.0262 - val_mse: 22.8004\n",
      "Epoch 974/1000\n",
      "0s - loss: 0.0231 - mse: 20.1031 - val_loss: 0.0262 - val_mse: 22.7974\n",
      "Epoch 975/1000\n",
      "0s - loss: 0.0231 - mse: 20.0996 - val_loss: 0.0262 - val_mse: 22.7957\n",
      "Epoch 976/1000\n",
      "0s - loss: 0.0231 - mse: 20.0961 - val_loss: 0.0262 - val_mse: 22.7948\n",
      "Epoch 977/1000\n",
      "0s - loss: 0.0231 - mse: 20.0926 - val_loss: 0.0262 - val_mse: 22.7931\n",
      "Epoch 978/1000\n",
      "0s - loss: 0.0231 - mse: 20.0890 - val_loss: 0.0262 - val_mse: 22.7905\n",
      "Epoch 979/1000\n",
      "\n",
      "Epoch 00978: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0231 - mse: 20.0855 - val_loss: 0.0262 - val_mse: 22.7890\n",
      "Epoch 980/1000\n",
      "0s - loss: 0.0231 - mse: 20.0820 - val_loss: 0.0262 - val_mse: 22.7886\n",
      "Epoch 981/1000\n",
      "0s - loss: 0.0231 - mse: 20.0795 - val_loss: 0.0262 - val_mse: 22.7876\n",
      "Epoch 982/1000\n",
      "0s - loss: 0.0231 - mse: 20.0771 - val_loss: 0.0262 - val_mse: 22.7862\n",
      "Epoch 983/1000\n",
      "0s - loss: 0.0231 - mse: 20.0746 - val_loss: 0.0262 - val_mse: 22.7844\n",
      "Epoch 984/1000\n",
      "0s - loss: 0.0230 - mse: 20.0722 - val_loss: 0.0262 - val_mse: 22.7825\n",
      "Epoch 985/1000\n",
      "0s - loss: 0.0230 - mse: 20.0697 - val_loss: 0.0262 - val_mse: 22.7808\n",
      "Epoch 986/1000\n",
      "0s - loss: 0.0230 - mse: 20.0673 - val_loss: 0.0262 - val_mse: 22.7798\n",
      "Epoch 987/1000\n",
      "0s - loss: 0.0230 - mse: 20.0649 - val_loss: 0.0262 - val_mse: 22.7787\n",
      "Epoch 988/1000\n",
      "0s - loss: 0.0230 - mse: 20.0624 - val_loss: 0.0262 - val_mse: 22.7773\n",
      "Epoch 989/1000\n",
      "0s - loss: 0.0230 - mse: 20.0600 - val_loss: 0.0262 - val_mse: 22.7762\n",
      "Epoch 990/1000\n",
      "0s - loss: 0.0230 - mse: 20.0575 - val_loss: 0.0262 - val_mse: 22.7752\n",
      "Epoch 991/1000\n",
      "0s - loss: 0.0230 - mse: 20.0551 - val_loss: 0.0262 - val_mse: 22.7743\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}, {'loss': 0.0013604337582364678, 'dropout': 0.0, 'val_mse': 3.409489631652832, 'width': 210, 'mse': 1.1847187280654907, 'val_loss': 0.003915177658200264}, {'loss': 0.0069423709064722061, 'dropout': 0.0, 'val_mse': 9.2405805587768555, 'width': 310, 'mse': 6.0456881523132324, 'val_loss': 0.010611124336719513}, {'loss': 0.023029616102576256, 'dropout': 0.0, 'val_mse': 22.774328231811523, 'width': 410, 'mse': 20.055086135864258, 'val_loss': 0.026152169331908226}]\n"
     ]
    }
   ],
   "source": [
    "width_patience=2 # if increasing width this nr of times did not improve val_loss, stop\n",
    "width_perf = []\n",
    "prev_weights = []\n",
    "for width in range(10, 411, 100):\n",
    "    fit, results = run_depth1(width, learningrates1, prev_weights, 'bigsteps')\n",
    "    width_perf.append(results)\n",
    "    prev_weights=[layer.get_weights() for layer in model.layers]\n",
    "    print width_perf\n",
    "    if len(width_perf)>width_patience and width_perf[-1]['val_mse']>np.min(x['val_mse'] for x in width_perf[-width_patience:-1]):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17e2bfe50>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAENCAYAAAAG6bK5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1952502d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(width_perf).loc[:,['width','loss','val_loss']].set_index('width').plot(logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learningrates2 = {\n",
    "    'lr_intial' : 1e-2, # higher than the default\n",
    "    'lr_decay' : 0.01, # for adam it is 1 / (1 + decay * t) , thus with decay 0.001 and at t==1000, lr is divided by 2\n",
    "                    # note that the effect of this decay is not visible in tensorboard\n",
    "    'lr_plateau_factor' : 0.7, # if no convergence (possibly by too high lr), we boost the lr decay\n",
    "    'lr_plateau_patience' : 4, # nr of consequetive epochs without improvement before we boost the lr decay\n",
    "    'lr_plateau_cooldown' : 10, # first do this nr of iterations at new lr before detecting plateau\n",
    "    'lr_minimum' : 1e-6, # the minimum lr too which we decay (for plateau detection)\n",
    "\n",
    "    'stop_patience' : 30, # if no extra improvement after this nr of steps , we terminate learning\n",
    "    'stop_delta' : 0.0001, # the epsilon, changes below this threshold are 'no improvement'\n",
    "\n",
    "    'epochs_max' : 1000 # limit the total nr op epochs, very high, we will stop based on plateau\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_64 (LSTM)               (None, None, 50)          10600     \n",
      "_________________________________________________________________\n",
      "time_distributed_61 (TimeDis (None, None, 1)           51        \n",
      "=================================================================\n",
      "Total params: 10,651\n",
      "Trainable params: 10,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 0.8481 - mse: 738.5408 - val_loss: 0.6401 - val_mse: 557.3804\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.5927 - mse: 516.1166 - val_loss: 0.5083 - val_mse: 442.6301\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.4647 - mse: 404.7019 - val_loss: 0.4050 - val_mse: 352.6635\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.3734 - mse: 325.1505 - val_loss: 0.3196 - val_mse: 278.3061\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.3009 - mse: 262.0180 - val_loss: 0.2749 - val_mse: 239.4184\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2666 - mse: 232.1522 - val_loss: 0.2423 - val_mse: 210.9966\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.2395 - mse: 208.5226 - val_loss: 0.2007 - val_mse: 174.7970\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1999 - mse: 174.0808 - val_loss: 0.1763 - val_mse: 153.5540\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1735 - mse: 151.0896 - val_loss: 0.1504 - val_mse: 130.9589\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1462 - mse: 127.3290 - val_loss: 0.1235 - val_mse: 107.5748\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1185 - mse: 103.2099 - val_loss: 0.1111 - val_mse: 96.7488\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1038 - mse: 90.3867 - val_loss: 0.1148 - val_mse: 99.9612\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1052 - mse: 91.6135 - val_loss: 0.1189 - val_mse: 103.5004\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1073 - mse: 93.4314 - val_loss: 0.1159 - val_mse: 100.9622\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1027 - mse: 89.3941 - val_loss: 0.1148 - val_mse: 99.9322\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.00699999984354.\n",
      "4s - loss: 0.1003 - mse: 87.3219 - val_loss: 0.1181 - val_mse: 102.8645\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.1018 - mse: 88.6925 - val_loss: 0.1184 - val_mse: 103.1503\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0996 - mse: 86.7610 - val_loss: 0.1195 - val_mse: 104.0706\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0986 - mse: 85.8900 - val_loss: 0.1178 - val_mse: 102.6095\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0971 - mse: 84.5840 - val_loss: 0.1102 - val_mse: 95.9420\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0916 - mse: 79.7990 - val_loss: 0.1024 - val_mse: 89.1700\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0869 - mse: 75.6408 - val_loss: 0.0977 - val_mse: 85.1006\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0847 - mse: 73.7230 - val_loss: 0.0940 - val_mse: 81.8696\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0822 - mse: 71.6048 - val_loss: 0.0913 - val_mse: 79.5392\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0801 - mse: 69.7728 - val_loss: 0.0899 - val_mse: 78.2826\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0792 - mse: 68.9606 - val_loss: 0.0880 - val_mse: 76.6268\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0779 - mse: 67.8473 - val_loss: 0.0851 - val_mse: 74.1094\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0757 - mse: 65.9328 - val_loss: 0.0826 - val_mse: 71.9186\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0739 - mse: 64.3762 - val_loss: 0.0814 - val_mse: 70.8506\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0733 - mse: 63.8206 - val_loss: 0.0805 - val_mse: 70.0886\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0726 - mse: 63.2398 - val_loss: 0.0796 - val_mse: 69.3252\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0717 - mse: 62.4156 - val_loss: 0.0791 - val_mse: 68.8641\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0712 - mse: 61.9997 - val_loss: 0.0782 - val_mse: 68.1364\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0707 - mse: 61.5886 - val_loss: 0.0769 - val_mse: 66.9704\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0700 - mse: 60.9218 - val_loss: 0.0759 - val_mse: 66.1264\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0696 - mse: 60.5786 - val_loss: 0.0754 - val_mse: 65.6408\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0694 - mse: 60.4116 - val_loss: 0.0746 - val_mse: 64.9303\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0686 - mse: 59.7322 - val_loss: 0.0738 - val_mse: 64.2276\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0676 - mse: 58.8786 - val_loss: 0.0732 - val_mse: 63.7545\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0669 - mse: 58.2322 - val_loss: 0.0725 - val_mse: 63.1526\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0661 - mse: 57.5454 - val_loss: 0.0715 - val_mse: 62.2822\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0652 - mse: 56.7418 - val_loss: 0.0704 - val_mse: 61.3389\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0643 - mse: 55.9940 - val_loss: 0.0694 - val_mse: 60.4149\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0635 - mse: 55.2730 - val_loss: 0.0683 - val_mse: 59.5020\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0626 - mse: 54.4788 - val_loss: 0.0675 - val_mse: 58.7888\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0618 - mse: 53.8125 - val_loss: 0.0669 - val_mse: 58.2789\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0612 - mse: 53.3302 - val_loss: 0.0662 - val_mse: 57.6831\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0606 - mse: 52.7690 - val_loss: 0.0654 - val_mse: 56.9139\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0598 - mse: 52.0452 - val_loss: 0.0645 - val_mse: 56.1790\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0590 - mse: 51.3481 - val_loss: 0.0638 - val_mse: 55.5817\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0583 - mse: 50.7342 - val_loss: 0.0632 - val_mse: 55.0504\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0575 - mse: 50.1094 - val_loss: 0.0627 - val_mse: 54.5700\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0569 - mse: 49.5132 - val_loss: 0.0621 - val_mse: 54.0905\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0562 - mse: 48.9757 - val_loss: 0.0614 - val_mse: 53.4877\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0556 - mse: 48.4031 - val_loss: 0.0606 - val_mse: 52.7947\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0549 - mse: 47.8080 - val_loss: 0.0598 - val_mse: 52.1016\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0542 - mse: 47.2392 - val_loss: 0.0590 - val_mse: 51.3853\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0535 - mse: 46.6305 - val_loss: 0.0581 - val_mse: 50.6372\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0528 - mse: 45.9657 - val_loss: 0.0573 - val_mse: 49.9108\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0521 - mse: 45.3286 - val_loss: 0.0565 - val_mse: 49.1914\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0514 - mse: 44.7321 - val_loss: 0.0556 - val_mse: 48.4351\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0507 - mse: 44.1358 - val_loss: 0.0548 - val_mse: 47.6916\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0500 - mse: 43.5547 - val_loss: 0.0539 - val_mse: 46.9581\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0493 - mse: 42.9402 - val_loss: 0.0531 - val_mse: 46.2519\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0486 - mse: 42.2935 - val_loss: 0.0524 - val_mse: 45.6105\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0479 - mse: 41.6869 - val_loss: 0.0517 - val_mse: 44.9975\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0472 - mse: 41.1228 - val_loss: 0.0509 - val_mse: 44.3512\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0466 - mse: 40.5548 - val_loss: 0.0502 - val_mse: 43.6787\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0459 - mse: 39.9871 - val_loss: 0.0493 - val_mse: 42.9676\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0452 - mse: 39.3982 - val_loss: 0.0485 - val_mse: 42.2410\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0446 - mse: 38.8107 - val_loss: 0.0477 - val_mse: 41.5168\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0439 - mse: 38.2487 - val_loss: 0.0468 - val_mse: 40.7833\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0433 - mse: 37.6901 - val_loss: 0.0460 - val_mse: 40.0766\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0426 - mse: 37.1372 - val_loss: 0.0453 - val_mse: 39.4207\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0420 - mse: 36.5780 - val_loss: 0.0446 - val_mse: 38.8283\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0414 - mse: 36.0244 - val_loss: 0.0440 - val_mse: 38.2784\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0408 - mse: 35.5011 - val_loss: 0.0433 - val_mse: 37.7065\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0402 - mse: 34.9841 - val_loss: 0.0426 - val_mse: 37.0789\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0396 - mse: 34.4679 - val_loss: 0.0418 - val_mse: 36.3837\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0390 - mse: 33.9432 - val_loss: 0.0410 - val_mse: 35.6739\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0384 - mse: 33.4283 - val_loss: 0.0402 - val_mse: 35.0002\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0378 - mse: 32.9366 - val_loss: 0.0395 - val_mse: 34.3675\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0373 - mse: 32.4396 - val_loss: 0.0388 - val_mse: 33.7954\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0367 - mse: 31.9417 - val_loss: 0.0382 - val_mse: 33.2721\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0361 - mse: 31.4509 - val_loss: 0.0376 - val_mse: 32.7582\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0356 - mse: 30.9886 - val_loss: 0.0370 - val_mse: 32.1950\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0351 - mse: 30.5341 - val_loss: 0.0363 - val_mse: 31.5967\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0346 - mse: 30.0876 - val_loss: 0.0356 - val_mse: 31.0359\n",
      "Epoch 89/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0341 - mse: 29.6757 - val_loss: 0.0351 - val_mse: 30.5445\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0336 - mse: 29.2839 - val_loss: 0.0346 - val_mse: 30.1368\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0332 - mse: 28.9111 - val_loss: 0.0342 - val_mse: 29.7950\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0328 - mse: 28.5564 - val_loss: 0.0339 - val_mse: 29.4914\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0324 - mse: 28.2270 - val_loss: 0.0335 - val_mse: 29.1757\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0321 - mse: 27.9128 - val_loss: 0.0331 - val_mse: 28.8336\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0317 - mse: 27.6147 - val_loss: 0.0327 - val_mse: 28.4980\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0314 - mse: 27.3323 - val_loss: 0.0324 - val_mse: 28.2078\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0311 - mse: 27.0656 - val_loss: 0.0321 - val_mse: 27.9715\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0308 - mse: 26.8052 - val_loss: 0.0319 - val_mse: 27.7747\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0305 - mse: 26.5516 - val_loss: 0.0317 - val_mse: 27.5872\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0302 - mse: 26.3075 - val_loss: 0.0314 - val_mse: 27.3734\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0299 - mse: 26.0708 - val_loss: 0.0311 - val_mse: 27.1198\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0297 - mse: 25.8361 - val_loss: 0.0308 - val_mse: 26.8534\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0294 - mse: 25.6059 - val_loss: 0.0305 - val_mse: 26.5992\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0291 - mse: 25.3756 - val_loss: 0.0303 - val_mse: 26.3719\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0289 - mse: 25.1487 - val_loss: 0.0300 - val_mse: 26.1669\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0286 - mse: 24.9280 - val_loss: 0.0298 - val_mse: 25.9683\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0284 - mse: 24.7169 - val_loss: 0.0296 - val_mse: 25.7579\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0282 - mse: 24.5145 - val_loss: 0.0293 - val_mse: 25.5310\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0279 - mse: 24.3171 - val_loss: 0.0291 - val_mse: 25.3049\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0277 - mse: 24.1272 - val_loss: 0.0288 - val_mse: 25.0942\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0275 - mse: 23.9431 - val_loss: 0.0286 - val_mse: 24.9084\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0273 - mse: 23.7631 - val_loss: 0.0284 - val_mse: 24.7423\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0271 - mse: 23.5853 - val_loss: 0.0282 - val_mse: 24.5833\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0269 - mse: 23.4126 - val_loss: 0.0280 - val_mse: 24.4150\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0267 - mse: 23.2426 - val_loss: 0.0278 - val_mse: 24.2307\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0265 - mse: 23.0733 - val_loss: 0.0276 - val_mse: 24.0394\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0263 - mse: 22.9059 - val_loss: 0.0274 - val_mse: 23.8535\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0261 - mse: 22.7409 - val_loss: 0.0272 - val_mse: 23.6803\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0259 - mse: 22.5774 - val_loss: 0.0270 - val_mse: 23.5180\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0257 - mse: 22.4155 - val_loss: 0.0268 - val_mse: 23.3578\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0256 - mse: 22.2565 - val_loss: 0.0266 - val_mse: 23.1917\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0254 - mse: 22.0989 - val_loss: 0.0264 - val_mse: 23.0207\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0252 - mse: 21.9428 - val_loss: 0.0262 - val_mse: 22.8524\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0250 - mse: 21.7884 - val_loss: 0.0261 - val_mse: 22.6942\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0248 - mse: 21.6343 - val_loss: 0.0259 - val_mse: 22.5458\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0247 - mse: 21.4809 - val_loss: 0.0257 - val_mse: 22.4053\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0245 - mse: 21.3301 - val_loss: 0.0256 - val_mse: 22.2647\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0243 - mse: 21.1792 - val_loss: 0.0254 - val_mse: 22.1227\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0241 - mse: 21.0278 - val_loss: 0.0252 - val_mse: 21.9820\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0240 - mse: 20.8757 - val_loss: 0.0251 - val_mse: 21.8476\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0238 - mse: 20.7254 - val_loss: 0.0249 - val_mse: 21.7215\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0236 - mse: 20.5765 - val_loss: 0.0248 - val_mse: 21.6029\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0235 - mse: 20.4283 - val_loss: 0.0247 - val_mse: 21.4886\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0233 - mse: 20.2810 - val_loss: 0.0245 - val_mse: 21.3770\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0231 - mse: 20.1397 - val_loss: 0.0244 - val_mse: 21.2552\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0230 - mse: 19.9988 - val_loss: 0.0243 - val_mse: 21.1367\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0228 - mse: 19.8584 - val_loss: 0.0241 - val_mse: 21.0296\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0226 - mse: 19.7173 - val_loss: 0.0240 - val_mse: 20.9298\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0225 - mse: 19.5760 - val_loss: 0.0239 - val_mse: 20.8197\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0223 - mse: 19.4349 - val_loss: 0.0238 - val_mse: 20.7032\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0222 - mse: 19.2959 - val_loss: 0.0236 - val_mse: 20.5756\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0220 - mse: 19.1660 - val_loss: 0.0235 - val_mse: 20.5080\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0219 - mse: 19.0807 - val_loss: 0.0237 - val_mse: 20.5971\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0221 - mse: 19.2116 - val_loss: 0.0239 - val_mse: 20.8393\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0223 - mse: 19.4596 - val_loss: 0.0241 - val_mse: 21.0207\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0226 - mse: 19.6710 - val_loss: 0.0229 - val_mse: 19.9836\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0213 - mse: 18.5330 - val_loss: 0.0236 - val_mse: 20.5135\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0220 - mse: 19.1230 - val_loss: 0.0237 - val_mse: 20.6050\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0221 - mse: 19.2357 - val_loss: 0.0227 - val_mse: 19.7307\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0210 - mse: 18.2962 - val_loss: 0.0237 - val_mse: 20.5991\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0221 - mse: 19.2745 - val_loss: 0.0226 - val_mse: 19.6826\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0209 - mse: 18.2221 - val_loss: 0.0229 - val_mse: 19.9629\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0213 - mse: 18.5143 - val_loss: 0.0225 - val_mse: 19.5509\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0208 - mse: 18.0769 - val_loss: 0.0224 - val_mse: 19.4651\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0206 - mse: 17.9767 - val_loss: 0.0224 - val_mse: 19.5119\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0207 - mse: 17.9885 - val_loss: 0.0221 - val_mse: 19.2335\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0203 - mse: 17.6603 - val_loss: 0.0222 - val_mse: 19.3493\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0204 - mse: 17.7955 - val_loss: 0.0217 - val_mse: 18.9290\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0199 - mse: 17.3207 - val_loss: 0.0220 - val_mse: 19.1428\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0201 - mse: 17.5392 - val_loss: 0.0215 - val_mse: 18.7408\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0196 - mse: 17.1084 - val_loss: 0.0217 - val_mse: 18.9181\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0199 - mse: 17.3318 - val_loss: 0.0213 - val_mse: 18.5304\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0194 - mse: 16.9146 - val_loss: 0.0214 - val_mse: 18.6326\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0196 - mse: 17.0449 - val_loss: 0.0211 - val_mse: 18.3401\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0192 - mse: 16.7388 - val_loss: 0.0211 - val_mse: 18.3895\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0193 - mse: 16.8019 - val_loss: 0.0209 - val_mse: 18.2235\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0191 - mse: 16.6013 - val_loss: 0.0209 - val_mse: 18.1924\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0190 - mse: 16.5470 - val_loss: 0.0208 - val_mse: 18.1228\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0189 - mse: 16.4425 - val_loss: 0.0207 - val_mse: 18.0259\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0187 - mse: 16.3068 - val_loss: 0.0207 - val_mse: 18.0214\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0187 - mse: 16.2816 - val_loss: 0.0205 - val_mse: 17.8753\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0185 - mse: 16.1024 - val_loss: 0.0206 - val_mse: 17.9000\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0185 - mse: 16.1092 - val_loss: 0.0204 - val_mse: 17.7429\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0183 - mse: 15.9128 - val_loss: 0.0204 - val_mse: 17.7431\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0183 - mse: 15.9093 - val_loss: 0.0202 - val_mse: 17.5812\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0181 - mse: 15.7477 - val_loss: 0.0202 - val_mse: 17.5510\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0180 - mse: 15.7084 - val_loss: 0.0201 - val_mse: 17.4867\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0179 - mse: 15.6009 - val_loss: 0.0200 - val_mse: 17.4240\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0178 - mse: 15.5069 - val_loss: 0.0199 - val_mse: 17.3627\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0177 - mse: 15.4475 - val_loss: 0.0198 - val_mse: 17.2530\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0176 - mse: 15.3190 - val_loss: 0.0198 - val_mse: 17.2408\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0175 - mse: 15.2779 - val_loss: 0.0197 - val_mse: 17.1437\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0174 - mse: 15.1526 - val_loss: 0.0196 - val_mse: 17.1002\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0173 - mse: 15.0953 - val_loss: 0.0195 - val_mse: 17.0226\n",
      "Epoch 185/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0172 - mse: 15.0034 - val_loss: 0.0195 - val_mse: 16.9413\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0171 - mse: 14.9069 - val_loss: 0.0194 - val_mse: 16.9026\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0171 - mse: 14.8484 - val_loss: 0.0193 - val_mse: 16.8205\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0169 - mse: 14.7366 - val_loss: 0.0192 - val_mse: 16.7559\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0168 - mse: 14.6730 - val_loss: 0.0191 - val_mse: 16.6682\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0167 - mse: 14.5840 - val_loss: 0.0191 - val_mse: 16.6047\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0166 - mse: 14.4944 - val_loss: 0.0190 - val_mse: 16.5610\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0166 - mse: 14.4317 - val_loss: 0.0189 - val_mse: 16.4729\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0165 - mse: 14.3342 - val_loss: 0.0189 - val_mse: 16.4289\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0164 - mse: 14.2655 - val_loss: 0.0188 - val_mse: 16.3971\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0163 - mse: 14.1878 - val_loss: 0.0187 - val_mse: 16.3277\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0162 - mse: 14.0996 - val_loss: 0.0187 - val_mse: 16.2682\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0161 - mse: 14.0367 - val_loss: 0.0186 - val_mse: 16.2048\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0160 - mse: 13.9530 - val_loss: 0.0186 - val_mse: 16.1544\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0159 - mse: 13.8763 - val_loss: 0.0185 - val_mse: 16.0883\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0159 - mse: 13.8090 - val_loss: 0.0184 - val_mse: 16.0046\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0158 - mse: 13.7251 - val_loss: 0.0183 - val_mse: 15.9457\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0157 - mse: 13.6514 - val_loss: 0.0183 - val_mse: 15.9053\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0156 - mse: 13.5809 - val_loss: 0.0182 - val_mse: 15.8377\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0155 - mse: 13.4990 - val_loss: 0.0181 - val_mse: 15.7655\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0154 - mse: 13.4269 - val_loss: 0.0180 - val_mse: 15.7104\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0153 - mse: 13.3553 - val_loss: 0.0180 - val_mse: 15.6688\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0152 - mse: 13.2759 - val_loss: 0.0179 - val_mse: 15.6172\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0152 - mse: 13.2039 - val_loss: 0.0179 - val_mse: 15.5532\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0151 - mse: 13.1341 - val_loss: 0.0178 - val_mse: 15.4862\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0150 - mse: 13.0589 - val_loss: 0.0177 - val_mse: 15.4330\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0149 - mse: 12.9866 - val_loss: 0.0177 - val_mse: 15.3845\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0148 - mse: 12.9209 - val_loss: 0.0176 - val_mse: 15.3270\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0148 - mse: 12.8513 - val_loss: 0.0175 - val_mse: 15.2740\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0147 - mse: 12.7813 - val_loss: 0.0175 - val_mse: 15.2317\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0146 - mse: 12.7151 - val_loss: 0.0174 - val_mse: 15.1869\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0145 - mse: 12.6481 - val_loss: 0.0174 - val_mse: 15.1288\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0144 - mse: 12.5781 - val_loss: 0.0173 - val_mse: 15.0630\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0144 - mse: 12.5093 - val_loss: 0.0172 - val_mse: 15.0083\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0143 - mse: 12.4423 - val_loss: 0.0172 - val_mse: 14.9630\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0142 - mse: 12.3747 - val_loss: 0.0171 - val_mse: 14.9067\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0141 - mse: 12.3053 - val_loss: 0.0170 - val_mse: 14.8455\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0141 - mse: 12.2363 - val_loss: 0.0170 - val_mse: 14.7875\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0140 - mse: 12.1683 - val_loss: 0.0169 - val_mse: 14.7308\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0139 - mse: 12.1008 - val_loss: 0.0168 - val_mse: 14.6699\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0138 - mse: 12.0340 - val_loss: 0.0168 - val_mse: 14.6077\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0137 - mse: 11.9667 - val_loss: 0.0167 - val_mse: 14.5466\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0137 - mse: 11.8991 - val_loss: 0.0166 - val_mse: 14.4809\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0136 - mse: 11.8316 - val_loss: 0.0166 - val_mse: 14.4184\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0135 - mse: 11.7650 - val_loss: 0.0165 - val_mse: 14.3579\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0134 - mse: 11.6992 - val_loss: 0.0164 - val_mse: 14.2944\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0134 - mse: 11.6332 - val_loss: 0.0164 - val_mse: 14.2396\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0133 - mse: 11.5672 - val_loss: 0.0163 - val_mse: 14.1709\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0132 - mse: 11.5024 - val_loss: 0.0162 - val_mse: 14.1122\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0131 - mse: 11.4389 - val_loss: 0.0161 - val_mse: 14.0579\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0131 - mse: 11.3782 - val_loss: 0.0161 - val_mse: 14.0150\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0130 - mse: 11.3228 - val_loss: 0.0160 - val_mse: 13.9601\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0130 - mse: 11.2834 - val_loss: 0.0160 - val_mse: 13.9765\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0129 - mse: 11.2735 - val_loss: 0.0161 - val_mse: 14.0182\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0130 - mse: 11.3557 - val_loss: 0.0163 - val_mse: 14.1979\n",
      "Epoch 240/1000\n",
      "\n",
      "Epoch 00239: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0132 - mse: 11.4992 - val_loss: 0.0167 - val_mse: 14.5115\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0137 - mse: 11.9275 - val_loss: 0.0158 - val_mse: 13.7422\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0126 - mse: 11.0128 - val_loss: 0.0163 - val_mse: 14.2317\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0132 - mse: 11.5177 - val_loss: 0.0161 - val_mse: 14.0412\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0131 - mse: 11.3655 - val_loss: 0.0158 - val_mse: 13.7926\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0127 - mse: 11.0903 - val_loss: 0.0162 - val_mse: 14.1320\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0131 - mse: 11.4100 - val_loss: 0.0156 - val_mse: 13.5540\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0124 - mse: 10.7725 - val_loss: 0.0160 - val_mse: 13.9281\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0129 - mse: 11.1963 - val_loss: 0.0155 - val_mse: 13.5021\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0123 - mse: 10.6895 - val_loss: 0.0159 - val_mse: 13.8690\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0127 - mse: 11.0691 - val_loss: 0.0155 - val_mse: 13.5030\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0123 - mse: 10.6761 - val_loss: 0.0157 - val_mse: 13.6695\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0125 - mse: 10.8464 - val_loss: 0.0155 - val_mse: 13.5044\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0122 - mse: 10.6169 - val_loss: 0.0156 - val_mse: 13.5544\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0123 - mse: 10.6729 - val_loss: 0.0154 - val_mse: 13.4476\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0122 - mse: 10.5942 - val_loss: 0.0154 - val_mse: 13.3991\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0121 - mse: 10.5372 - val_loss: 0.0154 - val_mse: 13.4233\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0121 - mse: 10.5260 - val_loss: 0.0153 - val_mse: 13.3107\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0119 - mse: 10.4025 - val_loss: 0.0153 - val_mse: 13.3305\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0120 - mse: 10.4551 - val_loss: 0.0152 - val_mse: 13.2044\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0118 - mse: 10.3125 - val_loss: 0.0153 - val_mse: 13.2877\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0119 - mse: 10.3788 - val_loss: 0.0151 - val_mse: 13.1495\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0117 - mse: 10.2232 - val_loss: 0.0152 - val_mse: 13.1994\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0118 - mse: 10.2879 - val_loss: 0.0150 - val_mse: 13.0855\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0117 - mse: 10.1508 - val_loss: 0.0151 - val_mse: 13.1443\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0117 - mse: 10.2044 - val_loss: 0.0150 - val_mse: 13.0321\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0116 - mse: 10.0868 - val_loss: 0.0150 - val_mse: 13.0600\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0116 - mse: 10.1097 - val_loss: 0.0149 - val_mse: 13.0039\n",
      "Epoch 268/1000\n",
      "\n",
      "Epoch 00267: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0115 - mse: 10.0243 - val_loss: 0.0149 - val_mse: 13.0005\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0115 - mse: 10.0205 - val_loss: 0.0149 - val_mse: 12.9332\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0114 - mse: 9.9577 - val_loss: 0.0149 - val_mse: 12.9442\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0114 - mse: 9.9710 - val_loss: 0.0148 - val_mse: 12.9030\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0114 - mse: 9.9097 - val_loss: 0.0148 - val_mse: 12.9154\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0114 - mse: 9.9105 - val_loss: 0.0148 - val_mse: 12.8739\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0113 - mse: 9.8699 - val_loss: 0.0147 - val_mse: 12.8428\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0113 - mse: 9.8488 - val_loss: 0.0147 - val_mse: 12.8277\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0113 - mse: 9.8302 - val_loss: 0.0147 - val_mse: 12.8076\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0112 - mse: 9.7916 - val_loss: 0.0147 - val_mse: 12.8101\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0112 - mse: 9.7847 - val_loss: 0.0147 - val_mse: 12.7663\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0112 - mse: 9.7422 - val_loss: 0.0146 - val_mse: 12.7540\n",
      "Epoch 280/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0112 - mse: 9.7343 - val_loss: 0.0146 - val_mse: 12.7291\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0111 - mse: 9.6975 - val_loss: 0.0146 - val_mse: 12.7271\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0111 - mse: 9.6821 - val_loss: 0.0146 - val_mse: 12.7036\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0111 - mse: 9.6540 - val_loss: 0.0146 - val_mse: 12.6781\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0111 - mse: 9.6310 - val_loss: 0.0145 - val_mse: 12.6610\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0110 - mse: 9.6105 - val_loss: 0.0145 - val_mse: 12.6459\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0110 - mse: 9.5823 - val_loss: 0.0145 - val_mse: 12.6378\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0110 - mse: 9.5657 - val_loss: 0.0145 - val_mse: 12.6099\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0110 - mse: 9.5360 - val_loss: 0.0145 - val_mse: 12.5965\n",
      "Epoch 289/1000\n",
      "\n",
      "Epoch 00288: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0109 - mse: 9.5197 - val_loss: 0.0144 - val_mse: 12.5798\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0109 - mse: 9.4921 - val_loss: 0.0144 - val_mse: 12.5741\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0109 - mse: 9.4783 - val_loss: 0.0144 - val_mse: 12.5651\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0109 - mse: 9.4636 - val_loss: 0.0144 - val_mse: 12.5521\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0108 - mse: 9.4465 - val_loss: 0.0144 - val_mse: 12.5432\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0108 - mse: 9.4331 - val_loss: 0.0144 - val_mse: 12.5338\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0108 - mse: 9.4172 - val_loss: 0.0144 - val_mse: 12.5235\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0108 - mse: 9.4016 - val_loss: 0.0144 - val_mse: 12.5121\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0108 - mse: 9.3878 - val_loss: 0.0144 - val_mse: 12.4970\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0108 - mse: 9.3717 - val_loss: 0.0143 - val_mse: 12.4853\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0107 - mse: 9.3570 - val_loss: 0.0143 - val_mse: 12.4772\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0107 - mse: 9.3423 - val_loss: 0.0143 - val_mse: 12.4690\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0107 - mse: 9.3267 - val_loss: 0.0143 - val_mse: 12.4597\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0107 - mse: 9.3123 - val_loss: 0.0143 - val_mse: 12.4475\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0107 - mse: 9.2973 - val_loss: 0.0143 - val_mse: 12.4369\n",
      "Epoch 304/1000\n",
      "\n",
      "Epoch 00303: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0107 - mse: 9.2823 - val_loss: 0.0143 - val_mse: 12.4302\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0106 - mse: 9.2676 - val_loss: 0.0143 - val_mse: 12.4264\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0106 - mse: 9.2571 - val_loss: 0.0143 - val_mse: 12.4216\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0106 - mse: 9.2468 - val_loss: 0.0143 - val_mse: 12.4144\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0106 - mse: 9.2366 - val_loss: 0.0142 - val_mse: 12.4053\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0106 - mse: 9.2261 - val_loss: 0.0142 - val_mse: 12.3970\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0106 - mse: 9.2159 - val_loss: 0.0142 - val_mse: 12.3910\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0106 - mse: 9.2056 - val_loss: 0.0142 - val_mse: 12.3865\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0106 - mse: 9.1952 - val_loss: 0.0142 - val_mse: 12.3818\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0105 - mse: 9.1850 - val_loss: 0.0142 - val_mse: 12.3751\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0105 - mse: 9.1748 - val_loss: 0.0142 - val_mse: 12.3660\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0105 - mse: 9.1644 - val_loss: 0.0142 - val_mse: 12.3569\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0105 - mse: 9.1543 - val_loss: 0.0142 - val_mse: 12.3495\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0105 - mse: 9.1442 - val_loss: 0.0142 - val_mse: 12.3440\n",
      "Epoch 318/1000\n",
      "\n",
      "Epoch 00317: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0105 - mse: 9.1338 - val_loss: 0.0142 - val_mse: 12.3399\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0105 - mse: 9.1237 - val_loss: 0.0142 - val_mse: 12.3364\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0105 - mse: 9.1167 - val_loss: 0.0142 - val_mse: 12.3318\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0105 - mse: 9.1095 - val_loss: 0.0142 - val_mse: 12.3264\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0105 - mse: 9.1023 - val_loss: 0.0141 - val_mse: 12.3212\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0104 - mse: 9.0952 - val_loss: 0.0141 - val_mse: 12.3169\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0104 - mse: 9.0882 - val_loss: 0.0141 - val_mse: 12.3135\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0104 - mse: 9.0810 - val_loss: 0.0141 - val_mse: 12.3108\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0104 - mse: 9.0739 - val_loss: 0.0141 - val_mse: 12.3075\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0104 - mse: 9.0670 - val_loss: 0.0141 - val_mse: 12.3029\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0104 - mse: 9.0599 - val_loss: 0.0141 - val_mse: 12.2971\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0104 - mse: 9.0528 - val_loss: 0.0141 - val_mse: 12.2911\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0104 - mse: 9.0457 - val_loss: 0.0141 - val_mse: 12.2860\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0104 - mse: 9.0388 - val_loss: 0.0141 - val_mse: 12.2819\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0104 - mse: 9.0317 - val_loss: 0.0141 - val_mse: 12.2786\n",
      "Epoch 333/1000\n",
      "\n",
      "Epoch 00332: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0104 - mse: 9.0247 - val_loss: 0.0141 - val_mse: 12.2752\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0104 - mse: 9.0177 - val_loss: 0.0141 - val_mse: 12.2724\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0103 - mse: 9.0129 - val_loss: 0.0141 - val_mse: 12.2691\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0103 - mse: 9.0079 - val_loss: 0.0141 - val_mse: 12.2657\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0103 - mse: 9.0031 - val_loss: 0.0141 - val_mse: 12.2625\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.0103 - mse: 8.9982 - val_loss: 0.0141 - val_mse: 12.2597\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0103 - mse: 8.9933 - val_loss: 0.0141 - val_mse: 12.2570\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0103 - mse: 8.9885 - val_loss: 0.0141 - val_mse: 12.2543\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0103 - mse: 8.9837 - val_loss: 0.0141 - val_mse: 12.2512\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0103 - mse: 8.9788 - val_loss: 0.0141 - val_mse: 12.2480\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.0103 - mse: 8.9740 - val_loss: 0.0141 - val_mse: 12.2448\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0103 - mse: 8.9692 - val_loss: 0.0141 - val_mse: 12.2419\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.0103 - mse: 8.9644 - val_loss: 0.0141 - val_mse: 12.2392\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.0103 - mse: 8.9596 - val_loss: 0.0141 - val_mse: 12.2367\n",
      "Epoch 347/1000\n",
      "\n",
      "Epoch 00346: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0103 - mse: 8.9548 - val_loss: 0.0140 - val_mse: 12.2341\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0103 - mse: 8.9500 - val_loss: 0.0140 - val_mse: 12.2319\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.0103 - mse: 8.9467 - val_loss: 0.0140 - val_mse: 12.2295\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0103 - mse: 8.9433 - val_loss: 0.0140 - val_mse: 12.2271\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.0103 - mse: 8.9400 - val_loss: 0.0140 - val_mse: 12.2248\n",
      "Epoch 352/1000\n",
      "0s - loss: 0.0103 - mse: 8.9366 - val_loss: 0.0140 - val_mse: 12.2226\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.0103 - mse: 8.9333 - val_loss: 0.0140 - val_mse: 12.2204\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.0103 - mse: 8.9300 - val_loss: 0.0140 - val_mse: 12.2182\n",
      "Epoch 355/1000\n",
      "0s - loss: 0.0103 - mse: 8.9267 - val_loss: 0.0140 - val_mse: 12.2160\n",
      "Epoch 356/1000\n",
      "0s - loss: 0.0102 - mse: 8.9234 - val_loss: 0.0140 - val_mse: 12.2138\n",
      "Epoch 357/1000\n",
      "0s - loss: 0.0102 - mse: 8.9201 - val_loss: 0.0140 - val_mse: 12.2118\n",
      "Epoch 358/1000\n",
      "0s - loss: 0.0102 - mse: 8.9167 - val_loss: 0.0140 - val_mse: 12.2098\n",
      "Epoch 359/1000\n",
      "0s - loss: 0.0102 - mse: 8.9134 - val_loss: 0.0140 - val_mse: 12.2079\n",
      "Epoch 360/1000\n",
      "0s - loss: 0.0102 - mse: 8.9101 - val_loss: 0.0140 - val_mse: 12.2060\n",
      "Epoch 361/1000\n",
      "0s - loss: 0.0102 - mse: 8.9068 - val_loss: 0.0140 - val_mse: 12.2040\n",
      "Epoch 362/1000\n",
      "0s - loss: 0.0102 - mse: 8.9035 - val_loss: 0.0140 - val_mse: 12.2019\n",
      "Epoch 363/1000\n",
      "\n",
      "Epoch 00362: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0102 - mse: 8.9002 - val_loss: 0.0140 - val_mse: 12.1997\n",
      "Epoch 364/1000\n",
      "0s - loss: 0.0102 - mse: 8.8970 - val_loss: 0.0140 - val_mse: 12.1982\n",
      "Epoch 365/1000\n",
      "0s - loss: 0.0102 - mse: 8.8947 - val_loss: 0.0140 - val_mse: 12.1966\n",
      "Epoch 366/1000\n",
      "0s - loss: 0.0102 - mse: 8.8924 - val_loss: 0.0140 - val_mse: 12.1950\n",
      "Epoch 367/1000\n",
      "0s - loss: 0.0102 - mse: 8.8901 - val_loss: 0.0140 - val_mse: 12.1935\n",
      "Epoch 368/1000\n",
      "0s - loss: 0.0102 - mse: 8.8878 - val_loss: 0.0140 - val_mse: 12.1920\n",
      "Epoch 369/1000\n",
      "0s - loss: 0.0102 - mse: 8.8855 - val_loss: 0.0140 - val_mse: 12.1905\n",
      "Epoch 370/1000\n",
      "0s - loss: 0.0102 - mse: 8.8832 - val_loss: 0.0140 - val_mse: 12.1891\n",
      "Epoch 371/1000\n",
      "0s - loss: 0.0102 - mse: 8.8810 - val_loss: 0.0140 - val_mse: 12.1877\n",
      "Epoch 372/1000\n",
      "0s - loss: 0.0102 - mse: 8.8787 - val_loss: 0.0140 - val_mse: 12.1862\n",
      "Epoch 373/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0102 - mse: 8.8764 - val_loss: 0.0140 - val_mse: 12.1846\n",
      "Epoch 374/1000\n",
      "0s - loss: 0.0102 - mse: 8.8741 - val_loss: 0.0140 - val_mse: 12.1831\n",
      "Epoch 375/1000\n",
      "0s - loss: 0.0102 - mse: 8.8719 - val_loss: 0.0140 - val_mse: 12.1817\n",
      "Epoch 376/1000\n",
      "0s - loss: 0.0102 - mse: 8.8696 - val_loss: 0.0140 - val_mse: 12.1803\n",
      "Epoch 377/1000\n",
      "\n",
      "Epoch 00376: reducing learning rate to 0.000282475253334.\n",
      "0s - loss: 0.0102 - mse: 8.8674 - val_loss: 0.0140 - val_mse: 12.1788\n",
      "Epoch 378/1000\n",
      "0s - loss: 0.0102 - mse: 8.8651 - val_loss: 0.0140 - val_mse: 12.1777\n",
      "Epoch 379/1000\n",
      "0s - loss: 0.0102 - mse: 8.8635 - val_loss: 0.0140 - val_mse: 12.1767\n",
      "Epoch 380/1000\n",
      "0s - loss: 0.0102 - mse: 8.8620 - val_loss: 0.0140 - val_mse: 12.1757\n",
      "Epoch 381/1000\n",
      "0s - loss: 0.0102 - mse: 8.8604 - val_loss: 0.0140 - val_mse: 12.1747\n",
      "Epoch 382/1000\n",
      "0s - loss: 0.0102 - mse: 8.8588 - val_loss: 0.0140 - val_mse: 12.1737\n",
      "Epoch 383/1000\n",
      "0s - loss: 0.0102 - mse: 8.8573 - val_loss: 0.0140 - val_mse: 12.1727\n",
      "Epoch 384/1000\n",
      "0s - loss: 0.0102 - mse: 8.8557 - val_loss: 0.0140 - val_mse: 12.1717\n",
      "Epoch 385/1000\n",
      "0s - loss: 0.0102 - mse: 8.8542 - val_loss: 0.0140 - val_mse: 12.1707\n",
      "Epoch 386/1000\n",
      "0s - loss: 0.0102 - mse: 8.8526 - val_loss: 0.0140 - val_mse: 12.1697\n",
      "Epoch 387/1000\n",
      "0s - loss: 0.0102 - mse: 8.8510 - val_loss: 0.0140 - val_mse: 12.1687\n",
      "Epoch 388/1000\n",
      "0s - loss: 0.0102 - mse: 8.8495 - val_loss: 0.0140 - val_mse: 12.1677\n",
      "Epoch 389/1000\n",
      "0s - loss: 0.0102 - mse: 8.8479 - val_loss: 0.0140 - val_mse: 12.1667\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}, {'loss': 0.0013604337582364678, 'dropout': 0.0, 'val_mse': 3.409489631652832, 'width': 210, 'mse': 1.1847187280654907, 'val_loss': 0.003915177658200264}, {'loss': 0.0069423709064722061, 'dropout': 0.0, 'val_mse': 9.2405805587768555, 'width': 310, 'mse': 6.0456881523132324, 'val_loss': 0.010611124336719513}, {'loss': 0.023029616102576256, 'dropout': 0.0, 'val_mse': 22.774328231811523, 'width': 410, 'mse': 20.055086135864258, 'val_loss': 0.026152169331908226}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_65 (LSTM)               (None, None, 100)         41200     \n",
      "_________________________________________________________________\n",
      "time_distributed_62 (TimeDis (None, None, 1)           101       \n",
      "=================================================================\n",
      "Total params: 41,301\n",
      "Trainable params: 41,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 1.1292 - mse: 983.3284 - val_loss: 0.6596 - val_mse: 574.4092\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.6116 - mse: 532.5966 - val_loss: 0.3756 - val_mse: 327.1118\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.3487 - mse: 303.6971 - val_loss: 0.3051 - val_mse: 265.6767\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.3062 - mse: 266.6526 - val_loss: 0.2438 - val_mse: 212.2954\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2418 - mse: 210.5301 - val_loss: 0.1719 - val_mse: 149.6708\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.1686 - mse: 146.7932 - val_loss: 0.1790 - val_mse: 155.8658\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.1723 - mse: 150.0370 - val_loss: 0.1508 - val_mse: 131.3632\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1391 - mse: 121.1223 - val_loss: 0.1453 - val_mse: 126.5366\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1306 - mse: 113.7700 - val_loss: 0.1320 - val_mse: 114.9591\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1159 - mse: 100.8913 - val_loss: 0.1266 - val_mse: 110.2739\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1074 - mse: 93.5707 - val_loss: 0.1390 - val_mse: 121.0580\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1131 - mse: 98.5346 - val_loss: 0.1411 - val_mse: 122.8473\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1132 - mse: 98.5521 - val_loss: 0.1227 - val_mse: 106.8723\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1005 - mse: 87.5319 - val_loss: 0.1118 - val_mse: 97.3559\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.0947 - mse: 82.4666 - val_loss: 0.1075 - val_mse: 93.6316\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.0931 - mse: 81.1041 - val_loss: 0.1022 - val_mse: 88.9614\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0893 - mse: 77.7793 - val_loss: 0.0935 - val_mse: 81.4215\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0818 - mse: 71.2130 - val_loss: 0.0846 - val_mse: 73.6452\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0738 - mse: 64.2927 - val_loss: 0.0797 - val_mse: 69.3850\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0700 - mse: 60.9566 - val_loss: 0.0785 - val_mse: 68.3841\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0701 - mse: 61.0859 - val_loss: 0.0772 - val_mse: 67.2715\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0703 - mse: 61.1898 - val_loss: 0.0743 - val_mse: 64.7445\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0684 - mse: 59.5822 - val_loss: 0.0709 - val_mse: 61.7158\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0655 - mse: 57.0262 - val_loss: 0.0675 - val_mse: 58.8228\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0625 - mse: 54.4218 - val_loss: 0.0653 - val_mse: 56.8621\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0604 - mse: 52.6093 - val_loss: 0.0632 - val_mse: 55.0733\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0584 - mse: 50.8640 - val_loss: 0.0609 - val_mse: 53.0608\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0562 - mse: 48.9292 - val_loss: 0.0591 - val_mse: 51.4552\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0544 - mse: 47.3838 - val_loss: 0.0572 - val_mse: 49.8158\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0525 - mse: 45.7440 - val_loss: 0.0552 - val_mse: 48.0320\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0504 - mse: 43.8807 - val_loss: 0.0538 - val_mse: 46.8840\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0488 - mse: 42.5155 - val_loss: 0.0528 - val_mse: 45.9933\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0474 - mse: 41.3112 - val_loss: 0.0512 - val_mse: 44.5664\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0456 - mse: 39.7250 - val_loss: 0.0499 - val_mse: 43.4654\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0445 - mse: 38.7135 - val_loss: 0.0498 - val_mse: 43.3421\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0444 - mse: 38.6667 - val_loss: 0.0495 - val_mse: 43.1459\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0439 - mse: 38.2430 - val_loss: 0.0486 - val_mse: 42.2832\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0426 - mse: 37.1065 - val_loss: 0.0468 - val_mse: 40.7142\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0409 - mse: 35.6454 - val_loss: 0.0450 - val_mse: 39.2229\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0398 - mse: 34.6351 - val_loss: 0.0434 - val_mse: 37.7926\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0387 - mse: 33.6815 - val_loss: 0.0417 - val_mse: 36.3515\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0375 - mse: 32.6161 - val_loss: 0.0404 - val_mse: 35.1768\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0364 - mse: 31.7418 - val_loss: 0.0395 - val_mse: 34.4073\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0358 - mse: 31.1394 - val_loss: 0.0387 - val_mse: 33.7255\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0350 - mse: 30.4471 - val_loss: 0.0378 - val_mse: 32.8950\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0340 - mse: 29.6048 - val_loss: 0.0366 - val_mse: 31.8644\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0331 - mse: 28.7882 - val_loss: 0.0356 - val_mse: 30.9794\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0324 - mse: 28.2265 - val_loss: 0.0350 - val_mse: 30.4671\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0320 - mse: 27.8290 - val_loss: 0.0346 - val_mse: 30.1490\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0315 - mse: 27.4141 - val_loss: 0.0341 - val_mse: 29.7338\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0309 - mse: 26.9226 - val_loss: 0.0336 - val_mse: 29.2729\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0303 - mse: 26.4286 - val_loss: 0.0331 - val_mse: 28.8584\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0298 - mse: 25.9156 - val_loss: 0.0326 - val_mse: 28.4072\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0293 - mse: 25.5297 - val_loss: 0.0321 - val_mse: 27.9137\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0290 - mse: 25.2577 - val_loss: 0.0315 - val_mse: 27.4310\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0286 - mse: 24.8928 - val_loss: 0.0309 - val_mse: 26.8654\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0280 - mse: 24.4089 - val_loss: 0.0302 - val_mse: 26.2809\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0275 - mse: 23.9811 - val_loss: 0.0297 - val_mse: 25.8674\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0271 - mse: 23.6321 - val_loss: 0.0292 - val_mse: 25.4345\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0267 - mse: 23.2667 - val_loss: 0.0287 - val_mse: 24.9637\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0264 - mse: 22.9500 - val_loss: 0.0283 - val_mse: 24.6857\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0261 - mse: 22.7011 - val_loss: 0.0279 - val_mse: 24.3087\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0257 - mse: 22.3909 - val_loss: 0.0274 - val_mse: 23.8900\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0253 - mse: 22.0736 - val_loss: 0.0272 - val_mse: 23.6514\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0251 - mse: 21.8320 - val_loss: 0.0268 - val_mse: 23.3450\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0248 - mse: 21.5952 - val_loss: 0.0266 - val_mse: 23.1479\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0245 - mse: 21.3126 - val_loss: 0.0264 - val_mse: 22.9657\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0241 - mse: 21.0247 - val_loss: 0.0262 - val_mse: 22.8463\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0239 - mse: 20.7751 - val_loss: 0.0261 - val_mse: 22.7145\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0236 - mse: 20.5636 - val_loss: 0.0258 - val_mse: 22.4417\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0234 - mse: 20.3553 - val_loss: 0.0255 - val_mse: 22.2192\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0231 - mse: 20.1507 - val_loss: 0.0251 - val_mse: 21.8563\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0230 - mse: 20.0308 - val_loss: 0.0257 - val_mse: 22.3630\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0235 - mse: 20.4359 - val_loss: 0.0269 - val_mse: 23.4645\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0256 - mse: 22.2861 - val_loss: 0.0320 - val_mse: 27.8845\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0304 - mse: 26.4327 - val_loss: 0.0246 - val_mse: 21.4380\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0232 - mse: 20.2175 - val_loss: 0.0261 - val_mse: 22.7526\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0250 - mse: 21.7840 - val_loss: 0.0271 - val_mse: 23.5806\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0253 - mse: 22.0064 - val_loss: 0.0246 - val_mse: 21.4107\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0227 - mse: 19.7693 - val_loss: 0.0260 - val_mse: 22.6566\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0245 - mse: 21.3396 - val_loss: 0.0235 - val_mse: 20.4234\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0217 - mse: 18.8914 - val_loss: 0.0250 - val_mse: 21.7620\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0230 - mse: 20.0474 - val_loss: 0.0238 - val_mse: 20.7589\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0217 - mse: 18.9286 - val_loss: 0.0237 - val_mse: 20.6476\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0218 - mse: 18.9551 - val_loss: 0.0236 - val_mse: 20.5359\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0216 - mse: 18.8521 - val_loss: 0.0228 - val_mse: 19.8482\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0208 - mse: 18.0760 - val_loss: 0.0235 - val_mse: 20.4809\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0215 - mse: 18.7417 - val_loss: 0.0220 - val_mse: 19.1530\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0201 - mse: 17.4900 - val_loss: 0.0227 - val_mse: 19.7919\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0210 - mse: 18.2598 - val_loss: 0.0218 - val_mse: 18.9643\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0199 - mse: 17.3460 - val_loss: 0.0220 - val_mse: 19.1527\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0201 - mse: 17.4999 - val_loss: 0.0218 - val_mse: 18.9645\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0199 - mse: 17.3665 - val_loss: 0.0211 - val_mse: 18.3952\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0193 - mse: 16.8328 - val_loss: 0.0214 - val_mse: 18.6570\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0197 - mse: 17.1274 - val_loss: 0.0207 - val_mse: 18.0695\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0189 - mse: 16.4754 - val_loss: 0.0210 - val_mse: 18.2687\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0191 - mse: 16.6515 - val_loss: 0.0206 - val_mse: 17.9040\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0187 - mse: 16.2523 - val_loss: 0.0206 - val_mse: 17.8972\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0186 - mse: 16.1726 - val_loss: 0.0203 - val_mse: 17.6713\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0184 - mse: 15.9905 - val_loss: 0.0200 - val_mse: 17.4159\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0181 - mse: 15.7907 - val_loss: 0.0198 - val_mse: 17.2370\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0180 - mse: 15.6765 - val_loss: 0.0196 - val_mse: 17.0685\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0177 - mse: 15.4413 - val_loss: 0.0196 - val_mse: 17.0550\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0176 - mse: 15.3511 - val_loss: 0.0193 - val_mse: 16.8270\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0173 - mse: 15.1066 - val_loss: 0.0192 - val_mse: 16.7469\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0173 - mse: 15.0377 - val_loss: 0.0190 - val_mse: 16.5243\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0170 - mse: 14.7941 - val_loss: 0.0190 - val_mse: 16.5351\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0169 - mse: 14.7328 - val_loss: 0.0187 - val_mse: 16.2807\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0166 - mse: 14.4718 - val_loss: 0.0187 - val_mse: 16.2424\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0166 - mse: 14.4342 - val_loss: 0.0183 - val_mse: 15.9774\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0163 - mse: 14.1677 - val_loss: 0.0184 - val_mse: 15.9898\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0162 - mse: 14.1306 - val_loss: 0.0181 - val_mse: 15.7568\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0159 - mse: 13.8814 - val_loss: 0.0181 - val_mse: 15.7397\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0159 - mse: 13.8213 - val_loss: 0.0179 - val_mse: 15.5563\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0156 - mse: 13.6152 - val_loss: 0.0177 - val_mse: 15.4291\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0155 - mse: 13.5046 - val_loss: 0.0175 - val_mse: 15.2516\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0153 - mse: 13.3550 - val_loss: 0.0174 - val_mse: 15.1716\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0152 - mse: 13.2030 - val_loss: 0.0174 - val_mse: 15.1228\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0150 - mse: 13.0866 - val_loss: 0.0171 - val_mse: 14.9246\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0148 - mse: 12.9237 - val_loss: 0.0170 - val_mse: 14.7717\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0147 - mse: 12.7914 - val_loss: 0.0169 - val_mse: 14.7027\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0145 - mse: 12.6659 - val_loss: 0.0167 - val_mse: 14.5601\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0143 - mse: 12.4907 - val_loss: 0.0166 - val_mse: 14.4634\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0142 - mse: 12.3925 - val_loss: 0.0165 - val_mse: 14.3278\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0140 - mse: 12.2209 - val_loss: 0.0163 - val_mse: 14.2055\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0139 - mse: 12.0949 - val_loss: 0.0162 - val_mse: 14.0700\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0137 - mse: 11.9614 - val_loss: 0.0160 - val_mse: 13.9700\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0136 - mse: 11.8142 - val_loss: 0.0159 - val_mse: 13.8231\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0134 - mse: 11.6685 - val_loss: 0.0157 - val_mse: 13.6830\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0133 - mse: 11.5536 - val_loss: 0.0156 - val_mse: 13.5481\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0131 - mse: 11.3945 - val_loss: 0.0154 - val_mse: 13.4229\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0129 - mse: 11.2576 - val_loss: 0.0153 - val_mse: 13.2948\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0128 - mse: 11.1308 - val_loss: 0.0151 - val_mse: 13.1844\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0126 - mse: 10.9957 - val_loss: 0.0150 - val_mse: 13.0545\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0125 - mse: 10.8512 - val_loss: 0.0148 - val_mse: 12.9289\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0123 - mse: 10.7111 - val_loss: 0.0147 - val_mse: 12.8232\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0122 - mse: 10.5947 - val_loss: 0.0146 - val_mse: 12.6943\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0120 - mse: 10.4683 - val_loss: 0.0145 - val_mse: 12.6008\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0119 - mse: 10.3370 - val_loss: 0.0143 - val_mse: 12.4704\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0117 - mse: 10.1961 - val_loss: 0.0142 - val_mse: 12.3436\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0115 - mse: 10.0538 - val_loss: 0.0140 - val_mse: 12.2090\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0114 - mse: 9.9208 - val_loss: 0.0139 - val_mse: 12.0768\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0112 - mse: 9.7844 - val_loss: 0.0137 - val_mse: 11.9577\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0111 - mse: 9.6531 - val_loss: 0.0136 - val_mse: 11.8251\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0109 - mse: 9.5238 - val_loss: 0.0134 - val_mse: 11.6776\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0108 - mse: 9.3924 - val_loss: 0.0133 - val_mse: 11.5467\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0106 - mse: 9.2647 - val_loss: 0.0131 - val_mse: 11.4328\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0105 - mse: 9.1514 - val_loss: 0.0130 - val_mse: 11.3513\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0104 - mse: 9.0973 - val_loss: 0.0134 - val_mse: 11.6528\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0108 - mse: 9.3856 - val_loss: 0.0152 - val_mse: 13.1960\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0128 - mse: 11.1173 - val_loss: 0.0233 - val_mse: 20.3008\n",
      "Epoch 152/1000\n",
      "\n",
      "Epoch 00151: reducing learning rate to 0.00699999984354.\n",
      "4s - loss: 0.0211 - mse: 18.3754 - val_loss: 0.0179 - val_mse: 15.6283\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0160 - mse: 13.9253 - val_loss: 0.0140 - val_mse: 12.1597\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0116 - mse: 10.1406 - val_loss: 0.0168 - val_mse: 14.6359\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0141 - mse: 12.2583 - val_loss: 0.0143 - val_mse: 12.4267\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0118 - mse: 10.2676 - val_loss: 0.0141 - val_mse: 12.2520\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0117 - mse: 10.2289 - val_loss: 0.0150 - val_mse: 13.0470\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0127 - mse: 11.0297 - val_loss: 0.0128 - val_mse: 11.1812\n",
      "Epoch 159/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0102 - mse: 8.8796 - val_loss: 0.0154 - val_mse: 13.3762\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0125 - mse: 10.8895 - val_loss: 0.0127 - val_mse: 11.0526\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0101 - mse: 8.7675 - val_loss: 0.0132 - val_mse: 11.5201\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0110 - mse: 9.6209 - val_loss: 0.0130 - val_mse: 11.2908\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0109 - mse: 9.5281 - val_loss: 0.0120 - val_mse: 10.4656\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0098 - mse: 8.5056 - val_loss: 0.0134 - val_mse: 11.6785\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0110 - mse: 9.5502 - val_loss: 0.0122 - val_mse: 10.6215\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0099 - mse: 8.5793 - val_loss: 0.0121 - val_mse: 10.5536\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0099 - mse: 8.6541 - val_loss: 0.0124 - val_mse: 10.8138\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0102 - mse: 8.9010 - val_loss: 0.0119 - val_mse: 10.3536\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0095 - mse: 8.2388 - val_loss: 0.0124 - val_mse: 10.8166\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0098 - mse: 8.5691 - val_loss: 0.0121 - val_mse: 10.5364\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0096 - mse: 8.3524 - val_loss: 0.0117 - val_mse: 10.2192\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0094 - mse: 8.1654 - val_loss: 0.0117 - val_mse: 10.2264\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0095 - mse: 8.2495 - val_loss: 0.0116 - val_mse: 10.0934\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0092 - mse: 8.0461 - val_loss: 0.0116 - val_mse: 10.1245\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0092 - mse: 8.0141 - val_loss: 0.0114 - val_mse: 9.9554\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0091 - mse: 7.9218 - val_loss: 0.0113 - val_mse: 9.7992\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0090 - mse: 7.8787 - val_loss: 0.0111 - val_mse: 9.6279\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0089 - mse: 7.7648 - val_loss: 0.0111 - val_mse: 9.6651\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0089 - mse: 7.7092 - val_loss: 0.0112 - val_mse: 9.7758\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0088 - mse: 7.6995 - val_loss: 0.0110 - val_mse: 9.5987\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0087 - mse: 7.5392 - val_loss: 0.0109 - val_mse: 9.5344\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0087 - mse: 7.5664 - val_loss: 0.0108 - val_mse: 9.3685\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0086 - mse: 7.4790 - val_loss: 0.0106 - val_mse: 9.2601\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0085 - mse: 7.3652 - val_loss: 0.0107 - val_mse: 9.3599\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0085 - mse: 7.4229 - val_loss: 0.0105 - val_mse: 9.1697\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0083 - mse: 7.2612 - val_loss: 0.0105 - val_mse: 9.1097\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0083 - mse: 7.2530 - val_loss: 0.0104 - val_mse: 9.0689\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0083 - mse: 7.2236 - val_loss: 0.0103 - val_mse: 8.9833\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0081 - mse: 7.0960 - val_loss: 0.0104 - val_mse: 9.0673\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0082 - mse: 7.1390 - val_loss: 0.0102 - val_mse: 8.9195\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0081 - mse: 7.0260 - val_loss: 0.0101 - val_mse: 8.8371\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0080 - mse: 6.9924 - val_loss: 0.0101 - val_mse: 8.8229\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0080 - mse: 6.9717 - val_loss: 0.0101 - val_mse: 8.7672\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0079 - mse: 6.8740 - val_loss: 0.0101 - val_mse: 8.7882\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0079 - mse: 6.8808 - val_loss: 0.0099 - val_mse: 8.6577\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0078 - mse: 6.7978 - val_loss: 0.0099 - val_mse: 8.5859\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0078 - mse: 6.7652 - val_loss: 0.0098 - val_mse: 8.5694\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0077 - mse: 6.7270 - val_loss: 0.0098 - val_mse: 8.5550\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0077 - mse: 6.6666 - val_loss: 0.0098 - val_mse: 8.5386\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0076 - mse: 6.6440 - val_loss: 0.0097 - val_mse: 8.4425\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0076 - mse: 6.5868 - val_loss: 0.0096 - val_mse: 8.3876\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0075 - mse: 6.5501 - val_loss: 0.0096 - val_mse: 8.3681\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0075 - mse: 6.5120 - val_loss: 0.0096 - val_mse: 8.3390\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0074 - mse: 6.4652 - val_loss: 0.0095 - val_mse: 8.2946\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0074 - mse: 6.4317 - val_loss: 0.0095 - val_mse: 8.2354\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0073 - mse: 6.3882 - val_loss: 0.0094 - val_mse: 8.2053\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0073 - mse: 6.3484 - val_loss: 0.0094 - val_mse: 8.1851\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0073 - mse: 6.3152 - val_loss: 0.0093 - val_mse: 8.1329\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0072 - mse: 6.2685 - val_loss: 0.0093 - val_mse: 8.0921\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0072 - mse: 6.2398 - val_loss: 0.0093 - val_mse: 8.0554\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0071 - mse: 6.1920 - val_loss: 0.0092 - val_mse: 8.0436\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0071 - mse: 6.1620 - val_loss: 0.0092 - val_mse: 7.9965\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0070 - mse: 6.1190 - val_loss: 0.0091 - val_mse: 7.9451\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0070 - mse: 6.0846 - val_loss: 0.0091 - val_mse: 7.9130\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0069 - mse: 6.0475 - val_loss: 0.0091 - val_mse: 7.9001\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0069 - mse: 6.0088 - val_loss: 0.0090 - val_mse: 7.8788\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0069 - mse: 5.9753 - val_loss: 0.0090 - val_mse: 7.8317\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0068 - mse: 5.9356 - val_loss: 0.0090 - val_mse: 7.7992\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0068 - mse: 5.9037 - val_loss: 0.0089 - val_mse: 7.7735\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0067 - mse: 5.8645 - val_loss: 0.0089 - val_mse: 7.7476\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0067 - mse: 5.8322 - val_loss: 0.0089 - val_mse: 7.7070\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0067 - mse: 5.7943 - val_loss: 0.0088 - val_mse: 7.6792\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0066 - mse: 5.7615 - val_loss: 0.0088 - val_mse: 7.6546\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0066 - mse: 5.7253 - val_loss: 0.0088 - val_mse: 7.6207\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0065 - mse: 5.6914 - val_loss: 0.0087 - val_mse: 7.5818\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0065 - mse: 5.6568 - val_loss: 0.0087 - val_mse: 7.5575\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0065 - mse: 5.6225 - val_loss: 0.0087 - val_mse: 7.5388\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0064 - mse: 5.5886 - val_loss: 0.0086 - val_mse: 7.5073\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0064 - mse: 5.5540 - val_loss: 0.0086 - val_mse: 7.4738\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0063 - mse: 5.5204 - val_loss: 0.0086 - val_mse: 7.4471\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0063 - mse: 5.4861 - val_loss: 0.0085 - val_mse: 7.4208\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0063 - mse: 5.4529 - val_loss: 0.0085 - val_mse: 7.3862\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0062 - mse: 5.4191 - val_loss: 0.0084 - val_mse: 7.3535\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0062 - mse: 5.3858 - val_loss: 0.0084 - val_mse: 7.3260\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0061 - mse: 5.3526 - val_loss: 0.0084 - val_mse: 7.2946\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0061 - mse: 5.3194 - val_loss: 0.0083 - val_mse: 7.2617\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0061 - mse: 5.2871 - val_loss: 0.0083 - val_mse: 7.2341\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0060 - mse: 5.2546 - val_loss: 0.0083 - val_mse: 7.2060\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0060 - mse: 5.2225 - val_loss: 0.0082 - val_mse: 7.1708\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0060 - mse: 5.1900 - val_loss: 0.0082 - val_mse: 7.1381\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0059 - mse: 5.1576 - val_loss: 0.0082 - val_mse: 7.1107\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0059 - mse: 5.1251 - val_loss: 0.0081 - val_mse: 7.0818\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0058 - mse: 5.0928 - val_loss: 0.0081 - val_mse: 7.0489\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0058 - mse: 5.0602 - val_loss: 0.0081 - val_mse: 7.0190\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0058 - mse: 5.0287 - val_loss: 0.0080 - val_mse: 6.9896\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0057 - mse: 4.9964 - val_loss: 0.0080 - val_mse: 6.9597\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0057 - mse: 4.9654 - val_loss: 0.0080 - val_mse: 6.9295\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0057 - mse: 4.9338 - val_loss: 0.0079 - val_mse: 6.8999\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0056 - mse: 4.9030 - val_loss: 0.0079 - val_mse: 6.8678\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0056 - mse: 4.8716 - val_loss: 0.0079 - val_mse: 6.8366\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0056 - mse: 4.8405 - val_loss: 0.0078 - val_mse: 6.8078\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0055 - mse: 4.8092 - val_loss: 0.0078 - val_mse: 6.7795\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0055 - mse: 4.7783 - val_loss: 0.0078 - val_mse: 6.7510\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0055 - mse: 4.7480 - val_loss: 0.0077 - val_mse: 6.7205\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0054 - mse: 4.7173 - val_loss: 0.0077 - val_mse: 6.6893\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0054 - mse: 4.6872 - val_loss: 0.0076 - val_mse: 6.6591\n",
      "Epoch 257/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0053 - mse: 4.6566 - val_loss: 0.0076 - val_mse: 6.6326\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0053 - mse: 4.6262 - val_loss: 0.0076 - val_mse: 6.6053\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0053 - mse: 4.5957 - val_loss: 0.0076 - val_mse: 6.5755\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0052 - mse: 4.5650 - val_loss: 0.0075 - val_mse: 6.5475\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0052 - mse: 4.5345 - val_loss: 0.0075 - val_mse: 6.5181\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0052 - mse: 4.5037 - val_loss: 0.0074 - val_mse: 6.4874\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0051 - mse: 4.4729 - val_loss: 0.0074 - val_mse: 6.4608\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0051 - mse: 4.4423 - val_loss: 0.0074 - val_mse: 6.4313\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0051 - mse: 4.4115 - val_loss: 0.0073 - val_mse: 6.4006\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0050 - mse: 4.3810 - val_loss: 0.0073 - val_mse: 6.3741\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0050 - mse: 4.3504 - val_loss: 0.0073 - val_mse: 6.3459\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.0050 - mse: 4.3199 - val_loss: 0.0073 - val_mse: 6.3159\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0049 - mse: 4.2891 - val_loss: 0.0072 - val_mse: 6.2899\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0049 - mse: 4.2585 - val_loss: 0.0072 - val_mse: 6.2587\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0049 - mse: 4.2278 - val_loss: 0.0072 - val_mse: 6.2270\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0048 - mse: 4.1973 - val_loss: 0.0071 - val_mse: 6.2012\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0048 - mse: 4.1669 - val_loss: 0.0071 - val_mse: 6.1733\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0047 - mse: 4.1364 - val_loss: 0.0071 - val_mse: 6.1447\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0047 - mse: 4.1059 - val_loss: 0.0070 - val_mse: 6.1213\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0047 - mse: 4.0753 - val_loss: 0.0070 - val_mse: 6.0914\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0046 - mse: 4.0450 - val_loss: 0.0070 - val_mse: 6.0650\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0046 - mse: 4.0152 - val_loss: 0.0069 - val_mse: 6.0405\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0046 - mse: 3.9855 - val_loss: 0.0069 - val_mse: 6.0127\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0045 - mse: 3.9562 - val_loss: 0.0069 - val_mse: 5.9862\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0045 - mse: 3.9271 - val_loss: 0.0068 - val_mse: 5.9633\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0045 - mse: 3.8989 - val_loss: 0.0068 - val_mse: 5.9351\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0044 - mse: 3.8741 - val_loss: 0.0068 - val_mse: 5.9392\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0044 - mse: 3.8651 - val_loss: 0.0069 - val_mse: 5.9906\n",
      "Epoch 285/1000\n",
      "\n",
      "Epoch 00284: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0045 - mse: 3.9317 - val_loss: 0.0074 - val_mse: 6.4849\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0050 - mse: 4.3676 - val_loss: 0.0076 - val_mse: 6.6419\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0053 - mse: 4.5842 - val_loss: 0.0068 - val_mse: 5.9248\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0044 - mse: 3.8386 - val_loss: 0.0070 - val_mse: 6.0838\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0046 - mse: 3.9959 - val_loss: 0.0073 - val_mse: 6.3759\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0049 - mse: 4.3048 - val_loss: 0.0067 - val_mse: 5.8603\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0043 - mse: 3.7663 - val_loss: 0.0069 - val_mse: 5.9947\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0045 - mse: 3.9078 - val_loss: 0.0071 - val_mse: 6.1498\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0047 - mse: 4.0698 - val_loss: 0.0066 - val_mse: 5.7527\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0042 - mse: 3.6649 - val_loss: 0.0068 - val_mse: 5.9603\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0044 - mse: 3.8745 - val_loss: 0.0068 - val_mse: 5.9401\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0044 - mse: 3.8498 - val_loss: 0.0065 - val_mse: 5.6931\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0041 - mse: 3.6130 - val_loss: 0.0068 - val_mse: 5.9140\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0044 - mse: 3.8251 - val_loss: 0.0067 - val_mse: 5.8132\n",
      "Epoch 299/1000\n",
      "\n",
      "Epoch 00298: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0042 - mse: 3.6756 - val_loss: 0.0066 - val_mse: 5.7334\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0041 - mse: 3.5992 - val_loss: 0.0066 - val_mse: 5.7317\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0041 - mse: 3.6070 - val_loss: 0.0066 - val_mse: 5.7135\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0041 - mse: 3.5713 - val_loss: 0.0066 - val_mse: 5.7245\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0041 - mse: 3.5693 - val_loss: 0.0065 - val_mse: 5.6916\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0041 - mse: 3.5444 - val_loss: 0.0065 - val_mse: 5.6687\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0041 - mse: 3.5355 - val_loss: 0.0065 - val_mse: 5.6639\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0040 - mse: 3.5157 - val_loss: 0.0065 - val_mse: 5.6728\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0040 - mse: 3.5045 - val_loss: 0.0065 - val_mse: 5.6549\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0040 - mse: 3.4874 - val_loss: 0.0065 - val_mse: 5.6308\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0040 - mse: 3.4759 - val_loss: 0.0065 - val_mse: 5.6232\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0040 - mse: 3.4590 - val_loss: 0.0065 - val_mse: 5.6317\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0040 - mse: 3.4489 - val_loss: 0.0065 - val_mse: 5.6177\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0039 - mse: 3.4313 - val_loss: 0.0064 - val_mse: 5.6026\n",
      "Epoch 313/1000\n",
      "\n",
      "Epoch 00312: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0039 - mse: 3.4225 - val_loss: 0.0064 - val_mse: 5.5928\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0039 - mse: 3.4040 - val_loss: 0.0064 - val_mse: 5.5868\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0039 - mse: 3.3882 - val_loss: 0.0064 - val_mse: 5.5927\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0039 - mse: 3.3915 - val_loss: 0.0064 - val_mse: 5.5659\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0039 - mse: 3.3685 - val_loss: 0.0064 - val_mse: 5.5641\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0039 - mse: 3.3703 - val_loss: 0.0064 - val_mse: 5.5560\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0039 - mse: 3.3557 - val_loss: 0.0064 - val_mse: 5.5565\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0038 - mse: 3.3466 - val_loss: 0.0064 - val_mse: 5.5547\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0038 - mse: 3.3424 - val_loss: 0.0064 - val_mse: 5.5342\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0038 - mse: 3.3262 - val_loss: 0.0064 - val_mse: 5.5305\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0038 - mse: 3.3249 - val_loss: 0.0063 - val_mse: 5.5233\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0038 - mse: 3.3102 - val_loss: 0.0063 - val_mse: 5.5269\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0038 - mse: 3.3043 - val_loss: 0.0063 - val_mse: 5.5205\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0038 - mse: 3.2955 - val_loss: 0.0063 - val_mse: 5.5050\n",
      "Epoch 327/1000\n",
      "\n",
      "Epoch 00326: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0038 - mse: 3.2842 - val_loss: 0.0063 - val_mse: 5.4979\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0038 - mse: 3.2794 - val_loss: 0.0063 - val_mse: 5.4926\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0038 - mse: 3.2697 - val_loss: 0.0063 - val_mse: 5.4941\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0037 - mse: 3.2646 - val_loss: 0.0063 - val_mse: 5.4940\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0037 - mse: 3.2603 - val_loss: 0.0063 - val_mse: 5.4853\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0037 - mse: 3.2516 - val_loss: 0.0063 - val_mse: 5.4783\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0037 - mse: 3.2466 - val_loss: 0.0063 - val_mse: 5.4733\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0037 - mse: 3.2415 - val_loss: 0.0063 - val_mse: 5.4695\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0037 - mse: 3.2336 - val_loss: 0.0063 - val_mse: 5.4700\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0037 - mse: 3.2288 - val_loss: 0.0063 - val_mse: 5.4677\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0037 - mse: 3.2230 - val_loss: 0.0063 - val_mse: 5.4612\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.0037 - mse: 3.2157 - val_loss: 0.0063 - val_mse: 5.4561\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0037 - mse: 3.2108 - val_loss: 0.0063 - val_mse: 5.4516\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0037 - mse: 3.2046 - val_loss: 0.0063 - val_mse: 5.4489\n",
      "Epoch 341/1000\n",
      "\n",
      "Epoch 00340: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0037 - mse: 3.1979 - val_loss: 0.0063 - val_mse: 5.4486\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0037 - mse: 3.1928 - val_loss: 0.0063 - val_mse: 5.4467\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.0037 - mse: 3.1883 - val_loss: 0.0063 - val_mse: 5.4432\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0037 - mse: 3.1836 - val_loss: 0.0062 - val_mse: 5.4400\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.0037 - mse: 3.1798 - val_loss: 0.0062 - val_mse: 5.4367\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.0036 - mse: 3.1758 - val_loss: 0.0062 - val_mse: 5.4335\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.0036 - mse: 3.1712 - val_loss: 0.0062 - val_mse: 5.4311\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0036 - mse: 3.1669 - val_loss: 0.0062 - val_mse: 5.4292\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.0036 - mse: 3.1631 - val_loss: 0.0062 - val_mse: 5.4268\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0036 - mse: 3.1587 - val_loss: 0.0062 - val_mse: 5.4242\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.0036 - mse: 3.1543 - val_loss: 0.0062 - val_mse: 5.4224\n",
      "Epoch 352/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0036 - mse: 3.1503 - val_loss: 0.0062 - val_mse: 5.4212\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.0036 - mse: 3.1462 - val_loss: 0.0062 - val_mse: 5.4204\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.0036 - mse: 3.1417 - val_loss: 0.0062 - val_mse: 5.4200\n",
      "Epoch 355/1000\n",
      "\n",
      "Epoch 00354: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0036 - mse: 3.1376 - val_loss: 0.0062 - val_mse: 5.4187\n",
      "Epoch 356/1000\n",
      "0s - loss: 0.0036 - mse: 3.1335 - val_loss: 0.0062 - val_mse: 5.4167\n",
      "Epoch 357/1000\n",
      "0s - loss: 0.0036 - mse: 3.1305 - val_loss: 0.0062 - val_mse: 5.4144\n",
      "Epoch 358/1000\n",
      "0s - loss: 0.0036 - mse: 3.1274 - val_loss: 0.0062 - val_mse: 5.4122\n",
      "Epoch 359/1000\n",
      "0s - loss: 0.0036 - mse: 3.1246 - val_loss: 0.0062 - val_mse: 5.4103\n",
      "Epoch 360/1000\n",
      "0s - loss: 0.0036 - mse: 3.1217 - val_loss: 0.0062 - val_mse: 5.4087\n",
      "Epoch 361/1000\n",
      "0s - loss: 0.0036 - mse: 3.1187 - val_loss: 0.0062 - val_mse: 5.4074\n",
      "Epoch 362/1000\n",
      "0s - loss: 0.0036 - mse: 3.1157 - val_loss: 0.0062 - val_mse: 5.4061\n",
      "Epoch 363/1000\n",
      "0s - loss: 0.0036 - mse: 3.1128 - val_loss: 0.0062 - val_mse: 5.4045\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}, {'loss': 0.0013604337582364678, 'dropout': 0.0, 'val_mse': 3.409489631652832, 'width': 210, 'mse': 1.1847187280654907, 'val_loss': 0.003915177658200264}, {'loss': 0.0069423709064722061, 'dropout': 0.0, 'val_mse': 9.2405805587768555, 'width': 310, 'mse': 6.0456881523132324, 'val_loss': 0.010611124336719513}, {'loss': 0.023029616102576256, 'dropout': 0.0, 'val_mse': 22.774328231811523, 'width': 410, 'mse': 20.055086135864258, 'val_loss': 0.026152169331908226}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_66 (LSTM)               (None, None, 150)         91800     \n",
      "_________________________________________________________________\n",
      "time_distributed_63 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 91,951\n",
      "Trainable params: 91,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 0.8630 - mse: 751.5712 - val_loss: 0.5130 - val_mse: 446.7815\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.4781 - mse: 416.3656 - val_loss: 0.3998 - val_mse: 348.1419\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.3805 - mse: 331.3315 - val_loss: 0.3704 - val_mse: 322.6012\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.3564 - mse: 310.4024 - val_loss: 0.2307 - val_mse: 200.9016\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2203 - mse: 191.8124 - val_loss: 0.2507 - val_mse: 218.3298\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2409 - mse: 209.7416 - val_loss: 0.1624 - val_mse: 141.4271\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.1515 - mse: 131.9211 - val_loss: 0.1455 - val_mse: 126.6745\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1361 - mse: 118.5520 - val_loss: 0.1455 - val_mse: 126.7248\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1359 - mse: 118.3692 - val_loss: 0.1394 - val_mse: 121.4077\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1299 - mse: 113.1359 - val_loss: 0.1353 - val_mse: 117.8349\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1245 - mse: 108.3821 - val_loss: 0.1368 - val_mse: 119.1356\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1218 - mse: 106.1045 - val_loss: 0.1359 - val_mse: 118.3055\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1158 - mse: 100.8062 - val_loss: 0.1263 - val_mse: 109.9823\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1046 - mse: 91.0828 - val_loss: 0.1140 - val_mse: 99.3099\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.0950 - mse: 82.7512 - val_loss: 0.1058 - val_mse: 92.1564\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.0909 - mse: 79.1201 - val_loss: 0.1000 - val_mse: 87.1258\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0887 - mse: 77.2689 - val_loss: 0.0928 - val_mse: 80.8555\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0845 - mse: 73.6098 - val_loss: 0.0844 - val_mse: 73.4777\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0784 - mse: 68.3110 - val_loss: 0.0771 - val_mse: 67.1210\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0726 - mse: 63.2596 - val_loss: 0.0721 - val_mse: 62.7916\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0682 - mse: 59.3904 - val_loss: 0.0688 - val_mse: 59.9564\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0652 - mse: 56.7516 - val_loss: 0.0659 - val_mse: 57.3633\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0613 - mse: 53.4228 - val_loss: 0.0657 - val_mse: 57.2506\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0599 - mse: 52.1836 - val_loss: 0.0617 - val_mse: 53.6987\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0559 - mse: 48.7128 - val_loss: 0.0593 - val_mse: 51.6100\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0534 - mse: 46.5149 - val_loss: 0.0582 - val_mse: 50.6626\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0525 - mse: 45.7007 - val_loss: 0.0563 - val_mse: 49.0643\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0507 - mse: 44.1691 - val_loss: 0.0545 - val_mse: 47.4181\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0491 - mse: 42.7447 - val_loss: 0.0534 - val_mse: 46.4839\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0481 - mse: 41.8919 - val_loss: 0.0515 - val_mse: 44.8623\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0458 - mse: 39.8668 - val_loss: 0.0493 - val_mse: 42.9422\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0441 - mse: 38.4342 - val_loss: 0.0463 - val_mse: 40.2958\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0422 - mse: 36.7609 - val_loss: 0.0444 - val_mse: 38.6655\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0402 - mse: 34.9881 - val_loss: 0.0442 - val_mse: 38.4729\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0392 - mse: 34.1697 - val_loss: 0.0441 - val_mse: 38.4033\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0391 - mse: 34.0197 - val_loss: 0.0424 - val_mse: 36.9087\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0380 - mse: 33.0499 - val_loss: 0.0413 - val_mse: 35.9698\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0373 - mse: 32.5004 - val_loss: 0.0404 - val_mse: 35.1709\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0365 - mse: 31.7811 - val_loss: 0.0396 - val_mse: 34.4672\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0354 - mse: 30.8358 - val_loss: 0.0388 - val_mse: 33.7848\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0343 - mse: 29.8948 - val_loss: 0.0379 - val_mse: 33.0157\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0333 - mse: 29.0378 - val_loss: 0.0367 - val_mse: 31.9662\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0321 - mse: 27.9604 - val_loss: 0.0363 - val_mse: 31.5704\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0317 - mse: 27.5694 - val_loss: 0.0360 - val_mse: 31.3159\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0313 - mse: 27.2160 - val_loss: 0.0357 - val_mse: 31.0750\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0308 - mse: 26.8148 - val_loss: 0.0355 - val_mse: 30.8894\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0305 - mse: 26.5977 - val_loss: 0.0345 - val_mse: 30.0874\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0299 - mse: 26.0113 - val_loss: 0.0333 - val_mse: 28.9657\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0291 - mse: 25.3463 - val_loss: 0.0322 - val_mse: 28.0363\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0285 - mse: 24.8263 - val_loss: 0.0313 - val_mse: 27.2951\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0278 - mse: 24.2528 - val_loss: 0.0309 - val_mse: 26.8816\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0274 - mse: 23.8649 - val_loss: 0.0305 - val_mse: 26.5791\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0271 - mse: 23.6121 - val_loss: 0.0299 - val_mse: 26.0136\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0266 - mse: 23.1924 - val_loss: 0.0294 - val_mse: 25.5686\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0263 - mse: 22.8730 - val_loss: 0.0289 - val_mse: 25.1634\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0258 - mse: 22.4982 - val_loss: 0.0285 - val_mse: 24.8341\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0254 - mse: 22.1050 - val_loss: 0.0283 - val_mse: 24.6190\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0251 - mse: 21.8452 - val_loss: 0.0278 - val_mse: 24.2502\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0247 - mse: 21.5473 - val_loss: 0.0273 - val_mse: 23.7616\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0243 - mse: 21.2030 - val_loss: 0.0268 - val_mse: 23.3624\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0240 - mse: 20.9054 - val_loss: 0.0265 - val_mse: 23.0499\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0236 - mse: 20.5885 - val_loss: 0.0262 - val_mse: 22.8561\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0234 - mse: 20.3582 - val_loss: 0.0260 - val_mse: 22.6032\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0231 - mse: 20.1140 - val_loss: 0.0255 - val_mse: 22.2359\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0227 - mse: 19.8058 - val_loss: 0.0252 - val_mse: 21.9072\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0224 - mse: 19.4952 - val_loss: 0.0249 - val_mse: 21.6480\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0220 - mse: 19.1558 - val_loss: 0.0247 - val_mse: 21.5136\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0217 - mse: 18.8714 - val_loss: 0.0245 - val_mse: 21.3597\n",
      "Epoch 69/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0213 - mse: 18.5920 - val_loss: 0.0242 - val_mse: 21.1137\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0210 - mse: 18.2901 - val_loss: 0.0239 - val_mse: 20.8521\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0207 - mse: 17.9994 - val_loss: 0.0236 - val_mse: 20.5794\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0203 - mse: 17.6925 - val_loss: 0.0233 - val_mse: 20.3116\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0200 - mse: 17.4268 - val_loss: 0.0230 - val_mse: 19.9873\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0197 - mse: 17.1374 - val_loss: 0.0226 - val_mse: 19.7041\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0194 - mse: 16.8576 - val_loss: 0.0224 - val_mse: 19.4932\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0190 - mse: 16.5655 - val_loss: 0.0222 - val_mse: 19.3458\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0187 - mse: 16.3115 - val_loss: 0.0219 - val_mse: 19.1090\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0184 - mse: 16.0324 - val_loss: 0.0216 - val_mse: 18.8344\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0181 - mse: 15.7477 - val_loss: 0.0214 - val_mse: 18.6606\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0178 - mse: 15.5053 - val_loss: 0.0220 - val_mse: 19.1249\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0183 - mse: 15.9142 - val_loss: 0.0275 - val_mse: 23.9812\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0244 - mse: 21.2131 - val_loss: 0.0388 - val_mse: 33.7950\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0366 - mse: 31.8440 - val_loss: 0.0301 - val_mse: 26.1889\n",
      "Epoch 84/1000\n",
      "\n",
      "Epoch 00083: reducing learning rate to 0.00699999984354.\n",
      "4s - loss: 0.0278 - mse: 24.2076 - val_loss: 0.0300 - val_mse: 26.0829\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0284 - mse: 24.7561 - val_loss: 0.0216 - val_mse: 18.7935\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0197 - mse: 17.1964 - val_loss: 0.0256 - val_mse: 22.3102\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0241 - mse: 20.9736 - val_loss: 0.0265 - val_mse: 23.0831\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0253 - mse: 22.0335 - val_loss: 0.0219 - val_mse: 19.0528\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0201 - mse: 17.4831 - val_loss: 0.0227 - val_mse: 19.7479\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0200 - mse: 17.4587 - val_loss: 0.0250 - val_mse: 21.7289\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0220 - mse: 19.1323 - val_loss: 0.0236 - val_mse: 20.5764\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0206 - mse: 17.9774 - val_loss: 0.0218 - val_mse: 19.0136\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0189 - mse: 16.4719 - val_loss: 0.0220 - val_mse: 19.1784\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0192 - mse: 16.7359 - val_loss: 0.0219 - val_mse: 19.0499\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0192 - mse: 16.6781 - val_loss: 0.0215 - val_mse: 18.7066\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0186 - mse: 16.2134 - val_loss: 0.0212 - val_mse: 18.4951\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0183 - mse: 15.9090 - val_loss: 0.0209 - val_mse: 18.1731\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0180 - mse: 15.6788 - val_loss: 0.0205 - val_mse: 17.8761\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0178 - mse: 15.5376 - val_loss: 0.0202 - val_mse: 17.6114\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0177 - mse: 15.3714 - val_loss: 0.0198 - val_mse: 17.2172\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0172 - mse: 14.9763 - val_loss: 0.0195 - val_mse: 16.9672\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0168 - mse: 14.6056 - val_loss: 0.0196 - val_mse: 17.0674\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0167 - mse: 14.5599 - val_loss: 0.0194 - val_mse: 16.8739\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0165 - mse: 14.3652 - val_loss: 0.0188 - val_mse: 16.3411\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0160 - mse: 13.9299 - val_loss: 0.0186 - val_mse: 16.2109\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0158 - mse: 13.7992 - val_loss: 0.0188 - val_mse: 16.3828\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0159 - mse: 13.8681 - val_loss: 0.0185 - val_mse: 16.1143\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0155 - mse: 13.5407 - val_loss: 0.0180 - val_mse: 15.6874\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0150 - mse: 13.0663 - val_loss: 0.0182 - val_mse: 15.8283\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0150 - mse: 13.0624 - val_loss: 0.0184 - val_mse: 16.0155\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0151 - mse: 13.1287 - val_loss: 0.0179 - val_mse: 15.6176\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0147 - mse: 12.7586 - val_loss: 0.0175 - val_mse: 15.2642\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0143 - mse: 12.4542 - val_loss: 0.0177 - val_mse: 15.3857\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0144 - mse: 12.5228 - val_loss: 0.0177 - val_mse: 15.4310\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0143 - mse: 12.4849 - val_loss: 0.0174 - val_mse: 15.1398\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0140 - mse: 12.1538 - val_loss: 0.0172 - val_mse: 14.9733\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0137 - mse: 11.9408 - val_loss: 0.0173 - val_mse: 15.0426\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0137 - mse: 11.9503 - val_loss: 0.0171 - val_mse: 14.9291\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0136 - mse: 11.8430 - val_loss: 0.0168 - val_mse: 14.6219\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0133 - mse: 11.6154 - val_loss: 0.0166 - val_mse: 14.4297\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0132 - mse: 11.5020 - val_loss: 0.0164 - val_mse: 14.3171\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0131 - mse: 11.4373 - val_loss: 0.0163 - val_mse: 14.1530\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0130 - mse: 11.2910 - val_loss: 0.0161 - val_mse: 14.0274\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0128 - mse: 11.1380 - val_loss: 0.0161 - val_mse: 13.9922\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0127 - mse: 11.0440 - val_loss: 0.0160 - val_mse: 13.9166\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0126 - mse: 10.9417 - val_loss: 0.0158 - val_mse: 13.7476\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0124 - mse: 10.8138 - val_loss: 0.0156 - val_mse: 13.5619\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0123 - mse: 10.6984 - val_loss: 0.0154 - val_mse: 13.4009\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0122 - mse: 10.5849 - val_loss: 0.0153 - val_mse: 13.2913\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0120 - mse: 10.4790 - val_loss: 0.0152 - val_mse: 13.2264\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0119 - mse: 10.3835 - val_loss: 0.0151 - val_mse: 13.1489\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0118 - mse: 10.2709 - val_loss: 0.0150 - val_mse: 13.0490\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0117 - mse: 10.1558 - val_loss: 0.0149 - val_mse: 12.9536\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0116 - mse: 10.0711 - val_loss: 0.0147 - val_mse: 12.8277\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0115 - mse: 9.9770 - val_loss: 0.0145 - val_mse: 12.6664\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0113 - mse: 9.8580 - val_loss: 0.0144 - val_mse: 12.5356\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0112 - mse: 9.7660 - val_loss: 0.0143 - val_mse: 12.4262\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0111 - mse: 9.6873 - val_loss: 0.0141 - val_mse: 12.2965\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0110 - mse: 9.5797 - val_loss: 0.0140 - val_mse: 12.1871\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0109 - mse: 9.4783 - val_loss: 0.0139 - val_mse: 12.1185\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0108 - mse: 9.4015 - val_loss: 0.0138 - val_mse: 12.0342\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0107 - mse: 9.3070 - val_loss: 0.0137 - val_mse: 11.9314\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0106 - mse: 9.2070 - val_loss: 0.0136 - val_mse: 11.8314\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0105 - mse: 9.1285 - val_loss: 0.0134 - val_mse: 11.7101\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0104 - mse: 9.0428 - val_loss: 0.0133 - val_mse: 11.5848\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0103 - mse: 8.9483 - val_loss: 0.0132 - val_mse: 11.5007\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0102 - mse: 8.8699 - val_loss: 0.0131 - val_mse: 11.4380\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0101 - mse: 8.7933 - val_loss: 0.0130 - val_mse: 11.3632\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0100 - mse: 8.7092 - val_loss: 0.0129 - val_mse: 11.2710\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0099 - mse: 8.6308 - val_loss: 0.0128 - val_mse: 11.1610\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0098 - mse: 8.5542 - val_loss: 0.0127 - val_mse: 11.0547\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0097 - mse: 8.4769 - val_loss: 0.0126 - val_mse: 10.9785\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0096 - mse: 8.4020 - val_loss: 0.0125 - val_mse: 10.9281\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0096 - mse: 8.3276 - val_loss: 0.0125 - val_mse: 10.8753\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0095 - mse: 8.2570 - val_loss: 0.0124 - val_mse: 10.7896\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0094 - mse: 8.1862 - val_loss: 0.0123 - val_mse: 10.6790\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0093 - mse: 8.1146 - val_loss: 0.0122 - val_mse: 10.5841\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0092 - mse: 8.0481 - val_loss: 0.0121 - val_mse: 10.5243\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0092 - mse: 7.9797 - val_loss: 0.0120 - val_mse: 10.4922\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0091 - mse: 7.9105 - val_loss: 0.0120 - val_mse: 10.4573\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0090 - mse: 7.8458 - val_loss: 0.0119 - val_mse: 10.3857\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0089 - mse: 7.7788 - val_loss: 0.0118 - val_mse: 10.2894\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0089 - mse: 7.7122 - val_loss: 0.0117 - val_mse: 10.2041\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0088 - mse: 7.6488 - val_loss: 0.0116 - val_mse: 10.1439\n",
      "Epoch 165/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0087 - mse: 7.5834 - val_loss: 0.0116 - val_mse: 10.1003\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0086 - mse: 7.5191 - val_loss: 0.0115 - val_mse: 10.0454\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0086 - mse: 7.4555 - val_loss: 0.0115 - val_mse: 9.9723\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0085 - mse: 7.3920 - val_loss: 0.0114 - val_mse: 9.8948\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0084 - mse: 7.3284 - val_loss: 0.0113 - val_mse: 9.8300\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0083 - mse: 7.2650 - val_loss: 0.0112 - val_mse: 9.7797\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0083 - mse: 7.2033 - val_loss: 0.0112 - val_mse: 9.7310\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0082 - mse: 7.1403 - val_loss: 0.0111 - val_mse: 9.6804\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0081 - mse: 7.0784 - val_loss: 0.0111 - val_mse: 9.6266\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0081 - mse: 7.0167 - val_loss: 0.0110 - val_mse: 9.5698\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0080 - mse: 6.9548 - val_loss: 0.0109 - val_mse: 9.5101\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0079 - mse: 6.8943 - val_loss: 0.0109 - val_mse: 9.4508\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0078 - mse: 6.8334 - val_loss: 0.0108 - val_mse: 9.4041\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0078 - mse: 6.7731 - val_loss: 0.0108 - val_mse: 9.3687\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0077 - mse: 6.7128 - val_loss: 0.0107 - val_mse: 9.3308\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0076 - mse: 6.6530 - val_loss: 0.0107 - val_mse: 9.2795\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0076 - mse: 6.5932 - val_loss: 0.0106 - val_mse: 9.2231\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0075 - mse: 6.5339 - val_loss: 0.0105 - val_mse: 9.1735\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0074 - mse: 6.4748 - val_loss: 0.0105 - val_mse: 9.1303\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0074 - mse: 6.4164 - val_loss: 0.0104 - val_mse: 9.0864\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0073 - mse: 6.3581 - val_loss: 0.0104 - val_mse: 9.0425\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0072 - mse: 6.3003 - val_loss: 0.0103 - val_mse: 9.0002\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0072 - mse: 6.2421 - val_loss: 0.0103 - val_mse: 8.9573\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0071 - mse: 6.1847 - val_loss: 0.0102 - val_mse: 8.9107\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0070 - mse: 6.1269 - val_loss: 0.0102 - val_mse: 8.8644\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0070 - mse: 6.0697 - val_loss: 0.0101 - val_mse: 8.8168\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0069 - mse: 6.0124 - val_loss: 0.0101 - val_mse: 8.7584\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0068 - mse: 5.9551 - val_loss: 0.0100 - val_mse: 8.6959\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0068 - mse: 5.8975 - val_loss: 0.0099 - val_mse: 8.6429\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0067 - mse: 5.8402 - val_loss: 0.0099 - val_mse: 8.5936\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0066 - mse: 5.7835 - val_loss: 0.0098 - val_mse: 8.5338\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0066 - mse: 5.7268 - val_loss: 0.0097 - val_mse: 8.4713\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0065 - mse: 5.6704 - val_loss: 0.0097 - val_mse: 8.4149\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0064 - mse: 5.6142 - val_loss: 0.0096 - val_mse: 8.3591\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0064 - mse: 5.5581 - val_loss: 0.0095 - val_mse: 8.3026\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0063 - mse: 5.5021 - val_loss: 0.0095 - val_mse: 8.2515\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0063 - mse: 5.4462 - val_loss: 0.0094 - val_mse: 8.2029\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0062 - mse: 5.3907 - val_loss: 0.0094 - val_mse: 8.1507\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0061 - mse: 5.3354 - val_loss: 0.0093 - val_mse: 8.1064\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0061 - mse: 5.2803 - val_loss: 0.0093 - val_mse: 8.0600\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0060 - mse: 5.2250 - val_loss: 0.0092 - val_mse: 8.0117\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0059 - mse: 5.1696 - val_loss: 0.0091 - val_mse: 7.9585\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0059 - mse: 5.1146 - val_loss: 0.0091 - val_mse: 7.9134\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0058 - mse: 5.0594 - val_loss: 0.0090 - val_mse: 7.8684\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0057 - mse: 5.0037 - val_loss: 0.0090 - val_mse: 7.8028\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0057 - mse: 4.9478 - val_loss: 0.0089 - val_mse: 7.7491\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0056 - mse: 4.8926 - val_loss: 0.0088 - val_mse: 7.6826\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0056 - mse: 4.8377 - val_loss: 0.0088 - val_mse: 7.6443\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0055 - mse: 4.7837 - val_loss: 0.0087 - val_mse: 7.5668\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0054 - mse: 4.7323 - val_loss: 0.0087 - val_mse: 7.5712\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0054 - mse: 4.6932 - val_loss: 0.0086 - val_mse: 7.5032\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0054 - mse: 4.7009 - val_loss: 0.0091 - val_mse: 7.8928\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0056 - mse: 4.9058 - val_loss: 0.0097 - val_mse: 8.4899\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0067 - mse: 5.8228 - val_loss: 0.0130 - val_mse: 11.3049\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0091 - mse: 7.9548 - val_loss: 0.0121 - val_mse: 10.5048\n",
      "Epoch 220/1000\n",
      "\n",
      "Epoch 00219: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0094 - mse: 8.1836 - val_loss: 0.0087 - val_mse: 7.5938\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0058 - mse: 5.0172 - val_loss: 0.0096 - val_mse: 8.3276\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0064 - mse: 5.5540 - val_loss: 0.0084 - val_mse: 7.2858\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0055 - mse: 4.7783 - val_loss: 0.0089 - val_mse: 7.7291\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0062 - mse: 5.3895 - val_loss: 0.0083 - val_mse: 7.2135\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0053 - mse: 4.6005 - val_loss: 0.0093 - val_mse: 8.0700\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0060 - mse: 5.1946 - val_loss: 0.0084 - val_mse: 7.3364\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0052 - mse: 4.5375 - val_loss: 0.0084 - val_mse: 7.3463\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0056 - mse: 4.8728 - val_loss: 0.0080 - val_mse: 7.0073\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0052 - mse: 4.5583 - val_loss: 0.0083 - val_mse: 7.2126\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0053 - mse: 4.5914 - val_loss: 0.0084 - val_mse: 7.3033\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0053 - mse: 4.5807 - val_loss: 0.0081 - val_mse: 7.0362\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0051 - mse: 4.4029 - val_loss: 0.0082 - val_mse: 7.0991\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0052 - mse: 4.5363 - val_loss: 0.0080 - val_mse: 6.9969\n",
      "Epoch 234/1000\n",
      "\n",
      "Epoch 00233: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0049 - mse: 4.3011 - val_loss: 0.0083 - val_mse: 7.2482\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0051 - mse: 4.4144 - val_loss: 0.0081 - val_mse: 7.0405\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0049 - mse: 4.2346 - val_loss: 0.0079 - val_mse: 6.9214\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0049 - mse: 4.2282 - val_loss: 0.0079 - val_mse: 6.8987\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0049 - mse: 4.2638 - val_loss: 0.0078 - val_mse: 6.8246\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0047 - mse: 4.1318 - val_loss: 0.0080 - val_mse: 6.9543\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0048 - mse: 4.1690 - val_loss: 0.0080 - val_mse: 6.9385\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0047 - mse: 4.1262 - val_loss: 0.0078 - val_mse: 6.8010\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0047 - mse: 4.0547 - val_loss: 0.0077 - val_mse: 6.7447\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0047 - mse: 4.0819 - val_loss: 0.0077 - val_mse: 6.6781\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0046 - mse: 4.0176 - val_loss: 0.0077 - val_mse: 6.7290\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0046 - mse: 3.9938 - val_loss: 0.0078 - val_mse: 6.7929\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0046 - mse: 3.9876 - val_loss: 0.0077 - val_mse: 6.7219\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0045 - mse: 3.9268 - val_loss: 0.0076 - val_mse: 6.6338\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0045 - mse: 3.9215 - val_loss: 0.0075 - val_mse: 6.5398\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0045 - mse: 3.8896 - val_loss: 0.0075 - val_mse: 6.5253\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0044 - mse: 3.8552 - val_loss: 0.0076 - val_mse: 6.5925\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0044 - mse: 3.8467 - val_loss: 0.0076 - val_mse: 6.5971\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0044 - mse: 3.8038 - val_loss: 0.0075 - val_mse: 6.5583\n",
      "Epoch 253/1000\n",
      "\n",
      "Epoch 00252: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0044 - mse: 3.7914 - val_loss: 0.0074 - val_mse: 6.4779\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0043 - mse: 3.7646 - val_loss: 0.0074 - val_mse: 6.4364\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0043 - mse: 3.7351 - val_loss: 0.0074 - val_mse: 6.4443\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0043 - mse: 3.7269 - val_loss: 0.0074 - val_mse: 6.4547\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0043 - mse: 3.7122 - val_loss: 0.0074 - val_mse: 6.4372\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0042 - mse: 3.6851 - val_loss: 0.0074 - val_mse: 6.4103\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0042 - mse: 3.6747 - val_loss: 0.0073 - val_mse: 6.3705\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0042 - mse: 3.6606 - val_loss: 0.0073 - val_mse: 6.3330\n",
      "Epoch 261/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0042 - mse: 3.6357 - val_loss: 0.0073 - val_mse: 6.3298\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0042 - mse: 3.6236 - val_loss: 0.0073 - val_mse: 6.3376\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0041 - mse: 3.6096 - val_loss: 0.0073 - val_mse: 6.3315\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0041 - mse: 3.5861 - val_loss: 0.0073 - val_mse: 6.3183\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0041 - mse: 3.5735 - val_loss: 0.0072 - val_mse: 6.2910\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0041 - mse: 3.5597 - val_loss: 0.0072 - val_mse: 6.2575\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0041 - mse: 3.5376 - val_loss: 0.0072 - val_mse: 6.2428\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.0040 - mse: 3.5239 - val_loss: 0.0072 - val_mse: 6.2338\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0040 - mse: 3.5092 - val_loss: 0.0071 - val_mse: 6.2159\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0040 - mse: 3.4882 - val_loss: 0.0071 - val_mse: 6.1947\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0040 - mse: 3.4741 - val_loss: 0.0071 - val_mse: 6.1652\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0040 - mse: 3.4586 - val_loss: 0.0070 - val_mse: 6.1344\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0039 - mse: 3.4387 - val_loss: 0.0070 - val_mse: 6.1200\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0039 - mse: 3.4242 - val_loss: 0.0070 - val_mse: 6.1119\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0039 - mse: 3.4078 - val_loss: 0.0070 - val_mse: 6.0998\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0039 - mse: 3.3893 - val_loss: 0.0070 - val_mse: 6.0828\n",
      "Epoch 277/1000\n",
      "\n",
      "Epoch 00276: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0039 - mse: 3.3750 - val_loss: 0.0070 - val_mse: 6.0571\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0039 - mse: 3.3585 - val_loss: 0.0069 - val_mse: 6.0384\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0038 - mse: 3.3456 - val_loss: 0.0069 - val_mse: 6.0247\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0038 - mse: 3.3347 - val_loss: 0.0069 - val_mse: 6.0143\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0038 - mse: 3.3239 - val_loss: 0.0069 - val_mse: 6.0025\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0038 - mse: 3.3115 - val_loss: 0.0069 - val_mse: 5.9888\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0038 - mse: 3.2996 - val_loss: 0.0069 - val_mse: 5.9741\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0038 - mse: 3.2888 - val_loss: 0.0068 - val_mse: 5.9585\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0038 - mse: 3.2772 - val_loss: 0.0068 - val_mse: 5.9449\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0037 - mse: 3.2649 - val_loss: 0.0068 - val_mse: 5.9353\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0037 - mse: 3.2540 - val_loss: 0.0068 - val_mse: 5.9277\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0037 - mse: 3.2431 - val_loss: 0.0068 - val_mse: 5.9183\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0037 - mse: 3.2314 - val_loss: 0.0068 - val_mse: 5.9064\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0037 - mse: 3.2201 - val_loss: 0.0068 - val_mse: 5.8926\n",
      "Epoch 291/1000\n",
      "\n",
      "Epoch 00290: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0037 - mse: 3.2093 - val_loss: 0.0067 - val_mse: 5.8773\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0037 - mse: 3.1978 - val_loss: 0.0067 - val_mse: 5.8677\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0037 - mse: 3.1896 - val_loss: 0.0067 - val_mse: 5.8599\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0037 - mse: 3.1818 - val_loss: 0.0067 - val_mse: 5.8536\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0036 - mse: 3.1740 - val_loss: 0.0067 - val_mse: 5.8472\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0036 - mse: 3.1661 - val_loss: 0.0067 - val_mse: 5.8397\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0036 - mse: 3.1580 - val_loss: 0.0067 - val_mse: 5.8310\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0036 - mse: 3.1500 - val_loss: 0.0067 - val_mse: 5.8215\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0036 - mse: 3.1422 - val_loss: 0.0067 - val_mse: 5.8118\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0036 - mse: 3.1341 - val_loss: 0.0067 - val_mse: 5.8026\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0036 - mse: 3.1260 - val_loss: 0.0067 - val_mse: 5.7945\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0036 - mse: 3.1179 - val_loss: 0.0066 - val_mse: 5.7872\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0036 - mse: 3.1100 - val_loss: 0.0066 - val_mse: 5.7804\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0036 - mse: 3.1019 - val_loss: 0.0066 - val_mse: 5.7734\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0036 - mse: 3.0938 - val_loss: 0.0066 - val_mse: 5.7652\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0035 - mse: 3.0858 - val_loss: 0.0066 - val_mse: 5.7561\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0035 - mse: 3.0777 - val_loss: 0.0066 - val_mse: 5.7467\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0035 - mse: 3.0696 - val_loss: 0.0066 - val_mse: 5.7381\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0035 - mse: 3.0614 - val_loss: 0.0066 - val_mse: 5.7307\n",
      "Epoch 310/1000\n",
      "\n",
      "Epoch 00309: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0035 - mse: 3.0533 - val_loss: 0.0066 - val_mse: 5.7235\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0035 - mse: 3.0452 - val_loss: 0.0066 - val_mse: 5.7181\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0035 - mse: 3.0394 - val_loss: 0.0066 - val_mse: 5.7121\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0035 - mse: 3.0336 - val_loss: 0.0066 - val_mse: 5.7059\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0035 - mse: 3.0279 - val_loss: 0.0065 - val_mse: 5.6998\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0035 - mse: 3.0222 - val_loss: 0.0065 - val_mse: 5.6939\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0035 - mse: 3.0164 - val_loss: 0.0065 - val_mse: 5.6885\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0035 - mse: 3.0107 - val_loss: 0.0065 - val_mse: 5.6838\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0035 - mse: 3.0049 - val_loss: 0.0065 - val_mse: 5.6791\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0034 - mse: 2.9992 - val_loss: 0.0065 - val_mse: 5.6740\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0034 - mse: 2.9934 - val_loss: 0.0065 - val_mse: 5.6685\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0034 - mse: 2.9877 - val_loss: 0.0065 - val_mse: 5.6628\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0034 - mse: 2.9820 - val_loss: 0.0065 - val_mse: 5.6572\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0034 - mse: 2.9762 - val_loss: 0.0065 - val_mse: 5.6519\n",
      "Epoch 324/1000\n",
      "\n",
      "Epoch 00323: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0034 - mse: 2.9705 - val_loss: 0.0065 - val_mse: 5.6468\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0034 - mse: 2.9648 - val_loss: 0.0065 - val_mse: 5.6430\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0034 - mse: 2.9608 - val_loss: 0.0065 - val_mse: 5.6393\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0034 - mse: 2.9568 - val_loss: 0.0065 - val_mse: 5.6359\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0034 - mse: 2.9528 - val_loss: 0.0065 - val_mse: 5.6327\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0034 - mse: 2.9488 - val_loss: 0.0065 - val_mse: 5.6294\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0034 - mse: 2.9448 - val_loss: 0.0065 - val_mse: 5.6263\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0034 - mse: 2.9408 - val_loss: 0.0065 - val_mse: 5.6228\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0034 - mse: 2.9369 - val_loss: 0.0065 - val_mse: 5.6191\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0034 - mse: 2.9329 - val_loss: 0.0064 - val_mse: 5.6152\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0034 - mse: 2.9289 - val_loss: 0.0064 - val_mse: 5.6114\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0034 - mse: 2.9249 - val_loss: 0.0064 - val_mse: 5.6076\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0034 - mse: 2.9210 - val_loss: 0.0064 - val_mse: 5.6037\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0033 - mse: 2.9170 - val_loss: 0.0064 - val_mse: 5.5996\n",
      "Epoch 338/1000\n",
      "\n",
      "Epoch 00337: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0033 - mse: 2.9130 - val_loss: 0.0064 - val_mse: 5.5957\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0033 - mse: 2.9090 - val_loss: 0.0064 - val_mse: 5.5932\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0033 - mse: 2.9062 - val_loss: 0.0064 - val_mse: 5.5908\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0033 - mse: 2.9034 - val_loss: 0.0064 - val_mse: 5.5883\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0033 - mse: 2.9007 - val_loss: 0.0064 - val_mse: 5.5858\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.0033 - mse: 2.8979 - val_loss: 0.0064 - val_mse: 5.5831\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0033 - mse: 2.8951 - val_loss: 0.0064 - val_mse: 5.5806\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.0033 - mse: 2.8923 - val_loss: 0.0064 - val_mse: 5.5782\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.0033 - mse: 2.8896 - val_loss: 0.0064 - val_mse: 5.5759\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.0033 - mse: 2.8868 - val_loss: 0.0064 - val_mse: 5.5734\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0033 - mse: 2.8840 - val_loss: 0.0064 - val_mse: 5.5708\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.0033 - mse: 2.8813 - val_loss: 0.0064 - val_mse: 5.5681\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0033 - mse: 2.8785 - val_loss: 0.0064 - val_mse: 5.5653\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.0033 - mse: 2.8757 - val_loss: 0.0064 - val_mse: 5.5626\n",
      "Epoch 352/1000\n",
      "\n",
      "Epoch 00351: reducing learning rate to 0.000282475253334.\n",
      "0s - loss: 0.0033 - mse: 2.8730 - val_loss: 0.0064 - val_mse: 5.5601\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.0033 - mse: 2.8702 - val_loss: 0.0064 - val_mse: 5.5583\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.0033 - mse: 2.8683 - val_loss: 0.0064 - val_mse: 5.5567\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0033 - mse: 2.8663 - val_loss: 0.0064 - val_mse: 5.5551\n",
      "Epoch 356/1000\n",
      "0s - loss: 0.0033 - mse: 2.8644 - val_loss: 0.0064 - val_mse: 5.5533\n",
      "Epoch 357/1000\n",
      "0s - loss: 0.0033 - mse: 2.8625 - val_loss: 0.0064 - val_mse: 5.5516\n",
      "Epoch 358/1000\n",
      "0s - loss: 0.0033 - mse: 2.8606 - val_loss: 0.0064 - val_mse: 5.5498\n",
      "Epoch 359/1000\n",
      "0s - loss: 0.0033 - mse: 2.8586 - val_loss: 0.0064 - val_mse: 5.5480\n",
      "Epoch 360/1000\n",
      "0s - loss: 0.0033 - mse: 2.8567 - val_loss: 0.0064 - val_mse: 5.5462\n",
      "Epoch 361/1000\n",
      "0s - loss: 0.0033 - mse: 2.8548 - val_loss: 0.0064 - val_mse: 5.5445\n",
      "Epoch 362/1000\n",
      "0s - loss: 0.0033 - mse: 2.8529 - val_loss: 0.0064 - val_mse: 5.5428\n",
      "Epoch 363/1000\n",
      "0s - loss: 0.0033 - mse: 2.8510 - val_loss: 0.0064 - val_mse: 5.5410\n",
      "Epoch 364/1000\n",
      "0s - loss: 0.0033 - mse: 2.8491 - val_loss: 0.0064 - val_mse: 5.5392\n",
      "Epoch 365/1000\n",
      "0s - loss: 0.0033 - mse: 2.8471 - val_loss: 0.0064 - val_mse: 5.5375\n",
      "Epoch 366/1000\n",
      "\n",
      "Epoch 00365: reducing learning rate to 0.000197732681409.\n",
      "0s - loss: 0.0033 - mse: 2.8452 - val_loss: 0.0064 - val_mse: 5.5357\n",
      "Epoch 367/1000\n",
      "0s - loss: 0.0033 - mse: 2.8433 - val_loss: 0.0064 - val_mse: 5.5345\n",
      "Epoch 368/1000\n",
      "0s - loss: 0.0033 - mse: 2.8420 - val_loss: 0.0064 - val_mse: 5.5333\n",
      "Epoch 369/1000\n",
      "0s - loss: 0.0033 - mse: 2.8406 - val_loss: 0.0064 - val_mse: 5.5321\n",
      "Epoch 370/1000\n",
      "0s - loss: 0.0033 - mse: 2.8393 - val_loss: 0.0064 - val_mse: 5.5308\n",
      "Epoch 371/1000\n",
      "0s - loss: 0.0033 - mse: 2.8380 - val_loss: 0.0063 - val_mse: 5.5295\n",
      "Epoch 372/1000\n",
      "0s - loss: 0.0033 - mse: 2.8366 - val_loss: 0.0063 - val_mse: 5.5283\n",
      "Epoch 373/1000\n",
      "0s - loss: 0.0033 - mse: 2.8353 - val_loss: 0.0063 - val_mse: 5.5271\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}, {'loss': 0.0013604337582364678, 'dropout': 0.0, 'val_mse': 3.409489631652832, 'width': 210, 'mse': 1.1847187280654907, 'val_loss': 0.003915177658200264}, {'loss': 0.0069423709064722061, 'dropout': 0.0, 'val_mse': 9.2405805587768555, 'width': 310, 'mse': 6.0456881523132324, 'val_loss': 0.010611124336719513}, {'loss': 0.023029616102576256, 'dropout': 0.0, 'val_mse': 22.774328231811523, 'width': 410, 'mse': 20.055086135864258, 'val_loss': 0.026152169331908226}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_67 (LSTM)               (None, None, 200)         162400    \n",
      "_________________________________________________________________\n",
      "time_distributed_64 (TimeDis (None, None, 1)           201       \n",
      "=================================================================\n",
      "Total params: 162,601\n",
      "Trainable params: 162,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 1.0735 - mse: 934.8682 - val_loss: 0.6219 - val_mse: 541.5839\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.5841 - mse: 508.6770 - val_loss: 0.5004 - val_mse: 435.7998\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.4829 - mse: 420.5453 - val_loss: 0.4784 - val_mse: 416.6251\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.4611 - mse: 401.5368 - val_loss: 0.2917 - val_mse: 254.0587\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2690 - mse: 234.2787 - val_loss: 1.5644 - val_mse: 1362.3813\n",
      "Epoch 6/1000\n",
      "0s - loss: 1.5636 - mse: 1361.6821 - val_loss: 0.2422 - val_mse: 210.8924\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.2358 - mse: 205.3062 - val_loss: 0.2528 - val_mse: 220.1516\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.2397 - mse: 208.7741 - val_loss: 0.2190 - val_mse: 190.6854\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.2067 - mse: 180.0369 - val_loss: 0.1930 - val_mse: 168.0999\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1836 - mse: 159.8609 - val_loss: 0.1871 - val_mse: 162.9625\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1797 - mse: 156.5287 - val_loss: 0.1742 - val_mse: 151.6966\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1658 - mse: 144.4123 - val_loss: 0.1427 - val_mse: 124.2503\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1314 - mse: 114.4498 - val_loss: 0.1292 - val_mse: 112.5016\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1153 - mse: 100.3863 - val_loss: 0.1391 - val_mse: 121.1467\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1231 - mse: 107.1845 - val_loss: 0.1475 - val_mse: 128.4394\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.1302 - mse: 113.3563 - val_loss: 0.1423 - val_mse: 123.9052\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.1243 - mse: 108.2307 - val_loss: 0.1317 - val_mse: 114.6507\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.1128 - mse: 98.2068 - val_loss: 0.1283 - val_mse: 111.7542\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.1079 - mse: 93.9668 - val_loss: 0.1323 - val_mse: 115.2030\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.1110 - mse: 96.6963 - val_loss: 0.1286 - val_mse: 111.9743\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.1092 - mse: 95.1258 - val_loss: 0.1171 - val_mse: 101.9381\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.1014 - mse: 88.2822 - val_loss: 0.1066 - val_mse: 92.8634\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0945 - mse: 82.2883 - val_loss: 0.0997 - val_mse: 86.8597\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0901 - mse: 78.4854 - val_loss: 0.0948 - val_mse: 82.5530\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0867 - mse: 75.5389 - val_loss: 0.0908 - val_mse: 79.0566\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0834 - mse: 72.6153 - val_loss: 0.0871 - val_mse: 75.8413\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0796 - mse: 69.3585 - val_loss: 0.0834 - val_mse: 72.6373\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0756 - mse: 65.8514 - val_loss: 0.0795 - val_mse: 69.2014\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0716 - mse: 62.3854 - val_loss: 0.0751 - val_mse: 65.3985\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0680 - mse: 59.2189 - val_loss: 0.0701 - val_mse: 61.0460\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0645 - mse: 56.1413 - val_loss: 0.0647 - val_mse: 56.3754\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0607 - mse: 52.8867 - val_loss: 0.0604 - val_mse: 52.5615\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0575 - mse: 50.1154 - val_loss: 0.0578 - val_mse: 50.3669\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0556 - mse: 48.4519 - val_loss: 0.0564 - val_mse: 49.1186\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0544 - mse: 47.3765 - val_loss: 0.0547 - val_mse: 47.6274\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0527 - mse: 45.9029 - val_loss: 0.0523 - val_mse: 45.5082\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0502 - mse: 43.7251 - val_loss: 0.0499 - val_mse: 43.4593\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0476 - mse: 41.4843 - val_loss: 0.0487 - val_mse: 42.4207\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0460 - mse: 40.0370 - val_loss: 0.0488 - val_mse: 42.4950\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0455 - mse: 39.5800 - val_loss: 0.0488 - val_mse: 42.4797\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0450 - mse: 39.1883 - val_loss: 0.0472 - val_mse: 41.1427\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0435 - mse: 37.9073 - val_loss: 0.0446 - val_mse: 38.8586\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0414 - mse: 36.0221 - val_loss: 0.0424 - val_mse: 36.9120\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0396 - mse: 34.5170 - val_loss: 0.0410 - val_mse: 35.7341\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0386 - mse: 33.6540 - val_loss: 0.0401 - val_mse: 34.9226\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0379 - mse: 33.0152 - val_loss: 0.0392 - val_mse: 34.1103\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0371 - mse: 32.2647 - val_loss: 0.0383 - val_mse: 33.3880\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0362 - mse: 31.4942 - val_loss: 0.0379 - val_mse: 32.9776\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0355 - mse: 30.9364 - val_loss: 0.0376 - val_mse: 32.7840\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0351 - mse: 30.5432 - val_loss: 0.0372 - val_mse: 32.4185\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0344 - mse: 29.9976 - val_loss: 0.0364 - val_mse: 31.6896\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0335 - mse: 29.1527 - val_loss: 0.0354 - val_mse: 30.8314\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0324 - mse: 28.2331 - val_loss: 0.0347 - val_mse: 30.1786\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0316 - mse: 27.5459 - val_loss: 0.0342 - val_mse: 29.7579\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0311 - mse: 27.0970 - val_loss: 0.0337 - val_mse: 29.3845\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0306 - mse: 26.6874 - val_loss: 0.0332 - val_mse: 28.9524\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0301 - mse: 26.2008 - val_loss: 0.0328 - val_mse: 28.5581\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0295 - mse: 25.7224 - val_loss: 0.0325 - val_mse: 28.2810\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0291 - mse: 25.3551 - val_loss: 0.0322 - val_mse: 28.0021\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0288 - mse: 25.0402 - val_loss: 0.0316 - val_mse: 27.5487\n",
      "Epoch 61/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0283 - mse: 24.6514 - val_loss: 0.0310 - val_mse: 26.9609\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0278 - mse: 24.2094 - val_loss: 0.0304 - val_mse: 26.4385\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0274 - mse: 23.8468 - val_loss: 0.0300 - val_mse: 26.0903\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0271 - mse: 23.6103 - val_loss: 0.0297 - val_mse: 25.8637\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0269 - mse: 23.4135 - val_loss: 0.0295 - val_mse: 25.6852\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0266 - mse: 23.1712 - val_loss: 0.0294 - val_mse: 25.5704\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0263 - mse: 22.9007 - val_loss: 0.0293 - val_mse: 25.5552\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0260 - mse: 22.6732 - val_loss: 0.0294 - val_mse: 25.5840\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0258 - mse: 22.5016 - val_loss: 0.0293 - val_mse: 25.5253\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0256 - mse: 22.3342 - val_loss: 0.0291 - val_mse: 25.3026\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0254 - mse: 22.1320 - val_loss: 0.0287 - val_mse: 24.9615\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0252 - mse: 21.9142 - val_loss: 0.0283 - val_mse: 24.6044\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0249 - mse: 21.7157 - val_loss: 0.0279 - val_mse: 24.2896\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0247 - mse: 21.5359 - val_loss: 0.0276 - val_mse: 24.0346\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0245 - mse: 21.3596 - val_loss: 0.0274 - val_mse: 23.8391\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0243 - mse: 21.1846 - val_loss: 0.0272 - val_mse: 23.6913\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0241 - mse: 21.0170 - val_loss: 0.0271 - val_mse: 23.5630\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0239 - mse: 20.8559 - val_loss: 0.0269 - val_mse: 23.4143\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0238 - mse: 20.6874 - val_loss: 0.0267 - val_mse: 23.2302\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0236 - mse: 20.5102 - val_loss: 0.0264 - val_mse: 23.0255\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0234 - mse: 20.3380 - val_loss: 0.0262 - val_mse: 22.8215\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0232 - mse: 20.1827 - val_loss: 0.0260 - val_mse: 22.6197\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0230 - mse: 20.0348 - val_loss: 0.0257 - val_mse: 22.4228\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0228 - mse: 19.8865 - val_loss: 0.0255 - val_mse: 22.2365\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0227 - mse: 19.7385 - val_loss: 0.0253 - val_mse: 22.0700\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0225 - mse: 19.5975 - val_loss: 0.0252 - val_mse: 21.9176\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0223 - mse: 19.4601 - val_loss: 0.0250 - val_mse: 21.7715\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0222 - mse: 19.3220 - val_loss: 0.0248 - val_mse: 21.6348\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0220 - mse: 19.1863 - val_loss: 0.0247 - val_mse: 21.5126\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0219 - mse: 19.0564 - val_loss: 0.0246 - val_mse: 21.3981\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0217 - mse: 18.9265 - val_loss: 0.0244 - val_mse: 21.2830\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0216 - mse: 18.7909 - val_loss: 0.0243 - val_mse: 21.1693\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0214 - mse: 18.6545 - val_loss: 0.0242 - val_mse: 21.0615\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0213 - mse: 18.5245 - val_loss: 0.0241 - val_mse: 20.9534\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0211 - mse: 18.3989 - val_loss: 0.0239 - val_mse: 20.8344\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0210 - mse: 18.2712 - val_loss: 0.0238 - val_mse: 20.7019\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0208 - mse: 18.1401 - val_loss: 0.0236 - val_mse: 20.5640\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0207 - mse: 18.0118 - val_loss: 0.0235 - val_mse: 20.4243\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0205 - mse: 17.8874 - val_loss: 0.0233 - val_mse: 20.2824\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0204 - mse: 17.7638 - val_loss: 0.0231 - val_mse: 20.1401\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0203 - mse: 17.6394 - val_loss: 0.0230 - val_mse: 20.0030\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0201 - mse: 17.5171 - val_loss: 0.0228 - val_mse: 19.8728\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0200 - mse: 17.3946 - val_loss: 0.0227 - val_mse: 19.7471\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0198 - mse: 17.2692 - val_loss: 0.0225 - val_mse: 19.6280\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0197 - mse: 17.1435 - val_loss: 0.0224 - val_mse: 19.5181\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0195 - mse: 17.0204 - val_loss: 0.0223 - val_mse: 19.4127\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0194 - mse: 16.8985 - val_loss: 0.0222 - val_mse: 19.3061\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0193 - mse: 16.7751 - val_loss: 0.0220 - val_mse: 19.1972\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0191 - mse: 16.6516 - val_loss: 0.0219 - val_mse: 19.0871\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0190 - mse: 16.5300 - val_loss: 0.0218 - val_mse: 18.9756\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0188 - mse: 16.4101 - val_loss: 0.0217 - val_mse: 18.8617\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0187 - mse: 16.2900 - val_loss: 0.0215 - val_mse: 18.7473\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0186 - mse: 16.1711 - val_loss: 0.0214 - val_mse: 18.6308\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0184 - mse: 16.0534 - val_loss: 0.0213 - val_mse: 18.5115\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0183 - mse: 15.9359 - val_loss: 0.0211 - val_mse: 18.3919\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0182 - mse: 15.8193 - val_loss: 0.0210 - val_mse: 18.2746\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0180 - mse: 15.7039 - val_loss: 0.0209 - val_mse: 18.1610\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0179 - mse: 15.5898 - val_loss: 0.0207 - val_mse: 18.0502\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0178 - mse: 15.4759 - val_loss: 0.0206 - val_mse: 17.9419\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0176 - mse: 15.3628 - val_loss: 0.0205 - val_mse: 17.8337\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0175 - mse: 15.2507 - val_loss: 0.0204 - val_mse: 17.7248\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0174 - mse: 15.1388 - val_loss: 0.0202 - val_mse: 17.6176\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0173 - mse: 15.0268 - val_loss: 0.0201 - val_mse: 17.5139\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0171 - mse: 14.9139 - val_loss: 0.0200 - val_mse: 17.4151\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0170 - mse: 14.8010 - val_loss: 0.0199 - val_mse: 17.3208\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0169 - mse: 14.6883 - val_loss: 0.0198 - val_mse: 17.2281\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0167 - mse: 14.5758 - val_loss: 0.0197 - val_mse: 17.1332\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0166 - mse: 14.4640 - val_loss: 0.0196 - val_mse: 17.0342\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0165 - mse: 14.3518 - val_loss: 0.0194 - val_mse: 16.9348\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0164 - mse: 14.2414 - val_loss: 0.0193 - val_mse: 16.8354\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0162 - mse: 14.1311 - val_loss: 0.0192 - val_mse: 16.7375\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0161 - mse: 14.0208 - val_loss: 0.0191 - val_mse: 16.6422\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0160 - mse: 13.9116 - val_loss: 0.0190 - val_mse: 16.5489\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0159 - mse: 13.8034 - val_loss: 0.0189 - val_mse: 16.4564\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0157 - mse: 13.6953 - val_loss: 0.0188 - val_mse: 16.3654\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0156 - mse: 13.5870 - val_loss: 0.0187 - val_mse: 16.2766\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0155 - mse: 13.4788 - val_loss: 0.0186 - val_mse: 16.1894\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0154 - mse: 13.3705 - val_loss: 0.0185 - val_mse: 16.1020\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0152 - mse: 13.2624 - val_loss: 0.0184 - val_mse: 16.0122\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0151 - mse: 13.1545 - val_loss: 0.0183 - val_mse: 15.9185\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0150 - mse: 13.0461 - val_loss: 0.0182 - val_mse: 15.8240\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0149 - mse: 12.9383 - val_loss: 0.0181 - val_mse: 15.7338\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0147 - mse: 12.8316 - val_loss: 0.0180 - val_mse: 15.6490\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0146 - mse: 12.7248 - val_loss: 0.0179 - val_mse: 15.5680\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0145 - mse: 12.6181 - val_loss: 0.0178 - val_mse: 15.4878\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0144 - mse: 12.5114 - val_loss: 0.0177 - val_mse: 15.4035\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0142 - mse: 12.4036 - val_loss: 0.0176 - val_mse: 15.3188\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0141 - mse: 12.2957 - val_loss: 0.0175 - val_mse: 15.2360\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0140 - mse: 12.1877 - val_loss: 0.0174 - val_mse: 15.1555\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0139 - mse: 12.0789 - val_loss: 0.0173 - val_mse: 15.0767\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0137 - mse: 11.9706 - val_loss: 0.0172 - val_mse: 14.9965\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0136 - mse: 11.8618 - val_loss: 0.0171 - val_mse: 14.9152\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0135 - mse: 11.7531 - val_loss: 0.0170 - val_mse: 14.8367\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0134 - mse: 11.6455 - val_loss: 0.0170 - val_mse: 14.7629\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0133 - mse: 11.5389 - val_loss: 0.0169 - val_mse: 14.6914\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0131 - mse: 11.4332 - val_loss: 0.0168 - val_mse: 14.6190\n",
      "Epoch 157/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0130 - mse: 11.3277 - val_loss: 0.0167 - val_mse: 14.5452\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0129 - mse: 11.2227 - val_loss: 0.0166 - val_mse: 14.4720\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0128 - mse: 11.1186 - val_loss: 0.0165 - val_mse: 14.4016\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0127 - mse: 11.0162 - val_loss: 0.0165 - val_mse: 14.3334\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0125 - mse: 10.9148 - val_loss: 0.0164 - val_mse: 14.2643\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0124 - mse: 10.8140 - val_loss: 0.0163 - val_mse: 14.1921\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0123 - mse: 10.7139 - val_loss: 0.0162 - val_mse: 14.1173\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0122 - mse: 10.6149 - val_loss: 0.0161 - val_mse: 14.0421\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0121 - mse: 10.5172 - val_loss: 0.0160 - val_mse: 13.9694\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0120 - mse: 10.4200 - val_loss: 0.0160 - val_mse: 13.9015\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0119 - mse: 10.3230 - val_loss: 0.0159 - val_mse: 13.8377\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0117 - mse: 10.2276 - val_loss: 0.0158 - val_mse: 13.7748\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0116 - mse: 10.1342 - val_loss: 0.0157 - val_mse: 13.7085\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0115 - mse: 10.0411 - val_loss: 0.0157 - val_mse: 13.6398\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0114 - mse: 9.9488 - val_loss: 0.0156 - val_mse: 13.5715\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0113 - mse: 9.8570 - val_loss: 0.0155 - val_mse: 13.5063\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0112 - mse: 9.7656 - val_loss: 0.0154 - val_mse: 13.4450\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0111 - mse: 9.6745 - val_loss: 0.0154 - val_mse: 13.3846\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0110 - mse: 9.5838 - val_loss: 0.0153 - val_mse: 13.3202\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0109 - mse: 9.4934 - val_loss: 0.0152 - val_mse: 13.2499\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0108 - mse: 9.4030 - val_loss: 0.0151 - val_mse: 13.1765\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0107 - mse: 9.3130 - val_loss: 0.0150 - val_mse: 13.1041\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0106 - mse: 9.2235 - val_loss: 0.0150 - val_mse: 13.0374\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0105 - mse: 9.1345 - val_loss: 0.0149 - val_mse: 12.9751\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0104 - mse: 9.0454 - val_loss: 0.0148 - val_mse: 12.9133\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0103 - mse: 8.9561 - val_loss: 0.0148 - val_mse: 12.8495\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0102 - mse: 8.8669 - val_loss: 0.0147 - val_mse: 12.7848\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0101 - mse: 8.7783 - val_loss: 0.0146 - val_mse: 12.7224\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0100 - mse: 8.6919 - val_loss: 0.0145 - val_mse: 12.6611\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0099 - mse: 8.6046 - val_loss: 0.0145 - val_mse: 12.5941\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0098 - mse: 8.5160 - val_loss: 0.0144 - val_mse: 12.5276\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0097 - mse: 8.4282 - val_loss: 0.0143 - val_mse: 12.4645\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0096 - mse: 8.3388 - val_loss: 0.0142 - val_mse: 12.4024\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0095 - mse: 8.2504 - val_loss: 0.0142 - val_mse: 12.3349\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0094 - mse: 8.1617 - val_loss: 0.0141 - val_mse: 12.2602\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0093 - mse: 8.0727 - val_loss: 0.0140 - val_mse: 12.1865\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0092 - mse: 7.9833 - val_loss: 0.0139 - val_mse: 12.1186\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0091 - mse: 7.8933 - val_loss: 0.0138 - val_mse: 12.0507\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0090 - mse: 7.8017 - val_loss: 0.0138 - val_mse: 11.9809\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0089 - mse: 7.7082 - val_loss: 0.0137 - val_mse: 11.9120\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0087 - mse: 7.6141 - val_loss: 0.0136 - val_mse: 11.8440\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0086 - mse: 7.5194 - val_loss: 0.0135 - val_mse: 11.7704\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0085 - mse: 7.4256 - val_loss: 0.0134 - val_mse: 11.6906\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0084 - mse: 7.3315 - val_loss: 0.0133 - val_mse: 11.6142\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0083 - mse: 7.2350 - val_loss: 0.0133 - val_mse: 11.5476\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0082 - mse: 7.1379 - val_loss: 0.0132 - val_mse: 11.4842\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0081 - mse: 7.0429 - val_loss: 0.0131 - val_mse: 11.4183\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0080 - mse: 6.9506 - val_loss: 0.0130 - val_mse: 11.3588\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0079 - mse: 6.8630 - val_loss: 0.0130 - val_mse: 11.3056\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0078 - mse: 6.7770 - val_loss: 0.0129 - val_mse: 11.2528\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0077 - mse: 6.6932 - val_loss: 0.0129 - val_mse: 11.1985\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0076 - mse: 6.6102 - val_loss: 0.0128 - val_mse: 11.1551\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0075 - mse: 6.5278 - val_loss: 0.0128 - val_mse: 11.1214\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0074 - mse: 6.4467 - val_loss: 0.0127 - val_mse: 11.0843\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0073 - mse: 6.3661 - val_loss: 0.0127 - val_mse: 11.0416\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0072 - mse: 6.2868 - val_loss: 0.0126 - val_mse: 10.9978\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0071 - mse: 6.2077 - val_loss: 0.0126 - val_mse: 10.9534\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0070 - mse: 6.1302 - val_loss: 0.0125 - val_mse: 10.9101\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0070 - mse: 6.0547 - val_loss: 0.0125 - val_mse: 10.8776\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0069 - mse: 5.9816 - val_loss: 0.0125 - val_mse: 10.8459\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0068 - mse: 5.9093 - val_loss: 0.0124 - val_mse: 10.8142\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0067 - mse: 5.8374 - val_loss: 0.0124 - val_mse: 10.7906\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0066 - mse: 5.7668 - val_loss: 0.0124 - val_mse: 10.7688\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0065 - mse: 5.6971 - val_loss: 0.0123 - val_mse: 10.7351\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0065 - mse: 5.6276 - val_loss: 0.0123 - val_mse: 10.6983\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0064 - mse: 5.5593 - val_loss: 0.0122 - val_mse: 10.6640\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0063 - mse: 5.4929 - val_loss: 0.0122 - val_mse: 10.6271\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0062 - mse: 5.4274 - val_loss: 0.0122 - val_mse: 10.5945\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0062 - mse: 5.3636 - val_loss: 0.0121 - val_mse: 10.5570\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0061 - mse: 5.3007 - val_loss: 0.0121 - val_mse: 10.5080\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0060 - mse: 5.2384 - val_loss: 0.0120 - val_mse: 10.4619\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0059 - mse: 5.1777 - val_loss: 0.0120 - val_mse: 10.4161\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0059 - mse: 5.1182 - val_loss: 0.0119 - val_mse: 10.3723\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0058 - mse: 5.0594 - val_loss: 0.0119 - val_mse: 10.3402\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0057 - mse: 5.0010 - val_loss: 0.0118 - val_mse: 10.3097\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0057 - mse: 4.9442 - val_loss: 0.0118 - val_mse: 10.2687\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0056 - mse: 4.8892 - val_loss: 0.0117 - val_mse: 10.2247\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0056 - mse: 4.8357 - val_loss: 0.0117 - val_mse: 10.1854\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0055 - mse: 4.7834 - val_loss: 0.0117 - val_mse: 10.1463\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0054 - mse: 4.7323 - val_loss: 0.0116 - val_mse: 10.1126\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0054 - mse: 4.6829 - val_loss: 0.0116 - val_mse: 10.0787\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0053 - mse: 4.6345 - val_loss: 0.0115 - val_mse: 10.0367\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0053 - mse: 4.5866 - val_loss: 0.0115 - val_mse: 9.9931\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0052 - mse: 4.5405 - val_loss: 0.0114 - val_mse: 9.9556\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0052 - mse: 4.4946 - val_loss: 0.0114 - val_mse: 9.9145\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0051 - mse: 4.4500 - val_loss: 0.0113 - val_mse: 9.8738\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0051 - mse: 4.4061 - val_loss: 0.0113 - val_mse: 9.8358\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0050 - mse: 4.3631 - val_loss: 0.0112 - val_mse: 9.7965\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0050 - mse: 4.3211 - val_loss: 0.0112 - val_mse: 9.7567\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0049 - mse: 4.2794 - val_loss: 0.0112 - val_mse: 9.7155\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0049 - mse: 4.2389 - val_loss: 0.0111 - val_mse: 9.6676\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0048 - mse: 4.1995 - val_loss: 0.0110 - val_mse: 9.6159\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0048 - mse: 4.1606 - val_loss: 0.0110 - val_mse: 9.5725\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0047 - mse: 4.1219 - val_loss: 0.0109 - val_mse: 9.5321\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0047 - mse: 4.0840 - val_loss: 0.0109 - val_mse: 9.4906\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0046 - mse: 4.0466 - val_loss: 0.0108 - val_mse: 9.4423\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0046 - mse: 4.0104 - val_loss: 0.0108 - val_mse: 9.3932\n",
      "Epoch 254/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0046 - mse: 3.9746 - val_loss: 0.0107 - val_mse: 9.3559\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0045 - mse: 3.9390 - val_loss: 0.0107 - val_mse: 9.3132\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0045 - mse: 3.9041 - val_loss: 0.0106 - val_mse: 9.2677\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0044 - mse: 3.8694 - val_loss: 0.0106 - val_mse: 9.2320\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0044 - mse: 3.8352 - val_loss: 0.0106 - val_mse: 9.2048\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0044 - mse: 3.8013 - val_loss: 0.0105 - val_mse: 9.1731\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0043 - mse: 3.7678 - val_loss: 0.0105 - val_mse: 9.1301\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0043 - mse: 3.7344 - val_loss: 0.0104 - val_mse: 9.0917\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0043 - mse: 3.7011 - val_loss: 0.0104 - val_mse: 9.0584\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0042 - mse: 3.6685 - val_loss: 0.0104 - val_mse: 9.0194\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0042 - mse: 3.6362 - val_loss: 0.0103 - val_mse: 8.9808\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0041 - mse: 3.6040 - val_loss: 0.0103 - val_mse: 8.9509\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0041 - mse: 3.5720 - val_loss: 0.0102 - val_mse: 8.9151\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0041 - mse: 3.5410 - val_loss: 0.0102 - val_mse: 8.8766\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.0040 - mse: 3.5104 - val_loss: 0.0102 - val_mse: 8.8460\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0040 - mse: 3.4796 - val_loss: 0.0101 - val_mse: 8.8173\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0040 - mse: 3.4482 - val_loss: 0.0101 - val_mse: 8.7846\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0039 - mse: 3.4168 - val_loss: 0.0100 - val_mse: 8.7511\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0039 - mse: 3.3853 - val_loss: 0.0100 - val_mse: 8.7290\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0039 - mse: 3.3535 - val_loss: 0.0100 - val_mse: 8.6943\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0038 - mse: 3.3213 - val_loss: 0.0099 - val_mse: 8.6610\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0038 - mse: 3.2892 - val_loss: 0.0099 - val_mse: 8.6261\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0037 - mse: 3.2581 - val_loss: 0.0099 - val_mse: 8.6229\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0037 - mse: 3.2293 - val_loss: 0.0099 - val_mse: 8.5832\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0037 - mse: 3.2070 - val_loss: 0.0099 - val_mse: 8.6093\n",
      "Epoch 279/1000\n",
      "\n",
      "Epoch 00278: reducing learning rate to 0.00699999984354.\n",
      "4s - loss: 0.0037 - mse: 3.2048 - val_loss: 0.0099 - val_mse: 8.5976\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0037 - mse: 3.2505 - val_loss: 0.0099 - val_mse: 8.6246\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0037 - mse: 3.1958 - val_loss: 0.0098 - val_mse: 8.4987\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0035 - mse: 3.0831 - val_loss: 0.0098 - val_mse: 8.5291\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0036 - mse: 3.1700 - val_loss: 0.0098 - val_mse: 8.5231\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0036 - mse: 3.0979 - val_loss: 0.0097 - val_mse: 8.4392\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0035 - mse: 3.0166 - val_loss: 0.0097 - val_mse: 8.4637\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0035 - mse: 3.0841 - val_loss: 0.0097 - val_mse: 8.4374\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0034 - mse: 2.9984 - val_loss: 0.0096 - val_mse: 8.3794\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0034 - mse: 2.9492 - val_loss: 0.0096 - val_mse: 8.3726\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0034 - mse: 2.9915 - val_loss: 0.0096 - val_mse: 8.3398\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0033 - mse: 2.9045 - val_loss: 0.0095 - val_mse: 8.3097\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0033 - mse: 2.8741 - val_loss: 0.0095 - val_mse: 8.2856\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0033 - mse: 2.8936 - val_loss: 0.0095 - val_mse: 8.2515\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0032 - mse: 2.8151 - val_loss: 0.0095 - val_mse: 8.2381\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0032 - mse: 2.7907 - val_loss: 0.0094 - val_mse: 8.2200\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0032 - mse: 2.7963 - val_loss: 0.0094 - val_mse: 8.1964\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0031 - mse: 2.7277 - val_loss: 0.0094 - val_mse: 8.1766\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0031 - mse: 2.7004 - val_loss: 0.0094 - val_mse: 8.1520\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0031 - mse: 2.6965 - val_loss: 0.0093 - val_mse: 8.1382\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0030 - mse: 2.6388 - val_loss: 0.0093 - val_mse: 8.1158\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0030 - mse: 2.6056 - val_loss: 0.0093 - val_mse: 8.0957\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0030 - mse: 2.5978 - val_loss: 0.0093 - val_mse: 8.0898\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0029 - mse: 2.5516 - val_loss: 0.0093 - val_mse: 8.0650\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0029 - mse: 2.5106 - val_loss: 0.0093 - val_mse: 8.0568\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0029 - mse: 2.4978 - val_loss: 0.0093 - val_mse: 8.0612\n",
      "Epoch 305/1000\n",
      "\n",
      "Epoch 00304: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0028 - mse: 2.4644 - val_loss: 0.0092 - val_mse: 8.0153\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0028 - mse: 2.4171 - val_loss: 0.0092 - val_mse: 8.0048\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0028 - mse: 2.3997 - val_loss: 0.0092 - val_mse: 8.0038\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0027 - mse: 2.3743 - val_loss: 0.0092 - val_mse: 7.9951\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0027 - mse: 2.3530 - val_loss: 0.0092 - val_mse: 7.9728\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0027 - mse: 2.3315 - val_loss: 0.0091 - val_mse: 7.9604\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0026 - mse: 2.3069 - val_loss: 0.0091 - val_mse: 7.9628\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0026 - mse: 2.2869 - val_loss: 0.0091 - val_mse: 7.9494\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0026 - mse: 2.2608 - val_loss: 0.0091 - val_mse: 7.9382\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0026 - mse: 2.2410 - val_loss: 0.0091 - val_mse: 7.9306\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0025 - mse: 2.2161 - val_loss: 0.0091 - val_mse: 7.9255\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0025 - mse: 2.1949 - val_loss: 0.0091 - val_mse: 7.9169\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0025 - mse: 2.1715 - val_loss: 0.0091 - val_mse: 7.9158\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0025 - mse: 2.1481 - val_loss: 0.0091 - val_mse: 7.9129\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0024 - mse: 2.1269 - val_loss: 0.0091 - val_mse: 7.8945\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0024 - mse: 2.1030 - val_loss: 0.0091 - val_mse: 7.8817\n",
      "Epoch 321/1000\n",
      "\n",
      "Epoch 00320: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0024 - mse: 2.0819 - val_loss: 0.0090 - val_mse: 7.8790\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0024 - mse: 2.0588 - val_loss: 0.0090 - val_mse: 7.8759\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0023 - mse: 2.0431 - val_loss: 0.0090 - val_mse: 7.8692\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0023 - mse: 2.0274 - val_loss: 0.0090 - val_mse: 7.8624\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0023 - mse: 2.0121 - val_loss: 0.0090 - val_mse: 7.8569\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0023 - mse: 1.9964 - val_loss: 0.0090 - val_mse: 7.8520\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0023 - mse: 1.9813 - val_loss: 0.0090 - val_mse: 7.8462\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0023 - mse: 1.9656 - val_loss: 0.0090 - val_mse: 7.8429\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0022 - mse: 1.9508 - val_loss: 0.0090 - val_mse: 7.8412\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0022 - mse: 1.9354 - val_loss: 0.0090 - val_mse: 7.8393\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0022 - mse: 1.9208 - val_loss: 0.0090 - val_mse: 7.8348\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0022 - mse: 1.9055 - val_loss: 0.0090 - val_mse: 7.8314\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0022 - mse: 1.8910 - val_loss: 0.0090 - val_mse: 7.8296\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0022 - mse: 1.8761 - val_loss: 0.0090 - val_mse: 7.8268\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0021 - mse: 1.8618 - val_loss: 0.0090 - val_mse: 7.8213\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0021 - mse: 1.8474 - val_loss: 0.0090 - val_mse: 7.8163\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0021 - mse: 1.8337 - val_loss: 0.0090 - val_mse: 7.8132\n",
      "Epoch 338/1000\n",
      "\n",
      "Epoch 00337: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0021 - mse: 1.8198 - val_loss: 0.0090 - val_mse: 7.8120\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0021 - mse: 1.8063 - val_loss: 0.0090 - val_mse: 7.8112\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0021 - mse: 1.7969 - val_loss: 0.0090 - val_mse: 7.8114\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0021 - mse: 1.7876 - val_loss: 0.0090 - val_mse: 7.8122\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0020 - mse: 1.7782 - val_loss: 0.0090 - val_mse: 7.8124\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.0020 - mse: 1.7691 - val_loss: 0.0090 - val_mse: 7.8109\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0020 - mse: 1.7601 - val_loss: 0.0090 - val_mse: 7.8086\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.0020 - mse: 1.7510 - val_loss: 0.0090 - val_mse: 7.8072\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.0020 - mse: 1.7422 - val_loss: 0.0090 - val_mse: 7.8081\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.0020 - mse: 1.7332 - val_loss: 0.0090 - val_mse: 7.8108\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0020 - mse: 1.7244 - val_loss: 0.0090 - val_mse: 7.8121\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0020 - mse: 1.7156 - val_loss: 0.0090 - val_mse: 7.8107\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0020 - mse: 1.7069 - val_loss: 0.0090 - val_mse: 7.8081\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.0020 - mse: 1.6982 - val_loss: 0.0090 - val_mse: 7.8057\n",
      "Epoch 352/1000\n",
      "\n",
      "Epoch 00351: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0019 - mse: 1.6895 - val_loss: 0.0090 - val_mse: 7.8041\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.0019 - mse: 1.6810 - val_loss: 0.0090 - val_mse: 7.8035\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.0019 - mse: 1.6750 - val_loss: 0.0090 - val_mse: 7.8035\n",
      "Epoch 355/1000\n",
      "0s - loss: 0.0019 - mse: 1.6690 - val_loss: 0.0090 - val_mse: 7.8036\n",
      "Epoch 356/1000\n",
      "0s - loss: 0.0019 - mse: 1.6631 - val_loss: 0.0090 - val_mse: 7.8035\n",
      "Epoch 357/1000\n",
      "0s - loss: 0.0019 - mse: 1.6571 - val_loss: 0.0090 - val_mse: 7.8026\n",
      "Epoch 358/1000\n",
      "0s - loss: 0.0019 - mse: 1.6512 - val_loss: 0.0090 - val_mse: 7.8015\n",
      "Epoch 359/1000\n",
      "0s - loss: 0.0019 - mse: 1.6455 - val_loss: 0.0090 - val_mse: 7.8007\n",
      "Epoch 360/1000\n",
      "0s - loss: 0.0019 - mse: 1.6398 - val_loss: 0.0090 - val_mse: 7.8004\n",
      "Epoch 361/1000\n",
      "0s - loss: 0.0019 - mse: 1.6342 - val_loss: 0.0090 - val_mse: 7.8007\n",
      "Epoch 362/1000\n",
      "0s - loss: 0.0019 - mse: 1.6287 - val_loss: 0.0090 - val_mse: 7.8009\n",
      "Epoch 363/1000\n",
      "0s - loss: 0.0019 - mse: 1.6232 - val_loss: 0.0090 - val_mse: 7.8004\n",
      "Epoch 364/1000\n",
      "0s - loss: 0.0019 - mse: 1.6178 - val_loss: 0.0090 - val_mse: 7.7996\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}, {'loss': 0.0013604337582364678, 'dropout': 0.0, 'val_mse': 3.409489631652832, 'width': 210, 'mse': 1.1847187280654907, 'val_loss': 0.003915177658200264}, {'loss': 0.0069423709064722061, 'dropout': 0.0, 'val_mse': 9.2405805587768555, 'width': 310, 'mse': 6.0456881523132324, 'val_loss': 0.010611124336719513}, {'loss': 0.023029616102576256, 'dropout': 0.0, 'val_mse': 22.774328231811523, 'width': 410, 'mse': 20.055086135864258, 'val_loss': 0.026152169331908226}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_68 (LSTM)               (None, None, 250)         253000    \n",
      "_________________________________________________________________\n",
      "time_distributed_65 (TimeDis (None, None, 1)           251       \n",
      "=================================================================\n",
      "Total params: 253,251\n",
      "Trainable params: 253,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 0.9635 - mse: 839.0684 - val_loss: 0.4927 - val_mse: 429.0824\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.4588 - mse: 399.5082 - val_loss: 0.6047 - val_mse: 526.6235\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.5803 - mse: 505.3800 - val_loss: 0.4310 - val_mse: 375.3112\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.4100 - mse: 357.0021 - val_loss: 1.4106 - val_mse: 1228.4028\n",
      "Epoch 5/1000\n",
      "0s - loss: 1.2789 - mse: 1113.7079 - val_loss: 0.3311 - val_mse: 288.2928\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.3105 - mse: 270.4366 - val_loss: 0.3909 - val_mse: 340.3718\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.3658 - mse: 318.5628 - val_loss: 0.3979 - val_mse: 346.5008\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.3781 - mse: 329.3042 - val_loss: 0.3244 - val_mse: 282.4670\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.3105 - mse: 270.3890 - val_loss: 0.2406 - val_mse: 209.5569\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.2250 - mse: 195.9279 - val_loss: 0.1981 - val_mse: 172.4815\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1743 - mse: 151.8158 - val_loss: 0.2082 - val_mse: 181.3236\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1789 - mse: 155.8193 - val_loss: 0.1811 - val_mse: 157.6782\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1581 - mse: 137.6856 - val_loss: 0.1781 - val_mse: 155.1200\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1655 - mse: 144.1079 - val_loss: 0.1474 - val_mse: 128.3308\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1369 - mse: 119.1877 - val_loss: 0.1144 - val_mse: 99.6275\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.1034 - mse: 90.0292 - val_loss: 0.1106 - val_mse: 96.3099\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0995 - mse: 86.6752 - val_loss: 0.1159 - val_mse: 100.8932\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.1050 - mse: 91.4205 - val_loss: 0.1173 - val_mse: 102.1278\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.1066 - mse: 92.7969 - val_loss: 0.1132 - val_mse: 98.5621\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.1027 - mse: 89.4587 - val_loss: 0.1048 - val_mse: 91.2821\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0947 - mse: 82.5042 - val_loss: 0.0945 - val_mse: 82.3328\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0847 - mse: 73.7650 - val_loss: 0.0864 - val_mse: 75.2579\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0763 - mse: 66.4089 - val_loss: 0.0841 - val_mse: 73.2222\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0728 - mse: 63.4017 - val_loss: 0.0853 - val_mse: 74.3081\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0729 - mse: 63.4494 - val_loss: 0.0826 - val_mse: 71.9134\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0703 - mse: 61.2362 - val_loss: 0.0744 - val_mse: 64.7803\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0640 - mse: 55.7071 - val_loss: 0.0667 - val_mse: 58.0941\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0584 - mse: 50.8663 - val_loss: 0.0630 - val_mse: 54.8469\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0561 - mse: 48.8966 - val_loss: 0.0616 - val_mse: 53.6396\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0557 - mse: 48.4941 - val_loss: 0.0596 - val_mse: 51.9050\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0545 - mse: 47.4203 - val_loss: 0.0559 - val_mse: 48.6757\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0515 - mse: 44.8101 - val_loss: 0.0516 - val_mse: 44.9541\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0477 - mse: 41.5202 - val_loss: 0.0486 - val_mse: 42.3046\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0447 - mse: 38.9406 - val_loss: 0.0472 - val_mse: 41.1156\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0431 - mse: 37.5593 - val_loss: 0.0463 - val_mse: 40.3354\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0422 - mse: 36.7323 - val_loss: 0.0447 - val_mse: 38.9281\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0409 - mse: 35.6470 - val_loss: 0.0424 - val_mse: 36.9161\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0392 - mse: 34.1047 - val_loss: 0.0401 - val_mse: 34.9253\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0373 - mse: 32.4774 - val_loss: 0.0384 - val_mse: 33.4199\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0358 - mse: 31.2184 - val_loss: 0.0370 - val_mse: 32.2340\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0347 - mse: 30.2439 - val_loss: 0.0355 - val_mse: 30.9418\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0334 - mse: 29.1262 - val_loss: 0.0341 - val_mse: 29.7068\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0321 - mse: 27.9773 - val_loss: 0.0333 - val_mse: 28.9799\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0313 - mse: 27.2571 - val_loss: 0.0329 - val_mse: 28.6449\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0309 - mse: 26.9290 - val_loss: 0.0324 - val_mse: 28.2372\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0305 - mse: 26.5928 - val_loss: 0.0315 - val_mse: 27.4284\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0297 - mse: 25.9003 - val_loss: 0.0301 - val_mse: 26.2319\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0285 - mse: 24.7986 - val_loss: 0.0288 - val_mse: 25.1136\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0272 - mse: 23.7173 - val_loss: 0.0281 - val_mse: 24.4934\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0265 - mse: 23.0918 - val_loss: 0.0278 - val_mse: 24.2409\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0262 - mse: 22.8143 - val_loss: 0.0276 - val_mse: 24.0081\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0259 - mse: 22.5461 - val_loss: 0.0271 - val_mse: 23.6431\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0254 - mse: 22.1427 - val_loss: 0.0266 - val_mse: 23.1912\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0249 - mse: 21.6651 - val_loss: 0.0262 - val_mse: 22.7988\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0244 - mse: 21.2703 - val_loss: 0.0259 - val_mse: 22.5575\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0242 - mse: 21.0440 - val_loss: 0.0257 - val_mse: 22.3477\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0240 - mse: 20.8599 - val_loss: 0.0253 - val_mse: 21.9983\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0236 - mse: 20.5496 - val_loss: 0.0247 - val_mse: 21.5458\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0231 - mse: 20.1387 - val_loss: 0.0243 - val_mse: 21.1690\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0227 - mse: 19.7870 - val_loss: 0.0240 - val_mse: 20.9397\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0225 - mse: 19.5572 - val_loss: 0.0239 - val_mse: 20.7737\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0222 - mse: 19.3677 - val_loss: 0.0236 - val_mse: 20.5882\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0220 - mse: 19.1412 - val_loss: 0.0234 - val_mse: 20.3750\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0217 - mse: 18.8706 - val_loss: 0.0232 - val_mse: 20.1741\n",
      "Epoch 65/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0214 - mse: 18.5962 - val_loss: 0.0230 - val_mse: 20.0118\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0211 - mse: 18.3486 - val_loss: 0.0228 - val_mse: 19.8553\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0208 - mse: 18.1024 - val_loss: 0.0226 - val_mse: 19.6783\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0205 - mse: 17.8349 - val_loss: 0.0224 - val_mse: 19.5208\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0202 - mse: 17.5809 - val_loss: 0.0223 - val_mse: 19.4129\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0199 - mse: 17.3641 - val_loss: 0.0222 - val_mse: 19.3451\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0197 - mse: 17.1735 - val_loss: 0.0221 - val_mse: 19.2854\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0195 - mse: 16.9876 - val_loss: 0.0220 - val_mse: 19.1944\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0193 - mse: 16.7883 - val_loss: 0.0219 - val_mse: 19.0595\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0190 - mse: 16.5797 - val_loss: 0.0217 - val_mse: 18.8965\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0188 - mse: 16.3782 - val_loss: 0.0215 - val_mse: 18.7137\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0186 - mse: 16.1822 - val_loss: 0.0213 - val_mse: 18.5274\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0184 - mse: 15.9943 - val_loss: 0.0211 - val_mse: 18.3637\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0182 - mse: 15.8234 - val_loss: 0.0209 - val_mse: 18.2283\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0180 - mse: 15.6568 - val_loss: 0.0208 - val_mse: 18.1160\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0178 - mse: 15.4811 - val_loss: 0.0207 - val_mse: 18.0279\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0176 - mse: 15.3051 - val_loss: 0.0206 - val_mse: 17.9596\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0174 - mse: 15.1389 - val_loss: 0.0205 - val_mse: 17.8917\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0172 - mse: 14.9779 - val_loss: 0.0204 - val_mse: 17.8002\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0170 - mse: 14.8073 - val_loss: 0.0203 - val_mse: 17.6812\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0168 - mse: 14.6254 - val_loss: 0.0202 - val_mse: 17.5588\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0166 - mse: 14.4507 - val_loss: 0.0200 - val_mse: 17.4456\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0164 - mse: 14.2878 - val_loss: 0.0199 - val_mse: 17.3259\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0162 - mse: 14.1187 - val_loss: 0.0197 - val_mse: 17.1945\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0160 - mse: 13.9385 - val_loss: 0.0196 - val_mse: 17.0628\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0158 - mse: 13.7596 - val_loss: 0.0194 - val_mse: 16.9325\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0156 - mse: 13.5835 - val_loss: 0.0193 - val_mse: 16.7888\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0154 - mse: 13.3994 - val_loss: 0.0191 - val_mse: 16.6314\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0152 - mse: 13.2105 - val_loss: 0.0189 - val_mse: 16.4719\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0150 - mse: 13.0293 - val_loss: 0.0187 - val_mse: 16.3128\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0148 - mse: 12.8513 - val_loss: 0.0185 - val_mse: 16.1479\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0145 - mse: 12.6599 - val_loss: 0.0184 - val_mse: 15.9888\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0143 - mse: 12.4639 - val_loss: 0.0182 - val_mse: 15.8415\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0141 - mse: 12.2754 - val_loss: 0.0180 - val_mse: 15.6870\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0139 - mse: 12.0848 - val_loss: 0.0178 - val_mse: 15.5289\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0137 - mse: 11.8929 - val_loss: 0.0177 - val_mse: 15.3907\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0134 - mse: 11.7122 - val_loss: 0.0175 - val_mse: 15.2742\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0132 - mse: 11.5323 - val_loss: 0.0174 - val_mse: 15.1750\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0130 - mse: 11.3502 - val_loss: 0.0173 - val_mse: 15.0891\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0128 - mse: 11.1772 - val_loss: 0.0172 - val_mse: 14.9898\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0126 - mse: 11.0070 - val_loss: 0.0171 - val_mse: 14.8650\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0124 - mse: 10.8362 - val_loss: 0.0169 - val_mse: 14.7367\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0123 - mse: 10.6723 - val_loss: 0.0168 - val_mse: 14.6138\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0121 - mse: 10.5045 - val_loss: 0.0166 - val_mse: 14.4991\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0119 - mse: 10.3389 - val_loss: 0.0165 - val_mse: 14.3829\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0117 - mse: 10.1778 - val_loss: 0.0164 - val_mse: 14.2606\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0115 - mse: 10.0182 - val_loss: 0.0163 - val_mse: 14.1537\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0113 - mse: 9.8648 - val_loss: 0.0162 - val_mse: 14.0701\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0112 - mse: 9.7114 - val_loss: 0.0161 - val_mse: 13.9916\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0110 - mse: 9.5649 - val_loss: 0.0159 - val_mse: 13.8840\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0108 - mse: 9.4203 - val_loss: 0.0158 - val_mse: 13.7601\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0107 - mse: 9.2765 - val_loss: 0.0157 - val_mse: 13.6496\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0105 - mse: 9.1327 - val_loss: 0.0155 - val_mse: 13.5338\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0103 - mse: 8.9899 - val_loss: 0.0154 - val_mse: 13.3962\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0102 - mse: 8.8494 - val_loss: 0.0152 - val_mse: 13.2629\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0100 - mse: 8.7126 - val_loss: 0.0151 - val_mse: 13.1508\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0099 - mse: 8.5795 - val_loss: 0.0150 - val_mse: 13.0290\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0097 - mse: 8.4522 - val_loss: 0.0148 - val_mse: 12.9019\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0096 - mse: 8.3311 - val_loss: 0.0147 - val_mse: 12.7870\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0094 - mse: 8.2153 - val_loss: 0.0145 - val_mse: 12.6652\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0093 - mse: 8.1050 - val_loss: 0.0144 - val_mse: 12.5181\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0092 - mse: 7.9983 - val_loss: 0.0142 - val_mse: 12.3942\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0091 - mse: 7.8943 - val_loss: 0.0141 - val_mse: 12.2870\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0089 - mse: 7.7938 - val_loss: 0.0139 - val_mse: 12.1480\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0088 - mse: 7.6940 - val_loss: 0.0138 - val_mse: 12.0491\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0087 - mse: 7.5963 - val_loss: 0.0137 - val_mse: 11.9480\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0086 - mse: 7.5004 - val_loss: 0.0136 - val_mse: 11.8161\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0085 - mse: 7.4045 - val_loss: 0.0134 - val_mse: 11.7010\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0084 - mse: 7.3099 - val_loss: 0.0133 - val_mse: 11.5954\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0083 - mse: 7.2170 - val_loss: 0.0132 - val_mse: 11.4687\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0082 - mse: 7.1318 - val_loss: 0.0131 - val_mse: 11.4112\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0081 - mse: 7.0590 - val_loss: 0.0130 - val_mse: 11.3131\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0081 - mse: 7.0256 - val_loss: 0.0131 - val_mse: 11.4265\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0082 - mse: 7.1350 - val_loss: 0.0137 - val_mse: 11.9635\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0089 - mse: 7.7297 - val_loss: 0.0153 - val_mse: 13.2808\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0106 - mse: 9.2212 - val_loss: 0.0175 - val_mse: 15.2726\n",
      "Epoch 141/1000\n",
      "\n",
      "Epoch 00140: reducing learning rate to 0.00699999984354.\n",
      "4s - loss: 0.0129 - mse: 11.2758 - val_loss: 0.0136 - val_mse: 11.8847\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0092 - mse: 8.0040 - val_loss: 0.0130 - val_mse: 11.3396\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0090 - mse: 7.8657 - val_loss: 0.0131 - val_mse: 11.4190\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0088 - mse: 7.6348 - val_loss: 0.0130 - val_mse: 11.3315\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0087 - mse: 7.5886 - val_loss: 0.0124 - val_mse: 10.7620\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0081 - mse: 7.0734 - val_loss: 0.0128 - val_mse: 11.1331\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0087 - mse: 7.5736 - val_loss: 0.0120 - val_mse: 10.4668\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0076 - mse: 6.6533 - val_loss: 0.0130 - val_mse: 11.2855\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0084 - mse: 7.3035 - val_loss: 0.0121 - val_mse: 10.5084\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0076 - mse: 6.6049 - val_loss: 0.0121 - val_mse: 10.5763\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0078 - mse: 6.7909 - val_loss: 0.0120 - val_mse: 10.4275\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0077 - mse: 6.6958 - val_loss: 0.0118 - val_mse: 10.2907\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0073 - mse: 6.3654 - val_loss: 0.0122 - val_mse: 10.6673\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0077 - mse: 6.6764 - val_loss: 0.0116 - val_mse: 10.0907\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0071 - mse: 6.1648 - val_loss: 0.0118 - val_mse: 10.2761\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0074 - mse: 6.4417 - val_loss: 0.0115 - val_mse: 10.0075\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0070 - mse: 6.1172 - val_loss: 0.0117 - val_mse: 10.1783\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0070 - mse: 6.0954 - val_loss: 0.0118 - val_mse: 10.2417\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0070 - mse: 6.1111 - val_loss: 0.0113 - val_mse: 9.8440\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0067 - mse: 5.8155 - val_loss: 0.0114 - val_mse: 9.9550\n",
      "Epoch 161/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0069 - mse: 6.0062 - val_loss: 0.0111 - val_mse: 9.6930\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0065 - mse: 5.6664 - val_loss: 0.0114 - val_mse: 9.8976\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0066 - mse: 5.7847 - val_loss: 0.0111 - val_mse: 9.6531\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0064 - mse: 5.5829 - val_loss: 0.0109 - val_mse: 9.4973\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0064 - mse: 5.5518 - val_loss: 0.0108 - val_mse: 9.4280\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0063 - mse: 5.4835 - val_loss: 0.0108 - val_mse: 9.3768\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0061 - mse: 5.3498 - val_loss: 0.0108 - val_mse: 9.3935\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0061 - mse: 5.3315 - val_loss: 0.0106 - val_mse: 9.1977\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0060 - mse: 5.1815 - val_loss: 0.0105 - val_mse: 9.1817\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0059 - mse: 5.1496 - val_loss: 0.0105 - val_mse: 9.1460\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0058 - mse: 5.0213 - val_loss: 0.0104 - val_mse: 9.0869\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0057 - mse: 4.9584 - val_loss: 0.0102 - val_mse: 8.9094\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0056 - mse: 4.8408 - val_loss: 0.0102 - val_mse: 8.8442\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0055 - mse: 4.7548 - val_loss: 0.0101 - val_mse: 8.8243\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0053 - mse: 4.6582 - val_loss: 0.0099 - val_mse: 8.6435\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0052 - mse: 4.5433 - val_loss: 0.0097 - val_mse: 8.4438\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0051 - mse: 4.4622 - val_loss: 0.0096 - val_mse: 8.3587\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0049 - mse: 4.3100 - val_loss: 0.0096 - val_mse: 8.3991\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0049 - mse: 4.2435 - val_loss: 0.0094 - val_mse: 8.1618\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0047 - mse: 4.0771 - val_loss: 0.0092 - val_mse: 7.9725\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0046 - mse: 3.9859 - val_loss: 0.0091 - val_mse: 7.9136\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0044 - mse: 3.8588 - val_loss: 0.0089 - val_mse: 7.7819\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0043 - mse: 3.7091 - val_loss: 0.0087 - val_mse: 7.6065\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0041 - mse: 3.6063 - val_loss: 0.0086 - val_mse: 7.4776\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0040 - mse: 3.4828 - val_loss: 0.0084 - val_mse: 7.3089\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0038 - mse: 3.3231 - val_loss: 0.0083 - val_mse: 7.2469\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0037 - mse: 3.2300 - val_loss: 0.0082 - val_mse: 7.1537\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0036 - mse: 3.1250 - val_loss: 0.0079 - val_mse: 6.9169\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0034 - mse: 2.9917 - val_loss: 0.0079 - val_mse: 6.8455\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0033 - mse: 2.8557 - val_loss: 0.0078 - val_mse: 6.7690\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0032 - mse: 2.7437 - val_loss: 0.0076 - val_mse: 6.6359\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0031 - mse: 2.6566 - val_loss: 0.0076 - val_mse: 6.6424\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0030 - mse: 2.5771 - val_loss: 0.0075 - val_mse: 6.5315\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0029 - mse: 2.4930 - val_loss: 0.0076 - val_mse: 6.5861\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0028 - mse: 2.4461 - val_loss: 0.0075 - val_mse: 6.5105\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0028 - mse: 2.4437 - val_loss: 0.0078 - val_mse: 6.7520\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0029 - mse: 2.5650 - val_loss: 0.0081 - val_mse: 7.0232\n",
      "Epoch 198/1000\n",
      "\n",
      "Epoch 00197: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0034 - mse: 2.9482 - val_loss: 0.0094 - val_mse: 8.1841\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0045 - mse: 3.8941 - val_loss: 0.0077 - val_mse: 6.7100\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0030 - mse: 2.5701 - val_loss: 0.0076 - val_mse: 6.5789\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0029 - mse: 2.5129 - val_loss: 0.0083 - val_mse: 7.2641\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0035 - mse: 3.0541 - val_loss: 0.0071 - val_mse: 6.1902\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0023 - mse: 1.9727 - val_loss: 0.0080 - val_mse: 6.9281\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0033 - mse: 2.8843 - val_loss: 0.0071 - val_mse: 6.2104\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0022 - mse: 1.9539 - val_loss: 0.0077 - val_mse: 6.7200\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0029 - mse: 2.4992 - val_loss: 0.0070 - val_mse: 6.0667\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0023 - mse: 1.9977 - val_loss: 0.0072 - val_mse: 6.2734\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0025 - mse: 2.1570 - val_loss: 0.0072 - val_mse: 6.3021\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0023 - mse: 2.0231 - val_loss: 0.0071 - val_mse: 6.1492\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0022 - mse: 1.9530 - val_loss: 0.0069 - val_mse: 5.9911\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0023 - mse: 2.0036 - val_loss: 0.0068 - val_mse: 5.9044\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0020 - mse: 1.7803 - val_loss: 0.0071 - val_mse: 6.1855\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0022 - mse: 1.9214 - val_loss: 0.0067 - val_mse: 5.8773\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0019 - mse: 1.6813 - val_loss: 0.0068 - val_mse: 5.8812\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0021 - mse: 1.8560 - val_loss: 0.0066 - val_mse: 5.7448\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0018 - mse: 1.6060 - val_loss: 0.0069 - val_mse: 5.9689\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0020 - mse: 1.7622 - val_loss: 0.0065 - val_mse: 5.6779\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0018 - mse: 1.5455 - val_loss: 0.0066 - val_mse: 5.7269\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0019 - mse: 1.6827 - val_loss: 0.0065 - val_mse: 5.6664\n",
      "Epoch 220/1000\n",
      "\n",
      "Epoch 00219: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0017 - mse: 1.5089 - val_loss: 0.0066 - val_mse: 5.7681\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0018 - mse: 1.5960 - val_loss: 0.0064 - val_mse: 5.5595\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0017 - mse: 1.4531 - val_loss: 0.0064 - val_mse: 5.5914\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0018 - mse: 1.5471 - val_loss: 0.0064 - val_mse: 5.5459\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0017 - mse: 1.4427 - val_loss: 0.0065 - val_mse: 5.6443\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0017 - mse: 1.4680 - val_loss: 0.0065 - val_mse: 5.6202\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0017 - mse: 1.4502 - val_loss: 0.0063 - val_mse: 5.5013\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0016 - mse: 1.3976 - val_loss: 0.0063 - val_mse: 5.5179\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0017 - mse: 1.4381 - val_loss: 0.0063 - val_mse: 5.5109\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0016 - mse: 1.3596 - val_loss: 0.0064 - val_mse: 5.6064\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0016 - mse: 1.3995 - val_loss: 0.0064 - val_mse: 5.5384\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0015 - mse: 1.3465 - val_loss: 0.0063 - val_mse: 5.4850\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0015 - mse: 1.3498 - val_loss: 0.0063 - val_mse: 5.4716\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0015 - mse: 1.3373 - val_loss: 0.0063 - val_mse: 5.5031\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0015 - mse: 1.3080 - val_loss: 0.0064 - val_mse: 5.5551\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0015 - mse: 1.3188 - val_loss: 0.0063 - val_mse: 5.5031\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0015 - mse: 1.2810 - val_loss: 0.0063 - val_mse: 5.4766\n",
      "Epoch 237/1000\n",
      "\n",
      "Epoch 00236: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0015 - mse: 1.2901 - val_loss: 0.0063 - val_mse: 5.4652\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0015 - mse: 1.2636 - val_loss: 0.0063 - val_mse: 5.4868\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0014 - mse: 1.2537 - val_loss: 0.0063 - val_mse: 5.5119\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0014 - mse: 1.2534 - val_loss: 0.0063 - val_mse: 5.5007\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0014 - mse: 1.2396 - val_loss: 0.0063 - val_mse: 5.4778\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0014 - mse: 1.2316 - val_loss: 0.0063 - val_mse: 5.4689\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0014 - mse: 1.2294 - val_loss: 0.0063 - val_mse: 5.4689\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0014 - mse: 1.2169 - val_loss: 0.0063 - val_mse: 5.4822\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0014 - mse: 1.2103 - val_loss: 0.0063 - val_mse: 5.4898\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0014 - mse: 1.2069 - val_loss: 0.0063 - val_mse: 5.4720\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0014 - mse: 1.1952 - val_loss: 0.0063 - val_mse: 5.4555\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0014 - mse: 1.1900 - val_loss: 0.0063 - val_mse: 5.4516\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0014 - mse: 1.1854 - val_loss: 0.0063 - val_mse: 5.4538\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0013 - mse: 1.1746 - val_loss: 0.0063 - val_mse: 5.4643\n",
      "Epoch 251/1000\n",
      "\n",
      "Epoch 00250: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0013 - mse: 1.1708 - val_loss: 0.0063 - val_mse: 5.4608\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0013 - mse: 1.1648 - val_loss: 0.0063 - val_mse: 5.4475\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0013 - mse: 1.1575 - val_loss: 0.0062 - val_mse: 5.4384\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0013 - mse: 1.1543 - val_loss: 0.0062 - val_mse: 5.4351\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0013 - mse: 1.1515 - val_loss: 0.0062 - val_mse: 5.4350\n",
      "Epoch 256/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0013 - mse: 1.1453 - val_loss: 0.0062 - val_mse: 5.4395\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0013 - mse: 1.1409 - val_loss: 0.0062 - val_mse: 5.4424\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0013 - mse: 1.1380 - val_loss: 0.0062 - val_mse: 5.4374\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0013 - mse: 1.1332 - val_loss: 0.0062 - val_mse: 5.4283\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0013 - mse: 1.1283 - val_loss: 0.0062 - val_mse: 5.4225\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0013 - mse: 1.1249 - val_loss: 0.0062 - val_mse: 5.4209\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0013 - mse: 1.1209 - val_loss: 0.0062 - val_mse: 5.4232\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0013 - mse: 1.1163 - val_loss: 0.0062 - val_mse: 5.4268\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}, {'loss': 0.0013604337582364678, 'dropout': 0.0, 'val_mse': 3.409489631652832, 'width': 210, 'mse': 1.1847187280654907, 'val_loss': 0.003915177658200264}, {'loss': 0.0069423709064722061, 'dropout': 0.0, 'val_mse': 9.2405805587768555, 'width': 310, 'mse': 6.0456881523132324, 'val_loss': 0.010611124336719513}, {'loss': 0.023029616102576256, 'dropout': 0.0, 'val_mse': 22.774328231811523, 'width': 410, 'mse': 20.055086135864258, 'val_loss': 0.026152169331908226}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_69 (LSTM)               (None, None, 300)         363600    \n",
      "_________________________________________________________________\n",
      "time_distributed_66 (TimeDis (None, None, 1)           301       \n",
      "=================================================================\n",
      "Total params: 363,901\n",
      "Trainable params: 363,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 0.9170 - mse: 798.5510 - val_loss: 0.8931 - val_mse: 777.7495\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.8757 - mse: 762.5922 - val_loss: 0.7370 - val_mse: 641.8371\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.7000 - mse: 609.5845 - val_loss: 0.7261 - val_mse: 632.3378\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.6942 - mse: 604.5554 - val_loss: 0.5673 - val_mse: 494.0233\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.5422 - mse: 472.1487 - val_loss: 0.3308 - val_mse: 288.1050\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.3070 - mse: 267.3444 - val_loss: 0.4779 - val_mse: 416.1436\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.4632 - mse: 403.3993 - val_loss: 0.3262 - val_mse: 284.0400\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.3241 - mse: 282.2263 - val_loss: 0.2727 - val_mse: 237.4731\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.2622 - mse: 228.3368 - val_loss: 0.2551 - val_mse: 222.1876\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.2414 - mse: 210.1838 - val_loss: 0.2208 - val_mse: 192.3170\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.2085 - mse: 181.5340 - val_loss: 0.1722 - val_mse: 149.9648\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1623 - mse: 141.3564 - val_loss: 0.1323 - val_mse: 115.2125\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.1255 - mse: 109.3324 - val_loss: 0.1249 - val_mse: 108.7889\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.1165 - mse: 101.4150 - val_loss: 0.1365 - val_mse: 118.9016\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1220 - mse: 106.2790 - val_loss: 0.1294 - val_mse: 112.6820\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.1145 - mse: 99.7101 - val_loss: 0.0984 - val_mse: 85.6778\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0868 - mse: 75.5701 - val_loss: 0.0985 - val_mse: 85.7816\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0879 - mse: 76.5392 - val_loss: 0.1047 - val_mse: 91.1632\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0942 - mse: 82.0458 - val_loss: 0.1034 - val_mse: 90.0018\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0932 - mse: 81.1757 - val_loss: 0.0962 - val_mse: 83.7387\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0867 - mse: 75.4857 - val_loss: 0.0886 - val_mse: 77.1483\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0797 - mse: 69.4384 - val_loss: 0.0827 - val_mse: 71.9947\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0740 - mse: 64.4547 - val_loss: 0.0787 - val_mse: 68.5427\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0699 - mse: 60.8545 - val_loss: 0.0760 - val_mse: 66.1897\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0672 - mse: 58.4835 - val_loss: 0.0732 - val_mse: 63.7041\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0652 - mse: 56.8011 - val_loss: 0.0730 - val_mse: 63.5463\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0664 - mse: 57.8627 - val_loss: 0.0679 - val_mse: 59.1440\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0615 - mse: 53.5700 - val_loss: 0.0642 - val_mse: 55.9101\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0587 - mse: 51.1345 - val_loss: 0.0600 - val_mse: 52.2145\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0552 - mse: 48.0473 - val_loss: 0.0573 - val_mse: 49.8989\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0530 - mse: 46.1815 - val_loss: 0.0554 - val_mse: 48.2835\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0518 - mse: 45.0669 - val_loss: 0.0532 - val_mse: 46.3516\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0501 - mse: 43.6191 - val_loss: 0.0512 - val_mse: 44.6213\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0485 - mse: 42.2618 - val_loss: 0.0498 - val_mse: 43.3783\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0475 - mse: 41.3229 - val_loss: 0.0476 - val_mse: 41.4109\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0455 - mse: 39.6000 - val_loss: 0.0441 - val_mse: 38.3945\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0422 - mse: 36.7140 - val_loss: 0.0418 - val_mse: 36.3764\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0399 - mse: 34.7575 - val_loss: 0.0416 - val_mse: 36.2630\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0399 - mse: 34.7606 - val_loss: 0.0405 - val_mse: 35.2313\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0390 - mse: 33.9276 - val_loss: 0.0391 - val_mse: 34.0841\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0378 - mse: 32.9316 - val_loss: 0.0391 - val_mse: 34.0842\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0379 - mse: 33.0177 - val_loss: 0.0377 - val_mse: 32.8700\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0366 - mse: 31.8852 - val_loss: 0.0359 - val_mse: 31.2606\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0348 - mse: 30.3363 - val_loss: 0.0353 - val_mse: 30.7298\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0341 - mse: 29.6620 - val_loss: 0.0347 - val_mse: 30.2240\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0330 - mse: 28.7719 - val_loss: 0.0345 - val_mse: 30.0211\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0324 - mse: 28.1763 - val_loss: 0.0347 - val_mse: 30.2320\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0323 - mse: 28.1386 - val_loss: 0.0342 - val_mse: 29.7627\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0317 - mse: 27.5965 - val_loss: 0.0331 - val_mse: 28.7971\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0308 - mse: 26.7860 - val_loss: 0.0321 - val_mse: 27.9813\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0302 - mse: 26.2932 - val_loss: 0.0312 - val_mse: 27.1575\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0295 - mse: 25.7266 - val_loss: 0.0304 - val_mse: 26.5140\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0289 - mse: 25.1672 - val_loss: 0.0303 - val_mse: 26.3840\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0288 - mse: 25.0370 - val_loss: 0.0301 - val_mse: 26.2192\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0285 - mse: 24.8604 - val_loss: 0.0295 - val_mse: 25.7158\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0280 - mse: 24.3480 - val_loss: 0.0291 - val_mse: 25.3334\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0275 - mse: 23.9608 - val_loss: 0.0289 - val_mse: 25.1344\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0273 - mse: 23.7415 - val_loss: 0.0284 - val_mse: 24.7681\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0268 - mse: 23.3330 - val_loss: 0.0280 - val_mse: 24.3874\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0263 - mse: 22.9359 - val_loss: 0.0278 - val_mse: 24.1726\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0261 - mse: 22.7566 - val_loss: 0.0274 - val_mse: 23.9004\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0259 - mse: 22.5465 - val_loss: 0.0270 - val_mse: 23.4992\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0255 - mse: 22.2066 - val_loss: 0.0266 - val_mse: 23.1438\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0251 - mse: 21.8855 - val_loss: 0.0263 - val_mse: 22.8875\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0248 - mse: 21.6081 - val_loss: 0.0261 - val_mse: 22.6970\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0245 - mse: 21.3566 - val_loss: 0.0259 - val_mse: 22.5271\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0243 - mse: 21.1531 - val_loss: 0.0256 - val_mse: 22.2732\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0240 - mse: 20.9318 - val_loss: 0.0252 - val_mse: 21.9613\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0238 - mse: 20.6910 - val_loss: 0.0249 - val_mse: 21.6723\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0235 - mse: 20.4512 - val_loss: 0.0246 - val_mse: 21.4033\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0232 - mse: 20.1794 - val_loss: 0.0244 - val_mse: 21.2139\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0229 - mse: 19.9580 - val_loss: 0.0242 - val_mse: 21.0520\n",
      "Epoch 73/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0227 - mse: 19.7703 - val_loss: 0.0239 - val_mse: 20.8226\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0224 - mse: 19.5345 - val_loss: 0.0236 - val_mse: 20.5866\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0222 - mse: 19.3098 - val_loss: 0.0234 - val_mse: 20.3555\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0219 - mse: 19.0951 - val_loss: 0.0231 - val_mse: 20.0937\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0216 - mse: 18.8414 - val_loss: 0.0228 - val_mse: 19.8618\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0214 - mse: 18.6057 - val_loss: 0.0226 - val_mse: 19.6600\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0211 - mse: 18.3950 - val_loss: 0.0223 - val_mse: 19.4365\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0209 - mse: 18.1608 - val_loss: 0.0221 - val_mse: 19.2178\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0206 - mse: 17.9206 - val_loss: 0.0218 - val_mse: 19.0070\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0203 - mse: 17.6788 - val_loss: 0.0216 - val_mse: 18.8014\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0200 - mse: 17.4543 - val_loss: 0.0213 - val_mse: 18.5700\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0198 - mse: 17.2302 - val_loss: 0.0211 - val_mse: 18.3341\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0195 - mse: 17.0191 - val_loss: 0.0208 - val_mse: 18.1016\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0193 - mse: 16.7985 - val_loss: 0.0205 - val_mse: 17.8744\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0190 - mse: 16.5601 - val_loss: 0.0203 - val_mse: 17.6708\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0188 - mse: 16.3427 - val_loss: 0.0200 - val_mse: 17.4563\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0185 - mse: 16.1189 - val_loss: 0.0198 - val_mse: 17.2486\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0183 - mse: 15.9018 - val_loss: 0.0196 - val_mse: 17.0422\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0180 - mse: 15.6937 - val_loss: 0.0193 - val_mse: 16.7970\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0178 - mse: 15.4603 - val_loss: 0.0190 - val_mse: 16.5722\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0175 - mse: 15.2444 - val_loss: 0.0188 - val_mse: 16.3545\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0173 - mse: 15.0305 - val_loss: 0.0185 - val_mse: 16.1451\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0170 - mse: 14.8158 - val_loss: 0.0183 - val_mse: 15.9640\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0168 - mse: 14.5958 - val_loss: 0.0181 - val_mse: 15.7824\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0165 - mse: 14.3815 - val_loss: 0.0179 - val_mse: 15.5871\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0163 - mse: 14.1664 - val_loss: 0.0177 - val_mse: 15.3861\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0160 - mse: 13.9650 - val_loss: 0.0174 - val_mse: 15.1694\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0158 - mse: 13.7510 - val_loss: 0.0172 - val_mse: 14.9660\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0156 - mse: 13.5500 - val_loss: 0.0170 - val_mse: 14.7617\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0154 - mse: 13.3839 - val_loss: 0.0171 - val_mse: 14.8659\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0155 - mse: 13.5025 - val_loss: 0.0184 - val_mse: 16.0093\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0170 - mse: 14.8227 - val_loss: 0.0175 - val_mse: 15.2537\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0158 - mse: 13.7685 - val_loss: 0.0172 - val_mse: 14.9456\n",
      "Epoch 106/1000\n",
      "\n",
      "Epoch 00105: reducing learning rate to 0.00699999984354.\n",
      "6s - loss: 0.0154 - mse: 13.3704 - val_loss: 0.0173 - val_mse: 15.0700\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0158 - mse: 13.7742 - val_loss: 0.0167 - val_mse: 14.5274\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0151 - mse: 13.1248 - val_loss: 0.0170 - val_mse: 14.7681\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0154 - mse: 13.3784 - val_loss: 0.0160 - val_mse: 13.9457\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0144 - mse: 12.5640 - val_loss: 0.0162 - val_mse: 14.0722\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0146 - mse: 12.6856 - val_loss: 0.0161 - val_mse: 13.9841\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0142 - mse: 12.3914 - val_loss: 0.0157 - val_mse: 13.6990\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0139 - mse: 12.1359 - val_loss: 0.0153 - val_mse: 13.2818\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0136 - mse: 11.8840 - val_loss: 0.0151 - val_mse: 13.1554\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0135 - mse: 11.7641 - val_loss: 0.0154 - val_mse: 13.4015\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0136 - mse: 11.8596 - val_loss: 0.0151 - val_mse: 13.1142\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0132 - mse: 11.4612 - val_loss: 0.0152 - val_mse: 13.2434\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0133 - mse: 11.5444 - val_loss: 0.0149 - val_mse: 12.9580\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0128 - mse: 11.1245 - val_loss: 0.0148 - val_mse: 12.8807\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0127 - mse: 11.0606 - val_loss: 0.0144 - val_mse: 12.5805\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0126 - mse: 10.9337 - val_loss: 0.0142 - val_mse: 12.3772\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0124 - mse: 10.7987 - val_loss: 0.0142 - val_mse: 12.3625\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0123 - mse: 10.6888 - val_loss: 0.0140 - val_mse: 12.2292\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0121 - mse: 10.5167 - val_loss: 0.0139 - val_mse: 12.1312\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0120 - mse: 10.4430 - val_loss: 0.0137 - val_mse: 11.9650\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0117 - mse: 10.2087 - val_loss: 0.0136 - val_mse: 11.8280\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0116 - mse: 10.0819 - val_loss: 0.0134 - val_mse: 11.6745\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0114 - mse: 9.9693 - val_loss: 0.0134 - val_mse: 11.6821\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0113 - mse: 9.8584 - val_loss: 0.0134 - val_mse: 11.6689\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0112 - mse: 9.7536 - val_loss: 0.0132 - val_mse: 11.4932\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0110 - mse: 9.6151 - val_loss: 0.0130 - val_mse: 11.3519\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0109 - mse: 9.4551 - val_loss: 0.0129 - val_mse: 11.2361\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0107 - mse: 9.3340 - val_loss: 0.0127 - val_mse: 11.0719\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0106 - mse: 9.2242 - val_loss: 0.0126 - val_mse: 10.9679\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0104 - mse: 9.0851 - val_loss: 0.0125 - val_mse: 10.9081\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0103 - mse: 8.9797 - val_loss: 0.0124 - val_mse: 10.8079\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0102 - mse: 8.8414 - val_loss: 0.0124 - val_mse: 10.7645\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0100 - mse: 8.7025 - val_loss: 0.0122 - val_mse: 10.6125\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0098 - mse: 8.5736 - val_loss: 0.0120 - val_mse: 10.4790\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0097 - mse: 8.4454 - val_loss: 0.0120 - val_mse: 10.4502\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0096 - mse: 8.3362 - val_loss: 0.0119 - val_mse: 10.3867\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0094 - mse: 8.2268 - val_loss: 0.0119 - val_mse: 10.3340\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0093 - mse: 8.0891 - val_loss: 0.0117 - val_mse: 10.1556\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0092 - mse: 7.9974 - val_loss: 0.0117 - val_mse: 10.1726\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0091 - mse: 7.9169 - val_loss: 0.0116 - val_mse: 10.0925\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0091 - mse: 7.8978 - val_loss: 0.0121 - val_mse: 10.5079\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0094 - mse: 8.1449 - val_loss: 0.0122 - val_mse: 10.6485\n",
      "Epoch 148/1000\n",
      "\n",
      "Epoch 00147: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0099 - mse: 8.6458 - val_loss: 0.0149 - val_mse: 12.9826\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0123 - mse: 10.7221 - val_loss: 0.0112 - val_mse: 9.7269\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0086 - mse: 7.4915 - val_loss: 0.0133 - val_mse: 11.5789\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0111 - mse: 9.6975 - val_loss: 0.0129 - val_mse: 11.2221\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0100 - mse: 8.6948 - val_loss: 0.0129 - val_mse: 11.2562\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0101 - mse: 8.7540 - val_loss: 0.0117 - val_mse: 10.1936\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0093 - mse: 8.0849 - val_loss: 0.0119 - val_mse: 10.3615\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0095 - mse: 8.2522 - val_loss: 0.0120 - val_mse: 10.4284\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0090 - mse: 7.8160 - val_loss: 0.0124 - val_mse: 10.7800\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0094 - mse: 8.1498 - val_loss: 0.0110 - val_mse: 9.5686\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0083 - mse: 7.2099 - val_loss: 0.0117 - val_mse: 10.1810\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0092 - mse: 7.9833 - val_loss: 0.0111 - val_mse: 9.6458\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0080 - mse: 6.9765 - val_loss: 0.0121 - val_mse: 10.5080\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0089 - mse: 7.7216 - val_loss: 0.0107 - val_mse: 9.3307\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0077 - mse: 6.6971 - val_loss: 0.0113 - val_mse: 9.8619\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0086 - mse: 7.5064 - val_loss: 0.0106 - val_mse: 9.2047\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0075 - mse: 6.5623 - val_loss: 0.0114 - val_mse: 9.9200\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0082 - mse: 7.1695 - val_loss: 0.0104 - val_mse: 9.0809\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0074 - mse: 6.4222 - val_loss: 0.0107 - val_mse: 9.3582\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0080 - mse: 6.9598 - val_loss: 0.0102 - val_mse: 8.8682\n",
      "Epoch 168/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0072 - mse: 6.2935 - val_loss: 0.0108 - val_mse: 9.3831\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0077 - mse: 6.6949 - val_loss: 0.0101 - val_mse: 8.8310\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0071 - mse: 6.1736 - val_loss: 0.0103 - val_mse: 9.0121\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0075 - mse: 6.5064 - val_loss: 0.0100 - val_mse: 8.7165\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0069 - mse: 6.0395 - val_loss: 0.0104 - val_mse: 9.0635\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0072 - mse: 6.2890 - val_loss: 0.0099 - val_mse: 8.6243\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0068 - mse: 5.9175 - val_loss: 0.0100 - val_mse: 8.7221\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0070 - mse: 6.1288 - val_loss: 0.0098 - val_mse: 8.5712\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0066 - mse: 5.7900 - val_loss: 0.0101 - val_mse: 8.8163\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0068 - mse: 5.9458 - val_loss: 0.0097 - val_mse: 8.4447\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0065 - mse: 5.6733 - val_loss: 0.0098 - val_mse: 8.4958\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0067 - mse: 5.7923 - val_loss: 0.0097 - val_mse: 8.4761\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0064 - mse: 5.5806 - val_loss: 0.0099 - val_mse: 8.5878\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0065 - mse: 5.6245 - val_loss: 0.0096 - val_mse: 8.3412\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0063 - mse: 5.4898 - val_loss: 0.0095 - val_mse: 8.2992\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0063 - mse: 5.4629 - val_loss: 0.0096 - val_mse: 8.3822\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0062 - mse: 5.4108 - val_loss: 0.0095 - val_mse: 8.2927\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0061 - mse: 5.3100 - val_loss: 0.0094 - val_mse: 8.2157\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0061 - mse: 5.3254 - val_loss: 0.0093 - val_mse: 8.1421\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0059 - mse: 5.1800 - val_loss: 0.0095 - val_mse: 8.2328\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0060 - mse: 5.2066 - val_loss: 0.0093 - val_mse: 8.0617\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0058 - mse: 5.0869 - val_loss: 0.0092 - val_mse: 8.0351\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0058 - mse: 5.0662 - val_loss: 0.0093 - val_mse: 8.0728\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0058 - mse: 5.0151 - val_loss: 0.0091 - val_mse: 7.9666\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0057 - mse: 4.9281 - val_loss: 0.0091 - val_mse: 7.9106\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0057 - mse: 4.9259 - val_loss: 0.0091 - val_mse: 7.8897\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0055 - mse: 4.8261 - val_loss: 0.0090 - val_mse: 7.8775\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0055 - mse: 4.7983 - val_loss: 0.0089 - val_mse: 7.7548\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0055 - mse: 4.7497 - val_loss: 0.0089 - val_mse: 7.7132\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0054 - mse: 4.6711 - val_loss: 0.0089 - val_mse: 7.7532\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0053 - mse: 4.6504 - val_loss: 0.0088 - val_mse: 7.6462\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0053 - mse: 4.5775 - val_loss: 0.0087 - val_mse: 7.5948\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0052 - mse: 4.5281 - val_loss: 0.0087 - val_mse: 7.6085\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0052 - mse: 4.4942 - val_loss: 0.0086 - val_mse: 7.5279\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0051 - mse: 4.4228 - val_loss: 0.0086 - val_mse: 7.5004\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0050 - mse: 4.3888 - val_loss: 0.0086 - val_mse: 7.5107\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0050 - mse: 4.3462 - val_loss: 0.0085 - val_mse: 7.4326\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0049 - mse: 4.2833 - val_loss: 0.0085 - val_mse: 7.3993\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0049 - mse: 4.2517 - val_loss: 0.0085 - val_mse: 7.4160\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0048 - mse: 4.2117 - val_loss: 0.0084 - val_mse: 7.3505\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0048 - mse: 4.1550 - val_loss: 0.0084 - val_mse: 7.3113\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0047 - mse: 4.1200 - val_loss: 0.0084 - val_mse: 7.3144\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0047 - mse: 4.0847 - val_loss: 0.0083 - val_mse: 7.2534\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0046 - mse: 4.0337 - val_loss: 0.0083 - val_mse: 7.2225\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0046 - mse: 3.9947 - val_loss: 0.0083 - val_mse: 7.2140\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0046 - mse: 3.9629 - val_loss: 0.0082 - val_mse: 7.1508\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0045 - mse: 3.9191 - val_loss: 0.0082 - val_mse: 7.1295\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0045 - mse: 3.8756 - val_loss: 0.0082 - val_mse: 7.1173\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0044 - mse: 3.8424 - val_loss: 0.0081 - val_mse: 7.0602\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0044 - mse: 3.8071 - val_loss: 0.0081 - val_mse: 7.0393\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0043 - mse: 3.7646 - val_loss: 0.0081 - val_mse: 7.0103\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0043 - mse: 3.7244 - val_loss: 0.0080 - val_mse: 6.9784\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0042 - mse: 3.6905 - val_loss: 0.0080 - val_mse: 6.9638\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0042 - mse: 3.6559 - val_loss: 0.0079 - val_mse: 6.9151\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0042 - mse: 3.6164 - val_loss: 0.0079 - val_mse: 6.8919\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0041 - mse: 3.5767 - val_loss: 0.0079 - val_mse: 6.8567\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0041 - mse: 3.5406 - val_loss: 0.0078 - val_mse: 6.8161\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0040 - mse: 3.5076 - val_loss: 0.0078 - val_mse: 6.7989\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0040 - mse: 3.4743 - val_loss: 0.0078 - val_mse: 6.7536\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0039 - mse: 3.4390 - val_loss: 0.0077 - val_mse: 6.7239\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0039 - mse: 3.4031 - val_loss: 0.0077 - val_mse: 6.6859\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0039 - mse: 3.3686 - val_loss: 0.0076 - val_mse: 6.6615\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0038 - mse: 3.3357 - val_loss: 0.0076 - val_mse: 6.6443\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0038 - mse: 3.3047 - val_loss: 0.0076 - val_mse: 6.6125\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0038 - mse: 3.2741 - val_loss: 0.0076 - val_mse: 6.6008\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0037 - mse: 3.2443 - val_loss: 0.0075 - val_mse: 6.5739\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0037 - mse: 3.2150 - val_loss: 0.0076 - val_mse: 6.5792\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0037 - mse: 3.1879 - val_loss: 0.0075 - val_mse: 6.5390\n",
      "Epoch 236/1000\n",
      "\n",
      "Epoch 00235: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0036 - mse: 3.1634 - val_loss: 0.0075 - val_mse: 6.5487\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0036 - mse: 3.1481 - val_loss: 0.0075 - val_mse: 6.4891\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0036 - mse: 3.1028 - val_loss: 0.0074 - val_mse: 6.4709\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0035 - mse: 3.0706 - val_loss: 0.0075 - val_mse: 6.4878\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0035 - mse: 3.0725 - val_loss: 0.0074 - val_mse: 6.4258\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0035 - mse: 3.0300 - val_loss: 0.0074 - val_mse: 6.4080\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0035 - mse: 3.0057 - val_loss: 0.0074 - val_mse: 6.4245\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0034 - mse: 3.0002 - val_loss: 0.0073 - val_mse: 6.3749\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0034 - mse: 2.9622 - val_loss: 0.0073 - val_mse: 6.3541\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0034 - mse: 2.9398 - val_loss: 0.0073 - val_mse: 6.3645\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0034 - mse: 2.9310 - val_loss: 0.0073 - val_mse: 6.3251\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0033 - mse: 2.8974 - val_loss: 0.0072 - val_mse: 6.3070\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0033 - mse: 2.8731 - val_loss: 0.0072 - val_mse: 6.3086\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0033 - mse: 2.8622 - val_loss: 0.0072 - val_mse: 6.2707\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0033 - mse: 2.8348 - val_loss: 0.0072 - val_mse: 6.2556\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0032 - mse: 2.8075 - val_loss: 0.0072 - val_mse: 6.2566\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0032 - mse: 2.7936 - val_loss: 0.0072 - val_mse: 6.2274\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0032 - mse: 2.7724 - val_loss: 0.0071 - val_mse: 6.2107\n",
      "Epoch 254/1000\n",
      "\n",
      "Epoch 00253: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0032 - mse: 2.7444 - val_loss: 0.0071 - val_mse: 6.1964\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0031 - mse: 2.7247 - val_loss: 0.0071 - val_mse: 6.1788\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0031 - mse: 2.7093 - val_loss: 0.0071 - val_mse: 6.1690\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0031 - mse: 2.6931 - val_loss: 0.0071 - val_mse: 6.1614\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0031 - mse: 2.6792 - val_loss: 0.0071 - val_mse: 6.1396\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0031 - mse: 2.6619 - val_loss: 0.0070 - val_mse: 6.1209\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0030 - mse: 2.6486 - val_loss: 0.0070 - val_mse: 6.1100\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0030 - mse: 2.6313 - val_loss: 0.0070 - val_mse: 6.1015\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0030 - mse: 2.6174 - val_loss: 0.0070 - val_mse: 6.0826\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0030 - mse: 2.6010 - val_loss: 0.0070 - val_mse: 6.0660\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0030 - mse: 2.5859 - val_loss: 0.0070 - val_mse: 6.0550\n",
      "Epoch 265/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0030 - mse: 2.5710 - val_loss: 0.0069 - val_mse: 6.0396\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0029 - mse: 2.5550 - val_loss: 0.0069 - val_mse: 6.0240\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0029 - mse: 2.5410 - val_loss: 0.0069 - val_mse: 6.0104\n",
      "Epoch 268/1000\n",
      "\n",
      "Epoch 00267: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0029 - mse: 2.5249 - val_loss: 0.0069 - val_mse: 5.9958\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0029 - mse: 2.5104 - val_loss: 0.0069 - val_mse: 5.9837\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0029 - mse: 2.4993 - val_loss: 0.0069 - val_mse: 5.9747\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0029 - mse: 2.4892 - val_loss: 0.0069 - val_mse: 5.9675\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0028 - mse: 2.4781 - val_loss: 0.0068 - val_mse: 5.9594\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0028 - mse: 2.4681 - val_loss: 0.0068 - val_mse: 5.9473\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0028 - mse: 2.4571 - val_loss: 0.0068 - val_mse: 5.9363\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0028 - mse: 2.4472 - val_loss: 0.0068 - val_mse: 5.9272\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0028 - mse: 2.4365 - val_loss: 0.0068 - val_mse: 5.9193\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0028 - mse: 2.4265 - val_loss: 0.0068 - val_mse: 5.9085\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0028 - mse: 2.4160 - val_loss: 0.0068 - val_mse: 5.8972\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0028 - mse: 2.4060 - val_loss: 0.0068 - val_mse: 5.8870\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0028 - mse: 2.3957 - val_loss: 0.0068 - val_mse: 5.8782\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0027 - mse: 2.3857 - val_loss: 0.0067 - val_mse: 5.8680\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0027 - mse: 2.3758 - val_loss: 0.0067 - val_mse: 5.8579\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0027 - mse: 2.3661 - val_loss: 0.0067 - val_mse: 5.8487\n",
      "Epoch 284/1000\n",
      "\n",
      "Epoch 00283: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0027 - mse: 2.3562 - val_loss: 0.0067 - val_mse: 5.8388\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0027 - mse: 2.3465 - val_loss: 0.0067 - val_mse: 5.8304\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0027 - mse: 2.3396 - val_loss: 0.0067 - val_mse: 5.8216\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0027 - mse: 2.3327 - val_loss: 0.0067 - val_mse: 5.8143\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0027 - mse: 2.3261 - val_loss: 0.0067 - val_mse: 5.8079\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0027 - mse: 2.3194 - val_loss: 0.0067 - val_mse: 5.8017\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0027 - mse: 2.3128 - val_loss: 0.0067 - val_mse: 5.7945\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0026 - mse: 2.3062 - val_loss: 0.0066 - val_mse: 5.7866\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0026 - mse: 2.2996 - val_loss: 0.0066 - val_mse: 5.7795\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0026 - mse: 2.2931 - val_loss: 0.0066 - val_mse: 5.7733\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0026 - mse: 2.2864 - val_loss: 0.0066 - val_mse: 5.7674\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0026 - mse: 2.2799 - val_loss: 0.0066 - val_mse: 5.7605\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0026 - mse: 2.2734 - val_loss: 0.0066 - val_mse: 5.7527\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0026 - mse: 2.2668 - val_loss: 0.0066 - val_mse: 5.7449\n",
      "Epoch 298/1000\n",
      "\n",
      "Epoch 00297: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0026 - mse: 2.2603 - val_loss: 0.0066 - val_mse: 5.7381\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0026 - mse: 2.2537 - val_loss: 0.0066 - val_mse: 5.7335\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0026 - mse: 2.2492 - val_loss: 0.0066 - val_mse: 5.7288\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0026 - mse: 2.2447 - val_loss: 0.0066 - val_mse: 5.7238\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0026 - mse: 2.2402 - val_loss: 0.0066 - val_mse: 5.7186\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0026 - mse: 2.2357 - val_loss: 0.0066 - val_mse: 5.7134\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0026 - mse: 2.2312 - val_loss: 0.0066 - val_mse: 5.7080\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0026 - mse: 2.2267 - val_loss: 0.0065 - val_mse: 5.7029\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0026 - mse: 2.2222 - val_loss: 0.0065 - val_mse: 5.6979\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0025 - mse: 2.2178 - val_loss: 0.0065 - val_mse: 5.6926\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0025 - mse: 2.2133 - val_loss: 0.0065 - val_mse: 5.6872\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0025 - mse: 2.2089 - val_loss: 0.0065 - val_mse: 5.6818\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0025 - mse: 2.2045 - val_loss: 0.0065 - val_mse: 5.6760\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0025 - mse: 2.2001 - val_loss: 0.0065 - val_mse: 5.6705\n",
      "Epoch 312/1000\n",
      "\n",
      "Epoch 00311: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0025 - mse: 2.1956 - val_loss: 0.0065 - val_mse: 5.6654\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0025 - mse: 2.1912 - val_loss: 0.0065 - val_mse: 5.6617\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0025 - mse: 2.1881 - val_loss: 0.0065 - val_mse: 5.6580\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0025 - mse: 2.1850 - val_loss: 0.0065 - val_mse: 5.6540\n",
      "Epoch 316/1000\n",
      "1s - loss: 0.0025 - mse: 2.1820 - val_loss: 0.0065 - val_mse: 5.6501\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0025 - mse: 2.1789 - val_loss: 0.0065 - val_mse: 5.6465\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0025 - mse: 2.1758 - val_loss: 0.0065 - val_mse: 5.6431\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0025 - mse: 2.1727 - val_loss: 0.0065 - val_mse: 5.6395\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0025 - mse: 2.1696 - val_loss: 0.0065 - val_mse: 5.6356\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0025 - mse: 2.1665 - val_loss: 0.0065 - val_mse: 5.6319\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0025 - mse: 2.1634 - val_loss: 0.0065 - val_mse: 5.6286\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0025 - mse: 2.1603 - val_loss: 0.0065 - val_mse: 5.6256\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0025 - mse: 2.1572 - val_loss: 0.0065 - val_mse: 5.6224\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0025 - mse: 2.1541 - val_loss: 0.0065 - val_mse: 5.6191\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0025 - mse: 2.1510 - val_loss: 0.0064 - val_mse: 5.6157\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0025 - mse: 2.1479 - val_loss: 0.0064 - val_mse: 5.6122\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0025 - mse: 2.1448 - val_loss: 0.0064 - val_mse: 5.6084\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0025 - mse: 2.1417 - val_loss: 0.0064 - val_mse: 5.6046\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0025 - mse: 2.1387 - val_loss: 0.0064 - val_mse: 5.6009\n",
      "Epoch 331/1000\n",
      "\n",
      "Epoch 00330: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0025 - mse: 2.1356 - val_loss: 0.0064 - val_mse: 5.5973\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0024 - mse: 2.1325 - val_loss: 0.0064 - val_mse: 5.5948\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0024 - mse: 2.1303 - val_loss: 0.0064 - val_mse: 5.5924\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0024 - mse: 2.1282 - val_loss: 0.0064 - val_mse: 5.5900\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0024 - mse: 2.1261 - val_loss: 0.0064 - val_mse: 5.5873\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0024 - mse: 2.1239 - val_loss: 0.0064 - val_mse: 5.5847\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0024 - mse: 2.1218 - val_loss: 0.0064 - val_mse: 5.5821\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.0024 - mse: 2.1196 - val_loss: 0.0064 - val_mse: 5.5795\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0024 - mse: 2.1174 - val_loss: 0.0064 - val_mse: 5.5771\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0024 - mse: 2.1153 - val_loss: 0.0064 - val_mse: 5.5747\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0024 - mse: 2.1132 - val_loss: 0.0064 - val_mse: 5.5721\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0024 - mse: 2.1111 - val_loss: 0.0064 - val_mse: 5.5696\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.0024 - mse: 2.1090 - val_loss: 0.0064 - val_mse: 5.5673\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0024 - mse: 2.1069 - val_loss: 0.0064 - val_mse: 5.5651\n",
      "Epoch 345/1000\n",
      "\n",
      "Epoch 00344: reducing learning rate to 0.000282475253334.\n",
      "0s - loss: 0.0024 - mse: 2.1047 - val_loss: 0.0064 - val_mse: 5.5629\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.0024 - mse: 2.1026 - val_loss: 0.0064 - val_mse: 5.5613\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.0024 - mse: 2.1011 - val_loss: 0.0064 - val_mse: 5.5598\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0024 - mse: 2.0996 - val_loss: 0.0064 - val_mse: 5.5582\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.0024 - mse: 2.0982 - val_loss: 0.0064 - val_mse: 5.5565\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0024 - mse: 2.0967 - val_loss: 0.0064 - val_mse: 5.5547\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.0024 - mse: 2.0952 - val_loss: 0.0064 - val_mse: 5.5528\n",
      "Epoch 352/1000\n",
      "0s - loss: 0.0024 - mse: 2.0938 - val_loss: 0.0064 - val_mse: 5.5509\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.0024 - mse: 2.0923 - val_loss: 0.0064 - val_mse: 5.5492\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.0024 - mse: 2.0908 - val_loss: 0.0064 - val_mse: 5.5474\n",
      "Epoch 355/1000\n",
      "0s - loss: 0.0024 - mse: 2.0893 - val_loss: 0.0064 - val_mse: 5.5457\n",
      "Epoch 356/1000\n",
      "0s - loss: 0.0024 - mse: 2.0879 - val_loss: 0.0064 - val_mse: 5.5441\n",
      "Epoch 357/1000\n",
      "0s - loss: 0.0024 - mse: 2.0864 - val_loss: 0.0064 - val_mse: 5.5423\n",
      "[{'loss': 0.024293258786201477, 'dropout': 0.0, 'val_mse': 23.78474235534668, 'width': 10, 'mse': 21.155517578125, 'val_loss': 0.027312444522976875}, {'loss': 0.0041144080460071564, 'dropout': 0.0, 'val_mse': 7.8667654991149902, 'width': 110, 'mse': 3.5829868316650391, 'val_loss': 0.0090335477143526077}, {'loss': 0.0013604337582364678, 'dropout': 0.0, 'val_mse': 3.409489631652832, 'width': 210, 'mse': 1.1847187280654907, 'val_loss': 0.003915177658200264}, {'loss': 0.0069423709064722061, 'dropout': 0.0, 'val_mse': 9.2405805587768555, 'width': 310, 'mse': 6.0456881523132324, 'val_loss': 0.010611124336719513}, {'loss': 0.023029616102576256, 'dropout': 0.0, 'val_mse': 22.774328231811523, 'width': 410, 'mse': 20.055086135864258, 'val_loss': 0.026152169331908226}]\n"
     ]
    }
   ],
   "source": [
    "width_patience=2 # if increasing width this nr of times did not improve val_loss, stop\n",
    "width2_perf = []\n",
    "prev_weights = []\n",
    "for width in range(50, 301, 50):\n",
    "    fit, results = run_depth1(width, learningrates2, prev_weights, 'mediumsteps')\n",
    "    width2_perf.append(results)\n",
    "    prev_weights=[layer.get_weights() for layer in model.layers]\n",
    "    print width_perf\n",
    "    if len(width_perf)>width_patience and width_perf[-1]['val_mse']>np.min(x['val_mse'] for x in width_perf[-width_patience:-1]):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1793aef90>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAENCAYAAADgwHn9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e8237a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(width2_perf).loc[:,['width','loss','val_loss']].set_index('width').plot(logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_70 (LSTM)               (None, None, 75)          23400     \n",
      "_________________________________________________________________\n",
      "time_distributed_67 (TimeDis (None, None, 1)           76        \n",
      "=================================================================\n",
      "Total params: 23,476\n",
      "Trainable params: 23,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 1.0390 - mse: 904.8274 - val_loss: 0.7489 - val_mse: 652.1671\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.7070 - mse: 615.6877 - val_loss: 0.4976 - val_mse: 433.3028\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.4583 - mse: 399.0903 - val_loss: 0.3232 - val_mse: 281.4737\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.2991 - mse: 260.4613 - val_loss: 0.2695 - val_mse: 234.6826\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2615 - mse: 227.7144 - val_loss: 0.2829 - val_mse: 246.3958\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2820 - mse: 245.5874 - val_loss: 0.2222 - val_mse: 193.5296\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.2218 - mse: 193.1415 - val_loss: 0.1911 - val_mse: 166.4041\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1891 - mse: 164.7025 - val_loss: 0.1686 - val_mse: 146.8249\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1610 - mse: 140.1980 - val_loss: 0.1681 - val_mse: 146.3538\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1578 - mse: 137.3996 - val_loss: 0.1531 - val_mse: 133.3293\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1434 - mse: 124.8550 - val_loss: 0.1307 - val_mse: 113.8366\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1222 - mse: 106.3880 - val_loss: 0.1142 - val_mse: 99.4323\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1022 - mse: 89.0337 - val_loss: 0.1231 - val_mse: 107.1939\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1020 - mse: 88.7939 - val_loss: 0.1378 - val_mse: 119.9802\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1111 - mse: 96.7923 - val_loss: 0.1297 - val_mse: 112.9359\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.1063 - mse: 92.5607 - val_loss: 0.1162 - val_mse: 101.2286\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0973 - mse: 84.7437 - val_loss: 0.1084 - val_mse: 94.4051\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0923 - mse: 80.3688 - val_loss: 0.1081 - val_mse: 94.1070\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0941 - mse: 81.9077 - val_loss: 0.1056 - val_mse: 91.9860\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0931 - mse: 81.0668 - val_loss: 0.1012 - val_mse: 88.1716\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0900 - mse: 78.3533 - val_loss: 0.0972 - val_mse: 84.6386\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0870 - mse: 75.7777 - val_loss: 0.0909 - val_mse: 79.1904\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0812 - mse: 70.7314 - val_loss: 0.0844 - val_mse: 73.4933\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0746 - mse: 64.9530 - val_loss: 0.0825 - val_mse: 71.8409\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0728 - mse: 63.3702 - val_loss: 0.0819 - val_mse: 71.3266\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0728 - mse: 63.4166 - val_loss: 0.0798 - val_mse: 69.5106\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0720 - mse: 62.6679 - val_loss: 0.0773 - val_mse: 67.3512\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0705 - mse: 61.3662 - val_loss: 0.0746 - val_mse: 64.9656\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0678 - mse: 59.0699 - val_loss: 0.0745 - val_mse: 64.8989\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0679 - mse: 59.1098 - val_loss: 0.0736 - val_mse: 64.1093\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0672 - mse: 58.5283 - val_loss: 0.0720 - val_mse: 62.7293\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0660 - mse: 57.4934 - val_loss: 0.0708 - val_mse: 61.6244\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0653 - mse: 56.8610 - val_loss: 0.0688 - val_mse: 59.9309\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0638 - mse: 55.5598 - val_loss: 0.0671 - val_mse: 58.3931\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0624 - mse: 54.3334 - val_loss: 0.0660 - val_mse: 57.4979\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0617 - mse: 53.7231 - val_loss: 0.0650 - val_mse: 56.6115\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0609 - mse: 53.0453 - val_loss: 0.0639 - val_mse: 55.6147\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0599 - mse: 52.1578 - val_loss: 0.0625 - val_mse: 54.4523\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0586 - mse: 51.0095 - val_loss: 0.0612 - val_mse: 53.3042\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0571 - mse: 49.7173 - val_loss: 0.0608 - val_mse: 52.9106\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0563 - mse: 49.0352 - val_loss: 0.0609 - val_mse: 53.0344\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0561 - mse: 48.8768 - val_loss: 0.0603 - val_mse: 52.4978\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0553 - mse: 48.1464 - val_loss: 0.0588 - val_mse: 51.2182\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0537 - mse: 46.7402 - val_loss: 0.0576 - val_mse: 50.1884\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0523 - mse: 45.5722 - val_loss: 0.0571 - val_mse: 49.7557\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0516 - mse: 44.9400 - val_loss: 0.0568 - val_mse: 49.4345\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0511 - mse: 44.4657 - val_loss: 0.0560 - val_mse: 48.7338\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0503 - mse: 43.8084 - val_loss: 0.0547 - val_mse: 47.6381\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0493 - mse: 42.9092 - val_loss: 0.0535 - val_mse: 46.5634\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0482 - mse: 41.9940 - val_loss: 0.0525 - val_mse: 45.7617\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0472 - mse: 41.1398 - val_loss: 0.0519 - val_mse: 45.1722\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0462 - mse: 40.2683 - val_loss: 0.0513 - val_mse: 44.6829\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0453 - mse: 39.4696 - val_loss: 0.0505 - val_mse: 43.9389\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0444 - mse: 38.6492 - val_loss: 0.0492 - val_mse: 42.8438\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0434 - mse: 37.8164 - val_loss: 0.0479 - val_mse: 41.7354\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0426 - mse: 37.1175 - val_loss: 0.0469 - val_mse: 40.8070\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0419 - mse: 36.4764 - val_loss: 0.0459 - val_mse: 40.0009\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0411 - mse: 35.8173 - val_loss: 0.0451 - val_mse: 39.2357\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0403 - mse: 35.1227 - val_loss: 0.0444 - val_mse: 38.6930\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0397 - mse: 34.5750 - val_loss: 0.0440 - val_mse: 38.3223\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0392 - mse: 34.1253 - val_loss: 0.0434 - val_mse: 37.8030\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0385 - mse: 33.5497 - val_loss: 0.0426 - val_mse: 37.0959\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0378 - mse: 32.9149 - val_loss: 0.0418 - val_mse: 36.4307\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0372 - mse: 32.3691 - val_loss: 0.0413 - val_mse: 35.9737\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0366 - mse: 31.8764 - val_loss: 0.0409 - val_mse: 35.5880\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0359 - mse: 31.2826 - val_loss: 0.0404 - val_mse: 35.1432\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0353 - mse: 30.7328 - val_loss: 0.0397 - val_mse: 34.5720\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0348 - mse: 30.2852 - val_loss: 0.0390 - val_mse: 33.9786\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0343 - mse: 29.8278 - val_loss: 0.0383 - val_mse: 33.3803\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0337 - mse: 29.3404 - val_loss: 0.0376 - val_mse: 32.7288\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0331 - mse: 28.8483 - val_loss: 0.0370 - val_mse: 32.2359\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0326 - mse: 28.4295 - val_loss: 0.0366 - val_mse: 31.8362\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0322 - mse: 28.0531 - val_loss: 0.0358 - val_mse: 31.1695\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0317 - mse: 27.6087 - val_loss: 0.0350 - val_mse: 30.5195\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0312 - mse: 27.1857 - val_loss: 0.0345 - val_mse: 30.0571\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0308 - mse: 26.8426 - val_loss: 0.0339 - val_mse: 29.5523\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0304 - mse: 26.4777 - val_loss: 0.0335 - val_mse: 29.1649\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0300 - mse: 26.0892 - val_loss: 0.0331 - val_mse: 28.8389\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0296 - mse: 25.7375 - val_loss: 0.0326 - val_mse: 28.4197\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0292 - mse: 25.4038 - val_loss: 0.0323 - val_mse: 28.1129\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0288 - mse: 25.0535 - val_loss: 0.0318 - val_mse: 27.6964\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0284 - mse: 24.6998 - val_loss: 0.0316 - val_mse: 27.4775\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0280 - mse: 24.3796 - val_loss: 0.0312 - val_mse: 27.1326\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0276 - mse: 24.0582 - val_loss: 0.0309 - val_mse: 26.8935\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0272 - mse: 23.7285 - val_loss: 0.0304 - val_mse: 26.5032\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0269 - mse: 23.4422 - val_loss: 0.0304 - val_mse: 26.4622\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0267 - mse: 23.2357 - val_loss: 0.0299 - val_mse: 26.0809\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0269 - mse: 23.3966 - val_loss: 0.0328 - val_mse: 28.5831\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0290 - mse: 25.2628 - val_loss: 0.0336 - val_mse: 29.2692\n",
      "Epoch 90/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0318 - mse: 27.6930 - val_loss: 0.0335 - val_mse: 29.1810\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0302 - mse: 26.2636 - val_loss: 0.0284 - val_mse: 24.7445\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0255 - mse: 22.2304 - val_loss: 0.0316 - val_mse: 27.4789\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0300 - mse: 26.1373 - val_loss: 0.0285 - val_mse: 24.8033\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0255 - mse: 22.2398 - val_loss: 0.0299 - val_mse: 26.0707\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0270 - mse: 23.5125 - val_loss: 0.0277 - val_mse: 24.0860\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0253 - mse: 22.0445 - val_loss: 0.0278 - val_mse: 24.2430\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0257 - mse: 22.4072 - val_loss: 0.0284 - val_mse: 24.7337\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0250 - mse: 21.8000 - val_loss: 0.0278 - val_mse: 24.2067\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0246 - mse: 21.4184 - val_loss: 0.0271 - val_mse: 23.6071\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0245 - mse: 21.3011 - val_loss: 0.0266 - val_mse: 23.1553\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0241 - mse: 21.0291 - val_loss: 0.0270 - val_mse: 23.5510\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0238 - mse: 20.7093 - val_loss: 0.0268 - val_mse: 23.3676\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0236 - mse: 20.5441 - val_loss: 0.0256 - val_mse: 22.2654\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0232 - mse: 20.1930 - val_loss: 0.0254 - val_mse: 22.1567\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0233 - mse: 20.3130 - val_loss: 0.0252 - val_mse: 21.9384\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0225 - mse: 19.6061 - val_loss: 0.0261 - val_mse: 22.7425\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0230 - mse: 20.0440 - val_loss: 0.0244 - val_mse: 21.2229\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0221 - mse: 19.2597 - val_loss: 0.0245 - val_mse: 21.3054\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0226 - mse: 19.6521 - val_loss: 0.0241 - val_mse: 20.9552\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0218 - mse: 18.9423 - val_loss: 0.0251 - val_mse: 21.8652\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0221 - mse: 19.2622 - val_loss: 0.0240 - val_mse: 20.9202\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0215 - mse: 18.6840 - val_loss: 0.0237 - val_mse: 20.6319\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0216 - mse: 18.8076 - val_loss: 0.0235 - val_mse: 20.4379\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0212 - mse: 18.4738 - val_loss: 0.0239 - val_mse: 20.8021\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0211 - mse: 18.3895 - val_loss: 0.0236 - val_mse: 20.5278\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0209 - mse: 18.1937 - val_loss: 0.0229 - val_mse: 19.9410\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0207 - mse: 18.0394 - val_loss: 0.0227 - val_mse: 19.8030\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0206 - mse: 17.9044 - val_loss: 0.0229 - val_mse: 19.9258\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0203 - mse: 17.7166 - val_loss: 0.0227 - val_mse: 19.8014\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0202 - mse: 17.5823 - val_loss: 0.0222 - val_mse: 19.2955\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0200 - mse: 17.4346 - val_loss: 0.0220 - val_mse: 19.1408\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0198 - mse: 17.2740 - val_loss: 0.0222 - val_mse: 19.2996\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0197 - mse: 17.1565 - val_loss: 0.0219 - val_mse: 19.1112\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0195 - mse: 16.9721 - val_loss: 0.0216 - val_mse: 18.7817\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0194 - mse: 16.8978 - val_loss: 0.0214 - val_mse: 18.6547\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0192 - mse: 16.6854 - val_loss: 0.0216 - val_mse: 18.7956\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0191 - mse: 16.6370 - val_loss: 0.0212 - val_mse: 18.4505\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0189 - mse: 16.4264 - val_loss: 0.0210 - val_mse: 18.2642\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0188 - mse: 16.3672 - val_loss: 0.0210 - val_mse: 18.2689\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0186 - mse: 16.2127 - val_loss: 0.0208 - val_mse: 18.1495\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0185 - mse: 16.0798 - val_loss: 0.0206 - val_mse: 17.9268\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0184 - mse: 16.0142 - val_loss: 0.0205 - val_mse: 17.8322\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0182 - mse: 15.8235 - val_loss: 0.0205 - val_mse: 17.8225\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0181 - mse: 15.7699 - val_loss: 0.0202 - val_mse: 17.5621\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0180 - mse: 15.6445 - val_loss: 0.0201 - val_mse: 17.4961\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0178 - mse: 15.5028 - val_loss: 0.0201 - val_mse: 17.5074\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0177 - mse: 15.4404 - val_loss: 0.0198 - val_mse: 17.2474\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0176 - mse: 15.3183 - val_loss: 0.0197 - val_mse: 17.1875\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0174 - mse: 15.1860 - val_loss: 0.0197 - val_mse: 17.1865\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0174 - mse: 15.1162 - val_loss: 0.0195 - val_mse: 16.9525\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0172 - mse: 15.0198 - val_loss: 0.0194 - val_mse: 16.9141\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0171 - mse: 14.8820 - val_loss: 0.0194 - val_mse: 16.8570\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0170 - mse: 14.7934 - val_loss: 0.0191 - val_mse: 16.6613\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0169 - mse: 14.7202 - val_loss: 0.0192 - val_mse: 16.6786\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0168 - mse: 14.6107 - val_loss: 0.0190 - val_mse: 16.5136\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0166 - mse: 14.4886 - val_loss: 0.0188 - val_mse: 16.4033\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0165 - mse: 14.3879 - val_loss: 0.0188 - val_mse: 16.4082\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0164 - mse: 14.3126 - val_loss: 0.0186 - val_mse: 16.2390\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0163 - mse: 14.2178 - val_loss: 0.0187 - val_mse: 16.2534\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0162 - mse: 14.1054 - val_loss: 0.0185 - val_mse: 16.0918\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0161 - mse: 13.9942 - val_loss: 0.0184 - val_mse: 16.0427\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0159 - mse: 13.8894 - val_loss: 0.0184 - val_mse: 16.0227\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0158 - mse: 13.8001 - val_loss: 0.0182 - val_mse: 15.8681\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0158 - mse: 13.7172 - val_loss: 0.0183 - val_mse: 15.9169\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0156 - mse: 13.6240 - val_loss: 0.0181 - val_mse: 15.7257\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0155 - mse: 13.5281 - val_loss: 0.0181 - val_mse: 15.7663\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0154 - mse: 13.4305 - val_loss: 0.0179 - val_mse: 15.5758\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0153 - mse: 13.3285 - val_loss: 0.0179 - val_mse: 15.6213\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0152 - mse: 13.2295 - val_loss: 0.0177 - val_mse: 15.4282\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0151 - mse: 13.1349 - val_loss: 0.0178 - val_mse: 15.4870\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0150 - mse: 13.0451 - val_loss: 0.0175 - val_mse: 15.2830\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0149 - mse: 12.9692 - val_loss: 0.0177 - val_mse: 15.4345\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0148 - mse: 12.9288 - val_loss: 0.0175 - val_mse: 15.2124\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0149 - mse: 12.9542 - val_loss: 0.0181 - val_mse: 15.7714\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0151 - mse: 13.1498 - val_loss: 0.0181 - val_mse: 15.7313\n",
      "Epoch 166/1000\n",
      "\n",
      "Epoch 00165: reducing learning rate to 0.00699999984354.\n",
      "5s - loss: 0.0157 - mse: 13.6443 - val_loss: 0.0193 - val_mse: 16.8145\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0162 - mse: 14.1009 - val_loss: 0.0171 - val_mse: 14.8922\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0145 - mse: 12.6391 - val_loss: 0.0175 - val_mse: 15.2009\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0150 - mse: 13.0790 - val_loss: 0.0181 - val_mse: 15.7291\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0150 - mse: 13.0568 - val_loss: 0.0171 - val_mse: 14.8739\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0142 - mse: 12.3324 - val_loss: 0.0175 - val_mse: 15.1973\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0149 - mse: 12.9656 - val_loss: 0.0168 - val_mse: 14.5926\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0139 - mse: 12.0800 - val_loss: 0.0177 - val_mse: 15.4003\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0145 - mse: 12.6580 - val_loss: 0.0167 - val_mse: 14.5644\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0139 - mse: 12.0757 - val_loss: 0.0168 - val_mse: 14.6342\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0141 - mse: 12.2495 - val_loss: 0.0169 - val_mse: 14.7173\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0139 - mse: 12.0628 - val_loss: 0.0166 - val_mse: 14.4928\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0136 - mse: 11.8842 - val_loss: 0.0165 - val_mse: 14.3959\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0138 - mse: 12.0081 - val_loss: 0.0162 - val_mse: 14.1004\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0134 - mse: 11.6355 - val_loss: 0.0167 - val_mse: 14.5434\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0137 - mse: 11.8872 - val_loss: 0.0161 - val_mse: 13.9861\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0132 - mse: 11.4724 - val_loss: 0.0162 - val_mse: 14.1014\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0134 - mse: 11.6614 - val_loss: 0.0160 - val_mse: 13.9759\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0131 - mse: 11.3833 - val_loss: 0.0161 - val_mse: 14.0484\n",
      "Epoch 185/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0131 - mse: 11.4148 - val_loss: 0.0159 - val_mse: 13.8264\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0130 - mse: 11.3202 - val_loss: 0.0158 - val_mse: 13.7478\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0128 - mse: 11.1823 - val_loss: 0.0159 - val_mse: 13.8758\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0129 - mse: 11.2189 - val_loss: 0.0156 - val_mse: 13.5678\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0126 - mse: 11.0039 - val_loss: 0.0156 - val_mse: 13.5687\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0127 - mse: 11.0565 - val_loss: 0.0155 - val_mse: 13.5052\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0125 - mse: 10.8950 - val_loss: 0.0154 - val_mse: 13.4447\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0125 - mse: 10.8496 - val_loss: 0.0153 - val_mse: 13.2943\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0124 - mse: 10.8092 - val_loss: 0.0152 - val_mse: 13.2130\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0122 - mse: 10.6624 - val_loss: 0.0152 - val_mse: 13.2682\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0123 - mse: 10.6724 - val_loss: 0.0150 - val_mse: 13.0560\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0121 - mse: 10.5394 - val_loss: 0.0149 - val_mse: 13.0099\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0120 - mse: 10.4821 - val_loss: 0.0150 - val_mse: 13.0444\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0120 - mse: 10.4430 - val_loss: 0.0148 - val_mse: 12.8651\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0118 - mse: 10.3114 - val_loss: 0.0147 - val_mse: 12.8056\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0118 - mse: 10.2946 - val_loss: 0.0147 - val_mse: 12.7831\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0117 - mse: 10.2063 - val_loss: 0.0146 - val_mse: 12.6776\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0116 - mse: 10.1119 - val_loss: 0.0145 - val_mse: 12.5938\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0116 - mse: 10.0831 - val_loss: 0.0144 - val_mse: 12.5311\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0115 - mse: 9.9870 - val_loss: 0.0143 - val_mse: 12.4681\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0114 - mse: 9.9078 - val_loss: 0.0142 - val_mse: 12.3987\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0113 - mse: 9.8685 - val_loss: 0.0141 - val_mse: 12.3204\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0112 - mse: 9.7840 - val_loss: 0.0141 - val_mse: 12.2606\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0111 - mse: 9.7005 - val_loss: 0.0140 - val_mse: 12.2154\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0111 - mse: 9.6579 - val_loss: 0.0139 - val_mse: 12.1304\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0110 - mse: 9.5901 - val_loss: 0.0139 - val_mse: 12.0645\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0109 - mse: 9.4995 - val_loss: 0.0138 - val_mse: 12.0237\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0108 - mse: 9.4443 - val_loss: 0.0137 - val_mse: 11.9354\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0108 - mse: 9.3891 - val_loss: 0.0136 - val_mse: 11.8775\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0107 - mse: 9.3083 - val_loss: 0.0136 - val_mse: 11.8256\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0106 - mse: 9.2351 - val_loss: 0.0135 - val_mse: 11.7425\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0105 - mse: 9.1800 - val_loss: 0.0135 - val_mse: 11.7132\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0105 - mse: 9.1217 - val_loss: 0.0134 - val_mse: 11.6356\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0104 - mse: 9.0471 - val_loss: 0.0133 - val_mse: 11.5718\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0103 - mse: 8.9698 - val_loss: 0.0132 - val_mse: 11.5052\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0102 - mse: 8.9062 - val_loss: 0.0131 - val_mse: 11.4393\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0102 - mse: 8.8522 - val_loss: 0.0131 - val_mse: 11.4350\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0101 - mse: 8.7967 - val_loss: 0.0130 - val_mse: 11.3120\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0100 - mse: 8.7348 - val_loss: 0.0130 - val_mse: 11.3271\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0100 - mse: 8.6670 - val_loss: 0.0129 - val_mse: 11.2111\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0099 - mse: 8.5987 - val_loss: 0.0129 - val_mse: 11.1943\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0098 - mse: 8.5302 - val_loss: 0.0128 - val_mse: 11.1242\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0097 - mse: 8.4646 - val_loss: 0.0127 - val_mse: 11.0819\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0096 - mse: 8.4017 - val_loss: 0.0127 - val_mse: 11.0210\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0096 - mse: 8.3414 - val_loss: 0.0126 - val_mse: 11.0030\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0095 - mse: 8.2821 - val_loss: 0.0126 - val_mse: 10.9299\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0094 - mse: 8.2222 - val_loss: 0.0125 - val_mse: 10.8976\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0094 - mse: 8.1627 - val_loss: 0.0125 - val_mse: 10.8896\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0093 - mse: 8.1055 - val_loss: 0.0124 - val_mse: 10.7996\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0092 - mse: 8.0489 - val_loss: 0.0124 - val_mse: 10.8360\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0092 - mse: 7.9950 - val_loss: 0.0123 - val_mse: 10.7337\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0091 - mse: 7.9579 - val_loss: 0.0126 - val_mse: 10.9440\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0092 - mse: 7.9823 - val_loss: 0.0125 - val_mse: 10.9152\n",
      "Epoch 238/1000\n",
      "\n",
      "Epoch 00237: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0095 - mse: 8.2671 - val_loss: 0.0149 - val_mse: 12.9381\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0110 - mse: 9.5430 - val_loss: 0.0132 - val_mse: 11.5308\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0104 - mse: 9.0980 - val_loss: 0.0121 - val_mse: 10.5725\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0089 - mse: 7.7796 - val_loss: 0.0141 - val_mse: 12.3098\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0104 - mse: 9.0545 - val_loss: 0.0120 - val_mse: 10.4801\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0090 - mse: 7.8808 - val_loss: 0.0124 - val_mse: 10.8318\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0095 - mse: 8.2862 - val_loss: 0.0131 - val_mse: 11.4366\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0094 - mse: 8.2024 - val_loss: 0.0123 - val_mse: 10.7501\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0089 - mse: 7.7554 - val_loss: 0.0124 - val_mse: 10.7670\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0095 - mse: 8.2689 - val_loss: 0.0120 - val_mse: 10.4484\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0087 - mse: 7.5365 - val_loss: 0.0130 - val_mse: 11.3291\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0093 - mse: 8.0917 - val_loss: 0.0118 - val_mse: 10.2514\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0086 - mse: 7.4981 - val_loss: 0.0120 - val_mse: 10.4237\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0090 - mse: 7.8611 - val_loss: 0.0121 - val_mse: 10.5520\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0086 - mse: 7.5091 - val_loss: 0.0124 - val_mse: 10.7812\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0088 - mse: 7.6358 - val_loss: 0.0118 - val_mse: 10.2360\n",
      "Epoch 254/1000\n",
      "\n",
      "Epoch 00253: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0086 - mse: 7.5071 - val_loss: 0.0118 - val_mse: 10.2452\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0086 - mse: 7.4574 - val_loss: 0.0120 - val_mse: 10.4478\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0084 - mse: 7.3379 - val_loss: 0.0123 - val_mse: 10.7237\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0086 - mse: 7.4529 - val_loss: 0.0119 - val_mse: 10.3349\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0083 - mse: 7.2601 - val_loss: 0.0118 - val_mse: 10.2651\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0085 - mse: 7.3809 - val_loss: 0.0118 - val_mse: 10.2412\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0083 - mse: 7.2432 - val_loss: 0.0121 - val_mse: 10.5083\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0084 - mse: 7.2756 - val_loss: 0.0120 - val_mse: 10.4778\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0083 - mse: 7.2408 - val_loss: 0.0117 - val_mse: 10.2101\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0082 - mse: 7.1833 - val_loss: 0.0117 - val_mse: 10.1567\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0083 - mse: 7.2211 - val_loss: 0.0117 - val_mse: 10.2037\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0082 - mse: 7.1229 - val_loss: 0.0119 - val_mse: 10.3986\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0082 - mse: 7.1725 - val_loss: 0.0118 - val_mse: 10.2552\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0081 - mse: 7.0904 - val_loss: 0.0116 - val_mse: 10.1319\n",
      "Epoch 268/1000\n",
      "\n",
      "Epoch 00267: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0082 - mse: 7.1088 - val_loss: 0.0116 - val_mse: 10.1145\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0081 - mse: 7.0679 - val_loss: 0.0117 - val_mse: 10.1936\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0081 - mse: 7.0365 - val_loss: 0.0118 - val_mse: 10.3003\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0081 - mse: 7.0508 - val_loss: 0.0118 - val_mse: 10.2603\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0081 - mse: 7.0177 - val_loss: 0.0117 - val_mse: 10.1695\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0080 - mse: 7.0009 - val_loss: 0.0116 - val_mse: 10.1347\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0080 - mse: 7.0055 - val_loss: 0.0117 - val_mse: 10.1506\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0080 - mse: 6.9734 - val_loss: 0.0117 - val_mse: 10.2255\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0080 - mse: 6.9662 - val_loss: 0.0118 - val_mse: 10.2514\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0080 - mse: 6.9606 - val_loss: 0.0117 - val_mse: 10.1841\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0080 - mse: 6.9331 - val_loss: 0.0116 - val_mse: 10.1302\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0080 - mse: 6.9298 - val_loss: 0.0116 - val_mse: 10.1189\n",
      "Epoch 280/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0079 - mse: 6.9171 - val_loss: 0.0117 - val_mse: 10.1498\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0079 - mse: 6.8960 - val_loss: 0.0117 - val_mse: 10.1919\n",
      "Epoch 282/1000\n",
      "\n",
      "Epoch 00281: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0079 - mse: 6.8924 - val_loss: 0.0117 - val_mse: 10.1703\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0079 - mse: 6.8759 - val_loss: 0.0116 - val_mse: 10.1328\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0079 - mse: 6.8631 - val_loss: 0.0116 - val_mse: 10.1084\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0079 - mse: 6.8583 - val_loss: 0.0116 - val_mse: 10.1091\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0079 - mse: 6.8506 - val_loss: 0.0116 - val_mse: 10.1309\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0079 - mse: 6.8382 - val_loss: 0.0117 - val_mse: 10.1581\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0078 - mse: 6.8309 - val_loss: 0.0117 - val_mse: 10.1647\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0078 - mse: 6.8246 - val_loss: 0.0116 - val_mse: 10.1434\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0078 - mse: 6.8138 - val_loss: 0.0116 - val_mse: 10.1187\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0078 - mse: 6.8049 - val_loss: 0.0116 - val_mse: 10.1116\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0078 - mse: 6.7985 - val_loss: 0.0116 - val_mse: 10.1249\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0078 - mse: 6.7892 - val_loss: 0.0117 - val_mse: 10.1462\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0078 - mse: 6.7799 - val_loss: 0.0117 - val_mse: 10.1581\n",
      "[{'loss': 0.0077854436822235584, 'dropout': 0.0, 'val_mse': 10.15809154510498, 'width': 75, 'mse': 6.7798690795898438, 'val_loss': 0.011664718389511108}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_71 (LSTM)               (None, None, 90)          33480     \n",
      "_________________________________________________________________\n",
      "time_distributed_68 (TimeDis (None, None, 1)           91        \n",
      "=================================================================\n",
      "Total params: 33,571\n",
      "Trainable params: 33,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 1.1689 - mse: 1017.9467 - val_loss: 0.7711 - val_mse: 671.5232\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.7222 - mse: 628.8787 - val_loss: 0.4866 - val_mse: 423.7838\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.4451 - mse: 387.6071 - val_loss: 0.2882 - val_mse: 250.9570\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.2730 - mse: 237.7436 - val_loss: 0.2968 - val_mse: 258.4979\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.3043 - mse: 265.0014 - val_loss: 0.2375 - val_mse: 206.8383\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2386 - mse: 207.8196 - val_loss: 0.2042 - val_mse: 177.8098\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.2003 - mse: 174.4047 - val_loss: 0.1798 - val_mse: 156.6105\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1727 - mse: 150.4045 - val_loss: 0.1723 - val_mse: 150.0242\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1625 - mse: 141.5265 - val_loss: 0.1568 - val_mse: 136.5253\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1462 - mse: 127.3133 - val_loss: 0.1350 - val_mse: 117.5821\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1236 - mse: 107.6595 - val_loss: 0.1207 - val_mse: 105.0946\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1059 - mse: 92.2429 - val_loss: 0.1273 - val_mse: 110.8262\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1055 - mse: 91.9039 - val_loss: 0.1473 - val_mse: 128.2342\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1189 - mse: 103.5282 - val_loss: 0.1500 - val_mse: 130.6197\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1216 - mse: 105.8953 - val_loss: 0.1324 - val_mse: 115.2597\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.1093 - mse: 95.2205 - val_loss: 0.1160 - val_mse: 101.0091\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0987 - mse: 85.9769 - val_loss: 0.1085 - val_mse: 94.5107\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0952 - mse: 82.8744 - val_loss: 0.1046 - val_mse: 91.0584\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0934 - mse: 81.3417 - val_loss: 0.0988 - val_mse: 86.0817\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0888 - mse: 77.3508 - val_loss: 0.0912 - val_mse: 79.4101\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0816 - mse: 71.0862 - val_loss: 0.0857 - val_mse: 74.6599\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0764 - mse: 66.5091 - val_loss: 0.0848 - val_mse: 73.8122\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0759 - mse: 66.1358 - val_loss: 0.0829 - val_mse: 72.1989\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0754 - mse: 65.6782 - val_loss: 0.0784 - val_mse: 68.2340\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0723 - mse: 62.9746 - val_loss: 0.0759 - val_mse: 66.0964\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0709 - mse: 61.7309 - val_loss: 0.0743 - val_mse: 64.7323\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0699 - mse: 60.8590 - val_loss: 0.0713 - val_mse: 62.1018\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0669 - mse: 58.2767 - val_loss: 0.0677 - val_mse: 58.9364\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0630 - mse: 54.8728 - val_loss: 0.0657 - val_mse: 57.1875\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0606 - mse: 52.8087 - val_loss: 0.0642 - val_mse: 55.9155\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0591 - mse: 51.4580 - val_loss: 0.0623 - val_mse: 54.2165\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0575 - mse: 50.0441 - val_loss: 0.0606 - val_mse: 52.7365\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0562 - mse: 48.9830 - val_loss: 0.0588 - val_mse: 51.2029\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0549 - mse: 47.7684 - val_loss: 0.0568 - val_mse: 49.4856\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0529 - mse: 46.0775 - val_loss: 0.0559 - val_mse: 48.6573\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0516 - mse: 44.9120 - val_loss: 0.0553 - val_mse: 48.1760\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0505 - mse: 43.9969 - val_loss: 0.0536 - val_mse: 46.7188\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0486 - mse: 42.3453 - val_loss: 0.0514 - val_mse: 44.7624\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0465 - mse: 40.4816 - val_loss: 0.0501 - val_mse: 43.6472\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0455 - mse: 39.6240 - val_loss: 0.0494 - val_mse: 42.9881\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0449 - mse: 39.0971 - val_loss: 0.0485 - val_mse: 42.1955\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0442 - mse: 38.4499 - val_loss: 0.0473 - val_mse: 41.1911\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0432 - mse: 37.6146 - val_loss: 0.0470 - val_mse: 40.9027\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0427 - mse: 37.2005 - val_loss: 0.0467 - val_mse: 40.7108\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0421 - mse: 36.6458 - val_loss: 0.0456 - val_mse: 39.7331\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0408 - mse: 35.5064 - val_loss: 0.0445 - val_mse: 38.7611\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0397 - mse: 34.5735 - val_loss: 0.0439 - val_mse: 38.2102\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0391 - mse: 34.0115 - val_loss: 0.0427 - val_mse: 37.1598\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0382 - mse: 33.2887 - val_loss: 0.0412 - val_mse: 35.8704\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0372 - mse: 32.4139 - val_loss: 0.0404 - val_mse: 35.1791\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0366 - mse: 31.8906 - val_loss: 0.0397 - val_mse: 34.5372\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0362 - mse: 31.5176 - val_loss: 0.0388 - val_mse: 33.7949\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0354 - mse: 30.8307 - val_loss: 0.0379 - val_mse: 33.0395\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0345 - mse: 30.0703 - val_loss: 0.0373 - val_mse: 32.4733\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0340 - mse: 29.6459 - val_loss: 0.0369 - val_mse: 32.1319\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0336 - mse: 29.2657 - val_loss: 0.0359 - val_mse: 31.2196\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0329 - mse: 28.6334 - val_loss: 0.0353 - val_mse: 30.7336\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0324 - mse: 28.1896 - val_loss: 0.0348 - val_mse: 30.3405\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0321 - mse: 27.9593 - val_loss: 0.0343 - val_mse: 29.8537\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0316 - mse: 27.5143 - val_loss: 0.0337 - val_mse: 29.3349\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0310 - mse: 27.0123 - val_loss: 0.0333 - val_mse: 28.9627\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0306 - mse: 26.6872 - val_loss: 0.0330 - val_mse: 28.6981\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0302 - mse: 26.2840 - val_loss: 0.0323 - val_mse: 28.1414\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0297 - mse: 25.8952 - val_loss: 0.0325 - val_mse: 28.3342\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0296 - mse: 25.8011 - val_loss: 0.0319 - val_mse: 27.7485\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0295 - mse: 25.6494 - val_loss: 0.0322 - val_mse: 28.0457\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0293 - mse: 25.5013 - val_loss: 0.0310 - val_mse: 26.9615\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0286 - mse: 24.9344 - val_loss: 0.0308 - val_mse: 26.8423\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0280 - mse: 24.3908 - val_loss: 0.0298 - val_mse: 25.9688\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0273 - mse: 23.7451 - val_loss: 0.0295 - val_mse: 25.6842\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0270 - mse: 23.5173 - val_loss: 0.0297 - val_mse: 25.8602\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0270 - mse: 23.4893 - val_loss: 0.0289 - val_mse: 25.1849\n",
      "Epoch 73/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0265 - mse: 23.1024 - val_loss: 0.0286 - val_mse: 24.9366\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0261 - mse: 22.6954 - val_loss: 0.0286 - val_mse: 24.8836\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0259 - mse: 22.5440 - val_loss: 0.0282 - val_mse: 24.5184\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0257 - mse: 22.3783 - val_loss: 0.0281 - val_mse: 24.4637\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0253 - mse: 22.0511 - val_loss: 0.0276 - val_mse: 24.0318\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0249 - mse: 21.6998 - val_loss: 0.0273 - val_mse: 23.8117\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0247 - mse: 21.5008 - val_loss: 0.0274 - val_mse: 23.8791\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0245 - mse: 21.3657 - val_loss: 0.0269 - val_mse: 23.4348\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0243 - mse: 21.1258 - val_loss: 0.0268 - val_mse: 23.3261\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0239 - mse: 20.8356 - val_loss: 0.0264 - val_mse: 23.0040\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0236 - mse: 20.5791 - val_loss: 0.0261 - val_mse: 22.7670\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0234 - mse: 20.3929 - val_loss: 0.0261 - val_mse: 22.7244\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0232 - mse: 20.2341 - val_loss: 0.0257 - val_mse: 22.3482\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0230 - mse: 20.0547 - val_loss: 0.0257 - val_mse: 22.3487\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0228 - mse: 19.8529 - val_loss: 0.0252 - val_mse: 21.9491\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0225 - mse: 19.6084 - val_loss: 0.0251 - val_mse: 21.8521\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0222 - mse: 19.3688 - val_loss: 0.0248 - val_mse: 21.6106\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0220 - mse: 19.1547 - val_loss: 0.0246 - val_mse: 21.4359\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0218 - mse: 18.9614 - val_loss: 0.0246 - val_mse: 21.3889\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0216 - mse: 18.7906 - val_loss: 0.0243 - val_mse: 21.1472\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0214 - mse: 18.6387 - val_loss: 0.0244 - val_mse: 21.2902\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0213 - mse: 18.5178 - val_loss: 0.0241 - val_mse: 20.9573\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0212 - mse: 18.4373 - val_loss: 0.0246 - val_mse: 21.4504\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0213 - mse: 18.5082 - val_loss: 0.0241 - val_mse: 20.9613\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0213 - mse: 18.5907 - val_loss: 0.0252 - val_mse: 21.9584\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0218 - mse: 18.9418 - val_loss: 0.0238 - val_mse: 20.7062\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0212 - mse: 18.4685 - val_loss: 0.0237 - val_mse: 20.6669\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0205 - mse: 17.8315 - val_loss: 0.0226 - val_mse: 19.7007\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0197 - mse: 17.1490 - val_loss: 0.0225 - val_mse: 19.6121\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0198 - mse: 17.2449 - val_loss: 0.0233 - val_mse: 20.2928\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0201 - mse: 17.4781 - val_loss: 0.0221 - val_mse: 19.2886\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0194 - mse: 16.8845 - val_loss: 0.0219 - val_mse: 19.0732\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0189 - mse: 16.4314 - val_loss: 0.0222 - val_mse: 19.3533\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0189 - mse: 16.4984 - val_loss: 0.0218 - val_mse: 18.9458\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0189 - mse: 16.4332 - val_loss: 0.0218 - val_mse: 18.9672\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0184 - mse: 16.0606 - val_loss: 0.0213 - val_mse: 18.5161\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0181 - mse: 15.7209 - val_loss: 0.0211 - val_mse: 18.3367\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0180 - mse: 15.6887 - val_loss: 0.0214 - val_mse: 18.6257\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0180 - mse: 15.6683 - val_loss: 0.0206 - val_mse: 17.9814\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0176 - mse: 15.3445 - val_loss: 0.0205 - val_mse: 17.8394\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0172 - mse: 15.0030 - val_loss: 0.0204 - val_mse: 17.7533\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0171 - mse: 14.8506 - val_loss: 0.0201 - val_mse: 17.5225\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0170 - mse: 14.8025 - val_loss: 0.0204 - val_mse: 17.8068\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0169 - mse: 14.6988 - val_loss: 0.0198 - val_mse: 17.2772\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0166 - mse: 14.4363 - val_loss: 0.0199 - val_mse: 17.3304\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0162 - mse: 14.1510 - val_loss: 0.0195 - val_mse: 16.9397\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0159 - mse: 13.8772 - val_loss: 0.0193 - val_mse: 16.8285\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0157 - mse: 13.6559 - val_loss: 0.0192 - val_mse: 16.7038\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0155 - mse: 13.4720 - val_loss: 0.0189 - val_mse: 16.4494\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0153 - mse: 13.3284 - val_loss: 0.0191 - val_mse: 16.6502\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0153 - mse: 13.2945 - val_loss: 0.0189 - val_mse: 16.4872\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0156 - mse: 13.6030 - val_loss: 0.0218 - val_mse: 18.9687\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0178 - mse: 15.4708 - val_loss: 0.0240 - val_mse: 20.9099\n",
      "Epoch 126/1000\n",
      "\n",
      "Epoch 00125: reducing learning rate to 0.00699999984354.\n",
      "5s - loss: 0.0220 - mse: 19.1540 - val_loss: 0.0308 - val_mse: 26.8605\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0273 - mse: 23.8030 - val_loss: 0.0193 - val_mse: 16.7915\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0160 - mse: 13.9250 - val_loss: 0.0247 - val_mse: 21.5441\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0235 - mse: 20.4515 - val_loss: 0.0183 - val_mse: 15.9360\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0161 - mse: 14.0379 - val_loss: 0.0216 - val_mse: 18.7689\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0184 - mse: 15.9888 - val_loss: 0.0223 - val_mse: 19.4520\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0190 - mse: 16.5116 - val_loss: 0.0174 - val_mse: 15.1169\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0144 - mse: 12.5201 - val_loss: 0.0208 - val_mse: 18.0981\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0185 - mse: 16.1524 - val_loss: 0.0183 - val_mse: 15.9542\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0155 - mse: 13.4653 - val_loss: 0.0187 - val_mse: 16.2763\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0150 - mse: 13.0644 - val_loss: 0.0209 - val_mse: 18.2374\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0170 - mse: 14.8253 - val_loss: 0.0181 - val_mse: 15.7500\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0144 - mse: 12.5487 - val_loss: 0.0177 - val_mse: 15.4385\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0147 - mse: 12.8421 - val_loss: 0.0185 - val_mse: 16.0820\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0158 - mse: 13.7527 - val_loss: 0.0168 - val_mse: 14.6223\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0137 - mse: 11.9545 - val_loss: 0.0177 - val_mse: 15.3931\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0144 - mse: 12.5057 - val_loss: 0.0181 - val_mse: 15.7972\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0149 - mse: 12.9502 - val_loss: 0.0165 - val_mse: 14.3844\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0135 - mse: 11.7442 - val_loss: 0.0164 - val_mse: 14.3085\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0138 - mse: 11.9848 - val_loss: 0.0168 - val_mse: 14.5935\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0142 - mse: 12.3309 - val_loss: 0.0161 - val_mse: 13.9850\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0131 - mse: 11.4164 - val_loss: 0.0166 - val_mse: 14.4512\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0133 - mse: 11.6026 - val_loss: 0.0169 - val_mse: 14.7194\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0135 - mse: 11.7902 - val_loss: 0.0160 - val_mse: 13.9467\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0128 - mse: 11.1251 - val_loss: 0.0159 - val_mse: 13.8675\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0129 - mse: 11.2579 - val_loss: 0.0160 - val_mse: 13.9658\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0131 - mse: 11.3788 - val_loss: 0.0157 - val_mse: 13.6568\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0124 - mse: 10.8330 - val_loss: 0.0160 - val_mse: 13.9672\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0126 - mse: 10.9537 - val_loss: 0.0161 - val_mse: 13.9820\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0126 - mse: 10.9852 - val_loss: 0.0153 - val_mse: 13.3663\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0121 - mse: 10.5683 - val_loss: 0.0152 - val_mse: 13.2238\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0123 - mse: 10.6720 - val_loss: 0.0151 - val_mse: 13.1593\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0122 - mse: 10.6516 - val_loss: 0.0149 - val_mse: 13.0061\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0119 - mse: 10.3364 - val_loss: 0.0152 - val_mse: 13.2300\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0120 - mse: 10.4206 - val_loss: 0.0151 - val_mse: 13.1453\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0119 - mse: 10.3321 - val_loss: 0.0147 - val_mse: 12.7826\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0116 - mse: 10.1007 - val_loss: 0.0146 - val_mse: 12.7186\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0117 - mse: 10.1770 - val_loss: 0.0145 - val_mse: 12.6157\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0115 - mse: 10.0392 - val_loss: 0.0145 - val_mse: 12.6294\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0114 - mse: 9.8994 - val_loss: 0.0147 - val_mse: 12.7649\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0114 - mse: 9.9433 - val_loss: 0.0144 - val_mse: 12.5629\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0112 - mse: 9.7820 - val_loss: 0.0142 - val_mse: 12.3510\n",
      "Epoch 168/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0111 - mse: 9.7068 - val_loss: 0.0141 - val_mse: 12.2594\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0111 - mse: 9.6969 - val_loss: 0.0140 - val_mse: 12.1746\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0110 - mse: 9.5454 - val_loss: 0.0141 - val_mse: 12.2602\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0109 - mse: 9.5180 - val_loss: 0.0141 - val_mse: 12.2374\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0109 - mse: 9.4527 - val_loss: 0.0139 - val_mse: 12.0642\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0107 - mse: 9.3418 - val_loss: 0.0137 - val_mse: 11.9540\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0107 - mse: 9.3177 - val_loss: 0.0137 - val_mse: 11.8887\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0106 - mse: 9.2244 - val_loss: 0.0137 - val_mse: 11.9505\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0105 - mse: 9.1511 - val_loss: 0.0138 - val_mse: 12.0244\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0105 - mse: 9.1099 - val_loss: 0.0137 - val_mse: 11.9137\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0104 - mse: 9.0168 - val_loss: 0.0135 - val_mse: 11.7538\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0103 - mse: 8.9667 - val_loss: 0.0134 - val_mse: 11.6638\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0102 - mse: 8.9082 - val_loss: 0.0134 - val_mse: 11.6753\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0101 - mse: 8.8264 - val_loss: 0.0135 - val_mse: 11.7253\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0101 - mse: 8.7867 - val_loss: 0.0133 - val_mse: 11.5904\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0100 - mse: 8.7106 - val_loss: 0.0131 - val_mse: 11.3963\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0099 - mse: 8.6541 - val_loss: 0.0130 - val_mse: 11.3213\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0099 - mse: 8.6019 - val_loss: 0.0131 - val_mse: 11.3687\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0098 - mse: 8.5304 - val_loss: 0.0131 - val_mse: 11.4227\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0097 - mse: 8.4878 - val_loss: 0.0130 - val_mse: 11.3128\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0097 - mse: 8.4202 - val_loss: 0.0128 - val_mse: 11.1845\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0096 - mse: 8.3729 - val_loss: 0.0128 - val_mse: 11.1640\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0096 - mse: 8.3176 - val_loss: 0.0129 - val_mse: 11.2137\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0095 - mse: 8.2617 - val_loss: 0.0129 - val_mse: 11.1992\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0094 - mse: 8.2139 - val_loss: 0.0127 - val_mse: 11.0891\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0094 - mse: 8.1567 - val_loss: 0.0126 - val_mse: 11.0023\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0093 - mse: 8.1081 - val_loss: 0.0126 - val_mse: 10.9716\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0093 - mse: 8.0558 - val_loss: 0.0126 - val_mse: 10.9669\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0092 - mse: 8.0040 - val_loss: 0.0126 - val_mse: 10.9489\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0091 - mse: 7.9566 - val_loss: 0.0125 - val_mse: 10.8982\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0091 - mse: 7.9031 - val_loss: 0.0124 - val_mse: 10.8401\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0090 - mse: 7.8579 - val_loss: 0.0124 - val_mse: 10.7943\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0090 - mse: 7.8056 - val_loss: 0.0124 - val_mse: 10.7999\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0089 - mse: 7.7603 - val_loss: 0.0124 - val_mse: 10.7704\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0089 - mse: 7.7114 - val_loss: 0.0123 - val_mse: 10.6755\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0088 - mse: 7.6648 - val_loss: 0.0122 - val_mse: 10.6074\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0087 - mse: 7.6190 - val_loss: 0.0122 - val_mse: 10.6003\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0087 - mse: 7.5714 - val_loss: 0.0122 - val_mse: 10.5863\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0086 - mse: 7.5277 - val_loss: 0.0121 - val_mse: 10.4960\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0086 - mse: 7.4798 - val_loss: 0.0120 - val_mse: 10.4139\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0085 - mse: 7.4368 - val_loss: 0.0119 - val_mse: 10.3898\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0085 - mse: 7.3904 - val_loss: 0.0119 - val_mse: 10.3699\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0084 - mse: 7.3472 - val_loss: 0.0119 - val_mse: 10.3212\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0084 - mse: 7.3023 - val_loss: 0.0118 - val_mse: 10.2708\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0083 - mse: 7.2582 - val_loss: 0.0118 - val_mse: 10.2365\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0083 - mse: 7.2151 - val_loss: 0.0117 - val_mse: 10.2249\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0082 - mse: 7.1710 - val_loss: 0.0117 - val_mse: 10.2061\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0082 - mse: 7.1290 - val_loss: 0.0117 - val_mse: 10.1537\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0081 - mse: 7.0857 - val_loss: 0.0116 - val_mse: 10.1136\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0081 - mse: 7.0446 - val_loss: 0.0116 - val_mse: 10.0846\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0080 - mse: 7.0022 - val_loss: 0.0116 - val_mse: 10.0776\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0080 - mse: 6.9615 - val_loss: 0.0116 - val_mse: 10.0598\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0079 - mse: 6.9201 - val_loss: 0.0115 - val_mse: 10.0221\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0079 - mse: 6.8797 - val_loss: 0.0115 - val_mse: 10.0117\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0079 - mse: 6.8385 - val_loss: 0.0115 - val_mse: 9.9940\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0078 - mse: 6.7985 - val_loss: 0.0115 - val_mse: 9.9778\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0078 - mse: 6.7582 - val_loss: 0.0114 - val_mse: 9.9565\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0077 - mse: 6.7187 - val_loss: 0.0114 - val_mse: 9.9565\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0077 - mse: 6.6790 - val_loss: 0.0114 - val_mse: 9.9602\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0076 - mse: 6.6403 - val_loss: 0.0114 - val_mse: 9.9255\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0076 - mse: 6.6012 - val_loss: 0.0114 - val_mse: 9.9260\n",
      "Epoch 229/1000\n",
      "\n",
      "Epoch 00228: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0075 - mse: 6.5627 - val_loss: 0.0114 - val_mse: 9.8951\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0075 - mse: 6.5236 - val_loss: 0.0114 - val_mse: 9.9062\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0075 - mse: 6.4964 - val_loss: 0.0114 - val_mse: 9.9112\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0074 - mse: 6.4696 - val_loss: 0.0113 - val_mse: 9.8786\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0074 - mse: 6.4426 - val_loss: 0.0113 - val_mse: 9.8636\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0074 - mse: 6.4159 - val_loss: 0.0113 - val_mse: 9.8778\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0073 - mse: 6.3891 - val_loss: 0.0113 - val_mse: 9.8645\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0073 - mse: 6.3621 - val_loss: 0.0113 - val_mse: 9.8459\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0073 - mse: 6.3351 - val_loss: 0.0113 - val_mse: 9.8412\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0072 - mse: 6.3082 - val_loss: 0.0113 - val_mse: 9.8111\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0072 - mse: 6.2811 - val_loss: 0.0113 - val_mse: 9.8002\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0072 - mse: 6.2543 - val_loss: 0.0113 - val_mse: 9.8075\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0072 - mse: 6.2274 - val_loss: 0.0112 - val_mse: 9.7862\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0071 - mse: 6.2003 - val_loss: 0.0112 - val_mse: 9.7754\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0071 - mse: 6.1737 - val_loss: 0.0112 - val_mse: 9.7748\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0071 - mse: 6.1474 - val_loss: 0.0112 - val_mse: 9.7489\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0070 - mse: 6.1207 - val_loss: 0.0112 - val_mse: 9.7458\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0070 - mse: 6.0941 - val_loss: 0.0112 - val_mse: 9.7241\n",
      "Epoch 247/1000\n",
      "\n",
      "Epoch 00246: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0070 - mse: 6.0674 - val_loss: 0.0112 - val_mse: 9.7131\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0069 - mse: 6.0408 - val_loss: 0.0112 - val_mse: 9.7137\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0069 - mse: 6.0221 - val_loss: 0.0111 - val_mse: 9.7091\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0069 - mse: 6.0034 - val_loss: 0.0111 - val_mse: 9.6983\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0069 - mse: 5.9847 - val_loss: 0.0111 - val_mse: 9.6890\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0069 - mse: 5.9660 - val_loss: 0.0111 - val_mse: 9.6763\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0068 - mse: 5.9473 - val_loss: 0.0111 - val_mse: 9.6730\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0068 - mse: 5.9284 - val_loss: 0.0111 - val_mse: 9.6640\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0068 - mse: 5.9094 - val_loss: 0.0111 - val_mse: 9.6549\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0068 - mse: 5.8906 - val_loss: 0.0111 - val_mse: 9.6422\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0067 - mse: 5.8717 - val_loss: 0.0111 - val_mse: 9.6314\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0067 - mse: 5.8528 - val_loss: 0.0110 - val_mse: 9.6128\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0067 - mse: 5.8339 - val_loss: 0.0110 - val_mse: 9.6055\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0067 - mse: 5.8149 - val_loss: 0.0110 - val_mse: 9.6044\n",
      "Epoch 261/1000\n",
      "\n",
      "Epoch 00260: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0067 - mse: 5.7960 - val_loss: 0.0110 - val_mse: 9.6004\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0066 - mse: 5.7771 - val_loss: 0.0110 - val_mse: 9.5843\n",
      "Epoch 263/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0066 - mse: 5.7637 - val_loss: 0.0110 - val_mse: 9.5684\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0066 - mse: 5.7504 - val_loss: 0.0110 - val_mse: 9.5600\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0066 - mse: 5.7370 - val_loss: 0.0110 - val_mse: 9.5505\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0066 - mse: 5.7235 - val_loss: 0.0110 - val_mse: 9.5360\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0066 - mse: 5.7099 - val_loss: 0.0109 - val_mse: 9.5266\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.0065 - mse: 5.6963 - val_loss: 0.0109 - val_mse: 9.5222\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0065 - mse: 5.6827 - val_loss: 0.0109 - val_mse: 9.5160\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0065 - mse: 5.6690 - val_loss: 0.0109 - val_mse: 9.5035\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0065 - mse: 5.6553 - val_loss: 0.0109 - val_mse: 9.4903\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0065 - mse: 5.6416 - val_loss: 0.0109 - val_mse: 9.4797\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0065 - mse: 5.6278 - val_loss: 0.0109 - val_mse: 9.4763\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0064 - mse: 5.6141 - val_loss: 0.0109 - val_mse: 9.4696\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0064 - mse: 5.6003 - val_loss: 0.0109 - val_mse: 9.4567\n",
      "Epoch 276/1000\n",
      "\n",
      "Epoch 00275: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0064 - mse: 5.5866 - val_loss: 0.0108 - val_mse: 9.4481\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0064 - mse: 5.5730 - val_loss: 0.0108 - val_mse: 9.4437\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0064 - mse: 5.5634 - val_loss: 0.0108 - val_mse: 9.4357\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0064 - mse: 5.5538 - val_loss: 0.0108 - val_mse: 9.4262\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0064 - mse: 5.5443 - val_loss: 0.0108 - val_mse: 9.4198\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0064 - mse: 5.5348 - val_loss: 0.0108 - val_mse: 9.4137\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0063 - mse: 5.5254 - val_loss: 0.0108 - val_mse: 9.4072\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0063 - mse: 5.5160 - val_loss: 0.0108 - val_mse: 9.4016\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0063 - mse: 5.5067 - val_loss: 0.0108 - val_mse: 9.3926\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0063 - mse: 5.4974 - val_loss: 0.0108 - val_mse: 9.3831\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0063 - mse: 5.4881 - val_loss: 0.0108 - val_mse: 9.3720\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0063 - mse: 5.4788 - val_loss: 0.0108 - val_mse: 9.3676\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0063 - mse: 5.4695 - val_loss: 0.0108 - val_mse: 9.3631\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0063 - mse: 5.4601 - val_loss: 0.0107 - val_mse: 9.3542\n",
      "Epoch 290/1000\n",
      "\n",
      "Epoch 00289: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0063 - mse: 5.4508 - val_loss: 0.0107 - val_mse: 9.3433\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0062 - mse: 5.4415 - val_loss: 0.0107 - val_mse: 9.3366\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0062 - mse: 5.4349 - val_loss: 0.0107 - val_mse: 9.3315\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0062 - mse: 5.4284 - val_loss: 0.0107 - val_mse: 9.3277\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0062 - mse: 5.4218 - val_loss: 0.0107 - val_mse: 9.3239\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0062 - mse: 5.4152 - val_loss: 0.0107 - val_mse: 9.3162\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0062 - mse: 5.4087 - val_loss: 0.0107 - val_mse: 9.3077\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0062 - mse: 5.4021 - val_loss: 0.0107 - val_mse: 9.3019\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0062 - mse: 5.3956 - val_loss: 0.0107 - val_mse: 9.3009\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0062 - mse: 5.3889 - val_loss: 0.0107 - val_mse: 9.3002\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0062 - mse: 5.3824 - val_loss: 0.0107 - val_mse: 9.2951\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0062 - mse: 5.3758 - val_loss: 0.0107 - val_mse: 9.2878\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0062 - mse: 5.3692 - val_loss: 0.0107 - val_mse: 9.2832\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0062 - mse: 5.3626 - val_loss: 0.0107 - val_mse: 9.2780\n",
      "Epoch 304/1000\n",
      "\n",
      "Epoch 00303: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0062 - mse: 5.3560 - val_loss: 0.0106 - val_mse: 9.2727\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0061 - mse: 5.3495 - val_loss: 0.0106 - val_mse: 9.2685\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0061 - mse: 5.3448 - val_loss: 0.0106 - val_mse: 9.2640\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0061 - mse: 5.3402 - val_loss: 0.0106 - val_mse: 9.2597\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0061 - mse: 5.3355 - val_loss: 0.0106 - val_mse: 9.2564\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0061 - mse: 5.3309 - val_loss: 0.0106 - val_mse: 9.2545\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0061 - mse: 5.3262 - val_loss: 0.0106 - val_mse: 9.2523\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0061 - mse: 5.3216 - val_loss: 0.0106 - val_mse: 9.2474\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0061 - mse: 5.3169 - val_loss: 0.0106 - val_mse: 9.2419\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0061 - mse: 5.3123 - val_loss: 0.0106 - val_mse: 9.2382\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0061 - mse: 5.3076 - val_loss: 0.0106 - val_mse: 9.2350\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0061 - mse: 5.3030 - val_loss: 0.0106 - val_mse: 9.2313\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0061 - mse: 5.2983 - val_loss: 0.0106 - val_mse: 9.2282\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0061 - mse: 5.2937 - val_loss: 0.0106 - val_mse: 9.2261\n",
      "Epoch 318/1000\n",
      "\n",
      "Epoch 00317: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0061 - mse: 5.2890 - val_loss: 0.0106 - val_mse: 9.2221\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0061 - mse: 5.2844 - val_loss: 0.0106 - val_mse: 9.2189\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0061 - mse: 5.2811 - val_loss: 0.0106 - val_mse: 9.2160\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0061 - mse: 5.2779 - val_loss: 0.0106 - val_mse: 9.2135\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0061 - mse: 5.2746 - val_loss: 0.0106 - val_mse: 9.2117\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0061 - mse: 5.2714 - val_loss: 0.0106 - val_mse: 9.2095\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0060 - mse: 5.2681 - val_loss: 0.0106 - val_mse: 9.2065\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0060 - mse: 5.2649 - val_loss: 0.0106 - val_mse: 9.2034\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0060 - mse: 5.2616 - val_loss: 0.0106 - val_mse: 9.2003\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0060 - mse: 5.2584 - val_loss: 0.0106 - val_mse: 9.1973\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0060 - mse: 5.2552 - val_loss: 0.0106 - val_mse: 9.1944\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0060 - mse: 5.2519 - val_loss: 0.0106 - val_mse: 9.1914\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0060 - mse: 5.2487 - val_loss: 0.0106 - val_mse: 9.1875\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0060 - mse: 5.2455 - val_loss: 0.0105 - val_mse: 9.1847\n",
      "Epoch 332/1000\n",
      "\n",
      "Epoch 00331: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0060 - mse: 5.2422 - val_loss: 0.0105 - val_mse: 9.1832\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0060 - mse: 5.2390 - val_loss: 0.0105 - val_mse: 9.1817\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0060 - mse: 5.2367 - val_loss: 0.0105 - val_mse: 9.1794\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0060 - mse: 5.2345 - val_loss: 0.0105 - val_mse: 9.1775\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0060 - mse: 5.2322 - val_loss: 0.0105 - val_mse: 9.1760\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0060 - mse: 5.2299 - val_loss: 0.0105 - val_mse: 9.1742\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.0060 - mse: 5.2277 - val_loss: 0.0105 - val_mse: 9.1732\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0060 - mse: 5.2254 - val_loss: 0.0105 - val_mse: 9.1724\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0060 - mse: 5.2232 - val_loss: 0.0105 - val_mse: 9.1715\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0060 - mse: 5.2210 - val_loss: 0.0105 - val_mse: 9.1699\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0060 - mse: 5.2187 - val_loss: 0.0105 - val_mse: 9.1678\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.0060 - mse: 5.2165 - val_loss: 0.0105 - val_mse: 9.1656\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0060 - mse: 5.2142 - val_loss: 0.0105 - val_mse: 9.1635\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.0060 - mse: 5.2120 - val_loss: 0.0105 - val_mse: 9.1619\n",
      "Epoch 346/1000\n",
      "\n",
      "Epoch 00345: reducing learning rate to 0.000282475253334.\n",
      "0s - loss: 0.0060 - mse: 5.2098 - val_loss: 0.0105 - val_mse: 9.1602\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.0060 - mse: 5.2075 - val_loss: 0.0105 - val_mse: 9.1589\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0060 - mse: 5.2060 - val_loss: 0.0105 - val_mse: 9.1576\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.0060 - mse: 5.2044 - val_loss: 0.0105 - val_mse: 9.1560\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0060 - mse: 5.2028 - val_loss: 0.0105 - val_mse: 9.1544\n",
      "[{'loss': 0.0077854436822235584, 'dropout': 0.0, 'val_mse': 10.15809154510498, 'width': 75, 'mse': 6.7798690795898438, 'val_loss': 0.011664718389511108}, {'loss': 0.0059745172038674355, 'dropout': 0.0, 'val_mse': 9.1544342041015625, 'width': 90, 'mse': 5.202843189239502, 'val_loss': 0.010512201115489006}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_72 (LSTM)               (None, None, 105)         45360     \n",
      "_________________________________________________________________\n",
      "time_distributed_69 (TimeDis (None, None, 1)           106       \n",
      "=================================================================\n",
      "Total params: 45,466\n",
      "Trainable params: 45,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 1.0763 - mse: 937.2563 - val_loss: 0.5898 - val_mse: 513.5988\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.5400 - mse: 470.2236 - val_loss: 0.3584 - val_mse: 312.1037\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.3322 - mse: 289.3189 - val_loss: 0.2848 - val_mse: 247.9979\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.2847 - mse: 247.9124 - val_loss: 0.2599 - val_mse: 226.3305\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2603 - mse: 226.6826 - val_loss: 0.2129 - val_mse: 185.4069\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2115 - mse: 184.1610 - val_loss: 0.1784 - val_mse: 155.3302\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.1682 - mse: 146.4456 - val_loss: 0.1748 - val_mse: 152.1924\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1618 - mse: 140.9052 - val_loss: 0.1450 - val_mse: 126.3113\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1317 - mse: 114.7063 - val_loss: 0.1286 - val_mse: 111.9825\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1121 - mse: 97.6401 - val_loss: 0.1370 - val_mse: 119.3045\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1104 - mse: 96.1683 - val_loss: 0.1550 - val_mse: 134.9478\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1196 - mse: 104.1645 - val_loss: 0.1426 - val_mse: 124.1861\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1176 - mse: 102.4357 - val_loss: 0.1257 - val_mse: 109.4810\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1089 - mse: 94.8532 - val_loss: 0.1120 - val_mse: 97.5453\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.0978 - mse: 85.1422 - val_loss: 0.1063 - val_mse: 92.6003\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.0919 - mse: 80.0491 - val_loss: 0.1060 - val_mse: 92.2937\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0917 - mse: 79.8525 - val_loss: 0.0951 - val_mse: 82.7953\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0840 - mse: 73.1102 - val_loss: 0.0849 - val_mse: 73.9232\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0771 - mse: 67.1116 - val_loss: 0.0823 - val_mse: 71.6716\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0759 - mse: 66.1300 - val_loss: 0.0822 - val_mse: 71.5477\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0762 - mse: 66.3255 - val_loss: 0.0813 - val_mse: 70.8379\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0754 - mse: 65.6764 - val_loss: 0.0818 - val_mse: 71.2199\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0764 - mse: 66.5196 - val_loss: 0.0768 - val_mse: 66.9095\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0715 - mse: 62.2512 - val_loss: 0.0728 - val_mse: 63.4352\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0681 - mse: 59.3152 - val_loss: 0.0715 - val_mse: 62.3017\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0673 - mse: 58.5811 - val_loss: 0.0691 - val_mse: 60.2175\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0648 - mse: 56.4153 - val_loss: 0.0670 - val_mse: 58.3789\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0621 - mse: 54.0858 - val_loss: 0.0662 - val_mse: 57.6420\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0604 - mse: 52.6062 - val_loss: 0.0651 - val_mse: 56.6581\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0589 - mse: 51.3090 - val_loss: 0.0627 - val_mse: 54.6044\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0570 - mse: 49.6528 - val_loss: 0.0601 - val_mse: 52.3342\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0551 - mse: 47.9526 - val_loss: 0.0585 - val_mse: 50.9789\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0537 - mse: 46.7855 - val_loss: 0.0568 - val_mse: 49.4922\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0517 - mse: 45.0005 - val_loss: 0.0556 - val_mse: 48.3957\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0499 - mse: 43.4128 - val_loss: 0.0546 - val_mse: 47.5865\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0485 - mse: 42.2197 - val_loss: 0.0535 - val_mse: 46.6179\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0476 - mse: 41.4858 - val_loss: 0.0523 - val_mse: 45.5029\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0469 - mse: 40.8847 - val_loss: 0.0508 - val_mse: 44.2178\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0456 - mse: 39.6736 - val_loss: 0.0498 - val_mse: 43.3851\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0445 - mse: 38.7834 - val_loss: 0.0484 - val_mse: 42.1155\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0429 - mse: 37.3867 - val_loss: 0.0482 - val_mse: 41.9982\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0423 - mse: 36.8720 - val_loss: 0.0478 - val_mse: 41.6489\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0413 - mse: 35.9760 - val_loss: 0.0466 - val_mse: 40.5889\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0404 - mse: 35.1687 - val_loss: 0.0446 - val_mse: 38.8282\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0395 - mse: 34.4169 - val_loss: 0.0432 - val_mse: 37.5980\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0388 - mse: 33.7774 - val_loss: 0.0424 - val_mse: 36.8952\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0380 - mse: 33.1273 - val_loss: 0.0413 - val_mse: 35.9241\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0369 - mse: 32.1682 - val_loss: 0.0405 - val_mse: 35.3064\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0363 - mse: 31.6195 - val_loss: 0.0397 - val_mse: 34.5941\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0354 - mse: 30.8538 - val_loss: 0.0387 - val_mse: 33.7209\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0348 - mse: 30.2708 - val_loss: 0.0373 - val_mse: 32.5077\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0340 - mse: 29.6463 - val_loss: 0.0366 - val_mse: 31.8619\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0335 - mse: 29.1937 - val_loss: 0.0363 - val_mse: 31.6086\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0328 - mse: 28.5991 - val_loss: 0.0362 - val_mse: 31.5230\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0325 - mse: 28.3048 - val_loss: 0.0355 - val_mse: 30.9443\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0321 - mse: 27.9474 - val_loss: 0.0346 - val_mse: 30.1594\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0315 - mse: 27.4434 - val_loss: 0.0340 - val_mse: 29.6033\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0311 - mse: 27.0832 - val_loss: 0.0332 - val_mse: 28.9120\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0306 - mse: 26.6372 - val_loss: 0.0326 - val_mse: 28.3908\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0301 - mse: 26.2305 - val_loss: 0.0323 - val_mse: 28.0933\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0297 - mse: 25.8243 - val_loss: 0.0318 - val_mse: 27.6685\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0292 - mse: 25.4583 - val_loss: 0.0310 - val_mse: 27.0092\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0288 - mse: 25.0786 - val_loss: 0.0305 - val_mse: 26.5519\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0284 - mse: 24.7372 - val_loss: 0.0301 - val_mse: 26.2336\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0280 - mse: 24.4111 - val_loss: 0.0297 - val_mse: 25.8375\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0277 - mse: 24.0841 - val_loss: 0.0293 - val_mse: 25.5361\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0274 - mse: 23.8220 - val_loss: 0.0290 - val_mse: 25.2754\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0270 - mse: 23.5250 - val_loss: 0.0286 - val_mse: 24.8910\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0267 - mse: 23.2199 - val_loss: 0.0282 - val_mse: 24.5539\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0263 - mse: 22.9352 - val_loss: 0.0280 - val_mse: 24.3765\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0260 - mse: 22.6623 - val_loss: 0.0277 - val_mse: 24.1165\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0257 - mse: 22.3925 - val_loss: 0.0273 - val_mse: 23.8128\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0254 - mse: 22.1505 - val_loss: 0.0271 - val_mse: 23.6069\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0251 - mse: 21.8980 - val_loss: 0.0268 - val_mse: 23.2994\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0248 - mse: 21.6165 - val_loss: 0.0264 - val_mse: 23.0203\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0245 - mse: 21.3790 - val_loss: 0.0262 - val_mse: 22.8269\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0243 - mse: 21.1181 - val_loss: 0.0259 - val_mse: 22.5211\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0240 - mse: 20.8665 - val_loss: 0.0255 - val_mse: 22.2399\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0237 - mse: 20.6298 - val_loss: 0.0253 - val_mse: 22.0091\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0234 - mse: 20.3832 - val_loss: 0.0249 - val_mse: 21.6929\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0231 - mse: 20.1471 - val_loss: 0.0247 - val_mse: 21.4839\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0228 - mse: 19.8893 - val_loss: 0.0244 - val_mse: 21.2792\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0226 - mse: 19.6629 - val_loss: 0.0242 - val_mse: 21.0527\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0223 - mse: 19.4328 - val_loss: 0.0240 - val_mse: 20.9017\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0221 - mse: 19.2144 - val_loss: 0.0237 - val_mse: 20.6390\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0218 - mse: 18.9975 - val_loss: 0.0236 - val_mse: 20.5628\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0216 - mse: 18.7930 - val_loss: 0.0233 - val_mse: 20.2727\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0214 - mse: 18.6247 - val_loss: 0.0234 - val_mse: 20.3811\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0213 - mse: 18.5528 - val_loss: 0.0231 - val_mse: 20.1458\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0215 - mse: 18.7201 - val_loss: 0.0248 - val_mse: 21.5679\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0226 - mse: 19.6772 - val_loss: 0.0243 - val_mse: 21.1486\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0232 - mse: 20.1657 - val_loss: 0.0254 - val_mse: 22.0826\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0233 - mse: 20.2894 - val_loss: 0.0218 - val_mse: 18.9566\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0202 - mse: 17.6036 - val_loss: 0.0224 - val_mse: 19.5275\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0211 - mse: 18.4127 - val_loss: 0.0243 - val_mse: 21.1924\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0222 - mse: 19.3432 - val_loss: 0.0212 - val_mse: 18.4853\n",
      "Epoch 97/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0195 - mse: 16.9398 - val_loss: 0.0225 - val_mse: 19.6222\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0213 - mse: 18.5440 - val_loss: 0.0232 - val_mse: 20.1914\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0209 - mse: 18.1798 - val_loss: 0.0213 - val_mse: 18.5801\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0192 - mse: 16.7017 - val_loss: 0.0224 - val_mse: 19.4827\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0211 - mse: 18.4087 - val_loss: 0.0210 - val_mse: 18.2697\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0189 - mse: 16.4234 - val_loss: 0.0217 - val_mse: 18.9198\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0195 - mse: 16.9922 - val_loss: 0.0205 - val_mse: 17.8174\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0189 - mse: 16.4656 - val_loss: 0.0200 - val_mse: 17.4000\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0183 - mse: 15.9623 - val_loss: 0.0209 - val_mse: 18.2262\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0188 - mse: 16.3496 - val_loss: 0.0197 - val_mse: 17.1867\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0177 - mse: 15.4152 - val_loss: 0.0201 - val_mse: 17.4677\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0185 - mse: 16.1068 - val_loss: 0.0194 - val_mse: 16.8565\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0173 - mse: 15.0913 - val_loss: 0.0200 - val_mse: 17.4198\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0178 - mse: 15.4802 - val_loss: 0.0190 - val_mse: 16.5518\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0171 - mse: 14.9185 - val_loss: 0.0188 - val_mse: 16.4131\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0171 - mse: 14.8723 - val_loss: 0.0191 - val_mse: 16.6761\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0170 - mse: 14.8321 - val_loss: 0.0186 - val_mse: 16.1902\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0165 - mse: 14.3712 - val_loss: 0.0186 - val_mse: 16.1550\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0168 - mse: 14.6270 - val_loss: 0.0181 - val_mse: 15.7674\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0161 - mse: 14.0436 - val_loss: 0.0184 - val_mse: 16.0460\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0163 - mse: 14.1516 - val_loss: 0.0180 - val_mse: 15.6482\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0159 - mse: 13.8741 - val_loss: 0.0177 - val_mse: 15.3817\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0157 - mse: 13.6378 - val_loss: 0.0179 - val_mse: 15.6069\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0157 - mse: 13.6859 - val_loss: 0.0175 - val_mse: 15.2581\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0153 - mse: 13.2965 - val_loss: 0.0174 - val_mse: 15.1226\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0153 - mse: 13.3006 - val_loss: 0.0173 - val_mse: 15.1015\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0151 - mse: 13.1849 - val_loss: 0.0170 - val_mse: 14.8365\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0148 - mse: 12.8462 - val_loss: 0.0170 - val_mse: 14.7635\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0148 - mse: 12.8749 - val_loss: 0.0168 - val_mse: 14.6717\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0146 - mse: 12.7539 - val_loss: 0.0165 - val_mse: 14.4063\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0142 - mse: 12.4028 - val_loss: 0.0165 - val_mse: 14.3800\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0142 - mse: 12.3674 - val_loss: 0.0165 - val_mse: 14.4076\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0142 - mse: 12.3867 - val_loss: 0.0162 - val_mse: 14.1284\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0139 - mse: 12.1019 - val_loss: 0.0161 - val_mse: 13.9967\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0136 - mse: 11.8295 - val_loss: 0.0158 - val_mse: 13.7775\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0134 - mse: 11.6744 - val_loss: 0.0158 - val_mse: 13.7160\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0134 - mse: 11.6479 - val_loss: 0.0161 - val_mse: 14.0037\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0135 - mse: 11.7899 - val_loss: 0.0162 - val_mse: 14.0787\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0139 - mse: 12.0983 - val_loss: 0.0178 - val_mse: 15.5261\n",
      "Epoch 136/1000\n",
      "\n",
      "Epoch 00135: reducing learning rate to 0.00699999984354.\n",
      "5s - loss: 0.0155 - mse: 13.4768 - val_loss: 0.0179 - val_mse: 15.5935\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0159 - mse: 13.8824 - val_loss: 0.0158 - val_mse: 13.7995\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0134 - mse: 11.6614 - val_loss: 0.0164 - val_mse: 14.2919\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0141 - mse: 12.2646 - val_loss: 0.0155 - val_mse: 13.4937\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0134 - mse: 11.6930 - val_loss: 0.0152 - val_mse: 13.2411\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0130 - mse: 11.2781 - val_loss: 0.0160 - val_mse: 13.9207\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0134 - mse: 11.6905 - val_loss: 0.0151 - val_mse: 13.1277\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0125 - mse: 10.9174 - val_loss: 0.0154 - val_mse: 13.4209\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0132 - mse: 11.4908 - val_loss: 0.0146 - val_mse: 12.7234\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0121 - mse: 10.5132 - val_loss: 0.0155 - val_mse: 13.4850\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0128 - mse: 11.1528 - val_loss: 0.0144 - val_mse: 12.5628\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0119 - mse: 10.3460 - val_loss: 0.0148 - val_mse: 12.8680\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0125 - mse: 10.9022 - val_loss: 0.0142 - val_mse: 12.3300\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0117 - mse: 10.1465 - val_loss: 0.0147 - val_mse: 12.8411\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0122 - mse: 10.5849 - val_loss: 0.0140 - val_mse: 12.1988\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0115 - mse: 9.9968 - val_loss: 0.0143 - val_mse: 12.4390\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0119 - mse: 10.3536 - val_loss: 0.0139 - val_mse: 12.1370\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0113 - mse: 9.8655 - val_loss: 0.0142 - val_mse: 12.3403\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0115 - mse: 10.0534 - val_loss: 0.0137 - val_mse: 11.9273\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0112 - mse: 9.7474 - val_loss: 0.0137 - val_mse: 11.9370\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0112 - mse: 9.7792 - val_loss: 0.0137 - val_mse: 11.9514\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0111 - mse: 9.6574 - val_loss: 0.0135 - val_mse: 11.7225\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0109 - mse: 9.4968 - val_loss: 0.0134 - val_mse: 11.6432\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0110 - mse: 9.5514 - val_loss: 0.0132 - val_mse: 11.4731\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0106 - mse: 9.2638 - val_loss: 0.0133 - val_mse: 11.6179\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0108 - mse: 9.3721 - val_loss: 0.0129 - val_mse: 11.2618\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0105 - mse: 9.1384 - val_loss: 0.0129 - val_mse: 11.2524\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0105 - mse: 9.1073 - val_loss: 0.0131 - val_mse: 11.3800\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0104 - mse: 9.0848 - val_loss: 0.0127 - val_mse: 11.0729\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0102 - mse: 8.8606 - val_loss: 0.0127 - val_mse: 11.0453\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0103 - mse: 8.9311 - val_loss: 0.0127 - val_mse: 11.0692\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0101 - mse: 8.8017 - val_loss: 0.0125 - val_mse: 10.8932\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0099 - mse: 8.6431 - val_loss: 0.0124 - val_mse: 10.8350\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0100 - mse: 8.6858 - val_loss: 0.0125 - val_mse: 10.8557\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0099 - mse: 8.5962 - val_loss: 0.0122 - val_mse: 10.6528\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0097 - mse: 8.4217 - val_loss: 0.0122 - val_mse: 10.5855\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0096 - mse: 8.4016 - val_loss: 0.0122 - val_mse: 10.6607\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0097 - mse: 8.4131 - val_loss: 0.0120 - val_mse: 10.4313\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0095 - mse: 8.2791 - val_loss: 0.0119 - val_mse: 10.3640\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0093 - mse: 8.1347 - val_loss: 0.0118 - val_mse: 10.2890\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0093 - mse: 8.0690 - val_loss: 0.0117 - val_mse: 10.2031\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0093 - mse: 8.0628 - val_loss: 0.0118 - val_mse: 10.2976\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0093 - mse: 8.0583 - val_loss: 0.0117 - val_mse: 10.1758\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0092 - mse: 8.0133 - val_loss: 0.0117 - val_mse: 10.2001\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0091 - mse: 7.9555 - val_loss: 0.0115 - val_mse: 10.0176\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0091 - mse: 7.8901 - val_loss: 0.0116 - val_mse: 10.1199\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0090 - mse: 7.8792 - val_loss: 0.0115 - val_mse: 10.0081\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0091 - mse: 7.8928 - val_loss: 0.0118 - val_mse: 10.2419\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0092 - mse: 8.0485 - val_loss: 0.0118 - val_mse: 10.2813\n",
      "Epoch 185/1000\n",
      "\n",
      "Epoch 00184: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0095 - mse: 8.2599 - val_loss: 0.0126 - val_mse: 10.9321\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0101 - mse: 8.7741 - val_loss: 0.0111 - val_mse: 9.6627\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0087 - mse: 7.5368 - val_loss: 0.0115 - val_mse: 9.9849\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0092 - mse: 7.9837 - val_loss: 0.0117 - val_mse: 10.2176\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0093 - mse: 8.0880 - val_loss: 0.0109 - val_mse: 9.5115\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0084 - mse: 7.3398 - val_loss: 0.0116 - val_mse: 10.1250\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0094 - mse: 8.1531 - val_loss: 0.0108 - val_mse: 9.4145\n",
      "Epoch 192/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0083 - mse: 7.2584 - val_loss: 0.0113 - val_mse: 9.8138\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0088 - mse: 7.6770 - val_loss: 0.0108 - val_mse: 9.4438\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0085 - mse: 7.3651 - val_loss: 0.0108 - val_mse: 9.3668\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0083 - mse: 7.2625 - val_loss: 0.0110 - val_mse: 9.6026\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0085 - mse: 7.4384 - val_loss: 0.0105 - val_mse: 9.1782\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0081 - mse: 7.0276 - val_loss: 0.0108 - val_mse: 9.4225\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0085 - mse: 7.3899 - val_loss: 0.0104 - val_mse: 9.0783\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0080 - mse: 6.9355 - val_loss: 0.0107 - val_mse: 9.3313\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0082 - mse: 7.1662 - val_loss: 0.0104 - val_mse: 9.0756\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0080 - mse: 6.9409 - val_loss: 0.0104 - val_mse: 9.0470\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0080 - mse: 6.9265 - val_loss: 0.0105 - val_mse: 9.1227\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0080 - mse: 6.9636 - val_loss: 0.0102 - val_mse: 8.9232\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0078 - mse: 6.7491 - val_loss: 0.0104 - val_mse: 9.0368\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0079 - mse: 6.9108 - val_loss: 0.0102 - val_mse: 8.8558\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0077 - mse: 6.6733 - val_loss: 0.0102 - val_mse: 8.9224\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0077 - mse: 6.7450 - val_loss: 0.0101 - val_mse: 8.8222\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0077 - mse: 6.6728 - val_loss: 0.0100 - val_mse: 8.7499\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0075 - mse: 6.5716 - val_loss: 0.0101 - val_mse: 8.8364\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0076 - mse: 6.6391 - val_loss: 0.0100 - val_mse: 8.6693\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0074 - mse: 6.4859 - val_loss: 0.0100 - val_mse: 8.6879\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0075 - mse: 6.5235 - val_loss: 0.0100 - val_mse: 8.6937\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0074 - mse: 6.4833 - val_loss: 0.0099 - val_mse: 8.6029\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0073 - mse: 6.3833 - val_loss: 0.0099 - val_mse: 8.6152\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0074 - mse: 6.4326 - val_loss: 0.0098 - val_mse: 8.5556\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0073 - mse: 6.3427 - val_loss: 0.0098 - val_mse: 8.5183\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0072 - mse: 6.2916 - val_loss: 0.0098 - val_mse: 8.5203\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0072 - mse: 6.3134 - val_loss: 0.0097 - val_mse: 8.4568\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0071 - mse: 6.2245 - val_loss: 0.0097 - val_mse: 8.4304\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0071 - mse: 6.1889 - val_loss: 0.0097 - val_mse: 8.4252\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0071 - mse: 6.1955 - val_loss: 0.0096 - val_mse: 8.3694\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0070 - mse: 6.1199 - val_loss: 0.0096 - val_mse: 8.3293\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0070 - mse: 6.0786 - val_loss: 0.0096 - val_mse: 8.3173\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0070 - mse: 6.0803 - val_loss: 0.0095 - val_mse: 8.2739\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0069 - mse: 6.0232 - val_loss: 0.0094 - val_mse: 8.2273\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0069 - mse: 5.9683 - val_loss: 0.0094 - val_mse: 8.2061\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0068 - mse: 5.9630 - val_loss: 0.0094 - val_mse: 8.1743\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0068 - mse: 5.9324 - val_loss: 0.0093 - val_mse: 8.1149\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0067 - mse: 5.8693 - val_loss: 0.0093 - val_mse: 8.0884\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0067 - mse: 5.8394 - val_loss: 0.0093 - val_mse: 8.0700\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0067 - mse: 5.8262 - val_loss: 0.0092 - val_mse: 8.0127\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0066 - mse: 5.7820 - val_loss: 0.0091 - val_mse: 7.9627\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0066 - mse: 5.7308 - val_loss: 0.0091 - val_mse: 7.9413\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0065 - mse: 5.7020 - val_loss: 0.0091 - val_mse: 7.9159\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0065 - mse: 5.6790 - val_loss: 0.0090 - val_mse: 7.8799\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0065 - mse: 5.6430 - val_loss: 0.0090 - val_mse: 7.8398\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0064 - mse: 5.5967 - val_loss: 0.0090 - val_mse: 7.8111\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0064 - mse: 5.5591 - val_loss: 0.0089 - val_mse: 7.7847\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0064 - mse: 5.5324 - val_loss: 0.0089 - val_mse: 7.7599\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0063 - mse: 5.5059 - val_loss: 0.0089 - val_mse: 7.7377\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0063 - mse: 5.4725 - val_loss: 0.0088 - val_mse: 7.7039\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0062 - mse: 5.4318 - val_loss: 0.0088 - val_mse: 7.6675\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0062 - mse: 5.3914 - val_loss: 0.0088 - val_mse: 7.6421\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0061 - mse: 5.3540 - val_loss: 0.0087 - val_mse: 7.6131\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0061 - mse: 5.3203 - val_loss: 0.0087 - val_mse: 7.5871\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0061 - mse: 5.2893 - val_loss: 0.0087 - val_mse: 7.5749\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0060 - mse: 5.2608 - val_loss: 0.0087 - val_mse: 7.5605\n",
      "Epoch 248/1000\n",
      "\n",
      "Epoch 00247: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0060 - mse: 5.2378 - val_loss: 0.0087 - val_mse: 7.5582\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0060 - mse: 5.2218 - val_loss: 0.0086 - val_mse: 7.5051\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0059 - mse: 5.1792 - val_loss: 0.0086 - val_mse: 7.4814\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0059 - mse: 5.1414 - val_loss: 0.0086 - val_mse: 7.5005\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0059 - mse: 5.1429 - val_loss: 0.0086 - val_mse: 7.4581\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0059 - mse: 5.1039 - val_loss: 0.0085 - val_mse: 7.4317\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0058 - mse: 5.0716 - val_loss: 0.0085 - val_mse: 7.4350\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0058 - mse: 5.0678 - val_loss: 0.0085 - val_mse: 7.3986\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0058 - mse: 5.0327 - val_loss: 0.0085 - val_mse: 7.3844\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0057 - mse: 5.0016 - val_loss: 0.0085 - val_mse: 7.3847\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0057 - mse: 4.9934 - val_loss: 0.0084 - val_mse: 7.3468\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0057 - mse: 4.9655 - val_loss: 0.0084 - val_mse: 7.3275\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0057 - mse: 4.9324 - val_loss: 0.0084 - val_mse: 7.3311\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0057 - mse: 4.9203 - val_loss: 0.0084 - val_mse: 7.3074\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0056 - mse: 4.9000 - val_loss: 0.0084 - val_mse: 7.2865\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0056 - mse: 4.8666 - val_loss: 0.0084 - val_mse: 7.2759\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0056 - mse: 4.8476 - val_loss: 0.0083 - val_mse: 7.2590\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0055 - mse: 4.8324 - val_loss: 0.0083 - val_mse: 7.2471\n",
      "Epoch 266/1000\n",
      "\n",
      "Epoch 00265: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0055 - mse: 4.8031 - val_loss: 0.0083 - val_mse: 7.2266\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0055 - mse: 4.7771 - val_loss: 0.0083 - val_mse: 7.2173\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.0055 - mse: 4.7640 - val_loss: 0.0083 - val_mse: 7.2093\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0054 - mse: 4.7459 - val_loss: 0.0083 - val_mse: 7.2050\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0054 - mse: 4.7326 - val_loss: 0.0083 - val_mse: 7.1911\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0054 - mse: 4.7154 - val_loss: 0.0083 - val_mse: 7.1845\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0054 - mse: 4.7012 - val_loss: 0.0082 - val_mse: 7.1800\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0054 - mse: 4.6852 - val_loss: 0.0082 - val_mse: 7.1691\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0054 - mse: 4.6697 - val_loss: 0.0082 - val_mse: 7.1596\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0053 - mse: 4.6553 - val_loss: 0.0082 - val_mse: 7.1550\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0053 - mse: 4.6389 - val_loss: 0.0082 - val_mse: 7.1516\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0053 - mse: 4.6248 - val_loss: 0.0082 - val_mse: 7.1386\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0053 - mse: 4.6086 - val_loss: 0.0082 - val_mse: 7.1294\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0053 - mse: 4.5936 - val_loss: 0.0082 - val_mse: 7.1240\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0053 - mse: 4.5788 - val_loss: 0.0082 - val_mse: 7.1134\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0052 - mse: 4.5628 - val_loss: 0.0082 - val_mse: 7.1006\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0052 - mse: 4.5482 - val_loss: 0.0081 - val_mse: 7.0916\n",
      "Epoch 283/1000\n",
      "\n",
      "Epoch 00282: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0052 - mse: 4.5328 - val_loss: 0.0081 - val_mse: 7.0815\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0052 - mse: 4.5176 - val_loss: 0.0081 - val_mse: 7.0694\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0052 - mse: 4.5071 - val_loss: 0.0081 - val_mse: 7.0585\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0052 - mse: 4.4965 - val_loss: 0.0081 - val_mse: 7.0531\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0052 - mse: 4.4860 - val_loss: 0.0081 - val_mse: 7.0494\n",
      "Epoch 288/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0051 - mse: 4.4755 - val_loss: 0.0081 - val_mse: 7.0438\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0051 - mse: 4.4650 - val_loss: 0.0081 - val_mse: 7.0351\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0051 - mse: 4.4545 - val_loss: 0.0081 - val_mse: 7.0248\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0051 - mse: 4.4442 - val_loss: 0.0081 - val_mse: 7.0188\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0051 - mse: 4.4337 - val_loss: 0.0081 - val_mse: 7.0142\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0051 - mse: 4.4234 - val_loss: 0.0080 - val_mse: 7.0065\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0051 - mse: 4.4129 - val_loss: 0.0080 - val_mse: 6.9961\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0051 - mse: 4.4026 - val_loss: 0.0080 - val_mse: 6.9888\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0050 - mse: 4.3923 - val_loss: 0.0080 - val_mse: 6.9820\n",
      "Epoch 297/1000\n",
      "\n",
      "Epoch 00296: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0050 - mse: 4.3818 - val_loss: 0.0080 - val_mse: 6.9744\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0050 - mse: 4.3715 - val_loss: 0.0080 - val_mse: 6.9686\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0050 - mse: 4.3641 - val_loss: 0.0080 - val_mse: 6.9627\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0050 - mse: 4.3569 - val_loss: 0.0080 - val_mse: 6.9573\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0050 - mse: 4.3497 - val_loss: 0.0080 - val_mse: 6.9533\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0050 - mse: 4.3424 - val_loss: 0.0080 - val_mse: 6.9496\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0050 - mse: 4.3352 - val_loss: 0.0080 - val_mse: 6.9463\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0050 - mse: 4.3280 - val_loss: 0.0080 - val_mse: 6.9395\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0050 - mse: 4.3208 - val_loss: 0.0080 - val_mse: 6.9321\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0050 - mse: 4.3136 - val_loss: 0.0080 - val_mse: 6.9271\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0049 - mse: 4.3064 - val_loss: 0.0080 - val_mse: 6.9249\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0049 - mse: 4.2992 - val_loss: 0.0079 - val_mse: 6.9204\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0049 - mse: 4.2921 - val_loss: 0.0079 - val_mse: 6.9137\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0049 - mse: 4.2848 - val_loss: 0.0079 - val_mse: 6.9076\n",
      "Epoch 311/1000\n",
      "\n",
      "Epoch 00310: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0049 - mse: 4.2778 - val_loss: 0.0079 - val_mse: 6.9038\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0049 - mse: 4.2706 - val_loss: 0.0079 - val_mse: 6.9018\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0049 - mse: 4.2657 - val_loss: 0.0079 - val_mse: 6.8976\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0049 - mse: 4.2607 - val_loss: 0.0079 - val_mse: 6.8928\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0049 - mse: 4.2557 - val_loss: 0.0079 - val_mse: 6.8894\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0049 - mse: 4.2507 - val_loss: 0.0079 - val_mse: 6.8868\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0049 - mse: 4.2457 - val_loss: 0.0079 - val_mse: 6.8839\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0049 - mse: 4.2407 - val_loss: 0.0079 - val_mse: 6.8811\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0049 - mse: 4.2357 - val_loss: 0.0079 - val_mse: 6.8778\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0049 - mse: 4.2307 - val_loss: 0.0079 - val_mse: 6.8733\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0049 - mse: 4.2258 - val_loss: 0.0079 - val_mse: 6.8685\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0048 - mse: 4.2208 - val_loss: 0.0079 - val_mse: 6.8645\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0048 - mse: 4.2158 - val_loss: 0.0079 - val_mse: 6.8609\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0048 - mse: 4.2109 - val_loss: 0.0079 - val_mse: 6.8585\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0048 - mse: 4.2059 - val_loss: 0.0079 - val_mse: 6.8546\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0048 - mse: 4.2009 - val_loss: 0.0079 - val_mse: 6.8491\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0048 - mse: 4.1960 - val_loss: 0.0079 - val_mse: 6.8453\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0048 - mse: 4.1910 - val_loss: 0.0079 - val_mse: 6.8433\n",
      "Epoch 329/1000\n",
      "\n",
      "Epoch 00328: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0048 - mse: 4.1860 - val_loss: 0.0079 - val_mse: 6.8410\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0048 - mse: 4.1811 - val_loss: 0.0079 - val_mse: 6.8381\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0048 - mse: 4.1776 - val_loss: 0.0078 - val_mse: 6.8352\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0048 - mse: 4.1741 - val_loss: 0.0078 - val_mse: 6.8327\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0048 - mse: 4.1706 - val_loss: 0.0078 - val_mse: 6.8306\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0048 - mse: 4.1671 - val_loss: 0.0078 - val_mse: 6.8288\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0048 - mse: 4.1636 - val_loss: 0.0078 - val_mse: 6.8266\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0048 - mse: 4.1601 - val_loss: 0.0078 - val_mse: 6.8243\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0048 - mse: 4.1566 - val_loss: 0.0078 - val_mse: 6.8219\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.0048 - mse: 4.1532 - val_loss: 0.0078 - val_mse: 6.8194\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0048 - mse: 4.1497 - val_loss: 0.0078 - val_mse: 6.8174\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0048 - mse: 4.1462 - val_loss: 0.0078 - val_mse: 6.8156\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0048 - mse: 4.1427 - val_loss: 0.0078 - val_mse: 6.8138\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0048 - mse: 4.1393 - val_loss: 0.0078 - val_mse: 6.8119\n",
      "Epoch 343/1000\n",
      "\n",
      "Epoch 00342: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0047 - mse: 4.1358 - val_loss: 0.0078 - val_mse: 6.8093\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0047 - mse: 4.1324 - val_loss: 0.0078 - val_mse: 6.8076\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.0047 - mse: 4.1299 - val_loss: 0.0078 - val_mse: 6.8062\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.0047 - mse: 4.1275 - val_loss: 0.0078 - val_mse: 6.8052\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.0047 - mse: 4.1252 - val_loss: 0.0078 - val_mse: 6.8036\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0047 - mse: 4.1228 - val_loss: 0.0078 - val_mse: 6.8016\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.0047 - mse: 4.1204 - val_loss: 0.0078 - val_mse: 6.7999\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0047 - mse: 4.1180 - val_loss: 0.0078 - val_mse: 6.7986\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.0047 - mse: 4.1156 - val_loss: 0.0078 - val_mse: 6.7970\n",
      "Epoch 352/1000\n",
      "0s - loss: 0.0047 - mse: 4.1132 - val_loss: 0.0078 - val_mse: 6.7950\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.0047 - mse: 4.1108 - val_loss: 0.0078 - val_mse: 6.7930\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.0047 - mse: 4.1084 - val_loss: 0.0078 - val_mse: 6.7912\n",
      "Epoch 355/1000\n",
      "0s - loss: 0.0047 - mse: 4.1060 - val_loss: 0.0078 - val_mse: 6.7899\n",
      "[{'loss': 0.0077854436822235584, 'dropout': 0.0, 'val_mse': 10.15809154510498, 'width': 75, 'mse': 6.7798690795898438, 'val_loss': 0.011664718389511108}, {'loss': 0.0059745172038674355, 'dropout': 0.0, 'val_mse': 9.1544342041015625, 'width': 90, 'mse': 5.202843189239502, 'val_loss': 0.010512201115489006}, {'loss': 0.0047149923630058765, 'dropout': 0.0, 'val_mse': 6.7898726463317871, 'width': 105, 'mse': 4.1059994697570801, 'val_loss': 0.007796931080520153}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_73 (LSTM)               (None, None, 120)         59040     \n",
      "_________________________________________________________________\n",
      "time_distributed_70 (TimeDis (None, None, 1)           121       \n",
      "=================================================================\n",
      "Total params: 59,161\n",
      "Trainable params: 59,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "3s - loss: 0.9153 - mse: 797.1002 - val_loss: 0.5424 - val_mse: 472.3399\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.5018 - mse: 437.0032 - val_loss: 0.3442 - val_mse: 299.7430\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.3253 - mse: 283.3073 - val_loss: 0.3390 - val_mse: 295.2536\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.3308 - mse: 288.0421 - val_loss: 0.2448 - val_mse: 213.1783\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2357 - mse: 205.2550 - val_loss: 0.1996 - val_mse: 173.7909\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.1943 - mse: 169.1762 - val_loss: 0.1759 - val_mse: 153.2011\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.1656 - mse: 144.2273 - val_loss: 0.1571 - val_mse: 136.8190\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1462 - mse: 127.3288 - val_loss: 0.1425 - val_mse: 124.0591\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1305 - mse: 113.6425 - val_loss: 0.1331 - val_mse: 115.9275\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1209 - mse: 105.2690 - val_loss: 0.1287 - val_mse: 112.1158\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1160 - mse: 101.0044 - val_loss: 0.1338 - val_mse: 116.4957\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1165 - mse: 101.4763 - val_loss: 0.1447 - val_mse: 125.9952\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1199 - mse: 104.4538 - val_loss: 0.1378 - val_mse: 120.0196\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1127 - mse: 98.1599 - val_loss: 0.1162 - val_mse: 101.1519\n",
      "Epoch 15/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0971 - mse: 84.5386 - val_loss: 0.1015 - val_mse: 88.3906\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.0880 - mse: 76.6626 - val_loss: 0.0953 - val_mse: 83.0279\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0852 - mse: 74.2283 - val_loss: 0.0921 - val_mse: 80.1904\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0838 - mse: 73.0128 - val_loss: 0.0885 - val_mse: 77.0815\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0814 - mse: 70.9101 - val_loss: 0.0844 - val_mse: 73.4703\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0781 - mse: 68.0204 - val_loss: 0.0809 - val_mse: 70.4101\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0752 - mse: 65.5047 - val_loss: 0.0785 - val_mse: 68.3490\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0734 - mse: 63.9154 - val_loss: 0.0764 - val_mse: 66.5645\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0719 - mse: 62.6370 - val_loss: 0.0742 - val_mse: 64.5868\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0703 - mse: 61.1780 - val_loss: 0.0710 - val_mse: 61.8598\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0675 - mse: 58.7843 - val_loss: 0.0672 - val_mse: 58.5463\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0638 - mse: 55.5910 - val_loss: 0.0644 - val_mse: 56.0722\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0609 - mse: 53.0220 - val_loss: 0.0630 - val_mse: 54.8982\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0593 - mse: 51.6151 - val_loss: 0.0617 - val_mse: 53.7060\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0578 - mse: 50.3616 - val_loss: 0.0589 - val_mse: 51.3136\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0553 - mse: 48.1777 - val_loss: 0.0567 - val_mse: 49.4054\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0534 - mse: 46.4748 - val_loss: 0.0560 - val_mse: 48.7828\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0526 - mse: 45.8354 - val_loss: 0.0554 - val_mse: 48.2418\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0518 - mse: 45.0764 - val_loss: 0.0541 - val_mse: 47.1051\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0500 - mse: 43.5663 - val_loss: 0.0528 - val_mse: 45.9880\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0481 - mse: 41.9101 - val_loss: 0.0520 - val_mse: 45.3115\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0467 - mse: 40.6449 - val_loss: 0.0512 - val_mse: 44.6012\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0454 - mse: 39.5370 - val_loss: 0.0500 - val_mse: 43.5797\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0441 - mse: 38.4125 - val_loss: 0.0486 - val_mse: 42.3056\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0428 - mse: 37.2716 - val_loss: 0.0469 - val_mse: 40.8549\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0414 - mse: 36.0767 - val_loss: 0.0455 - val_mse: 39.6478\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0404 - mse: 35.1432 - val_loss: 0.0446 - val_mse: 38.8346\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0397 - mse: 34.5759 - val_loss: 0.0435 - val_mse: 37.8889\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0389 - mse: 33.8537 - val_loss: 0.0421 - val_mse: 36.6223\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0377 - mse: 32.8234 - val_loss: 0.0408 - val_mse: 35.5520\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0368 - mse: 32.0141 - val_loss: 0.0400 - val_mse: 34.7955\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0362 - mse: 31.5144 - val_loss: 0.0390 - val_mse: 33.9712\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0355 - mse: 30.9140 - val_loss: 0.0379 - val_mse: 33.0141\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0346 - mse: 30.1350 - val_loss: 0.0369 - val_mse: 32.1473\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0338 - mse: 29.3966 - val_loss: 0.0361 - val_mse: 31.4097\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0330 - mse: 28.7407 - val_loss: 0.0353 - val_mse: 30.7572\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0323 - mse: 28.1033 - val_loss: 0.0347 - val_mse: 30.2066\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0316 - mse: 27.4830 - val_loss: 0.0341 - val_mse: 29.7183\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0309 - mse: 26.8667 - val_loss: 0.0337 - val_mse: 29.3451\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0303 - mse: 26.3463 - val_loss: 0.0333 - val_mse: 29.0392\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0298 - mse: 25.9316 - val_loss: 0.0328 - val_mse: 28.5595\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0292 - mse: 25.4390 - val_loss: 0.0320 - val_mse: 27.8804\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0285 - mse: 24.8333 - val_loss: 0.0312 - val_mse: 27.2008\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0279 - mse: 24.2869 - val_loss: 0.0305 - val_mse: 26.5571\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0274 - mse: 23.8194 - val_loss: 0.0297 - val_mse: 25.8915\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0268 - mse: 23.3195 - val_loss: 0.0291 - val_mse: 25.3781\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0263 - mse: 22.8822 - val_loss: 0.0287 - val_mse: 25.0077\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0259 - mse: 22.5199 - val_loss: 0.0282 - val_mse: 24.5895\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0254 - mse: 22.1273 - val_loss: 0.0278 - val_mse: 24.1881\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0249 - mse: 21.7272 - val_loss: 0.0274 - val_mse: 23.8491\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0245 - mse: 21.2931 - val_loss: 0.0271 - val_mse: 23.6096\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0240 - mse: 20.8996 - val_loss: 0.0268 - val_mse: 23.3641\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0236 - mse: 20.5354 - val_loss: 0.0266 - val_mse: 23.1289\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0232 - mse: 20.1753 - val_loss: 0.0264 - val_mse: 22.9770\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0228 - mse: 19.8680 - val_loss: 0.0261 - val_mse: 22.7103\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0224 - mse: 19.5367 - val_loss: 0.0257 - val_mse: 22.3388\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0220 - mse: 19.1989 - val_loss: 0.0253 - val_mse: 21.9907\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0217 - mse: 18.9045 - val_loss: 0.0249 - val_mse: 21.6448\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0214 - mse: 18.6316 - val_loss: 0.0245 - val_mse: 21.3622\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0211 - mse: 18.3581 - val_loss: 0.0242 - val_mse: 21.1042\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0208 - mse: 18.0811 - val_loss: 0.0239 - val_mse: 20.7726\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0204 - mse: 17.7844 - val_loss: 0.0236 - val_mse: 20.5311\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0201 - mse: 17.5185 - val_loss: 0.0233 - val_mse: 20.2756\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0198 - mse: 17.2826 - val_loss: 0.0233 - val_mse: 20.3024\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0196 - mse: 17.1101 - val_loss: 0.0233 - val_mse: 20.2559\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0199 - mse: 17.2918 - val_loss: 0.0252 - val_mse: 21.9217\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0214 - mse: 18.6101 - val_loss: 0.0253 - val_mse: 21.9982\n",
      "Epoch 82/1000\n",
      "\n",
      "Epoch 00081: reducing learning rate to 0.00699999984354.\n",
      "5s - loss: 0.0226 - mse: 19.6808 - val_loss: 0.0235 - val_mse: 20.4353\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0201 - mse: 17.5303 - val_loss: 0.0226 - val_mse: 19.6766\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0193 - mse: 16.8184 - val_loss: 0.0220 - val_mse: 19.1826\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0193 - mse: 16.7829 - val_loss: 0.0218 - val_mse: 18.9832\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0189 - mse: 16.4804 - val_loss: 0.0220 - val_mse: 19.1447\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0184 - mse: 16.0037 - val_loss: 0.0226 - val_mse: 19.6834\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0187 - mse: 16.2766 - val_loss: 0.0214 - val_mse: 18.6096\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0177 - mse: 15.4499 - val_loss: 0.0218 - val_mse: 18.9969\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0184 - mse: 16.0268 - val_loss: 0.0211 - val_mse: 18.3707\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0173 - mse: 15.0615 - val_loss: 0.0217 - val_mse: 18.9099\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0179 - mse: 15.5554 - val_loss: 0.0204 - val_mse: 17.7698\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0169 - mse: 14.7601 - val_loss: 0.0205 - val_mse: 17.8408\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0174 - mse: 15.1428 - val_loss: 0.0200 - val_mse: 17.4308\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0168 - mse: 14.5922 - val_loss: 0.0204 - val_mse: 17.7925\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0169 - mse: 14.6910 - val_loss: 0.0201 - val_mse: 17.5438\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0166 - mse: 14.4223 - val_loss: 0.0198 - val_mse: 17.2277\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0164 - mse: 14.2435 - val_loss: 0.0198 - val_mse: 17.2512\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0163 - mse: 14.2094 - val_loss: 0.0198 - val_mse: 17.2600\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0160 - mse: 13.9240 - val_loss: 0.0200 - val_mse: 17.3769\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0160 - mse: 13.9584 - val_loss: 0.0193 - val_mse: 16.8199\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0156 - mse: 13.6202 - val_loss: 0.0192 - val_mse: 16.6950\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0157 - mse: 13.6748 - val_loss: 0.0189 - val_mse: 16.4487\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0153 - mse: 13.3394 - val_loss: 0.0190 - val_mse: 16.5223\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0154 - mse: 13.3854 - val_loss: 0.0184 - val_mse: 16.0527\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0150 - mse: 13.0905 - val_loss: 0.0183 - val_mse: 15.9179\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0150 - mse: 13.1044 - val_loss: 0.0181 - val_mse: 15.7835\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0147 - mse: 12.8396 - val_loss: 0.0183 - val_mse: 15.9316\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0147 - mse: 12.8268 - val_loss: 0.0179 - val_mse: 15.6049\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0145 - mse: 12.5874 - val_loss: 0.0177 - val_mse: 15.4084\n",
      "Epoch 111/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0144 - mse: 12.5560 - val_loss: 0.0175 - val_mse: 15.2590\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0142 - mse: 12.3326 - val_loss: 0.0176 - val_mse: 15.3230\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0141 - mse: 12.2939 - val_loss: 0.0172 - val_mse: 14.9614\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0139 - mse: 12.0757 - val_loss: 0.0169 - val_mse: 14.7487\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0138 - mse: 12.0289 - val_loss: 0.0168 - val_mse: 14.6238\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0136 - mse: 11.8285 - val_loss: 0.0168 - val_mse: 14.6303\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0135 - mse: 11.7658 - val_loss: 0.0165 - val_mse: 14.3578\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0133 - mse: 11.5946 - val_loss: 0.0163 - val_mse: 14.2284\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0132 - mse: 11.4968 - val_loss: 0.0163 - val_mse: 14.2054\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0130 - mse: 11.3614 - val_loss: 0.0161 - val_mse: 14.0555\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0129 - mse: 11.2255 - val_loss: 0.0159 - val_mse: 13.8554\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0128 - mse: 11.1314 - val_loss: 0.0158 - val_mse: 13.7409\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0126 - mse: 10.9747 - val_loss: 0.0157 - val_mse: 13.6551\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0125 - mse: 10.8765 - val_loss: 0.0154 - val_mse: 13.4135\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0123 - mse: 10.7459 - val_loss: 0.0153 - val_mse: 13.2853\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0122 - mse: 10.6099 - val_loss: 0.0152 - val_mse: 13.2485\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0121 - mse: 10.5147 - val_loss: 0.0150 - val_mse: 13.0381\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0119 - mse: 10.3670 - val_loss: 0.0148 - val_mse: 12.9181\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0118 - mse: 10.2476 - val_loss: 0.0148 - val_mse: 12.8793\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0116 - mse: 10.1378 - val_loss: 0.0146 - val_mse: 12.7075\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0115 - mse: 9.9961 - val_loss: 0.0145 - val_mse: 12.5951\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0113 - mse: 9.8756 - val_loss: 0.0144 - val_mse: 12.5210\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0112 - mse: 9.7593 - val_loss: 0.0142 - val_mse: 12.3311\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0111 - mse: 9.6315 - val_loss: 0.0140 - val_mse: 12.2267\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0109 - mse: 9.5012 - val_loss: 0.0139 - val_mse: 12.1274\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0108 - mse: 9.3842 - val_loss: 0.0137 - val_mse: 11.9705\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0106 - mse: 9.2701 - val_loss: 0.0137 - val_mse: 11.9112\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0105 - mse: 9.1440 - val_loss: 0.0135 - val_mse: 11.7597\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0103 - mse: 9.0131 - val_loss: 0.0134 - val_mse: 11.6916\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0102 - mse: 8.8857 - val_loss: 0.0133 - val_mse: 11.5530\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0101 - mse: 8.7589 - val_loss: 0.0131 - val_mse: 11.4384\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0099 - mse: 8.6349 - val_loss: 0.0130 - val_mse: 11.3232\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0098 - mse: 8.5117 - val_loss: 0.0129 - val_mse: 11.2048\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0096 - mse: 8.3889 - val_loss: 0.0128 - val_mse: 11.1250\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0095 - mse: 8.2672 - val_loss: 0.0127 - val_mse: 11.0236\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0094 - mse: 8.1543 - val_loss: 0.0127 - val_mse: 11.0971\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0093 - mse: 8.1051 - val_loss: 0.0130 - val_mse: 11.3134\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0098 - mse: 8.4951 - val_loss: 0.0174 - val_mse: 15.1850\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0139 - mse: 12.1200 - val_loss: 0.0246 - val_mse: 21.4031\n",
      "Epoch 150/1000\n",
      "\n",
      "Epoch 00149: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0229 - mse: 19.9411 - val_loss: 0.0249 - val_mse: 21.6657\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0219 - mse: 19.0670 - val_loss: 0.0189 - val_mse: 16.4321\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0161 - mse: 14.0546 - val_loss: 0.0144 - val_mse: 12.5077\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0123 - mse: 10.6766 - val_loss: 0.0198 - val_mse: 17.2812\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0184 - mse: 16.0451 - val_loss: 0.0136 - val_mse: 11.8074\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0110 - mse: 9.5732 - val_loss: 0.0155 - val_mse: 13.4789\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0120 - mse: 10.4205 - val_loss: 0.0187 - val_mse: 16.2518\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0149 - mse: 12.9454 - val_loss: 0.0148 - val_mse: 12.8597\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0110 - mse: 9.5752 - val_loss: 0.0136 - val_mse: 11.8767\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0105 - mse: 9.1750 - val_loss: 0.0155 - val_mse: 13.4849\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0130 - mse: 11.3269 - val_loss: 0.0139 - val_mse: 12.1117\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0112 - mse: 9.7113 - val_loss: 0.0130 - val_mse: 11.3105\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0097 - mse: 8.4539 - val_loss: 0.0143 - val_mse: 12.4204\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0110 - mse: 9.5460 - val_loss: 0.0142 - val_mse: 12.3766\n",
      "Epoch 164/1000\n",
      "\n",
      "Epoch 00163: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0113 - mse: 9.8002 - val_loss: 0.0127 - val_mse: 11.0383\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0101 - mse: 8.7706 - val_loss: 0.0120 - val_mse: 10.4768\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0096 - mse: 8.3378 - val_loss: 0.0122 - val_mse: 10.6044\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0098 - mse: 8.4909 - val_loss: 0.0126 - val_mse: 10.9722\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0101 - mse: 8.7540 - val_loss: 0.0126 - val_mse: 10.9676\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0098 - mse: 8.5477 - val_loss: 0.0123 - val_mse: 10.6732\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0093 - mse: 8.0711 - val_loss: 0.0121 - val_mse: 10.5696\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0091 - mse: 7.9053 - val_loss: 0.0123 - val_mse: 10.7470\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0093 - mse: 8.1269 - val_loss: 0.0124 - val_mse: 10.7888\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0095 - mse: 8.2407 - val_loss: 0.0120 - val_mse: 10.4643\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0091 - mse: 7.9586 - val_loss: 0.0117 - val_mse: 10.1492\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0088 - mse: 7.6632 - val_loss: 0.0117 - val_mse: 10.1902\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0089 - mse: 7.7283 - val_loss: 0.0119 - val_mse: 10.3232\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0091 - mse: 7.8861 - val_loss: 0.0117 - val_mse: 10.1932\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0089 - mse: 7.7663 - val_loss: 0.0114 - val_mse: 9.9465\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0086 - mse: 7.5204 - val_loss: 0.0114 - val_mse: 9.8910\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0086 - mse: 7.4690 - val_loss: 0.0115 - val_mse: 9.9798\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0087 - mse: 7.5537 - val_loss: 0.0115 - val_mse: 9.9860\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0086 - mse: 7.5279 - val_loss: 0.0114 - val_mse: 9.8964\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0085 - mse: 7.3873 - val_loss: 0.0113 - val_mse: 9.8470\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0084 - mse: 7.3020 - val_loss: 0.0113 - val_mse: 9.8603\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0084 - mse: 7.3120 - val_loss: 0.0113 - val_mse: 9.8485\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0084 - mse: 7.3094 - val_loss: 0.0112 - val_mse: 9.7853\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0083 - mse: 7.2307 - val_loss: 0.0112 - val_mse: 9.7515\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0082 - mse: 7.1389 - val_loss: 0.0113 - val_mse: 9.8068\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0082 - mse: 7.1040 - val_loss: 0.0114 - val_mse: 9.8994\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0082 - mse: 7.1025 - val_loss: 0.0114 - val_mse: 9.9262\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0081 - mse: 7.0678 - val_loss: 0.0113 - val_mse: 9.8599\n",
      "Epoch 192/1000\n",
      "\n",
      "Epoch 00191: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0080 - mse: 6.9950 - val_loss: 0.0112 - val_mse: 9.7641\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0080 - mse: 6.9415 - val_loss: 0.0112 - val_mse: 9.7112\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0080 - mse: 6.9273 - val_loss: 0.0111 - val_mse: 9.6692\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0079 - mse: 6.9119 - val_loss: 0.0111 - val_mse: 9.6362\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0079 - mse: 6.8804 - val_loss: 0.0110 - val_mse: 9.6186\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0079 - mse: 6.8396 - val_loss: 0.0111 - val_mse: 9.6242\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0078 - mse: 6.8087 - val_loss: 0.0111 - val_mse: 9.6460\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0078 - mse: 6.7946 - val_loss: 0.0111 - val_mse: 9.6561\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0078 - mse: 6.7810 - val_loss: 0.0111 - val_mse: 9.6326\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0078 - mse: 6.7527 - val_loss: 0.0110 - val_mse: 9.5840\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0077 - mse: 6.7171 - val_loss: 0.0109 - val_mse: 9.5355\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0077 - mse: 6.6915 - val_loss: 0.0109 - val_mse: 9.5015\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0077 - mse: 6.6772 - val_loss: 0.0109 - val_mse: 9.4801\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0076 - mse: 6.6599 - val_loss: 0.0109 - val_mse: 9.4700\n",
      "Epoch 206/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0076 - mse: 6.6329 - val_loss: 0.0109 - val_mse: 9.4738\n",
      "Epoch 207/1000\n",
      "\n",
      "Epoch 00206: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0076 - mse: 6.6036 - val_loss: 0.0109 - val_mse: 9.4887\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0076 - mse: 6.5816 - val_loss: 0.0109 - val_mse: 9.4958\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0075 - mse: 6.5696 - val_loss: 0.0109 - val_mse: 9.4906\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0075 - mse: 6.5561 - val_loss: 0.0109 - val_mse: 9.4709\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0075 - mse: 6.5396 - val_loss: 0.0108 - val_mse: 9.4419\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0075 - mse: 6.5218 - val_loss: 0.0108 - val_mse: 9.4114\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0075 - mse: 6.5052 - val_loss: 0.0108 - val_mse: 9.3868\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0075 - mse: 6.4911 - val_loss: 0.0108 - val_mse: 9.3711\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0074 - mse: 6.4779 - val_loss: 0.0108 - val_mse: 9.3641\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0074 - mse: 6.4642 - val_loss: 0.0108 - val_mse: 9.3633\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0074 - mse: 6.4491 - val_loss: 0.0108 - val_mse: 9.3658\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0074 - mse: 6.4339 - val_loss: 0.0108 - val_mse: 9.3680\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0074 - mse: 6.4195 - val_loss: 0.0108 - val_mse: 9.3671\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0074 - mse: 6.4060 - val_loss: 0.0107 - val_mse: 9.3615\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0073 - mse: 6.3928 - val_loss: 0.0107 - val_mse: 9.3522\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0073 - mse: 6.3794 - val_loss: 0.0107 - val_mse: 9.3413\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0073 - mse: 6.3654 - val_loss: 0.0107 - val_mse: 9.3314\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0073 - mse: 6.3511 - val_loss: 0.0107 - val_mse: 9.3240\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0073 - mse: 6.3371 - val_loss: 0.0107 - val_mse: 9.3195\n",
      "Epoch 226/1000\n",
      "\n",
      "Epoch 00225: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0073 - mse: 6.3238 - val_loss: 0.0107 - val_mse: 9.3164\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0072 - mse: 6.3108 - val_loss: 0.0107 - val_mse: 9.3140\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0072 - mse: 6.3015 - val_loss: 0.0107 - val_mse: 9.3106\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0072 - mse: 6.2921 - val_loss: 0.0107 - val_mse: 9.3067\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0072 - mse: 6.2825 - val_loss: 0.0107 - val_mse: 9.3029\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0072 - mse: 6.2731 - val_loss: 0.0107 - val_mse: 9.3000\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0072 - mse: 6.2639 - val_loss: 0.0107 - val_mse: 9.2979\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0072 - mse: 6.2548 - val_loss: 0.0107 - val_mse: 9.2965\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0072 - mse: 6.2457 - val_loss: 0.0107 - val_mse: 9.2953\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0072 - mse: 6.2365 - val_loss: 0.0107 - val_mse: 9.2936\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0072 - mse: 6.2272 - val_loss: 0.0107 - val_mse: 9.2909\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0071 - mse: 6.2180 - val_loss: 0.0107 - val_mse: 9.2867\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0071 - mse: 6.2089 - val_loss: 0.0107 - val_mse: 9.2809\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0071 - mse: 6.1998 - val_loss: 0.0106 - val_mse: 9.2738\n",
      "Epoch 240/1000\n",
      "\n",
      "Epoch 00239: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0071 - mse: 6.1908 - val_loss: 0.0106 - val_mse: 9.2661\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0071 - mse: 6.1818 - val_loss: 0.0106 - val_mse: 9.2612\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0071 - mse: 6.1755 - val_loss: 0.0106 - val_mse: 9.2570\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0071 - mse: 6.1692 - val_loss: 0.0106 - val_mse: 9.2537\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0071 - mse: 6.1630 - val_loss: 0.0106 - val_mse: 9.2511\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0071 - mse: 6.1568 - val_loss: 0.0106 - val_mse: 9.2493\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0071 - mse: 6.1505 - val_loss: 0.0106 - val_mse: 9.2483\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0071 - mse: 6.1444 - val_loss: 0.0106 - val_mse: 9.2477\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0070 - mse: 6.1383 - val_loss: 0.0106 - val_mse: 9.2472\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0070 - mse: 6.1323 - val_loss: 0.0106 - val_mse: 9.2463\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0070 - mse: 6.1262 - val_loss: 0.0106 - val_mse: 9.2445\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0070 - mse: 6.1201 - val_loss: 0.0106 - val_mse: 9.2422\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0070 - mse: 6.1141 - val_loss: 0.0106 - val_mse: 9.2390\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0070 - mse: 6.1082 - val_loss: 0.0106 - val_mse: 9.2355\n",
      "Epoch 254/1000\n",
      "\n",
      "Epoch 00253: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0070 - mse: 6.1022 - val_loss: 0.0106 - val_mse: 9.2318\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0070 - mse: 6.0963 - val_loss: 0.0106 - val_mse: 9.2293\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0070 - mse: 6.0922 - val_loss: 0.0106 - val_mse: 9.2270\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0070 - mse: 6.0880 - val_loss: 0.0106 - val_mse: 9.2250\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0070 - mse: 6.0839 - val_loss: 0.0106 - val_mse: 9.2235\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0070 - mse: 6.0798 - val_loss: 0.0106 - val_mse: 9.2223\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0070 - mse: 6.0757 - val_loss: 0.0106 - val_mse: 9.2213\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0070 - mse: 6.0717 - val_loss: 0.0106 - val_mse: 9.2205\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0070 - mse: 6.0677 - val_loss: 0.0106 - val_mse: 9.2196\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0070 - mse: 6.0637 - val_loss: 0.0106 - val_mse: 9.2187\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0070 - mse: 6.0597 - val_loss: 0.0106 - val_mse: 9.2175\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0070 - mse: 6.0557 - val_loss: 0.0106 - val_mse: 9.2161\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0069 - mse: 6.0517 - val_loss: 0.0106 - val_mse: 9.2148\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0069 - mse: 6.0478 - val_loss: 0.0106 - val_mse: 9.2134\n",
      "Epoch 268/1000\n",
      "\n",
      "Epoch 00267: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0069 - mse: 6.0438 - val_loss: 0.0106 - val_mse: 9.2121\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0069 - mse: 6.0398 - val_loss: 0.0106 - val_mse: 9.2112\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0069 - mse: 6.0370 - val_loss: 0.0106 - val_mse: 9.2102\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0069 - mse: 6.0343 - val_loss: 0.0106 - val_mse: 9.2090\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0069 - mse: 6.0315 - val_loss: 0.0106 - val_mse: 9.2076\n",
      "[{'loss': 0.0077854436822235584, 'dropout': 0.0, 'val_mse': 10.15809154510498, 'width': 75, 'mse': 6.7798690795898438, 'val_loss': 0.011664718389511108}, {'loss': 0.0059745172038674355, 'dropout': 0.0, 'val_mse': 9.1544342041015625, 'width': 90, 'mse': 5.202843189239502, 'val_loss': 0.010512201115489006}, {'loss': 0.0047149923630058765, 'dropout': 0.0, 'val_mse': 6.7898726463317871, 'width': 105, 'mse': 4.1059994697570801, 'val_loss': 0.007796931080520153}, {'loss': 0.0069261062890291214, 'dropout': 0.0, 'val_mse': 9.2075967788696289, 'width': 120, 'mse': 6.0315232276916504, 'val_loss': 0.010573248378932476}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_74 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "time_distributed_71 (TimeDis (None, None, 1)           136       \n",
      "=================================================================\n",
      "Total params: 74,656\n",
      "Trainable params: 74,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "4s - loss: 0.9989 - mse: 869.8665 - val_loss: 0.5419 - val_mse: 471.8831\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.4974 - mse: 433.1248 - val_loss: 0.2903 - val_mse: 252.8065\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.2760 - mse: 240.3453 - val_loss: 0.2483 - val_mse: 216.2318\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.2508 - mse: 218.4299 - val_loss: 0.2303 - val_mse: 200.5875\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2282 - mse: 198.7171 - val_loss: 0.2292 - val_mse: 199.6307\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2099 - mse: 182.7479 - val_loss: 0.2185 - val_mse: 190.3174\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.1985 - mse: 172.8571 - val_loss: 0.1561 - val_mse: 135.9172\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1419 - mse: 123.5940 - val_loss: 0.1452 - val_mse: 126.4024\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1350 - mse: 117.5487 - val_loss: 0.1483 - val_mse: 129.1851\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1273 - mse: 110.8390 - val_loss: 0.1524 - val_mse: 132.7547\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1264 - mse: 110.1113 - val_loss: 0.1384 - val_mse: 120.5405\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1180 - mse: 102.7181 - val_loss: 0.1195 - val_mse: 104.0410\n",
      "Epoch 13/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.1053 - mse: 91.7167 - val_loss: 0.1128 - val_mse: 98.2398\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1027 - mse: 89.3929 - val_loss: 0.1076 - val_mse: 93.7338\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.0993 - mse: 86.4502 - val_loss: 0.0963 - val_mse: 83.8186\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.0881 - mse: 76.7559 - val_loss: 0.0862 - val_mse: 75.0818\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0781 - mse: 67.9830 - val_loss: 0.0838 - val_mse: 72.9926\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0756 - mse: 65.8428 - val_loss: 0.0862 - val_mse: 75.0521\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0780 - mse: 67.9638 - val_loss: 0.0825 - val_mse: 71.8661\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0760 - mse: 66.1531 - val_loss: 0.0769 - val_mse: 67.0035\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0713 - mse: 62.1281 - val_loss: 0.0754 - val_mse: 65.6601\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0706 - mse: 61.5081 - val_loss: 0.0711 - val_mse: 61.9516\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0670 - mse: 58.3098 - val_loss: 0.0662 - val_mse: 57.6472\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0625 - mse: 54.4274 - val_loss: 0.0624 - val_mse: 54.3527\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0591 - mse: 51.4971 - val_loss: 0.0617 - val_mse: 53.7236\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0582 - mse: 50.6915 - val_loss: 0.0617 - val_mse: 53.7389\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0578 - mse: 50.3518 - val_loss: 0.0600 - val_mse: 52.2286\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0559 - mse: 48.6675 - val_loss: 0.0568 - val_mse: 49.4264\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0523 - mse: 45.5430 - val_loss: 0.0553 - val_mse: 48.1770\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0502 - mse: 43.7546 - val_loss: 0.0527 - val_mse: 45.8905\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0480 - mse: 41.8342 - val_loss: 0.0512 - val_mse: 44.5656\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0472 - mse: 41.1357 - val_loss: 0.0512 - val_mse: 44.6004\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0471 - mse: 41.0470 - val_loss: 0.0492 - val_mse: 42.8186\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0445 - mse: 38.7360 - val_loss: 0.0489 - val_mse: 42.6204\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0436 - mse: 37.9940 - val_loss: 0.0472 - val_mse: 41.1283\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0418 - mse: 36.3631 - val_loss: 0.0459 - val_mse: 39.9820\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0407 - mse: 35.4678 - val_loss: 0.0447 - val_mse: 38.8854\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0399 - mse: 34.7658 - val_loss: 0.0434 - val_mse: 37.8110\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0393 - mse: 34.2170 - val_loss: 0.0415 - val_mse: 36.1645\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0378 - mse: 32.8807 - val_loss: 0.0408 - val_mse: 35.5458\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0373 - mse: 32.4684 - val_loss: 0.0396 - val_mse: 34.4825\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0364 - mse: 31.6806 - val_loss: 0.0381 - val_mse: 33.1842\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0353 - mse: 30.7055 - val_loss: 0.0372 - val_mse: 32.3920\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0344 - mse: 29.9566 - val_loss: 0.0368 - val_mse: 32.0178\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0337 - mse: 29.3632 - val_loss: 0.0364 - val_mse: 31.6798\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0331 - mse: 28.8017 - val_loss: 0.0359 - val_mse: 31.2691\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0325 - mse: 28.2638 - val_loss: 0.0350 - val_mse: 30.4541\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0316 - mse: 27.5532 - val_loss: 0.0335 - val_mse: 29.2109\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0305 - mse: 26.5763 - val_loss: 0.0329 - val_mse: 28.6640\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0301 - mse: 26.2374 - val_loss: 0.0324 - val_mse: 28.2182\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0297 - mse: 25.8404 - val_loss: 0.0320 - val_mse: 27.8699\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0291 - mse: 25.3414 - val_loss: 0.0317 - val_mse: 27.6130\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0285 - mse: 24.8422 - val_loss: 0.0313 - val_mse: 27.2233\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0281 - mse: 24.4287 - val_loss: 0.0304 - val_mse: 26.5117\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0275 - mse: 23.9251 - val_loss: 0.0296 - val_mse: 25.8078\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0270 - mse: 23.5050 - val_loss: 0.0290 - val_mse: 25.2203\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0265 - mse: 23.0610 - val_loss: 0.0285 - val_mse: 24.8320\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0260 - mse: 22.6484 - val_loss: 0.0282 - val_mse: 24.5274\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0256 - mse: 22.3247 - val_loss: 0.0276 - val_mse: 24.0706\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0252 - mse: 21.9624 - val_loss: 0.0270 - val_mse: 23.5132\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0247 - mse: 21.5130 - val_loss: 0.0267 - val_mse: 23.2453\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0244 - mse: 21.2382 - val_loss: 0.0263 - val_mse: 22.9377\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0240 - mse: 20.8798 - val_loss: 0.0261 - val_mse: 22.7309\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0236 - mse: 20.5658 - val_loss: 0.0258 - val_mse: 22.4965\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0232 - mse: 20.2380 - val_loss: 0.0254 - val_mse: 22.0981\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0228 - mse: 19.8531 - val_loss: 0.0251 - val_mse: 21.8425\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0224 - mse: 19.5490 - val_loss: 0.0248 - val_mse: 21.6271\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0220 - mse: 19.1899 - val_loss: 0.0246 - val_mse: 21.3995\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0217 - mse: 18.9036 - val_loss: 0.0243 - val_mse: 21.1664\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0213 - mse: 18.5777 - val_loss: 0.0237 - val_mse: 20.6685\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0209 - mse: 18.1708 - val_loss: 0.0233 - val_mse: 20.3134\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0205 - mse: 17.8240 - val_loss: 0.0228 - val_mse: 19.8459\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0200 - mse: 17.4421 - val_loss: 0.0224 - val_mse: 19.4767\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0196 - mse: 17.0947 - val_loss: 0.0218 - val_mse: 19.0106\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0192 - mse: 16.7472 - val_loss: 0.0215 - val_mse: 18.7571\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0189 - mse: 16.4410 - val_loss: 0.0211 - val_mse: 18.3836\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0187 - mse: 16.2880 - val_loss: 0.0227 - val_mse: 19.7594\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0199 - mse: 17.3696 - val_loss: 0.0255 - val_mse: 22.1859\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0241 - mse: 21.0011 - val_loss: 0.0359 - val_mse: 31.2364\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0337 - mse: 29.3665 - val_loss: 0.0209 - val_mse: 18.2207\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0186 - mse: 16.2225 - val_loss: 0.0311 - val_mse: 27.0991\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0309 - mse: 26.8718 - val_loss: 0.0195 - val_mse: 16.9959\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0175 - mse: 15.2401 - val_loss: 0.0258 - val_mse: 22.4482\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0235 - mse: 20.4353 - val_loss: 0.0233 - val_mse: 20.3260\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0209 - mse: 18.1825 - val_loss: 0.0202 - val_mse: 17.6061\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0179 - mse: 15.6163 - val_loss: 0.0231 - val_mse: 20.0838\n",
      "Epoch 87/1000\n",
      "\n",
      "Epoch 00086: reducing learning rate to 0.00699999984354.\n",
      "6s - loss: 0.0213 - mse: 18.5795 - val_loss: 0.0207 - val_mse: 17.9964\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0186 - mse: 16.2367 - val_loss: 0.0199 - val_mse: 17.3650\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0175 - mse: 15.2302 - val_loss: 0.0206 - val_mse: 17.9699\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0179 - mse: 15.6302 - val_loss: 0.0205 - val_mse: 17.8169\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0177 - mse: 15.4319 - val_loss: 0.0197 - val_mse: 17.1138\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0170 - mse: 14.7889 - val_loss: 0.0192 - val_mse: 16.7509\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0168 - mse: 14.5923 - val_loss: 0.0184 - val_mse: 16.0648\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0161 - mse: 14.0584 - val_loss: 0.0182 - val_mse: 15.8098\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0158 - mse: 13.8005 - val_loss: 0.0184 - val_mse: 16.0097\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0160 - mse: 13.9239 - val_loss: 0.0180 - val_mse: 15.6420\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0156 - mse: 13.5419 - val_loss: 0.0172 - val_mse: 14.9556\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0148 - mse: 12.8922 - val_loss: 0.0171 - val_mse: 14.9330\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0148 - mse: 12.8688 - val_loss: 0.0174 - val_mse: 15.1513\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0150 - mse: 13.0591 - val_loss: 0.0168 - val_mse: 14.6049\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0144 - mse: 12.5273 - val_loss: 0.0162 - val_mse: 14.1345\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0139 - mse: 12.0978 - val_loss: 0.0163 - val_mse: 14.2322\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0141 - mse: 12.2598 - val_loss: 0.0162 - val_mse: 14.1070\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0140 - mse: 12.2339 - val_loss: 0.0156 - val_mse: 13.5875\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0135 - mse: 11.7937 - val_loss: 0.0154 - val_mse: 13.3929\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0133 - mse: 11.5681 - val_loss: 0.0156 - val_mse: 13.6142\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0134 - mse: 11.6660 - val_loss: 0.0156 - val_mse: 13.5808\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0133 - mse: 11.5406 - val_loss: 0.0152 - val_mse: 13.1958\n",
      "Epoch 109/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0129 - mse: 11.1937 - val_loss: 0.0149 - val_mse: 12.9465\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0127 - mse: 11.0824 - val_loss: 0.0148 - val_mse: 12.8709\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0128 - mse: 11.1348 - val_loss: 0.0145 - val_mse: 12.6346\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0126 - mse: 10.9343 - val_loss: 0.0143 - val_mse: 12.4428\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0123 - mse: 10.6700 - val_loss: 0.0144 - val_mse: 12.5445\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0123 - mse: 10.6680 - val_loss: 0.0144 - val_mse: 12.5087\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0122 - mse: 10.6299 - val_loss: 0.0140 - val_mse: 12.1606\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0119 - mse: 10.4022 - val_loss: 0.0137 - val_mse: 11.9256\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0118 - mse: 10.2933 - val_loss: 0.0136 - val_mse: 11.8727\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0118 - mse: 10.2789 - val_loss: 0.0135 - val_mse: 11.7892\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0117 - mse: 10.1514 - val_loss: 0.0134 - val_mse: 11.6900\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0115 - mse: 10.0049 - val_loss: 0.0133 - val_mse: 11.6061\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0114 - mse: 9.9439 - val_loss: 0.0132 - val_mse: 11.4643\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0113 - mse: 9.8779 - val_loss: 0.0129 - val_mse: 11.2736\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0112 - mse: 9.7498 - val_loss: 0.0128 - val_mse: 11.1594\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0111 - mse: 9.6573 - val_loss: 0.0127 - val_mse: 11.0955\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0110 - mse: 9.6034 - val_loss: 0.0126 - val_mse: 10.9621\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0109 - mse: 9.5042 - val_loss: 0.0124 - val_mse: 10.8418\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0108 - mse: 9.4169 - val_loss: 0.0124 - val_mse: 10.7873\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0107 - mse: 9.3557 - val_loss: 0.0123 - val_mse: 10.7305\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0107 - mse: 9.2756 - val_loss: 0.0122 - val_mse: 10.6572\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0106 - mse: 9.2084 - val_loss: 0.0121 - val_mse: 10.5399\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0105 - mse: 9.1404 - val_loss: 0.0120 - val_mse: 10.4126\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0104 - mse: 9.0681 - val_loss: 0.0119 - val_mse: 10.3307\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0103 - mse: 9.0026 - val_loss: 0.0118 - val_mse: 10.2871\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0103 - mse: 8.9387 - val_loss: 0.0117 - val_mse: 10.2268\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0102 - mse: 8.8660 - val_loss: 0.0117 - val_mse: 10.1503\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0101 - mse: 8.8034 - val_loss: 0.0116 - val_mse: 10.0715\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0100 - mse: 8.7479 - val_loss: 0.0115 - val_mse: 9.9778\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0100 - mse: 8.6739 - val_loss: 0.0114 - val_mse: 9.9061\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0099 - mse: 8.6158 - val_loss: 0.0113 - val_mse: 9.8406\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0098 - mse: 8.5621 - val_loss: 0.0112 - val_mse: 9.7724\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0098 - mse: 8.4946 - val_loss: 0.0112 - val_mse: 9.7257\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0097 - mse: 8.4345 - val_loss: 0.0111 - val_mse: 9.6822\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0096 - mse: 8.3835 - val_loss: 0.0110 - val_mse: 9.6029\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0096 - mse: 8.3187 - val_loss: 0.0109 - val_mse: 9.5225\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0095 - mse: 8.2620 - val_loss: 0.0109 - val_mse: 9.4639\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0094 - mse: 8.2097 - val_loss: 0.0108 - val_mse: 9.4171\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0094 - mse: 8.1501 - val_loss: 0.0108 - val_mse: 9.3739\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0093 - mse: 8.0940 - val_loss: 0.0107 - val_mse: 9.3277\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0092 - mse: 8.0415 - val_loss: 0.0106 - val_mse: 9.2690\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0092 - mse: 7.9847 - val_loss: 0.0106 - val_mse: 9.2035\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0091 - mse: 7.9308 - val_loss: 0.0105 - val_mse: 9.1283\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0090 - mse: 7.8765 - val_loss: 0.0104 - val_mse: 9.0617\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0090 - mse: 7.8237 - val_loss: 0.0103 - val_mse: 9.0063\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0089 - mse: 7.7693 - val_loss: 0.0103 - val_mse: 8.9541\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0089 - mse: 7.7163 - val_loss: 0.0102 - val_mse: 8.8913\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0088 - mse: 7.6638 - val_loss: 0.0101 - val_mse: 8.8217\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0087 - mse: 7.6102 - val_loss: 0.0101 - val_mse: 8.7584\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0087 - mse: 7.5574 - val_loss: 0.0100 - val_mse: 8.7004\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0086 - mse: 7.5060 - val_loss: 0.0099 - val_mse: 8.6460\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0086 - mse: 7.4528 - val_loss: 0.0099 - val_mse: 8.5991\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0085 - mse: 7.4006 - val_loss: 0.0098 - val_mse: 8.5428\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0084 - mse: 7.3492 - val_loss: 0.0097 - val_mse: 8.4708\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0084 - mse: 7.2960 - val_loss: 0.0096 - val_mse: 8.4016\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0083 - mse: 7.2444 - val_loss: 0.0096 - val_mse: 8.3409\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0083 - mse: 7.1920 - val_loss: 0.0095 - val_mse: 8.2865\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0082 - mse: 7.1396 - val_loss: 0.0095 - val_mse: 8.2376\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0081 - mse: 7.0870 - val_loss: 0.0094 - val_mse: 8.1792\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0081 - mse: 7.0349 - val_loss: 0.0093 - val_mse: 8.1094\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0080 - mse: 6.9822 - val_loss: 0.0092 - val_mse: 8.0437\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0080 - mse: 6.9293 - val_loss: 0.0092 - val_mse: 7.9850\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0079 - mse: 6.8769 - val_loss: 0.0091 - val_mse: 7.9298\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0078 - mse: 6.8244 - val_loss: 0.0090 - val_mse: 7.8773\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0078 - mse: 6.7725 - val_loss: 0.0090 - val_mse: 7.8123\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0077 - mse: 6.7194 - val_loss: 0.0089 - val_mse: 7.7492\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0077 - mse: 6.6667 - val_loss: 0.0088 - val_mse: 7.6828\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0076 - mse: 6.6126 - val_loss: 0.0088 - val_mse: 7.6246\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0075 - mse: 6.5583 - val_loss: 0.0087 - val_mse: 7.5623\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0075 - mse: 6.5038 - val_loss: 0.0086 - val_mse: 7.4970\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0074 - mse: 6.4487 - val_loss: 0.0085 - val_mse: 7.4292\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0073 - mse: 6.3935 - val_loss: 0.0085 - val_mse: 7.3720\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0073 - mse: 6.3396 - val_loss: 0.0084 - val_mse: 7.3238\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0072 - mse: 6.2931 - val_loss: 0.0084 - val_mse: 7.3112\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0072 - mse: 6.2832 - val_loss: 0.0086 - val_mse: 7.4898\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0074 - mse: 6.4487 - val_loss: 0.0096 - val_mse: 8.3707\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0083 - mse: 7.2492 - val_loss: 0.0121 - val_mse: 10.5224\n",
      "Epoch 186/1000\n",
      "\n",
      "Epoch 00185: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0106 - mse: 9.2475 - val_loss: 0.0112 - val_mse: 9.7273\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0097 - mse: 8.4674 - val_loss: 0.0082 - val_mse: 7.1529\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0072 - mse: 6.2272 - val_loss: 0.0098 - val_mse: 8.5454\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0088 - mse: 7.6808 - val_loss: 0.0084 - val_mse: 7.2774\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0074 - mse: 6.4105 - val_loss: 0.0093 - val_mse: 8.1264\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0080 - mse: 6.9829 - val_loss: 0.0090 - val_mse: 7.8230\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0077 - mse: 6.6813 - val_loss: 0.0084 - val_mse: 7.3002\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0073 - mse: 6.3477 - val_loss: 0.0090 - val_mse: 7.8160\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0079 - mse: 6.8641 - val_loss: 0.0080 - val_mse: 6.9914\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0069 - mse: 6.0042 - val_loss: 0.0090 - val_mse: 7.8107\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0077 - mse: 6.6698 - val_loss: 0.0082 - val_mse: 7.1431\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0070 - mse: 6.0967 - val_loss: 0.0082 - val_mse: 7.1638\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0071 - mse: 6.2101 - val_loss: 0.0084 - val_mse: 7.2826\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0072 - mse: 6.2955 - val_loss: 0.0080 - val_mse: 6.9273\n",
      "Epoch 200/1000\n",
      "\n",
      "Epoch 00199: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0067 - mse: 5.8682 - val_loss: 0.0085 - val_mse: 7.3925\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0072 - mse: 6.2536 - val_loss: 0.0080 - val_mse: 6.9970\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0068 - mse: 5.9031 - val_loss: 0.0079 - val_mse: 6.9170\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0067 - mse: 5.8580 - val_loss: 0.0082 - val_mse: 7.1352\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0070 - mse: 6.0690 - val_loss: 0.0079 - val_mse: 6.8424\n",
      "Epoch 205/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0066 - mse: 5.7838 - val_loss: 0.0079 - val_mse: 6.8592\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0066 - mse: 5.7836 - val_loss: 0.0080 - val_mse: 6.9831\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0068 - mse: 5.9092 - val_loss: 0.0077 - val_mse: 6.6980\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0065 - mse: 5.6760 - val_loss: 0.0077 - val_mse: 6.7074\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0066 - mse: 5.7122 - val_loss: 0.0078 - val_mse: 6.7567\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0066 - mse: 5.7657 - val_loss: 0.0075 - val_mse: 6.5687\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0064 - mse: 5.5800 - val_loss: 0.0076 - val_mse: 6.6474\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0065 - mse: 5.6391 - val_loss: 0.0076 - val_mse: 6.6430\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0065 - mse: 5.6342 - val_loss: 0.0075 - val_mse: 6.4894\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0063 - mse: 5.4986 - val_loss: 0.0075 - val_mse: 6.5553\n",
      "Epoch 215/1000\n",
      "\n",
      "Epoch 00214: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0064 - mse: 5.5650 - val_loss: 0.0075 - val_mse: 6.4894\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0063 - mse: 5.5089 - val_loss: 0.0074 - val_mse: 6.4036\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0062 - mse: 5.4325 - val_loss: 0.0074 - val_mse: 6.4283\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0063 - mse: 5.4564 - val_loss: 0.0074 - val_mse: 6.4276\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0063 - mse: 5.4539 - val_loss: 0.0073 - val_mse: 6.3590\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0062 - mse: 5.3848 - val_loss: 0.0073 - val_mse: 6.3623\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0062 - mse: 5.3783 - val_loss: 0.0073 - val_mse: 6.3829\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0062 - mse: 5.3871 - val_loss: 0.0073 - val_mse: 6.3323\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0061 - mse: 5.3349 - val_loss: 0.0072 - val_mse: 6.3077\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0061 - mse: 5.3116 - val_loss: 0.0073 - val_mse: 6.3190\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0061 - mse: 5.3195 - val_loss: 0.0072 - val_mse: 6.2890\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0061 - mse: 5.2817 - val_loss: 0.0072 - val_mse: 6.2728\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0060 - mse: 5.2519 - val_loss: 0.0072 - val_mse: 6.2882\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0060 - mse: 5.2540 - val_loss: 0.0072 - val_mse: 6.2605\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0060 - mse: 5.2253 - val_loss: 0.0071 - val_mse: 6.2204\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0060 - mse: 5.1944 - val_loss: 0.0071 - val_mse: 6.2090\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0060 - mse: 5.1909 - val_loss: 0.0071 - val_mse: 6.1855\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0059 - mse: 5.1669 - val_loss: 0.0071 - val_mse: 6.1660\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0059 - mse: 5.1378 - val_loss: 0.0071 - val_mse: 6.1708\n",
      "Epoch 234/1000\n",
      "\n",
      "Epoch 00233: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0059 - mse: 5.1307 - val_loss: 0.0071 - val_mse: 6.1513\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0059 - mse: 5.1085 - val_loss: 0.0070 - val_mse: 6.1261\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0058 - mse: 5.0874 - val_loss: 0.0070 - val_mse: 6.1110\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0058 - mse: 5.0774 - val_loss: 0.0070 - val_mse: 6.1008\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0058 - mse: 5.0684 - val_loss: 0.0070 - val_mse: 6.0868\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0058 - mse: 5.0499 - val_loss: 0.0070 - val_mse: 6.0795\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0058 - mse: 5.0339 - val_loss: 0.0070 - val_mse: 6.0781\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0058 - mse: 5.0248 - val_loss: 0.0070 - val_mse: 6.0663\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0058 - mse: 5.0109 - val_loss: 0.0069 - val_mse: 6.0456\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0057 - mse: 4.9930 - val_loss: 0.0069 - val_mse: 6.0298\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0057 - mse: 4.9809 - val_loss: 0.0069 - val_mse: 6.0184\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0057 - mse: 4.9694 - val_loss: 0.0069 - val_mse: 6.0070\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0057 - mse: 4.9530 - val_loss: 0.0069 - val_mse: 6.0006\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0057 - mse: 4.9383 - val_loss: 0.0069 - val_mse: 5.9961\n",
      "Epoch 248/1000\n",
      "\n",
      "Epoch 00247: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0057 - mse: 4.9268 - val_loss: 0.0069 - val_mse: 5.9841\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0056 - mse: 4.9122 - val_loss: 0.0069 - val_mse: 5.9723\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0056 - mse: 4.9009 - val_loss: 0.0068 - val_mse: 5.9622\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0056 - mse: 4.8913 - val_loss: 0.0068 - val_mse: 5.9545\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0056 - mse: 4.8828 - val_loss: 0.0068 - val_mse: 5.9474\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0056 - mse: 4.8727 - val_loss: 0.0068 - val_mse: 5.9410\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0056 - mse: 4.8618 - val_loss: 0.0068 - val_mse: 5.9359\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0056 - mse: 4.8520 - val_loss: 0.0068 - val_mse: 5.9299\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0056 - mse: 4.8429 - val_loss: 0.0068 - val_mse: 5.9202\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0055 - mse: 4.8328 - val_loss: 0.0068 - val_mse: 5.9081\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0055 - mse: 4.8221 - val_loss: 0.0068 - val_mse: 5.8966\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0055 - mse: 4.8122 - val_loss: 0.0068 - val_mse: 5.8870\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0055 - mse: 4.8027 - val_loss: 0.0068 - val_mse: 5.8788\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0055 - mse: 4.7925 - val_loss: 0.0067 - val_mse: 5.8721\n",
      "Epoch 262/1000\n",
      "\n",
      "Epoch 00261: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0055 - mse: 4.7819 - val_loss: 0.0067 - val_mse: 5.8664\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0055 - mse: 4.7720 - val_loss: 0.0067 - val_mse: 5.8617\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0055 - mse: 4.7651 - val_loss: 0.0067 - val_mse: 5.8554\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0055 - mse: 4.7578 - val_loss: 0.0067 - val_mse: 5.8481\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0055 - mse: 4.7504 - val_loss: 0.0067 - val_mse: 5.8407\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0054 - mse: 4.7432 - val_loss: 0.0067 - val_mse: 5.8339\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.0054 - mse: 4.7362 - val_loss: 0.0067 - val_mse: 5.8279\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0054 - mse: 4.7290 - val_loss: 0.0067 - val_mse: 5.8226\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0054 - mse: 4.7217 - val_loss: 0.0067 - val_mse: 5.8180\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0054 - mse: 4.7143 - val_loss: 0.0067 - val_mse: 5.8135\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0054 - mse: 4.7071 - val_loss: 0.0067 - val_mse: 5.8085\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0054 - mse: 4.6999 - val_loss: 0.0067 - val_mse: 5.8026\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0054 - mse: 4.6926 - val_loss: 0.0067 - val_mse: 5.7960\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0054 - mse: 4.6851 - val_loss: 0.0066 - val_mse: 5.7896\n",
      "Epoch 276/1000\n",
      "\n",
      "Epoch 00275: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0054 - mse: 4.6778 - val_loss: 0.0066 - val_mse: 5.7834\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0054 - mse: 4.6705 - val_loss: 0.0066 - val_mse: 5.7794\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0054 - mse: 4.6653 - val_loss: 0.0066 - val_mse: 5.7755\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0054 - mse: 4.6600 - val_loss: 0.0066 - val_mse: 5.7721\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0053 - mse: 4.6548 - val_loss: 0.0066 - val_mse: 5.7687\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0053 - mse: 4.6496 - val_loss: 0.0066 - val_mse: 5.7653\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0053 - mse: 4.6444 - val_loss: 0.0066 - val_mse: 5.7617\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0053 - mse: 4.6392 - val_loss: 0.0066 - val_mse: 5.7578\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0053 - mse: 4.6339 - val_loss: 0.0066 - val_mse: 5.7537\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0053 - mse: 4.6287 - val_loss: 0.0066 - val_mse: 5.7496\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0053 - mse: 4.6234 - val_loss: 0.0066 - val_mse: 5.7453\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0053 - mse: 4.6182 - val_loss: 0.0066 - val_mse: 5.7411\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0053 - mse: 4.6129 - val_loss: 0.0066 - val_mse: 5.7370\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0053 - mse: 4.6077 - val_loss: 0.0066 - val_mse: 5.7329\n",
      "Epoch 290/1000\n",
      "\n",
      "Epoch 00289: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0053 - mse: 4.6024 - val_loss: 0.0066 - val_mse: 5.7289\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0053 - mse: 4.5972 - val_loss: 0.0066 - val_mse: 5.7262\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0053 - mse: 4.5936 - val_loss: 0.0066 - val_mse: 5.7235\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0053 - mse: 4.5899 - val_loss: 0.0066 - val_mse: 5.7209\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0053 - mse: 4.5863 - val_loss: 0.0066 - val_mse: 5.7183\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0053 - mse: 4.5826 - val_loss: 0.0066 - val_mse: 5.7156\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0053 - mse: 4.5789 - val_loss: 0.0066 - val_mse: 5.7130\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0053 - mse: 4.5753 - val_loss: 0.0066 - val_mse: 5.7104\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0052 - mse: 4.5716 - val_loss: 0.0066 - val_mse: 5.7078\n",
      "Epoch 299/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0052 - mse: 4.5679 - val_loss: 0.0066 - val_mse: 5.7052\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0052 - mse: 4.5642 - val_loss: 0.0065 - val_mse: 5.7027\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0052 - mse: 4.5606 - val_loss: 0.0065 - val_mse: 5.7000\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0052 - mse: 4.5569 - val_loss: 0.0065 - val_mse: 5.6972\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0052 - mse: 4.5532 - val_loss: 0.0065 - val_mse: 5.6943\n",
      "Epoch 304/1000\n",
      "\n",
      "Epoch 00303: reducing learning rate to 0.000282475253334.\n",
      "0s - loss: 0.0052 - mse: 4.5495 - val_loss: 0.0065 - val_mse: 5.6913\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0052 - mse: 4.5459 - val_loss: 0.0065 - val_mse: 5.6892\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0052 - mse: 4.5433 - val_loss: 0.0065 - val_mse: 5.6871\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0052 - mse: 4.5407 - val_loss: 0.0065 - val_mse: 5.6851\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0052 - mse: 4.5381 - val_loss: 0.0065 - val_mse: 5.6830\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0052 - mse: 4.5356 - val_loss: 0.0065 - val_mse: 5.6811\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0052 - mse: 4.5330 - val_loss: 0.0065 - val_mse: 5.6792\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0052 - mse: 4.5304 - val_loss: 0.0065 - val_mse: 5.6773\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0052 - mse: 4.5278 - val_loss: 0.0065 - val_mse: 5.6755\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0052 - mse: 4.5253 - val_loss: 0.0065 - val_mse: 5.6736\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0052 - mse: 4.5227 - val_loss: 0.0065 - val_mse: 5.6717\n",
      "[{'loss': 0.0077854436822235584, 'dropout': 0.0, 'val_mse': 10.15809154510498, 'width': 75, 'mse': 6.7798690795898438, 'val_loss': 0.011664718389511108}, {'loss': 0.0059745172038674355, 'dropout': 0.0, 'val_mse': 9.1544342041015625, 'width': 90, 'mse': 5.202843189239502, 'val_loss': 0.010512201115489006}, {'loss': 0.0047149923630058765, 'dropout': 0.0, 'val_mse': 6.7898726463317871, 'width': 105, 'mse': 4.1059994697570801, 'val_loss': 0.007796931080520153}, {'loss': 0.0069261062890291214, 'dropout': 0.0, 'val_mse': 9.2075967788696289, 'width': 120, 'mse': 6.0315232276916504, 'val_loss': 0.010573248378932476}, {'loss': 0.0051935072988271713, 'dropout': 0.0, 'val_mse': 5.6717453002929688, 'width': 135, 'mse': 4.5227084159851074, 'val_loss': 0.0065129664726555347}]\n"
     ]
    }
   ],
   "source": [
    "width_patience=2 # if increasing width this nr of times did not improve val_loss, stop\n",
    "width3_perf = []\n",
    "prev_weights = []\n",
    "for width in range(75, 150, 15):\n",
    "    fit, results, prev_weights = run_depth1(width, learningrates2, prev_weights, 'smallsteps')\n",
    "    width3_perf.append(results)\n",
    "    print width3_perf\n",
    "    if len(width2_perf)>width_patience and width2_perf[-1]['val_mse']>np.min(x['val_mse'] for x in width2_perf[-width_patience:-1]):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2037610>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAENCAYAAADOhVhvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XlYVdX+x/H32ocZBGQWEM0ppUErMuc5h/x1LSur261s\ndLilqZlajqmZFmqTV7vXrMwGu2qmpWapqVnmnImZljggiIAIgiCw1++Po6RdTURgn+H7ep6eJ/Fw\nznd5NnzO/u6111Jaa40QQghRDobVBQghhHBeEiJCCCHKTUJECCFEuUmICCGEKDcJESGEEOUmISKE\nEKLcJESEEEKUm4SIEEKIcpMQEUIIUW4SIkIIIcrNw+oCqsKRI0eq7LXCwsLIyMiosteraq48Plce\nG8j4nF1Vjy86OrpMj5MzESGEEOUmISKEEKLcJESEEEKUm4SIEEKIcpMQEUIIUW4SIkIIIcpNQkQI\nIUS5uUWIHD9VbHUJQgjhktziZsPeC/dRO9ibJjX8aVLDn/hwX7w93CI/hRCiUrlFiDzUJJztaXks\n3XOcz3Zn4Wko4iN8aRJlD5Xa1b0xlLK6TCGEcDpuESJ3XRPKXdeEUlhssis9n+2peWxPy+e97cd4\nb/sxgrxtNI7yp0kNP5rU8CfUz9PqkoUQwim4RYic5e1hcGN0ADdGBwCQdaqYHal5Z0Ilj7UHcgCo\nGeRVepZyTYQfvp7S+hJCiAtxqxD5sxBfD9rXCaJ9nSC01hzILmR7Wh7bU/NZsS+bJXuO42FAwzDf\n0uspdar7YDOk9SWEEODmIXIupRS1q/tQu7oPdzQK5XSJye5jp+xnKal5fLAjgw92ZFDNy+D6M2cp\nTaL8iQiQ1pcQwn1JiFyEl82gcZQ/jaP8efgGyC4o5qe0/NJQ+e5gLgDR1bzs11Ki/Lkuys/iqoUQ\nompJiJRRsI8HbWoH0qZ2IFprDuecLg2UVb+f4MtfszEUXBt1lGvC7ddU6odK60sI4dokRMpBKUXN\nIG9qBnlze8MQiko0ezLsra+fMwr5+KcMPvopA39Pg+ui/Eov0teo5mV16UIIUaEkRCqAp01xbaQf\n10b6ERYWxu8pR9mZlnfmIn0ePxw6CUBkgOeZQPHj+kh/ArxtFlcuhBBXRkKkEgR622hZK5CWteyt\nr9TcIral5rEjLY+1yTms2GdvfdUL8Smd9XV1mC8e0voSQjgZCZFKppQiOtCL6EAvul9dnWJTszfj\nFNvT8tiWms9/d2Uy/+dMfDwMrov0K71IHxPohZK76IUQDk5CpIp5GIpGEX40ivDj/uvh5OkSdh7N\nt9/0mJbHphR76yvMz6N0GnHjKD8CfeStEkI4HvnNZLEALxvNa1ajec1qABw9eZrtqflsS83j+0O5\nfP3bCRRQJ8SHJlH2ZVkahfviaZO76IUQ1pMQcTCRAV50qe9Fl/rBlJiafVkF7EjNY1tqHp/tzmJB\nUhbeZy7kNz4z6ysuSFpfQghrSIg4MJuhuDrMl6vDfOl1XRj5RSX8fDSf7Wn29tc7W9MBqO7rUXqW\n0iTKn2BfeVuFEFVDfts4ET9PG01jq9E01t76OpZXxI40+1nK5iN5rN5vX0BS9k4RQlQVCREnFu7v\nSae6wXSqG4ypNb9n2ReQ3JF6gb1TzpylyN4pQoiKJCHiIgylqBfqQ71QH+6+JpSCYpOkdPsF+h2p\n+by37RjvcWbvlBr+pe0v2TtFCHElJERclM+f9k7JzC9iR9ofU4nXJp+zd8qZs5RrI/3wkdaXEOIy\nSIi4iVA/TzrUCaLDOXunbDuzw+OKvdks+eXM3inhfqVnKbJ3inAWptaYWltdhluSEHFD5+6dcme8\nfe+UpPRTpTs8yt4pwpkcyC7k5bUpxFQ/yvCWkbJ8UBWTEBF42YzS2Vxg3ztlx5mzlHP3TokN9GJw\nBxt1/a2sVog/rD+Qw+vfp+JhUxw5mM17forHboq0uiy3IiEi/kewjwdtrwqi7VX21tehnNPsSM3j\nq33ZDPlsF/dfH8Y914bKLC9hmRJT88GOYyxMyuLqMF+GtY5m2f58Pt2eSr0QH9peFWR1iW5DQkT8\nJaUUcUHexAV507leMLN3HOfDn47xa8YpBrWIluXsRZXLKSjm1e+OsCMtn671g3n8pkg8bYqnWl3F\nrpRs3tyYRlywN1dV97G6VLcgU3FEmXl7GIzq3IA+N0eyPS2PwcuT+T2rwOqyhBv5PauAIcuT2ZV+\niqebRdGvaRSeNvsZsYfN4LnWMQR42Zi0NoXcwhKLq3UPEiLisiiluK1BdV66tRbFJZphXx3gm9+y\nrS5LuIHVv59g2FcHKNEw6dY4OtUN/p/HVPf1YFjrGDLzi5j63RFKTJmxVdkkRES5XB3my9TbatMw\nzJfXf0hjxsY0ikpMq8sSLqjY1Px781Gmf59Kg1AfpnarTYMw34s+vmG4L08kRLI1NY+Pfsqowkrd\nk1wTEeUW7OPB2A41mbfjGAuSsvj9eAHDWscQ7i9TgUXFyD5VzJT1KexKP8XtDavT+4aIMk3h7VIv\nmL2ZBXy6K5N6oT40O7PVgqh4ciYirojNUDx0QwQj2sSQknOaQcuS2Z6aZ3VZwgX8mnGKwcuS2ZtZ\nwKAWNXj8prLfA6KUos/NkdQP9WH6hlQOnyis5Grdl1OFyI8//sjMmTOZMmUKO3bssLoccY5mNavx\natfahPh4MHbVIT79OUPuIBbl9tW+bEasPIjNUEzuXIt25Ziy62UzGNY6Bi+bYtLaFPKL5EJ7Zaiy\ndtaMGTPYunUrQUFBJCYmln59+/btzJkzB9M06dixI3fcccdFn6Np06Y0bdqUkydPMnfuXBo3blwV\npYsyign0YkrXWry1MY0PdmSwJ6OAZ1rUIMBLpgGLsikqMfn35nRW7MumSZQfQ1rFEHgF08jD/T15\ntlU0Y1Yd4rXvUxneOkY2cKtgVRYi7dq1o2vXrrz11lulXzNNk9mzZzNy5EhCQ0MZMWIECQkJmKbJ\nhx9+eN739+vXj6Ag+6eRhQsX0qVLl6oqXVwGHw+DwS1qcHWYD+9sSWfIsmRGtImhtszZF5eQmV/E\n5HUp7MkooGd8CP9oHF4ha7ddH+VP7xsieGdrOgt2ZXH3taEVUK04q8pCJD4+nvT09PO+tm/fPqKi\nooiMtC9T0KJFCzZt2sSdd97J8OHD/+c5tNbMmzePJk2aUKdOnSqpW1w+pRT/d3UIdUN8mLLuCENX\nHKB/0yja15G7iMWFJaXnM3ldCgXFJs+1jqZlXGCFPv/fGlZnb+YpPthxjDoh3qWrW4srZ+nsrKys\nLEJD//hUEBoayt69ey/6+GXLlrFz507y8/NJS0ujc+fOF3zc119/zddffw3Ayy+/TFhYWMUW/hc8\nPDyq9PWq2uWMr3UYXBMXxehlvzD9+1QOnNQMaFMHLwddbl7eu6qntWbhT6m8tvYQ0YHevPF/jagT\nWr7F2S41vjHdQ+jzyQ6mbkjjnfubEB3kXGfHjvj+gcUhoi9w4fWv+pW33XYbt9122yWft1OnTnTq\n1Kn0zxkZVTdXPCwsrEpfr6qVZ3yj2tRg7nYPFu1MY9eRbIa1iSHMATfDkveuahUWm8zclMaq33NI\niPZnUMtoAvQpMjJOlev5yjK+oS2jGLI8mecW72Ry51pOtXV0Vb9/0dHRZXqcpf+CoaGhZGZmlv45\nMzOT6tWrW1iRqAw2Q9H7xgiGtY7m4InTDP4ymZ/SZBqwO0s/WcSIlQdY9XsO910XygvtYqtkAkaN\nal4MaRFN8vFC3tqYdsEPsuLyWBoidevWJTU1lfT0dIqLi9mwYQMJCQlWliQqUYu4QBK71qKat40x\nqw6xYFem/BC7oZ/S8hiyPJnU3CKebxvD/deHV+mK0DfFBPD368P4NjmHpXuOV9nruqoqa2dNnz6d\npKQkcnNz6du3L7169aJDhw48+uijTJw4EdM0ad++PTVr1qyqkoQFYoO8ebVrbd7cmMr724+xJ+MU\nA5vXwF+mAbs8rTWLf8nivW3HiK7mxYi2McQGeltSy93XhrI3q4B3tqZTp7oP10T6WVKHK1DaDT4K\nHjlypMpey9H6zhWtosantWbJnuO8uzWdyABPhrW2fhqwvHeVp6DY5M0fUll3IJfmNQMY0LwGfp4V\n+8HhcseXd7qEZ5cfIK+ohGndahPqgNfpziXXRIQ4h1KKvzUMYUKnOE4VmTy34gDf7j9hdVmiEqTm\nnua5FQdYfyCXBxuHM6x1TIUHSHn4e9kY0TaGwmLNy2tTZAHRcpIQEZaKj/Bj6m1XUTfEh6kbUnl7\n81GKSlz+5NhtbD1ykiHLk8nML2J0+1juvjbUoe4YjwvyZmDzKH7NLODfm9Mv/Q3if0iICMuF+How\nvlMcf2tYnS/2HOeFrw+SmV9kdVniCmit+fTnDF5cfZhwP08Su9Z22Bv8WsQFcld8CCv2ZfPVPtkb\n53JJiAiH4GEoHrspkqGtojmQXcCgZcnsPCrTgJ1RflEJL69L4YMdGbSuFcjkLrWIquZldVl/6YHG\n4TSJ8mPWpqP8Ws77VNyVhIhwKK1qBfJK19pU87Ix+ptDLEySacDO5HBOIUOXH+DHwyd59MYIBres\ngY8T3NBnMxRDWsUQ4uvBy+tSyC4otrokp+H4765wO3FB3rzStRbNalbjvW3HmLxOlvF2BhsP5fLs\nsgPkFJYwrkNNejQKcajrH5cS6G1jRJsYcgtLeGVdimytW0YSIsIh+XnaeK5VNI/cGM7Gwyd5dvkB\nDmbLxkKOyNSaeTuO8dLaFKIDvZjarTbXR5Vv/Sur1QnxoX/TKH5OP8W72+RCe1lIiAiHpZTijkah\njO8YR97pEoauSGZdco7VZYlznDxdwoQ1h5n/cyYd6gQx6dY4p98euX2dILpfXZ3Pfzku087LQEJE\nOLxrI/2Y2q02tYN9ePW7I/xny1GKpdVguQPZhTy73L4dcp+bIxnQLMqpFjT8K4/eGEF8uC9vbkwj\n+XiB1eU4NNd4x4XLC/XzZEKnOP7v6uos+eU4o74+SNYpufhple8O5PDcimQKikwmdIrjtgbVner6\nx6V4GIrnWscQ4GVj0toUcgvlmtzFSIgIp+FpUzyREMmQltH8llXAoC/3s+tovtVluZUSU/PetnSm\nrD9CrWAfErvVJj7CNdedqu7rwbDWMWTkFzH1uyNyof0iJESE02lT2z4N2M/TxshvDrJ4d5ZMA64C\nOYUljFt9iIVJWXStH8zETnEOv97UlWoY7ssTCZFsTc3j452uu67alZAQEU6pVrA3id1q0TQ2gHe2\npvPK+iMyDbgS/Z5VwJBlyexKP8VTt0TRr2kUnjbXaV/9lS71gulUN4j5P2fyw6Fcq8txOBIiwmn5\nedoY3jqGh28I5/tDuQxdfoBDJ2QacEVbs/8Ew746QInWTLo1jlvrBVtdUpVSStHn5kjqh/owfUMq\nh+UYO4+EiHBqSil6xocyrkNNcgvtS3t/d1CmAVeEYlPz781HmbYhlQahPkztVpsGYb5Wl2UJL5vB\nsNYxeNkUk9bKza/nkhARLuH6KH+m3labWsFeTFl3hDlb0+VC6BXIPlXM6G8OsnTPcW6/ujrjOsYR\n7FNle9g5pHB/T55tFc2R3NO8/n2qXIc7Q0JEuIwwP08mdqrFbQ2C+Wx3FqO+OchxmQZ82X7NOMXg\nZcnszSxgUIsaPJ4QiYfhHtc/LuX6KH963xDB94dOsiApy+pyHILLhsjmzZuZNWuW1WWIKuZpU/S5\nOYpBLWrYfwkuS2Z3ukwDLquV+7IZsfIgNkMxuXMt2l0VZHVJDudvDavTulY15u04xrZUWWnaZUMk\nISGBPn36WF2GsEi7q4J4pUstfDwUL3x9kCW/yDTgv1JUYjJjYxpvbkzjmghfErvVpk6ItdsVOyql\nFE81q0HNIG9eXZ/C0ZOnrS7JUi4bIkLUru7Dq11rkxATwH+2pJP43RFOFckWqH+WmV/EC18fZMW+\nbHrGhzCmfU0Cva3fvtaR+XgYjGgTgwYmrU2hsNh9jysJEeHSArxsDG8Tw4ONw/nuYC5DVyRzOEem\naJ6VlJ7P4GXJHMgu5LlW0Tx8QwQ2uf5RJjWqeTGkRTTJxwuZsTHNbc90JUSEyzOU4u5rQxnTviYn\nCkp4dtkBvj/o3jeNaa35Ys9xRn59EF9PgyldatOyVqDVZTmdm2ICuP/6MNYk57B0z3Gry7GEhIhw\nG01q+DO1W21ig7x4eV0K77rpNODCYpPXf0jl7c1HuaGGP692rU2tYG+ry3Ja91wbStPYAOZsTXfL\ntdwkRIRbCff3ZNKtcXSrH8yi3VmMXnWIbDeaBnwsr4gRKw+y6vcc7r0ulBfaxRLgJdc/roShFM80\nr0FkgBeT16eQmV9kdUlVSkJEuB1Pm0HfplEMbF6DXzNOMWhZMr8cO2V1WZVuy6FsBi9L5kjOaZ5v\nG8Pfrw/HcKHl263k72VjRNsYCos1L69NoajEfS60S4gIt9WhThCTO9fCy6Z44esD/HfHEZe8OKq1\nZvHuLJ5Z9DOB3jZe7VaLW2KrWV2Wy4kL8mZg8yh+zSzg35vdZ2tdCRHh1uqE+JDYtTY31PBn2prf\nmbYhlQIXmq5ZUGyS+N0R3tmaTus6obzStRaxgXL9o7K0iAukZ3wIK/Zls3JfttXlVAkJEeH2Arxt\nPN82lieax7E2OYfnlh/gSI7z30CWmnuaYSsOsP5ALg82Dmdi94b4ecr1j8r2j8bhNInyY+amo/ya\n4fptUgkRIbBfHO3dNI7R7WPJOlXEkOXJbHTivSO2HjnJkOXJZOQXMbp9LHdfG+pS29c6MpuhGNIq\nhhBfGy+vSyG7wLUnbkiICHGOG6MDmNrtKqKrefHS2hTe3+Zc04C11nz6cwYvrj5MuJ8niV1rc2N0\ngNVluZ1Abxsj2sSSW1jCK+tde2tdCREh/iQiwJNJnePoUi+YBUlZjF19iBNO8Gkyv6iEl9el8MGO\nDFrVqsbkLrWIquZldVluq06ID/2bRvHz0Xze3ea6F9olRIS4AC+bQf9boni6WRS70+3TgPc4cH/7\ncE4hQ5cf4MfDJ3n0xgiGtIzGx0N+vK3Wvk4Q3a+uzue/HGdtsmtuliZHmRB/oVPdYCZ3qYVNKZ5f\neYBlvx53uGnAGw/btwbOKSxhXIea9GgUItc/HMijN0YQH+7LGz+kkny8wOpyKpyEiBCXUDfEvjVs\n4yh/Zm46ymvfpzrEqq2m1nz40zFe+jaFGtW8mNqtNtdH+VtdlvgTD0PxXOsYArxsTFqbwslC19pa\nV0JEiDKo5m1jZLtY7r8ujDX7c3huxQFSc62bBnzydAkT1xzmk52ZdKgTxKRb4wj397SsHvHXqvt6\nMKx1DBn5RUzd4FoX2iVEhCgjQynuuz6MUe1iycgvYsiyZH48XPXTgA9mF/Ls8mS2pebR5+ZIBjSL\nwluufzi8huG+PH5TJFuO5PHxzgyry6kwcuQJcZluiglgarfaRFXzZOK3KXyw/ViVfbL87kAOQ1ck\nc6rIZEKnOG5rUF2ufziRrvWD6VQ3iPk/Zzr1fUjnkhARohwiA7x4uXMtOtUN4tNdmby4+hA5lTgN\nuMTUvLctnSnrj1Ar2Jup3WoTH+FXaa8nKodSij43R1IvxIdpG1JdYoM0CREhysnLZvB0sxr885Yo\ndqWfYvCyZPZmVvw04JzCEl5cfYiFSVl0qRfMxE5xhPrJ9Q9n5WUzGN4mBk+bYtK3KeQXOfeFdgkR\nIa5Q53rBTOoch1Iw/KuDrNibXWHTgH/PKmDIsmR+Tj/FU7dE0f+WKDxt8mPr7ML9PRnaKpojuad5\n/ftUh5s2fjnkaBSiAtQP9SWx21VcF+nHjB/TeOOHtCueBrxm/wmGfXWAElMz6dY4bq0XXEHVCkdw\nfZQ/vW+I4PtDJ1mQlGV1OeXmYXUBQriKQG8bo9rF8vHODOb/nMn+4wUMax1z2UuPFJuad7ems2TP\nca6J8OW5VjEE+8qPqiv6W8Pq7M08xbwdx6gb4sMNNZzvPh85ExGiAtkMxQONwxnZNpajeUUMXp7M\n5pSTZf7+7FPFjP7mIEv2HOf2q6vzYsc4CRAXppTiqWY1qBnoTeL6FI6edL4tCFw2RDZv3sysWbOs\nLkO4qZtjA5jatTYR/p6MX3OYD3+69DTgXzPOXpwvYFCLGjyeEImHIdN3XZ2Ph8GItjGYwKS1KQ6x\nGsLlcNkQSUhIoE+fPlaXIdxYVDUvJneuRYc6gXyyM5MJaw6Tc5ElL1buy2bEyoPYDJjcuRbtrgqq\n4mqFlWpU82Jwi2iSjxcyY2OaU11od9kQEcIReHsYDGhWg35NI/npaD5Dlu1nX+Yfi/AVlZjM2JjG\nmxvTuCbCfnG+ToiPhRULqyTEBHD/9WGsSc7hi1+PW11OmUmICFHJlFJ0rV+dSbfGYWoY/tUBVu7L\nJjO/iBe+PsSKfdn0jA9hTPuaBHrL9rXu7J5rQ2kaG8A7W9LZlZ5vdTllIiEiRBVpEObLtG61iY/w\n5c2NafRf8jsHsgsY2iqah2+IwCbXP9yeoRTPNK9BZIAXk9elkJlfZHVJl+QWIaK3b7S6BCEACPTx\nYEz7mvS6NpS4IG+mdKlNq1qBVpclHIi/l40RbWMoLDaZvC6FohLHvtDuFiFivjUR893X0aec4/RQ\nuLaz04Bf6VqbWsHeVpcjHFBckDcDmtdgT0YB/97s2FvrukWIqG53ozeswhw3AP3rz1aXI4QQl9Qy\nLpCe8SGs2JfNyn3ZVpdzUW4RIkbPhzCeewkMA/PVFzA/nYMucr6beoQQ7uUfjcNpHOXHzE1H2Z3m\nmEvHu0WIAKh68RijX0O17oL+ahHmxCHoQ/utLksIIS7KZiiebRVDiK+N57/YTXYlbjdQXm4TIgDK\nxxfjwf4YA0bDyRzMiUMwl/0XbTr3UsxCCNcV6G1jRJtYsk8V88p6x9ta161C5Cx1XQLGmDegSVP0\nwvcxp4xAp6daXZYQQlxQnRAfhnWsx89H83lvm2NdaHfLEAFQ1QIx+gxDPTYYjhzCfHEg5trlTrXc\ngBDCfXRtFEH3BsEs/uU4a5NzrC6nlNuGCNjvJDaatcMY+zrUuRo9dwbmG+PR2c67tr8QwnU9elMk\n8eG+vPFDKsnHCy79DVXArUPkLBUSjvHMONR9T8AvP2GOexq95TuryxJCiPN4GIrnWsfg72Vj0toU\nTl5kQc+qJCFyhjIMjI63Y4yaDqGRmDMnY/4nEZ1f9r0ghBCislX39WB46xgy8ouYusH6C+0SIn+i\nasRiDJ+Cuv1+9KZ1mGMHoJO2W12WEEKUahjuy+M3RbLlSB4f78ywtBYJkQtQHh4Yf7sfY/gr4O2N\nOW005kdvowsLrS5NCCEA6Fo/mE51g5j/cyYbD1l3I6KEyF9QV9XHGDkd1fF29KqlmBOeQe/fa3VZ\nQgiBUoo+N0dSL8SHaRtSOZxjzYdcCZFLUN7eGPc9gTHoRSgsxHx5KObnH6GLHe/OUSGEe/GyGQxv\nE4OnTTHp2xTyi6r+QruESBmp+CYYY19HNW2DXvIR5uRh6NTDVpclhHBz4f6eDG0VzZHc07z+fdVv\nrSshchmUXwDGY4Mx+g6DY2mY45/B/GYp2nTs9f6FEK7t+ih/Hr4hnO8P5bIgqWrvc5MQKQd1U0uM\nsW/A1dehP34bc/oYdNYxq8sSQrixHg1DaFWrGvN2HGNbal6Vva6ESDmp4BCMAaNR/+gPv/2COXYA\n5g9rZNkUIYQllFI83awGNQO9SVyfwtGTVbPdhYTIFVBKYbTtijHmNYiuiZ49lROvjESfdJx1bYQQ\n7sPHw2BE2xhMYNLaFAqLK7/VLiFSAVRENMZzk1B3PkjhpnWYY59G79xsdVlCCDdUo5oXg1tEk3y8\nkBk/Vv6FdgmRCqIMG8Zt9xAy5T8QEIj5+ouYc2egC05ZXZoQws0kxARw3/VhrNmfwxe/Hq/U15IQ\nqWCeVzXAeCER1flO9LoVmC8ORO/bbXVZQgg30+vaUJrGBvDOlnR2pedX2utIiFQC5emFcc8jGM9O\nBNPEnDICc+H76OIiq0sTQrgJQymeaV6DyABPpqxLITO/cn7/uGyIbN68mVmzZllag2pwLcaY11Et\nO6KX/Rdz4rPolAOW1iSEcB/+XvatdQuKTSavS6GopOIvtLtsiCQkJNCnTx+ry0D5+mE8/DTGP1+A\nE1mYEwZhrlgk+7oLUUG0aWIu+oCTn8yW5YguIC7YmwHNa7Ano4D/bKn4rXU9KvwZxQWpJrdg1Lna\nfrH9v3PQP/2I0XsgKjzK6tKEcFraNNEfzECv+4o8gM0bMJ54FhUSbnVpDqVlXCA94wtYmJRF/VAf\nOtUNrrDndtkzEUekAoMx+o9A9R4IB3/HHDcQc/1KuUFRiHI4N0BU914EDh4Lh5Ixxz8jU+wv4B+N\nw2kc5ce/fjzK3syKmzUqIVLFlFIYLTval02pXQ/93huYb01E51TuNDwhXMl5AXJbL1SPB/Bt3Rlj\n5FQIDrVPsV/wnrS3zmEzFM+2iiHE1761bnZBxfzbSIhYRIVGYAwej7rnUdi1zb6D4rYfrC5LCIen\nTRM9719nAuQe1B0PoJQCQEXFYIx4BdWmC3r5AszEF9BZ1u7850gCvW0MbxNLbmEJr6yvmK11JUQs\npAwDo/MdGCOnQfVQzBkvYc55DX2q8uZ0C+HM7AEyE712xZkA+UdpgJylvLwxHvwn6vEhZ9pbA9E7\nt1hUseOpG+JDv6ZR/Hw0n/e2XfmFdgkRB6Bi4uyfnm7rhf5+Nea4Aeg9O60uSwiH8keALEd1u/uC\nAXIu45a2GCMTz7S3xmEufA9dIrMiATrUCaJ7g2AW/3KctclXttafhIiDUB6eGHf+A2PYy2CzYSaO\nxJw/G11UNStxCuHItGmiPzwbIHeh7nzwLwPkLBUV+0d7a9kCzFelvXXWIzdGEh/uy5s/pJJ8vKDc\nzyMh4mBU3YYYo19Dte2KXrkYc/wg9MHfrC5LCMto00R/NAv97dkAeahMAXJWaXvrscFw6Hf77K2f\npb3laVM81zoGPy/7hfaTheU7S5MQcUDK2wfjgX4YA8dAfh7mS89ifjFfTsWF29Fa2wNkzTJU18sP\nkHMZzdobPoBlAAAgAElEQVTZZ28FVcd8bZx9KSI3/5mq7uvB8NYxZOQXMXXDEcxy3G4gIeLA1LU3\n2fd1v6E5+rMPMKcMRx89YnVZQlQJrbW9hbVmGapLT1TP8gfIWSoqFuP5V1GtO9uXInr1BfTxzAqq\n2Dk1DPfl8Zsi2XIkj49+uvxWn4SIg1MBgRh9nrPPNEk7jPniQMw1X8oNisKl2QNk1h8BctfDVxwg\nZykvb4yHnvqjvfXiQLdvb3WtH0zHOkHM/zmTjYdzL+t7JUSchHFLW4yxb0K9ePS8mZivj0Nnu/cn\nKOGa/giQL1Fd7qzQADmX0awdxgvntLcWzXXb9pZSir5NI6kX4sP0Dakcziks8/dKiDgRVT0U45mx\nqL/3gV9/tu/rvmm91WUJUWH+uAbyJarznai7eldKgJylasRijDjT3vryU/vNiW7a3vKyGQxvE4OH\noZj0bUqZv09CxMkopTDad8cY9RpE1EC/PQXz36+i805aXZoQV8QeIG+jV58JkLsrN0DOUt5n21uD\n7GvavTgQ/fPWSn9dRxTu78nQVtEcyS37rQUSIk5KRcVgDJuM6vF39Jbv7Pu6J22zuiwhyuWPAPkC\n1fmOKguQcxnN2p/T3hrrtu2t66P8eenWuDI/XkLEiSmbDeP/7sMY8Qr4+GJOG4P54Ux0Ydn7mUJY\nTWuN/vjf9gC5tQfq7keqPEDOkvaWXaNwvzI/VkLEBaha9TBGTUN1+ht69Zf2m6l+32N1WUJcktYa\n/cl/0KuW2gPknkctC5CzpL11eSREXITy8sa493GMweOhqBBz8jDMxfNkKWzhsEoD5JslqE6OESDn\nkvZW2UiIuBjVqLF9X/db2qKXfoI5aSj6yEGryxLiPOcHyN9QvRwrQM4qbW+1utXe3po6UqbW/4mE\niAtSfgEYjw7C6DscstIxxw/C/Hox2jStLk0Ie4DMn31OgDzmkAFylvL2xnj4adSjgyB5H+aLz6B3\nySSWsyREXJi6qYX9BsX4JuhPZmNOG43OPGZ1WcKNlQbI15+jOt7u8AFyLqN5e/vaW4HBZ9pbH0h7\nCwkRl6eCqmM8NRL10FOwfy/muKcxv18ty6aIKmcPkHf+CJB7H3eaADlL1ahpb2+17IT+cr60t5AQ\ncQtKKYzWnTHGvAYxtdHvTMOc+TI698o2oxGirLTW6E/fQX+92GkD5KzS9tYjz0h7CwkRt6LCozCG\nTkTd9TDs2IQ59in0jk1WlyVcXGmArHT+ADmX0aKDvb1VLcje3vrMPdtbEiJuRhk2jK532bcNrRaE\n+eZ4zPffRBfIvu6i4mmt0f+dYw+QDv/nMgFylqpRE+P5RFSLjugv5mNOHeV27S0JETelYq/CeGEq\nqutd6PUrMccNRO9Nsros4ULsAfIu+qvPUO27o+57wqUC5Czl7Y3Re8CZ9tZee3vLjZYgkhBxY8rT\nE+OuhzGGTgLAfGUE5n/fRRcVWVyZcHZ/BMgie4Dc/6RLBsi5jBYdMF44c4Y/faz9Zl/T9dtbEiIC\nVT8eY8xr9huqVizEfGkI+vB+q8sSTkprjV5wNkBuc4sAOUtFx/3R3lr6CebU0ejsLKvLqlROGSKH\nDx/m7bffJjExka+++srqclyC8vHDeOgpjKdGQU425oQhmMsWuMUnKVFx7AHyHnrFIlS721D393Gb\nADnrvPbW/l/ta2+5cHvLoywPysvLY+bMmRw6dAilFP369aNBgwaX/WIzZsxg69atBAUFkZiYeN7f\nbd++nTlz5mCaJh07duSOO+646PPExsby5JNPYpoms2bNuuw6xMWpxjdjjH0D84MZ6IXvoX/ahPHo\nM6jwKKtLEw7ujwBZaA+Qv7tfgJzLaNEBXbse5szJmNPHorr3Qt1+H8qwWV1ahSpTiMyZM4cmTZow\nZMgQiouLKfzTUuMnTpzAy8sLX1/f0q+lpaURFXX+L5527drRtWtX3nrrrfO+bpoms2fPZuTIkYSG\nhjJixAgSEhIwTZMPP/zwvMf269ePoKAgNm/ezGeffUbXrl0va8Di0lS1IIy+w9E/rEF/NAtz3EDU\nvY+hWt1qdWnCQWmt0QvfPxMg3dw+QM5S0XEYLyTad2tc+gl6bxLG40NQwSFWl1ZhLtnOys/PZ/fu\n3XTo0AEADw8P/P39z3tMUlISU6ZM4fRp+25YX3/9NXPmzPmf54qPjycgIOB/vr5v3z6ioqKIjIzE\nw8ODFi1asGnTJuLi4hg+fPh5/wUFBQGQkJDAhAkTWLdu3QXr3rx5s5ylXAGllH2ZhzFvwFX10e+/\nifnmBEpk2RTxJ1pr9KL30csX2APEDVtYf0V5+2D0Hoh6ZOA57a3tVpdVYS55JpKenk5gYCAzZszg\nwIED1KlTh969e+Pj41P6mObNm5Oens706dNp3rw5q1evZtSoUWUuIisri9DQ0NI/h4aGsnfv3os+\nfteuXWzcuJHi4mJuuOGGCz4mISGBhISEMtcgLkyFhmMMehG9ail64ftkPnU/9Pi7fcaNzbVOy8Xl\nKw2QZQtQbbvaA8Rwykutlc5o0RFdu/6Z9tYYVPd7Ubff6/TtrUu+2yUlJezfv5/OnTszZcoUvL29\n+eyzz/7ncT169MDLy4v//Oc/DBs27LyQuZQLreP0V59krrnmGh599FGefPJJaWdVAWUYGJ3+hjH2\nDTwbXYf+5D+YLz2L3n/xoBeuzx4gc+0B0qYr6u99JUAu4Wx7SzXvgF76sUvM3rrkOx4aGkpoaCj1\n69cHoFmzZuzf/7/TP3fv3s2hQ4e4+eab+fTTTy+riNDQUDIz/7jLMzMzk+rVq1/Wc4jKpyJqEDxq\nKkaf5+DEccxJz2LOm4nOP2l1aaKKaa3Rn32AXvZfe4A8IAFSVsrbB+ORgajeA2H/Hqdvb13yXQ8O\nDiY0NJQjR44AsHPnTmJjY897zP79+5k1axZDhw6lf//+nDx5ko8//rjMRdStW5fU1FTS09MpLi5m\nw4YN0opyUEopVEIrjPEzUB3+D/3tcsxR/TE3fisrA7uJ0gD58lNUmy4SIOVktOyI8fxUCAjEnD4G\nc/GHTjmlXuky/OQnJyczc+ZMiouLiYiIoH///uddIP/ll1/w8/MjLi4OgOLiYtasWUOnTp3Oe57p\n06eTlJREbm4uQUFB9OrVq/SC/datW3nvvfcwTZP27dvTs2fPChvk2QCsCmFhYWRkZFTZ61W1P49P\nH9iHOXcGHNgH8U0w/t4XFRltYYXl527vXXnYA2Qe+sv5qNadUf/o7zAB4qzvny4sQM+bif5+FVx9\n3UVnb1X1+KKjy/ZzXKYQcXYSIhXnQuPTZgn62+XoRXOhqAh12z2ornehPD0tqrJ83PG9uxxaa/Ti\neegvHC9AwPnfP/O7b9Af/gt8/OxB0qjxeX/vqCHiOEeAcFrKsGG0747x4gzUDc3Qn3+IOW4AevcO\nq0sTFcTRA8QVlLa3/KthThuN+blztLfkKBAVRgWHYDw5FOOZcWCWYE4dhfnvRPSJ41aXJq6A1hr9\n+YcSIFVAxZyZvdWsPXrJmdlbDv7zI0eCqHDqmhswxr6B+r970Vu+s194X/Ml2jStLk1cJnuAfIRe\n+gmq1a0SIFVAefvYlxo6d/aWA5/Vy9EgKoXy8sbo8QDGmNchrg563kzMl59DH/zd6tLEZbAHyMf2\nAHnwnxIgVejP7a2Tn8x2yPaWHBGiUqkasRhDJqAeGwwZRzEnDMb8ZLbspOgEzM8/tAdIy04SIBb5\no73VjryPZ2NOG+Nw7S05KkSlU0phNGuHMf5fqDad0d98jjnqn+itG+TeEgdlfv4hesnHqJYdUQ89\nJQFiIXt7axCBT78Av//icO0tOTJElVH+ARj/6I8xbLL9Bqt/vYz5xnj0sTSrSxPnMD//6JwAeVoC\nxEH4duiO8Xwi+AWcmb31kUO0t+ToEFVO1W2IMXIq6p5H4defMcc+hbnsv+hi2ZbXauaSj9FLPkK1\nkABxRCqmlr29dUs79JKPMKePRedY296SI0RYQtlsGJ3vwHjxLbj2JvTC9zFffAb9689Wl+a2zCUf\noz//0B4gD0sLy1EpH1/Uo8+geg+A33bbf25++cmyeuQoEZZSIeHY+o2wb8t7uhDzlecx330NnZtj\ndWluxVx6JkCadzgTIM69PLmrU0phtOxkb2/5+mNOHW3/EGBBe0tCRDgE1fhmjHFvorrehf5hDeao\nfpjrV8q9JVXAXPoJevGHqObtUb2flgBxIn+0t9rYV4qwoL0lISIchvL2wbjrYYxR06FGTfR7b2C+\n8jw65YDVpbkse4DMOxMgAyRAnJC9vTUI9fDTsK/q21sSIsLhqJhaGENfsv9QpB7CHP8M5oL30IUF\nVpfmUswv5tsDpJkEiLNTSmG0uhXj+VervL0lISIckjIM+w/F+H+hmrVDL1+AOeYp9I5NVpfmEswv\n5qM/+8AeII9IgLgKFVu7yttbEiLCoalqgRi9B2IMfQm8vDHfHE/JjJfQWcesLs1p/REg7SRAXFBV\nt7ckRIRTUA2uxRg9HdXzIdi1FXP0PzG/+gxdYv3NVs7E/PLTcwJkoASIi7pge2tp5bS3JESE01Ae\nnhjd7sYY+yY0uBb96TuYEwajf/vF6tKcQt6C99GL5qJuaSsB4ibOa28trpz2llvubKi1pqCgANM0\nUUpV6Gt5e3tTWFhYoc9pBa01hmHg4+Nz3r+Ro+wep7WGbd9jfvRvOJGFat0F1fMhlH/Apb/5Ihxl\nbJXBXPZf9ML3UU3boh57xiUDxJXfP7iy8Wmt0etXoj96G/z8MZ54FnX1dX/5PWXd2dCjXBU5uYKC\nAjw9PfHwqPjhe3h4YLO5xg9ocXExBQUF+Pr6Wl3K/1BKwY0tMOKboBd/hP5mCXrb96hej6JuaVfh\nHw6cmblsAXrh+/i06czpB/q5ZICIv6aUQrXujL6qAeasyZiJo1B/ux91291XfDy4ZTvLNM1KCRBX\n4+HhgengN/spHz+Mex/DGJkIYZHo2dMwE0ei0w5bXZpDMJcvQC98D9W0DYEDRkqAuDl7e2sqqmlr\n9OJ5mK+NQ+dkX9FzumWIyKfUsnOWfysVVxdj+GTUA/3g4O+Y4wZgLp6HPu38rcXyMpcvQC+wB4h6\ndBDKJh+cxJnZW48NRj30FOxNss/e2rOz3M/nliHiCOrXr291CS5HGTaMdt0wxs9A3dQSvfQTzLFP\no3dts7q0KmeuWGgPkJtbnwkQOQMRf1BKYbTujPH8K+Dji5k4yr56QTk6DxIiwuWooOoYjw/BGDwe\nlIE5fQzm26+gs7OsLq1KmCsWof/7rj1AHhssASIuSsVehTEyEXXz2fbW2Mtub0mIWExrzfjx4+nQ\noQMdO3Zk8eLFABw9epSePXty66230qFDBzZu3EhJSQnPPPNM6WPffvtti6t3bKpRY4yxr6P+9nf0\nth8wR/fHXLXUITbyqSz2AJkjASLKTPn4oR7/c3ur7FsyuH2T1Pz43+hD+yvu+ZSC2NoY9z1Rpsd/\n+eWX7Nq1i5UrV5KVlcVtt91Gs2bNWLRoEW3btmXgwIGUlJRw6tQpdu3aRVpaGqtWrQLgxIkTFVa3\nq1KeXqjb70M3bYM571/oj95Gb1iF8WB/VK16VpdXocyvJEBE+fwxe6s+5swpmIkjoX3nMn2vnIlY\n7Mcff+SOO+7AZrMRHh5Os2bN2LFjB02aNGH+/PkkJiaye/duAgICiIuL4+DBg4wcOZLVq1dTrVo1\nq8t3GioyGmPQi6gnnoXjGZgTn8X86G10fp7VpVUI86vP0J/OQSW0kgAR5Vba3mrVqczf4/ZnImU9\nYygrDw8PiouLy/z4i93r2axZMxYsWMA333zDwIED6du3L/fccw8rV65kzZo1vPvuuyxZsoSpU6dW\nVOkuTymFatoGfe2N6M8+QK/+Ar1lA+rex1EJLa0ur9zsAfIO6qaWqMeHSICIK6J8/OytrTKSMxGL\nNWvWjM8//5ySkhIyMzPZuHEjTZo04fDhw4SFhfHAAw9w3333sXPnTrKysjBNk+7duzN06FB27iz/\ntDx3pvwCMP7eF2PEqxAUjH57CuZrYylOdb57S8yVi/8IkCeelQARVc7tz0Ss1q1bN7Zs2cKtt96K\nUooXXniBiIgI5s+fz8yZM/Hw8MDf35/XXnuN1NRUBg8eXHoD4IgRIyyu3rmpq+pjPJ9oPyNZPI/M\nZ/6B6nYPqktPlKen1eVdkrlyMXr+bLiphZyBCMu45dpZ+fn5+Pn5VcprXW47y9H9+d/KVdcn0scz\n8fxsLoUbVkFULMY/+l1ybSErmV8vRn9iDxDj8WdRZViBwVXfu7NkfBWrrGtnSTtLCEBVDyV46ASM\nAWOguAjz1RcwZ0+74iUhKkNpgNxY9gARorJIiAhxDnXdTRhj30Tddg960zrMUf0x1y4v1528lcH8\n+vM/AuQJCRBhPQkRIf5EeXtj3PkgxujpEFsLPXcG5pTh6MMVdz9ReZjfLEF/8h+4sbkEiHAYLhsi\nmzdvZtasWVaXIZyYio7DePYl1CMD4egRzPGDMD+dgy44VeW1mN8sQX/87zMBMlQCRDgMlz0SExIS\nSEhIsLoM4eSUUqgWHdHX34xe+D76q0Xozesw7n8S1aRZldRgfrPUHiA3NJMAEQ7HZc9EhKhIKiAQ\n46GnMIa9bN+z+q2XKHlzAjozvVJf11y1FP3x2/YAeVICRDgeCREhLoOqF48xchrq7t6wewfm6H/a\nl12vhGnd5qql9u1Mm5wNEMe/d0W4HwkRJ/BXe48cOnSIDh06VGE1Qnl4YHTpifHiW9CoMfq/72JO\nGITel1Rhr2Gu/uJMgNyC0UcCRDguCREhykmFRmB7aiTGP5+HU3mYk4djvv8m+mTOFT2vufoL9Iez\nzgTIcxIgwqG5fYP1P5uPsv94QYU9n1KK2sHePJ4QedHHTJw4kZiYGHr37g1AYmIiSil++OEHTpw4\nQXFxMc899xxdunS5rNcuKChgxIgR/PTTT9hsNsaMGUPLli3Zs2cPgwcP5vTp02itefvtt4mKiqJP\nnz6kpqZimiYDBw6kR48eVzJ0t6WaNMNo2Bi95GP014vR235A3fMIqnmHy95e2Fz9pT1AGjeVABFO\nwe1DxAo9evRgzJgxpSGyZMkS5s2bxxNPPEG1atXIysri9ttvp3Pnzpf1S+jdd98F4JtvvmHfvn3c\nf//9rFu3jrlz5/LYY4/Rs2dPTp8+TUlJCatWrSIqKoq5c+cCkJNzZZ+e3Z3y8UXd8wi6eTvMD/6F\nnvMa+rtv7Mun1KhZpucw13yJ/nCmPUD6DpMAEU7B7UPkr84YyqMsa2dde+21ZGRkkJaWRmZmJkFB\nQURERDB27Fg2btyIUoq0tDSOHTtGREREmV9706ZNPPLIIwDUq1eP2NhYfv/9d2666SZef/11UlNT\n6datG3Xq1KFhw4aMHz+eiRMn0qlTJ2655ZYrGrewU7FXYTz3Mnr9SvSC9zDHDUR1uRN1Wy+Ut/dF\nv89csww9TwJEOB+5JmKR7t2788UXX/D555/To0cPFi5cSGZmJsuWLWPlypWEhYVRWFh4Wc95sbU0\n77zzTubMmYOPjw8PPPAA69evp27duixbtoyGDRsyadIkpk2bVhHDEoAyDIw2XTDGz0A1bY3+8lPM\nsU+hd26+4OPtAfKvMy0sCRDhXCRELNKjRw8WL17MF198Qffu3cnNzSUsLAxPT0++++47Dh++/L0t\nbrnlFhYtWgTAb7/9RkpKCnXr1uXAgQPUqlWLxx57jFtvvZXdu3eTlpaGr68vd911F3379pW9SSqB\nCgzGeHQQxpAJ4OGB+fqLlMx8GX08s/Qx5rfL7QFy/c32AHGCJeiFOJfbt7OscvXVV5OXl0dUVBSR\nkZH07NmThx9+mG7dunHNNddQr97l7//98MMPM3z4cDp27IjNZmPatGl4e3vz+eefs3DhQjw8PIiI\niGDQoEHs2LGDCRMmoJTC09OTSZMmVcIoBYBqeD3G6Nftd7t/MR/z522oOx4ADw97C+u6BIy+wyVA\nhFOS/UQqmOwn4ryqYmw6PRXzw5mwa5v9C9clYPQbUSUB4srvHcj4KlpZ9xORMxEhqpCKqIExcCxs\n+Q594DfU3/4uZyDCqUmIOIndu3czYMCA877m7e3N0qVLLapIlJdSChJaoRJaWV2KEFdMQsRJNGrU\niJUrV1pdhhBCnMctZ2e5wWWgCiP/VkKIv+KWIWIYhktd/K4sxcXFGIZbHiJCiDJyy3aWj48PBQUF\nFBYWXvbaRpfi7e192TcJOiKtNYZh4OPjY3UpQggH5pYhopTC19e3Up7b1acZCiHEuaRXIYQQotwk\nRIQQQpSbhIgQQohyc4tlT4QQQlQOOROpYMOHD7e6hErlyuNz5bGBjM/ZOer4JESEEEKUm4SIEEKI\ncrONHTt2rNVFuJo6depYXUKlcuXxufLYQMbn7BxxfHJhXQghRLlJO0sIIUS5ueWyJxVl6dKlrFq1\nCqUUNWvWpH///mRnZzN9+nROnjzJVVddxdNPP42Hh3P+M3/55Zd88803aK3p2LEj3bt35+TJk0yb\nNo1jx44RHh7OoEGDCAgIsLrUMpkxYwZbt24lKCiIxMREgIuOR2vNnDlz2LZtG97e3vTv398hWwnn\nutD4vv/+ez799FNSUlJ46aWXqFu3bunjFy1axKpVqzAMg0ceeYQmTZpYVXqZXGh8c+fOZcuWLXh4\neBAZGUn//v3x9/cHnGt8Fxrbxx9/zObNm1FKERQURP/+/QkJCXG8Y1OLcsnMzNT9+/fXhYWFWmut\nExMT9erVq3ViYqJev3691lrrWbNm6RUrVlhZZrkdOHBADx48WBcUFOji4mL94osv6iNHjui5c+fq\nRYsWaa21XrRokZ47d67FlZbdrl279G+//aYHDx5c+rWLjWfLli164sSJ2jRNvWfPHj1ixAhLar4c\nFxrfoUOHdEpKih4zZozet2/feV9/9tln9enTp/XRo0f1U089pUtKSqwou8wuNL7t27fr4uJirbX9\nvTz7/jnb+C40try8vNL//+KLL/SsWbO01o53bEo76wqYpsnp06cpKSnh9OnTBAcHs2vXLpo1awZA\nu3bt2LRpk8VVlk9KSgr169fH29sbm81Go0aN+PHHH9m0aRNt27YFoG3btk41vvj4+P85a7rYeDZv\n3kybNm1QStGgQQPy8vI4fvx4ldd8OS40vtjY2Avulb1p0yZatGiBp6cnERERREVFsW/fvqoqtVwu\nNL7GjRtjs9kAaNCgAVlZWYDzje9CY/Pz8yv9/3NXHHe0Y1NCpJxCQkK4/fbb6devH08++SR+fn7U\nqVMHPz+/0oM6JCSk9KB2NjVr1mT37t3k5uZSWFjItm3byMzM5MSJE1SvXh2A6tWrk5OTY3GlV+Zi\n48nKyiIsLKz0caGhoU77Xl5IVlYWoaGhpX925mP1rFWrVpW2rFxlfB999BH9+vVj/fr13HvvvYDj\nHZvO2ax3ACdPnmTTpk289dZb+Pn5MXXqVLZv3251WRUmNjaWHj16MGHCBHx8fKhVq5ZbbVClLzBp\nsaL3nrHShcbnzBYuXIjNZqN169aA64zv/vvv5/7772fRokUsX76cXr16Odyx6T6/FSrYzp07iYiI\nIDAwEA8PD2655Rb27NlDfn4+JSUlgP0TQ0hIiMWVll+HDh2YPHky48aNIyAggBo1ahAUFFR66nz8\n+HECAwMtrvLKXGw8oaGh5+0Lk5mZWXrG4gpCQ0PJzMws/bMzH6tr1qxhy5YtDBgwoPSXqSuND6BV\nq1Zs3LgRcLxjU0KknMLCwti7dy+FhYVordm5cyexsbFcc801/PDDD4D94E5ISLC40vI7ceIEABkZ\nGfz444+0bNmShIQEvv32WwC+/fZbbr75ZitLvGIXG09CQgJr165Fa82vv/6Kn5+fS4VIQkICGzZs\noKioiPT0dFJTU6lXr57VZV227du3s3jxYoYNG4a3t3fp111hfKmpqaX/v3nz5tJrW452bMrNhldg\n/vz5bNiwAZvNRu3atenbty9ZWVn/M8XX09PT6lLLZfTo0eTm5uLh4cFDDz3EddddR25uLtOmTSMj\nI4OwsDAGDx7sNFN8p0+fTlJSErm5uQQFBdGrVy9uvvnmC45Ha83s2bPZsWMHXl5e9O/f/7zpsY7o\nQuMLCAjgnXfeIScnB39/f2rXrs0LL7wA2FtAq1evxjAMevfuzQ033GDxCP7ahca3aNEiiouLS4/B\n+vXr8+STTwLONb4LjW3r1q2kpqailCIsLIwnn3yydIqvIx2bEiJCCCHKTdpZQgghyk1CRAghRLlJ\niAghhCg3CREhhBDlJiEihBCi3CREhKhkDz74IEePHr3g361Zs4ZRo0Zd9Ht37dpF3759K6s0Ia6Y\nhIgQlWzu3LlERkaW6bG9evUiLS2tkisSouJIiAghhCg3CREhymn16tW8/PLLpX9++umnmTp1aumf\n+/XrR3Jy8nlnF7m5uUyePJmHH36YESNGnHfWMWbMGACGDh3Kgw8+yIYNG0r/bsmSJTz++OM8+eST\nrF69urKHJkSZSYgIUU7x8fH88ssvmKbJ8ePHKSkpYc+ePQAcPXqUgoIC4uLizvue2bNn4+npyaxZ\ns+jXr995gTBu3DgAXnnlFebOnUuLFi0AyM7OJj8/n5kzZ9K3b19mz57NyZMnq2iUQvw1CREhyiky\nMhJfX1+Sk5NJSkqicePGhISEkJKSQlJSEg0bNjxv+XzTNNm4cSP33nsvPj4+xMXFlW6I9VdsNht3\n3303Hh4e3Hjjjfj4+HDkyJHKHJoQZSb7iQhxBRo1akRSUhJpaWnEx8fj7+9PUlISv/76K/Hx8ec9\nNicnh5KSkvM2SwoPD2f37t1/+RrVqlUr3egMwNvbm4KCgoodiBDlJGciQlyB+Ph4du3axe7du4mP\njyc+Pp6kpCSSkpL+J0QCAwOx2Wzn7XNx7r4QQjgjCREhrsDZEDl9+jShoaE0bNiQ7du3l24FcC7D\nMNTwn10AAACrSURBVGjatCmffvophYWFHD58uHQvk7OCgoIuek+JEI5IQkSIKxAdHY2Pjw+NGjUC\nwM/Pj8jISK6++uoLbif82GOPUVBQwJNPPslbb71Fu3btzvv7e+65h7feeovevXufNztLCEcl+4kI\nIYQoNzkTEUIIUW4SIkIIIcpNQkQIIUS5SYgIIYQoNwkRIYQQ5SYhIoQQotwkRIQQQpSbhIgQQohy\nkxARQghRbv8PtVDd+AD/NzIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x195250150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(width3_perf).loc[:,['width','mse','val_mse']].set_index('width').plot(logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final run for 1 layer\n",
    "Conclusion 1 layer of about 135 seems best, let's see if we can improve with another layer.  \n",
    "Because there can be a bit of variance, let's do a few runs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width1 = 135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, None, 1)           136       \n",
      "=================================================================\n",
      "Total params: 74,656\n",
      "Trainable params: 74,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 1.0755 - mse: 936.5521 - val_loss: 0.5915 - val_mse: 515.1248\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.5472 - mse: 476.5555 - val_loss: 0.2828 - val_mse: 246.3094\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.2685 - mse: 233.7912 - val_loss: 0.3783 - val_mse: 329.4532\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.3844 - mse: 334.7589 - val_loss: 0.2879 - val_mse: 250.6860\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2867 - mse: 249.6930 - val_loss: 0.2523 - val_mse: 219.7462\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2356 - mse: 205.1404 - val_loss: 0.2714 - val_mse: 236.3164\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.2522 - mse: 219.6522 - val_loss: 0.2393 - val_mse: 208.3498\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.2240 - mse: 195.0594 - val_loss: 0.1980 - val_mse: 172.4024\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1878 - mse: 163.5803 - val_loss: 0.1625 - val_mse: 141.4877\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1545 - mse: 134.5228 - val_loss: 0.1349 - val_mse: 117.5050\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1229 - mse: 107.0380 - val_loss: 0.1424 - val_mse: 124.0016\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1231 - mse: 107.1724 - val_loss: 0.1719 - val_mse: 149.6581\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1462 - mse: 127.3299 - val_loss: 0.1710 - val_mse: 148.9068\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1452 - mse: 126.4489 - val_loss: 0.1406 - val_mse: 122.4820\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1196 - mse: 104.1503 - val_loss: 0.1171 - val_mse: 101.9558\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.1017 - mse: 88.5271 - val_loss: 0.1094 - val_mse: 95.2968\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0979 - mse: 85.2883 - val_loss: 0.1085 - val_mse: 94.4832\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0985 - mse: 85.7488 - val_loss: 0.1105 - val_mse: 96.2572\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.1007 - mse: 87.7029 - val_loss: 0.1122 - val_mse: 97.6723\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.1024 - mse: 89.2119 - val_loss: 0.1097 - val_mse: 95.5449\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.1005 - mse: 87.4855 - val_loss: 0.1027 - val_mse: 89.4775\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0943 - mse: 82.1541 - val_loss: 0.0944 - val_mse: 82.2374\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0870 - mse: 75.7913 - val_loss: 0.0891 - val_mse: 77.5620\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0824 - mse: 71.7273 - val_loss: 0.0869 - val_mse: 75.6431\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0801 - mse: 69.7621 - val_loss: 0.0862 - val_mse: 75.0418\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0790 - mse: 68.8112 - val_loss: 0.0859 - val_mse: 74.8476\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0786 - mse: 68.4910 - val_loss: 0.0837 - val_mse: 72.8735\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0762 - mse: 66.3229 - val_loss: 0.0810 - val_mse: 70.4972\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0730 - mse: 63.5621 - val_loss: 0.0800 - val_mse: 69.6663\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0719 - mse: 62.6554 - val_loss: 0.0767 - val_mse: 66.7712\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0692 - mse: 60.2265 - val_loss: 0.0713 - val_mse: 62.1120\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0647 - mse: 56.3674 - val_loss: 0.0682 - val_mse: 59.3648\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0625 - mse: 54.4328 - val_loss: 0.0664 - val_mse: 57.7867\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0613 - mse: 53.3900 - val_loss: 0.0647 - val_mse: 56.3189\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0601 - mse: 52.3502 - val_loss: 0.0627 - val_mse: 54.6082\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0587 - mse: 51.1278 - val_loss: 0.0596 - val_mse: 51.9412\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0561 - mse: 48.8224 - val_loss: 0.0569 - val_mse: 49.5414\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0535 - mse: 46.5661 - val_loss: 0.0554 - val_mse: 48.2148\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0520 - mse: 45.2669 - val_loss: 0.0536 - val_mse: 46.6728\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0502 - mse: 43.7211 - val_loss: 0.0509 - val_mse: 44.2943\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0476 - mse: 41.4511 - val_loss: 0.0486 - val_mse: 42.3396\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0458 - mse: 39.8639 - val_loss: 0.0475 - val_mse: 41.3870\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0452 - mse: 39.3891 - val_loss: 0.0460 - val_mse: 40.0435\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0441 - mse: 38.3612 - val_loss: 0.0435 - val_mse: 37.9192\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0417 - mse: 36.3197 - val_loss: 0.0420 - val_mse: 36.5863\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0400 - mse: 34.8724 - val_loss: 0.0418 - val_mse: 36.4011\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0397 - mse: 34.5565 - val_loss: 0.0412 - val_mse: 35.8912\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0391 - mse: 34.0238 - val_loss: 0.0402 - val_mse: 35.0156\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0381 - mse: 33.2013 - val_loss: 0.0395 - val_mse: 34.3810\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0375 - mse: 32.6307 - val_loss: 0.0389 - val_mse: 33.8948\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0370 - mse: 32.1796 - val_loss: 0.0382 - val_mse: 33.2506\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0361 - mse: 31.4693 - val_loss: 0.0376 - val_mse: 32.7825\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0353 - mse: 30.7447 - val_loss: 0.0377 - val_mse: 32.8389\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0350 - mse: 30.4483 - val_loss: 0.0371 - val_mse: 32.3258\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0343 - mse: 29.8336 - val_loss: 0.0358 - val_mse: 31.1862\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0332 - mse: 28.8899 - val_loss: 0.0351 - val_mse: 30.5619\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0327 - mse: 28.4334 - val_loss: 0.0345 - val_mse: 30.0643\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0321 - mse: 27.9849 - val_loss: 0.0336 - val_mse: 29.2900\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0312 - mse: 27.1642 - val_loss: 0.0331 - val_mse: 28.8344\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0305 - mse: 26.5646 - val_loss: 0.0328 - val_mse: 28.5614\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0301 - mse: 26.2276 - val_loss: 0.0320 - val_mse: 27.8878\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0296 - mse: 25.7434 - val_loss: 0.0313 - val_mse: 27.2831\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0292 - mse: 25.4148 - val_loss: 0.0310 - val_mse: 26.9600\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0290 - mse: 25.2272 - val_loss: 0.0305 - val_mse: 26.5909\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0285 - mse: 24.7908 - val_loss: 0.0303 - val_mse: 26.3785\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0280 - mse: 24.4052 - val_loss: 0.0301 - val_mse: 26.2463\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0277 - mse: 24.1473 - val_loss: 0.0297 - val_mse: 25.8315\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0273 - mse: 23.7593 - val_loss: 0.0291 - val_mse: 25.3323\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0268 - mse: 23.3617 - val_loss: 0.0287 - val_mse: 25.0300\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0265 - mse: 23.1026 - val_loss: 0.0285 - val_mse: 24.7917\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0262 - mse: 22.7860 - val_loss: 0.0283 - val_mse: 24.6829\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0258 - mse: 22.4891 - val_loss: 0.0282 - val_mse: 24.5996\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0256 - mse: 22.2500 - val_loss: 0.0279 - val_mse: 24.2610\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0252 - mse: 21.9279 - val_loss: 0.0274 - val_mse: 23.8565\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0249 - mse: 21.6542 - val_loss: 0.0270 - val_mse: 23.5156\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0246 - mse: 21.4091 - val_loss: 0.0266 - val_mse: 23.1524\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0242 - mse: 21.0824 - val_loss: 0.0262 - val_mse: 22.8315\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0239 - mse: 20.7887 - val_loss: 0.0259 - val_mse: 22.5149\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0236 - mse: 20.5326 - val_loss: 0.0254 - val_mse: 22.1097\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0233 - mse: 20.2489 - val_loss: 0.0250 - val_mse: 21.7540\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0230 - mse: 20.0084 - val_loss: 0.0247 - val_mse: 21.4824\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0227 - mse: 19.7605 - val_loss: 0.0244 - val_mse: 21.2420\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0224 - mse: 19.4986 - val_loss: 0.0241 - val_mse: 21.0133\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0221 - mse: 19.2516 - val_loss: 0.0238 - val_mse: 20.7247\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0218 - mse: 18.9480 - val_loss: 0.0235 - val_mse: 20.4552\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0214 - mse: 18.6740 - val_loss: 0.0233 - val_mse: 20.2521\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0211 - mse: 18.4115 - val_loss: 0.0230 - val_mse: 20.0680\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0208 - mse: 18.1302 - val_loss: 0.0228 - val_mse: 19.8751\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0205 - mse: 17.8740 - val_loss: 0.0226 - val_mse: 19.6466\n",
      "Epoch 90/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0202 - mse: 17.5965 - val_loss: 0.0222 - val_mse: 19.3455\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0199 - mse: 17.3221 - val_loss: 0.0219 - val_mse: 19.1021\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0196 - mse: 17.0465 - val_loss: 0.0216 - val_mse: 18.8010\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0193 - mse: 16.7921 - val_loss: 0.0214 - val_mse: 18.6591\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0191 - mse: 16.5944 - val_loss: 0.0211 - val_mse: 18.4037\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0190 - mse: 16.5289 - val_loss: 0.0220 - val_mse: 19.1415\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0196 - mse: 17.0724 - val_loss: 0.0221 - val_mse: 19.2399\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0203 - mse: 17.6889 - val_loss: 0.0240 - val_mse: 20.9335\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0217 - mse: 18.9051 - val_loss: 0.0201 - val_mse: 17.4755\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0180 - mse: 15.6356 - val_loss: 0.0208 - val_mse: 18.1092\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0190 - mse: 16.5550 - val_loss: 0.0225 - val_mse: 19.6241\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0202 - mse: 17.6075 - val_loss: 0.0193 - val_mse: 16.8153\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0170 - mse: 14.8176 - val_loss: 0.0216 - val_mse: 18.8425\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0200 - mse: 17.4191 - val_loss: 0.0200 - val_mse: 17.4561\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0176 - mse: 15.2867 - val_loss: 0.0204 - val_mse: 17.8020\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0179 - mse: 15.6192 - val_loss: 0.0190 - val_mse: 16.5824\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0170 - mse: 14.7925 - val_loss: 0.0188 - val_mse: 16.4072\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0168 - mse: 14.6425 - val_loss: 0.0190 - val_mse: 16.5330\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0165 - mse: 14.4022 - val_loss: 0.0188 - val_mse: 16.3431\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0163 - mse: 14.1520 - val_loss: 0.0181 - val_mse: 15.7449\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0158 - mse: 13.7300 - val_loss: 0.0178 - val_mse: 15.5225\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0156 - mse: 13.5946 - val_loss: 0.0176 - val_mse: 15.3428\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0152 - mse: 13.2342 - val_loss: 0.0176 - val_mse: 15.3398\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0151 - mse: 13.1438 - val_loss: 0.0170 - val_mse: 14.8045\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0145 - mse: 12.6436 - val_loss: 0.0169 - val_mse: 14.7005\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0145 - mse: 12.6650 - val_loss: 0.0165 - val_mse: 14.3349\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0140 - mse: 12.2095 - val_loss: 0.0165 - val_mse: 14.3388\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0140 - mse: 12.1919 - val_loss: 0.0158 - val_mse: 13.7958\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0135 - mse: 11.7589 - val_loss: 0.0157 - val_mse: 13.6964\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0134 - mse: 11.6835 - val_loss: 0.0156 - val_mse: 13.5479\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0131 - mse: 11.4145 - val_loss: 0.0152 - val_mse: 13.2577\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0129 - mse: 11.2115 - val_loss: 0.0148 - val_mse: 12.9134\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0127 - mse: 11.0842 - val_loss: 0.0146 - val_mse: 12.7331\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0123 - mse: 10.7386 - val_loss: 0.0146 - val_mse: 12.7398\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0123 - mse: 10.7253 - val_loss: 0.0140 - val_mse: 12.1608\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0119 - mse: 10.3789 - val_loss: 0.0139 - val_mse: 12.1329\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0119 - mse: 10.3357 - val_loss: 0.0141 - val_mse: 12.2450\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0117 - mse: 10.1463 - val_loss: 0.0134 - val_mse: 11.6825\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0114 - mse: 9.9469 - val_loss: 0.0131 - val_mse: 11.4330\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0113 - mse: 9.8214 - val_loss: 0.0136 - val_mse: 11.8570\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0112 - mse: 9.7570 - val_loss: 0.0128 - val_mse: 11.1691\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0108 - mse: 9.4231 - val_loss: 0.0126 - val_mse: 11.0148\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0108 - mse: 9.4251 - val_loss: 0.0129 - val_mse: 11.2566\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0107 - mse: 9.2870 - val_loss: 0.0124 - val_mse: 10.7943\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0104 - mse: 9.0680 - val_loss: 0.0122 - val_mse: 10.6171\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0103 - mse: 8.9472 - val_loss: 0.0123 - val_mse: 10.6702\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0101 - mse: 8.8321 - val_loss: 0.0120 - val_mse: 10.4206\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0101 - mse: 8.7622 - val_loss: 0.0119 - val_mse: 10.3598\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0098 - mse: 8.5569 - val_loss: 0.0116 - val_mse: 10.0917\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0096 - mse: 8.4014 - val_loss: 0.0115 - val_mse: 10.0466\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0095 - mse: 8.3050 - val_loss: 0.0115 - val_mse: 10.0131\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0094 - mse: 8.1916 - val_loss: 0.0112 - val_mse: 9.7456\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0094 - mse: 8.1425 - val_loss: 0.0115 - val_mse: 9.9930\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0093 - mse: 8.0605 - val_loss: 0.0110 - val_mse: 9.5392\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0091 - mse: 7.9635 - val_loss: 0.0113 - val_mse: 9.8686\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0091 - mse: 7.9676 - val_loss: 0.0110 - val_mse: 9.5430\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0092 - mse: 7.9944 - val_loss: 0.0120 - val_mse: 10.4584\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0096 - mse: 8.3399 - val_loss: 0.0119 - val_mse: 10.3690\n",
      "Epoch 148/1000\n",
      "\n",
      "Epoch 00147: reducing learning rate to 0.00699999984354.\n",
      "1s - loss: 0.0104 - mse: 9.0342 - val_loss: 0.0147 - val_mse: 12.7904\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0119 - mse: 10.3878 - val_loss: 0.0105 - val_mse: 9.1692\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0087 - mse: 7.5578 - val_loss: 0.0118 - val_mse: 10.2933\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0104 - mse: 9.0477 - val_loss: 0.0120 - val_mse: 10.4125\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0096 - mse: 8.3369 - val_loss: 0.0117 - val_mse: 10.1804\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0092 - mse: 8.0345 - val_loss: 0.0112 - val_mse: 9.7848\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0096 - mse: 8.3179 - val_loss: 0.0105 - val_mse: 9.1223\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0085 - mse: 7.3934 - val_loss: 0.0117 - val_mse: 10.2152\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0092 - mse: 8.0245 - val_loss: 0.0108 - val_mse: 9.3628\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0083 - mse: 7.2470 - val_loss: 0.0108 - val_mse: 9.3849\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0089 - mse: 7.7663 - val_loss: 0.0102 - val_mse: 8.8696\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0081 - mse: 7.0377 - val_loss: 0.0110 - val_mse: 9.6086\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0086 - mse: 7.4594 - val_loss: 0.0104 - val_mse: 9.0293\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0080 - mse: 6.9595 - val_loss: 0.0102 - val_mse: 8.9126\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0083 - mse: 7.2135 - val_loss: 0.0099 - val_mse: 8.6020\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0078 - mse: 6.8064 - val_loss: 0.0104 - val_mse: 9.0408\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0080 - mse: 6.9880 - val_loss: 0.0100 - val_mse: 8.7134\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0077 - mse: 6.7055 - val_loss: 0.0098 - val_mse: 8.5277\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0078 - mse: 6.7915 - val_loss: 0.0096 - val_mse: 8.3508\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0075 - mse: 6.5551 - val_loss: 0.0100 - val_mse: 8.6716\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0076 - mse: 6.6373 - val_loss: 0.0097 - val_mse: 8.4093\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0074 - mse: 6.4195 - val_loss: 0.0095 - val_mse: 8.2887\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0075 - mse: 6.4994 - val_loss: 0.0093 - val_mse: 8.1304\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0072 - mse: 6.2592 - val_loss: 0.0096 - val_mse: 8.3943\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0073 - mse: 6.3720 - val_loss: 0.0092 - val_mse: 8.0434\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0070 - mse: 6.1247 - val_loss: 0.0092 - val_mse: 8.0326\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0072 - mse: 6.2401 - val_loss: 0.0091 - val_mse: 7.9400\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0069 - mse: 6.0086 - val_loss: 0.0092 - val_mse: 8.0507\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0070 - mse: 6.0742 - val_loss: 0.0090 - val_mse: 7.8158\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0068 - mse: 5.9288 - val_loss: 0.0090 - val_mse: 7.8060\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0068 - mse: 5.9044 - val_loss: 0.0090 - val_mse: 7.8770\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0067 - mse: 5.8578 - val_loss: 0.0088 - val_mse: 7.6825\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0066 - mse: 5.7474 - val_loss: 0.0088 - val_mse: 7.6673\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0066 - mse: 5.7469 - val_loss: 0.0088 - val_mse: 7.6600\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0065 - mse: 5.6393 - val_loss: 0.0087 - val_mse: 7.5393\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0064 - mse: 5.5991 - val_loss: 0.0085 - val_mse: 7.4356\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0064 - mse: 5.5434 - val_loss: 0.0086 - val_mse: 7.4762\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0063 - mse: 5.4666 - val_loss: 0.0085 - val_mse: 7.4024\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0062 - mse: 5.4171 - val_loss: 0.0083 - val_mse: 7.2560\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0062 - mse: 5.3663 - val_loss: 0.0084 - val_mse: 7.2975\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0061 - mse: 5.2937 - val_loss: 0.0083 - val_mse: 7.2670\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0060 - mse: 5.2393 - val_loss: 0.0082 - val_mse: 7.1176\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0060 - mse: 5.2067 - val_loss: 0.0082 - val_mse: 7.1454\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0059 - mse: 5.1199 - val_loss: 0.0082 - val_mse: 7.1066\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0058 - mse: 5.0607 - val_loss: 0.0080 - val_mse: 6.9723\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0058 - mse: 5.0360 - val_loss: 0.0081 - val_mse: 7.0190\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0057 - mse: 4.9602 - val_loss: 0.0080 - val_mse: 6.9307\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0056 - mse: 4.8778 - val_loss: 0.0079 - val_mse: 6.8556\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0056 - mse: 4.8416 - val_loss: 0.0079 - val_mse: 6.9222\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0055 - mse: 4.8016 - val_loss: 0.0078 - val_mse: 6.7730\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0054 - mse: 4.7253 - val_loss: 0.0078 - val_mse: 6.7688\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0053 - mse: 4.6511 - val_loss: 0.0077 - val_mse: 6.7100\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0053 - mse: 4.5975 - val_loss: 0.0076 - val_mse: 6.6389\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0052 - mse: 4.5478 - val_loss: 0.0077 - val_mse: 6.7222\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0052 - mse: 4.4983 - val_loss: 0.0075 - val_mse: 6.5593\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0051 - mse: 4.4582 - val_loss: 0.0077 - val_mse: 6.7429\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0051 - mse: 4.4392 - val_loss: 0.0076 - val_mse: 6.6107\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0052 - mse: 4.4865 - val_loss: 0.0082 - val_mse: 7.1186\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0054 - mse: 4.6896 - val_loss: 0.0084 - val_mse: 7.3211\n",
      "Epoch 207/1000\n",
      "\n",
      "Epoch 00206: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0060 - mse: 5.2586 - val_loss: 0.0104 - val_mse: 9.0501\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0074 - mse: 6.4182 - val_loss: 0.0078 - val_mse: 6.8254\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0053 - mse: 4.6477 - val_loss: 0.0081 - val_mse: 7.0296\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0057 - mse: 4.9531 - val_loss: 0.0088 - val_mse: 7.6577\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0059 - mse: 5.1807 - val_loss: 0.0079 - val_mse: 6.8771\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0050 - mse: 4.3332 - val_loss: 0.0083 - val_mse: 7.2661\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0059 - mse: 5.1415 - val_loss: 0.0074 - val_mse: 6.4797\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0049 - mse: 4.2437 - val_loss: 0.0083 - val_mse: 7.1909\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0054 - mse: 4.6955 - val_loss: 0.0076 - val_mse: 6.6422\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0049 - mse: 4.2301 - val_loss: 0.0075 - val_mse: 6.4973\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0050 - mse: 4.3697 - val_loss: 0.0074 - val_mse: 6.4369\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0049 - mse: 4.2653 - val_loss: 0.0076 - val_mse: 6.5901\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0047 - mse: 4.1241 - val_loss: 0.0076 - val_mse: 6.6264\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0048 - mse: 4.2121 - val_loss: 0.0070 - val_mse: 6.1384\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0045 - mse: 3.9262 - val_loss: 0.0073 - val_mse: 6.3305\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0047 - mse: 4.1255 - val_loss: 0.0073 - val_mse: 6.3219\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0044 - mse: 3.8241 - val_loss: 0.0075 - val_mse: 6.5332\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0046 - mse: 4.0070 - val_loss: 0.0070 - val_mse: 6.1301\n",
      "Epoch 225/1000\n",
      "\n",
      "Epoch 00224: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0043 - mse: 3.7577 - val_loss: 0.0071 - val_mse: 6.2026\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.0044 - mse: 3.8573 - val_loss: 0.0070 - val_mse: 6.1203\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0042 - mse: 3.6294 - val_loss: 0.0074 - val_mse: 6.4269\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0044 - mse: 3.8012 - val_loss: 0.0072 - val_mse: 6.2345\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0041 - mse: 3.6024 - val_loss: 0.0071 - val_mse: 6.1792\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0042 - mse: 3.6658 - val_loss: 0.0070 - val_mse: 6.0731\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0041 - mse: 3.6047 - val_loss: 0.0070 - val_mse: 6.1107\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0041 - mse: 3.5637 - val_loss: 0.0072 - val_mse: 6.2490\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0041 - mse: 3.5588 - val_loss: 0.0072 - val_mse: 6.2291\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0040 - mse: 3.5132 - val_loss: 0.0070 - val_mse: 6.1031\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0040 - mse: 3.4885 - val_loss: 0.0069 - val_mse: 6.0056\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0040 - mse: 3.4701 - val_loss: 0.0069 - val_mse: 6.0146\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0039 - mse: 3.4328 - val_loss: 0.0070 - val_mse: 6.1172\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0039 - mse: 3.4089 - val_loss: 0.0070 - val_mse: 6.1246\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0039 - mse: 3.3939 - val_loss: 0.0069 - val_mse: 5.9891\n",
      "Epoch 240/1000\n",
      "\n",
      "Epoch 00239: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0038 - mse: 3.3431 - val_loss: 0.0068 - val_mse: 5.9290\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0038 - mse: 3.3517 - val_loss: 0.0068 - val_mse: 5.9105\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0038 - mse: 3.2967 - val_loss: 0.0069 - val_mse: 5.9925\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0038 - mse: 3.2973 - val_loss: 0.0069 - val_mse: 6.0268\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0038 - mse: 3.2847 - val_loss: 0.0069 - val_mse: 5.9699\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0037 - mse: 3.2481 - val_loss: 0.0068 - val_mse: 5.9146\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0037 - mse: 3.2490 - val_loss: 0.0067 - val_mse: 5.8577\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0037 - mse: 3.2225 - val_loss: 0.0067 - val_mse: 5.8692\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0037 - mse: 3.2060 - val_loss: 0.0068 - val_mse: 5.9175\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0037 - mse: 3.1927 - val_loss: 0.0068 - val_mse: 5.9344\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0036 - mse: 3.1700 - val_loss: 0.0068 - val_mse: 5.9164\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0036 - mse: 3.1599 - val_loss: 0.0067 - val_mse: 5.8582\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0036 - mse: 3.1365 - val_loss: 0.0067 - val_mse: 5.8245\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0036 - mse: 3.1243 - val_loss: 0.0067 - val_mse: 5.8261\n",
      "Epoch 254/1000\n",
      "\n",
      "Epoch 00253: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0036 - mse: 3.1072 - val_loss: 0.0067 - val_mse: 5.8478\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0035 - mse: 3.0872 - val_loss: 0.0067 - val_mse: 5.8570\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0035 - mse: 3.0791 - val_loss: 0.0067 - val_mse: 5.8412\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0035 - mse: 3.0645 - val_loss: 0.0067 - val_mse: 5.8141\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0035 - mse: 3.0528 - val_loss: 0.0067 - val_mse: 5.7920\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0035 - mse: 3.0446 - val_loss: 0.0066 - val_mse: 5.7793\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0035 - mse: 3.0311 - val_loss: 0.0066 - val_mse: 5.7838\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0035 - mse: 3.0191 - val_loss: 0.0067 - val_mse: 5.7970\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0035 - mse: 3.0102 - val_loss: 0.0067 - val_mse: 5.7992\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0034 - mse: 2.9974 - val_loss: 0.0066 - val_mse: 5.7886\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0034 - mse: 2.9858 - val_loss: 0.0066 - val_mse: 5.7710\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0034 - mse: 2.9764 - val_loss: 0.0066 - val_mse: 5.7514\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0034 - mse: 2.9641 - val_loss: 0.0066 - val_mse: 5.7414\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0034 - mse: 2.9526 - val_loss: 0.0066 - val_mse: 5.7431\n",
      "Epoch 268/1000\n",
      "\n",
      "Epoch 00267: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0034 - mse: 2.9427 - val_loss: 0.0066 - val_mse: 5.7456\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0034 - mse: 2.9308 - val_loss: 0.0066 - val_mse: 5.7428\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0034 - mse: 2.9225 - val_loss: 0.0066 - val_mse: 5.7350\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0033 - mse: 2.9152 - val_loss: 0.0066 - val_mse: 5.7234\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0033 - mse: 2.9077 - val_loss: 0.0066 - val_mse: 5.7115\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0033 - mse: 2.8995 - val_loss: 0.0065 - val_mse: 5.7027\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0033 - mse: 2.8918 - val_loss: 0.0065 - val_mse: 5.6977\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0033 - mse: 2.8842 - val_loss: 0.0065 - val_mse: 5.6947\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0033 - mse: 2.8764 - val_loss: 0.0065 - val_mse: 5.6926\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0033 - mse: 2.8686 - val_loss: 0.0065 - val_mse: 5.6899\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0033 - mse: 2.8610 - val_loss: 0.0065 - val_mse: 5.6856\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0033 - mse: 2.8532 - val_loss: 0.0065 - val_mse: 5.6799\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0033 - mse: 2.8455 - val_loss: 0.0065 - val_mse: 5.6735\n",
      "Epoch 281/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0033 - mse: 2.8380 - val_loss: 0.0065 - val_mse: 5.6670\n",
      "Epoch 282/1000\n",
      "\n",
      "Epoch 00281: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0033 - mse: 2.8303 - val_loss: 0.0065 - val_mse: 5.6604\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0032 - mse: 2.8224 - val_loss: 0.0065 - val_mse: 5.6558\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0032 - mse: 2.8170 - val_loss: 0.0065 - val_mse: 5.6514\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0032 - mse: 2.8118 - val_loss: 0.0065 - val_mse: 5.6482\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0032 - mse: 2.8064 - val_loss: 0.0065 - val_mse: 5.6461\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0032 - mse: 2.8009 - val_loss: 0.0065 - val_mse: 5.6441\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0032 - mse: 2.7956 - val_loss: 0.0065 - val_mse: 5.6405\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0032 - mse: 2.7903 - val_loss: 0.0065 - val_mse: 5.6354\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0032 - mse: 2.7850 - val_loss: 0.0065 - val_mse: 5.6299\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0032 - mse: 2.7796 - val_loss: 0.0065 - val_mse: 5.6260\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0032 - mse: 2.7742 - val_loss: 0.0065 - val_mse: 5.6230\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0032 - mse: 2.7689 - val_loss: 0.0065 - val_mse: 5.6197\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0032 - mse: 2.7637 - val_loss: 0.0064 - val_mse: 5.6158\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0032 - mse: 2.7583 - val_loss: 0.0064 - val_mse: 5.6114\n",
      "Epoch 296/1000\n",
      "\n",
      "Epoch 00295: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0032 - mse: 2.7530 - val_loss: 0.0064 - val_mse: 5.6072\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0032 - mse: 2.7477 - val_loss: 0.0064 - val_mse: 5.6045\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0032 - mse: 2.7440 - val_loss: 0.0064 - val_mse: 5.6020\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0031 - mse: 2.7403 - val_loss: 0.0064 - val_mse: 5.6001\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0031 - mse: 2.7366 - val_loss: 0.0064 - val_mse: 5.5978\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0031 - mse: 2.7329 - val_loss: 0.0064 - val_mse: 5.5952\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0031 - mse: 2.7291 - val_loss: 0.0064 - val_mse: 5.5921\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0031 - mse: 2.7254 - val_loss: 0.0064 - val_mse: 5.5895\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0031 - mse: 2.7217 - val_loss: 0.0064 - val_mse: 5.5873\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0031 - mse: 2.7179 - val_loss: 0.0064 - val_mse: 5.5852\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0031 - mse: 2.7141 - val_loss: 0.0064 - val_mse: 5.5834\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0031 - mse: 2.7104 - val_loss: 0.0064 - val_mse: 5.5816\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0031 - mse: 2.7067 - val_loss: 0.0064 - val_mse: 5.5792\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0031 - mse: 2.7030 - val_loss: 0.0064 - val_mse: 5.5764\n",
      "Epoch 310/1000\n",
      "\n",
      "Epoch 00309: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0031 - mse: 2.6993 - val_loss: 0.0064 - val_mse: 5.5736\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0031 - mse: 2.6956 - val_loss: 0.0064 - val_mse: 5.5717\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0031 - mse: 2.6930 - val_loss: 0.0064 - val_mse: 5.5699\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0031 - mse: 2.6904 - val_loss: 0.0064 - val_mse: 5.5681\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0031 - mse: 2.6879 - val_loss: 0.0064 - val_mse: 5.5660\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0031 - mse: 2.6853 - val_loss: 0.0064 - val_mse: 5.5639\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0031 - mse: 2.6827 - val_loss: 0.0064 - val_mse: 5.5619\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0031 - mse: 2.6802 - val_loss: 0.0064 - val_mse: 5.5604\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0031 - mse: 2.6776 - val_loss: 0.0064 - val_mse: 5.5593\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0031 - mse: 2.6750 - val_loss: 0.0064 - val_mse: 5.5580\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0031 - mse: 2.6725 - val_loss: 0.0064 - val_mse: 5.5563\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0031 - mse: 2.6699 - val_loss: 0.0064 - val_mse: 5.5547\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0031 - mse: 2.6674 - val_loss: 0.0064 - val_mse: 5.5531\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0031 - mse: 2.6648 - val_loss: 0.0064 - val_mse: 5.5514\n",
      "Epoch 324/1000\n",
      "\n",
      "Epoch 00323: reducing learning rate to 0.000282475253334.\n",
      "0s - loss: 0.0031 - mse: 2.6623 - val_loss: 0.0064 - val_mse: 5.5492\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0031 - mse: 2.6597 - val_loss: 0.0064 - val_mse: 5.5478\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0031 - mse: 2.6579 - val_loss: 0.0064 - val_mse: 5.5465\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.0031 - mse: 2.6561 - val_loss: 0.0064 - val_mse: 5.5454\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0030 - mse: 2.6543 - val_loss: 0.0064 - val_mse: 5.5444\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0030 - mse: 2.6525 - val_loss: 0.0064 - val_mse: 5.5433\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0030 - mse: 2.6507 - val_loss: 0.0064 - val_mse: 5.5422\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0030 - mse: 2.6490 - val_loss: 0.0064 - val_mse: 5.5410\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0030 - mse: 2.6472 - val_loss: 0.0064 - val_mse: 5.5397\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0030 - mse: 2.6454 - val_loss: 0.0064 - val_mse: 5.5385\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0030 - mse: 2.6436 - val_loss: 0.0064 - val_mse: 5.5374\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0030 - mse: 2.6419 - val_loss: 0.0064 - val_mse: 5.5362\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0030 - mse: 2.6401 - val_loss: 0.0064 - val_mse: 5.5350\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0030 - mse: 2.6383 - val_loss: 0.0064 - val_mse: 5.5338\n",
      "Epoch 338/1000\n",
      "\n",
      "Epoch 00337: reducing learning rate to 0.000197732681409.\n",
      "0s - loss: 0.0030 - mse: 2.6366 - val_loss: 0.0064 - val_mse: 5.5328\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.0030 - mse: 2.6348 - val_loss: 0.0064 - val_mse: 5.5320\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.0030 - mse: 2.6336 - val_loss: 0.0064 - val_mse: 5.5312\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.0030 - mse: 2.6324 - val_loss: 0.0064 - val_mse: 5.5303\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.0030 - mse: 2.6312 - val_loss: 0.0063 - val_mse: 5.5295\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.0030 - mse: 2.6299 - val_loss: 0.0063 - val_mse: 5.5287\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.0030 - mse: 2.6287 - val_loss: 0.0063 - val_mse: 5.5278\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.0030 - mse: 2.6275 - val_loss: 0.0063 - val_mse: 5.5269\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.0030 - mse: 2.6263 - val_loss: 0.0063 - val_mse: 5.5258\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.0030 - mse: 2.6251 - val_loss: 0.0063 - val_mse: 5.5248\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.0030 - mse: 2.6238 - val_loss: 0.0063 - val_mse: 5.5240\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.0030 - mse: 2.6226 - val_loss: 0.0063 - val_mse: 5.5234\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.0030 - mse: 2.6214 - val_loss: 0.0063 - val_mse: 5.5227\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.0030 - mse: 2.6202 - val_loss: 0.0063 - val_mse: 5.5218\n",
      "Epoch 352/1000\n",
      "\n",
      "Epoch 00351: reducing learning rate to 0.000138412872911.\n",
      "0s - loss: 0.0030 - mse: 2.6190 - val_loss: 0.0063 - val_mse: 5.5209\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.0030 - mse: 2.6178 - val_loss: 0.0063 - val_mse: 5.5203\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.0030 - mse: 2.6169 - val_loss: 0.0063 - val_mse: 5.5197\n",
      "Epoch 355/1000\n",
      "0s - loss: 0.0030 - mse: 2.6161 - val_loss: 0.0063 - val_mse: 5.5192\n",
      "Epoch 356/1000\n",
      "0s - loss: 0.0030 - mse: 2.6152 - val_loss: 0.0063 - val_mse: 5.5186\n",
      "Epoch 357/1000\n",
      "0s - loss: 0.0030 - mse: 2.6144 - val_loss: 0.0063 - val_mse: 5.5179\n",
      "Epoch 358/1000\n",
      "0s - loss: 0.0030 - mse: 2.6135 - val_loss: 0.0063 - val_mse: 5.5173\n",
      "Epoch 359/1000\n",
      "0s - loss: 0.0030 - mse: 2.6127 - val_loss: 0.0063 - val_mse: 5.5166\n",
      "Epoch 360/1000\n",
      "0s - loss: 0.0030 - mse: 2.6118 - val_loss: 0.0063 - val_mse: 5.5160\n",
      "Epoch 361/1000\n",
      "0s - loss: 0.0030 - mse: 2.6110 - val_loss: 0.0063 - val_mse: 5.5155\n",
      "Epoch 362/1000\n",
      "0s - loss: 0.0030 - mse: 2.6102 - val_loss: 0.0063 - val_mse: 5.5150\n",
      "Epoch 363/1000\n",
      "0s - loss: 0.0030 - mse: 2.6093 - val_loss: 0.0063 - val_mse: 5.5146\n",
      "Epoch 364/1000\n",
      "0s - loss: 0.0030 - mse: 2.6085 - val_loss: 0.0063 - val_mse: 5.5142\n",
      "Epoch 365/1000\n",
      "0s - loss: 0.0030 - mse: 2.6076 - val_loss: 0.0063 - val_mse: 5.5137\n",
      "Epoch 366/1000\n",
      "\n",
      "Epoch 00365: reducing learning rate to 9.68890090007e-05.\n",
      "0s - loss: 0.0030 - mse: 2.6068 - val_loss: 0.0063 - val_mse: 5.5131\n",
      "Epoch 367/1000\n",
      "0s - loss: 0.0030 - mse: 2.6060 - val_loss: 0.0063 - val_mse: 5.5126\n",
      "Epoch 368/1000\n",
      "0s - loss: 0.0030 - mse: 2.6054 - val_loss: 0.0063 - val_mse: 5.5121\n",
      "Epoch 369/1000\n",
      "0s - loss: 0.0030 - mse: 2.6048 - val_loss: 0.0063 - val_mse: 5.5115\n",
      "Epoch 370/1000\n",
      "0s - loss: 0.0030 - mse: 2.6042 - val_loss: 0.0063 - val_mse: 5.5110\n",
      "Epoch 371/1000\n",
      "0s - loss: 0.0030 - mse: 2.6036 - val_loss: 0.0063 - val_mse: 5.5107\n",
      "Epoch 372/1000\n",
      "0s - loss: 0.0030 - mse: 2.6030 - val_loss: 0.0063 - val_mse: 5.5104\n",
      "Epoch 373/1000\n",
      "0s - loss: 0.0030 - mse: 2.6024 - val_loss: 0.0063 - val_mse: 5.5101\n",
      "Epoch 374/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0030 - mse: 2.6019 - val_loss: 0.0063 - val_mse: 5.5098\n",
      "Epoch 375/1000\n",
      "0s - loss: 0.0030 - mse: 2.6013 - val_loss: 0.0063 - val_mse: 5.5093\n",
      "Epoch 376/1000\n",
      "0s - loss: 0.0030 - mse: 2.6007 - val_loss: 0.0063 - val_mse: 5.5088\n",
      "[{'loss': 0.0029864141251891851, 'run': 0, 'dropout': 0.0, 'val_mse': 5.508796215057373, 'label': '', 'width': 135, 'mse': 2.6006863117218018, 'val_loss': 0.0063258488662540913}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, None, 1)           136       \n",
      "=================================================================\n",
      "Total params: 74,656\n",
      "Trainable params: 74,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.8552 - mse: 744.7449 - val_loss: 0.5704 - val_mse: 496.7196\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.5258 - mse: 457.9178 - val_loss: 0.3682 - val_mse: 320.6707\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.3456 - mse: 300.9524 - val_loss: 0.3177 - val_mse: 276.6601\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.3083 - mse: 268.4990 - val_loss: 0.2351 - val_mse: 204.7233\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2262 - mse: 196.9733 - val_loss: 0.2971 - val_mse: 258.7527\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2809 - mse: 244.6419 - val_loss: 0.1750 - val_mse: 152.4134\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.1628 - mse: 141.7626 - val_loss: 0.1768 - val_mse: 153.9745\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1647 - mse: 143.4472 - val_loss: 0.1682 - val_mse: 146.5147\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1598 - mse: 139.1431 - val_loss: 0.1466 - val_mse: 127.6349\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1383 - mse: 120.4576 - val_loss: 0.1342 - val_mse: 116.8553\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1219 - mse: 106.1221 - val_loss: 0.1435 - val_mse: 124.9460\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1257 - mse: 109.4503 - val_loss: 0.1435 - val_mse: 124.9850\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1220 - mse: 106.2501 - val_loss: 0.1316 - val_mse: 114.5865\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1108 - mse: 96.4923 - val_loss: 0.1214 - val_mse: 105.7237\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1042 - mse: 90.7804 - val_loss: 0.1128 - val_mse: 98.1986\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.0991 - mse: 86.3323 - val_loss: 0.1053 - val_mse: 91.6971\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0939 - mse: 81.7801 - val_loss: 0.0997 - val_mse: 86.8344\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0896 - mse: 77.9994 - val_loss: 0.0946 - val_mse: 82.3773\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0854 - mse: 74.4064 - val_loss: 0.0902 - val_mse: 78.5862\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0824 - mse: 71.7666 - val_loss: 0.0877 - val_mse: 76.3637\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0813 - mse: 70.8386 - val_loss: 0.0812 - val_mse: 70.6949\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0750 - mse: 65.3459 - val_loss: 0.0768 - val_mse: 66.8642\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0709 - mse: 61.7038 - val_loss: 0.0745 - val_mse: 64.8514\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0685 - mse: 59.6691 - val_loss: 0.0720 - val_mse: 62.6999\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0655 - mse: 57.0288 - val_loss: 0.0703 - val_mse: 61.2634\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0633 - mse: 55.1319 - val_loss: 0.0696 - val_mse: 60.6472\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0627 - mse: 54.6004 - val_loss: 0.0672 - val_mse: 58.4941\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0612 - mse: 53.2761 - val_loss: 0.0620 - val_mse: 53.9557\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0574 - mse: 49.9678 - val_loss: 0.0580 - val_mse: 50.5220\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0544 - mse: 47.3597 - val_loss: 0.0580 - val_mse: 50.5482\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0545 - mse: 47.4768 - val_loss: 0.0562 - val_mse: 48.9250\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0534 - mse: 46.4794 - val_loss: 0.0536 - val_mse: 46.7100\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0511 - mse: 44.4909 - val_loss: 0.0509 - val_mse: 44.3013\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0478 - mse: 41.6198 - val_loss: 0.0507 - val_mse: 44.1201\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0469 - mse: 40.8334 - val_loss: 0.0495 - val_mse: 43.1266\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0452 - mse: 39.3952 - val_loss: 0.0489 - val_mse: 42.5974\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0445 - mse: 38.7779 - val_loss: 0.0485 - val_mse: 42.2621\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0442 - mse: 38.5093 - val_loss: 0.0475 - val_mse: 41.3328\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0432 - mse: 37.5826 - val_loss: 0.0462 - val_mse: 40.2453\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0421 - mse: 36.6252 - val_loss: 0.0448 - val_mse: 39.0564\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0412 - mse: 35.8552 - val_loss: 0.0435 - val_mse: 37.8587\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0403 - mse: 35.0960 - val_loss: 0.0426 - val_mse: 37.0603\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0396 - mse: 34.4997 - val_loss: 0.0416 - val_mse: 36.2591\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0386 - mse: 33.6485 - val_loss: 0.0408 - val_mse: 35.5430\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0375 - mse: 32.6749 - val_loss: 0.0404 - val_mse: 35.1995\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0368 - mse: 32.0358 - val_loss: 0.0397 - val_mse: 34.5429\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0359 - mse: 31.2865 - val_loss: 0.0390 - val_mse: 33.9815\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0354 - mse: 30.7950 - val_loss: 0.0387 - val_mse: 33.6580\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0351 - mse: 30.5768 - val_loss: 0.0378 - val_mse: 32.8888\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0344 - mse: 29.9294 - val_loss: 0.0369 - val_mse: 32.1338\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0336 - mse: 29.2910 - val_loss: 0.0360 - val_mse: 31.3753\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0330 - mse: 28.7540 - val_loss: 0.0347 - val_mse: 30.2506\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0322 - mse: 28.0121 - val_loss: 0.0338 - val_mse: 29.4167\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0316 - mse: 27.5214 - val_loss: 0.0331 - val_mse: 28.8564\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0312 - mse: 27.1399 - val_loss: 0.0325 - val_mse: 28.3132\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0306 - mse: 26.6176 - val_loss: 0.0321 - val_mse: 27.9842\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0301 - mse: 26.2211 - val_loss: 0.0316 - val_mse: 27.5113\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0296 - mse: 25.7536 - val_loss: 0.0309 - val_mse: 26.9300\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0290 - mse: 25.2461 - val_loss: 0.0305 - val_mse: 26.6005\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0286 - mse: 24.9309 - val_loss: 0.0302 - val_mse: 26.3325\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0282 - mse: 24.5951 - val_loss: 0.0300 - val_mse: 26.1629\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0279 - mse: 24.2946 - val_loss: 0.0298 - val_mse: 25.9531\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0276 - mse: 23.9940 - val_loss: 0.0293 - val_mse: 25.5232\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0271 - mse: 23.6117 - val_loss: 0.0289 - val_mse: 25.1405\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0268 - mse: 23.2989 - val_loss: 0.0285 - val_mse: 24.8417\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0264 - mse: 23.0156 - val_loss: 0.0283 - val_mse: 24.6824\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0262 - mse: 22.8336 - val_loss: 0.0281 - val_mse: 24.4329\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0259 - mse: 22.5500 - val_loss: 0.0278 - val_mse: 24.2264\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0256 - mse: 22.3111 - val_loss: 0.0276 - val_mse: 24.0234\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0253 - mse: 22.0303 - val_loss: 0.0275 - val_mse: 23.9047\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0250 - mse: 21.7992 - val_loss: 0.0273 - val_mse: 23.7625\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0247 - mse: 21.5399 - val_loss: 0.0272 - val_mse: 23.6517\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0245 - mse: 21.3147 - val_loss: 0.0269 - val_mse: 23.4531\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0242 - mse: 21.0624 - val_loss: 0.0266 - val_mse: 23.1564\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0239 - mse: 20.8217 - val_loss: 0.0261 - val_mse: 22.7284\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0236 - mse: 20.5224 - val_loss: 0.0257 - val_mse: 22.3863\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0233 - mse: 20.2944 - val_loss: 0.0254 - val_mse: 22.0829\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0230 - mse: 20.0209 - val_loss: 0.0251 - val_mse: 21.8504\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0227 - mse: 19.7691 - val_loss: 0.0248 - val_mse: 21.6153\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0224 - mse: 19.5110 - val_loss: 0.0245 - val_mse: 21.3649\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0221 - mse: 19.2309 - val_loss: 0.0243 - val_mse: 21.1631\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0218 - mse: 18.9845 - val_loss: 0.0241 - val_mse: 20.9544\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0215 - mse: 18.7332 - val_loss: 0.0237 - val_mse: 20.6586\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0212 - mse: 18.4430 - val_loss: 0.0234 - val_mse: 20.3353\n",
      "Epoch 85/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0209 - mse: 18.1736 - val_loss: 0.0230 - val_mse: 20.0195\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0206 - mse: 17.9114 - val_loss: 0.0226 - val_mse: 19.7096\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0202 - mse: 17.6209 - val_loss: 0.0223 - val_mse: 19.3811\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0199 - mse: 17.3165 - val_loss: 0.0219 - val_mse: 19.0971\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0195 - mse: 17.0105 - val_loss: 0.0216 - val_mse: 18.8378\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0192 - mse: 16.6941 - val_loss: 0.0214 - val_mse: 18.6108\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0188 - mse: 16.3990 - val_loss: 0.0211 - val_mse: 18.4143\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0186 - mse: 16.2165 - val_loss: 0.0219 - val_mse: 19.0953\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.0194 - mse: 16.8865 - val_loss: 0.0280 - val_mse: 24.4132\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0264 - mse: 23.0050 - val_loss: 0.0278 - val_mse: 24.2372\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0256 - mse: 22.3024 - val_loss: 0.0218 - val_mse: 18.9757\n",
      "Epoch 96/1000\n",
      "\n",
      "Epoch 00095: reducing learning rate to 0.00699999984354.\n",
      "1s - loss: 0.0199 - mse: 17.2891 - val_loss: 0.0225 - val_mse: 19.6328\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0207 - mse: 17.9931 - val_loss: 0.0209 - val_mse: 18.2137\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0182 - mse: 15.8590 - val_loss: 0.0232 - val_mse: 20.2169\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0202 - mse: 17.5536 - val_loss: 0.0209 - val_mse: 18.1983\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0180 - mse: 15.6501 - val_loss: 0.0213 - val_mse: 18.5522\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0185 - mse: 16.1161 - val_loss: 0.0214 - val_mse: 18.6384\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0186 - mse: 16.1690 - val_loss: 0.0205 - val_mse: 17.8686\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0172 - mse: 15.0134 - val_loss: 0.0213 - val_mse: 18.5876\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0180 - mse: 15.6745 - val_loss: 0.0199 - val_mse: 17.3215\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0171 - mse: 14.8491 - val_loss: 0.0195 - val_mse: 17.0130\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0170 - mse: 14.8013 - val_loss: 0.0195 - val_mse: 16.9592\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0170 - mse: 14.8453 - val_loss: 0.0191 - val_mse: 16.6405\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0163 - mse: 14.2063 - val_loss: 0.0197 - val_mse: 17.1215\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0165 - mse: 14.3333 - val_loss: 0.0189 - val_mse: 16.4753\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0160 - mse: 13.8984 - val_loss: 0.0186 - val_mse: 16.1708\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.0157 - mse: 13.7121 - val_loss: 0.0186 - val_mse: 16.2190\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0157 - mse: 13.7081 - val_loss: 0.0185 - val_mse: 16.1128\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0153 - mse: 13.3224 - val_loss: 0.0188 - val_mse: 16.3475\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0153 - mse: 13.3572 - val_loss: 0.0180 - val_mse: 15.6775\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0150 - mse: 13.0755 - val_loss: 0.0174 - val_mse: 15.1755\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0148 - mse: 12.9039 - val_loss: 0.0173 - val_mse: 15.1040\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0148 - mse: 12.8588 - val_loss: 0.0172 - val_mse: 14.9524\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0144 - mse: 12.5377 - val_loss: 0.0173 - val_mse: 15.0706\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0144 - mse: 12.5260 - val_loss: 0.0168 - val_mse: 14.6194\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0141 - mse: 12.2525 - val_loss: 0.0166 - val_mse: 14.4452\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0140 - mse: 12.1997 - val_loss: 0.0165 - val_mse: 14.3732\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0138 - mse: 12.0521 - val_loss: 0.0165 - val_mse: 14.3973\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0137 - mse: 11.9066 - val_loss: 0.0165 - val_mse: 14.3313\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0136 - mse: 11.8042 - val_loss: 0.0159 - val_mse: 13.8784\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0133 - mse: 11.5754 - val_loss: 0.0157 - val_mse: 13.7084\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0133 - mse: 11.5474 - val_loss: 0.0156 - val_mse: 13.5877\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0130 - mse: 11.3214 - val_loss: 0.0158 - val_mse: 13.7462\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0130 - mse: 11.2804 - val_loss: 0.0156 - val_mse: 13.5904\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0127 - mse: 11.0814 - val_loss: 0.0154 - val_mse: 13.4014\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.0126 - mse: 10.9840 - val_loss: 0.0152 - val_mse: 13.2661\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0125 - mse: 10.8526 - val_loss: 0.0152 - val_mse: 13.2204\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0123 - mse: 10.7013 - val_loss: 0.0151 - val_mse: 13.1707\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0122 - mse: 10.6047 - val_loss: 0.0148 - val_mse: 12.9013\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0120 - mse: 10.4471 - val_loss: 0.0146 - val_mse: 12.6946\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0119 - mse: 10.3715 - val_loss: 0.0144 - val_mse: 12.5694\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0117 - mse: 10.2128 - val_loss: 0.0145 - val_mse: 12.5882\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0116 - mse: 10.1097 - val_loss: 0.0143 - val_mse: 12.4689\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0114 - mse: 9.9673 - val_loss: 0.0141 - val_mse: 12.2729\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0113 - mse: 9.8553 - val_loss: 0.0139 - val_mse: 12.1480\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0112 - mse: 9.7420 - val_loss: 0.0139 - val_mse: 12.0657\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0110 - mse: 9.5971 - val_loss: 0.0138 - val_mse: 11.9757\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0109 - mse: 9.5039 - val_loss: 0.0135 - val_mse: 11.7443\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0108 - mse: 9.3635 - val_loss: 0.0133 - val_mse: 11.5857\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0106 - mse: 9.2597 - val_loss: 0.0132 - val_mse: 11.5293\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0105 - mse: 9.1435 - val_loss: 0.0132 - val_mse: 11.4797\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0104 - mse: 9.0286 - val_loss: 0.0130 - val_mse: 11.3081\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0102 - mse: 8.9127 - val_loss: 0.0128 - val_mse: 11.1535\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0101 - mse: 8.8211 - val_loss: 0.0127 - val_mse: 11.0772\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0100 - mse: 8.7004 - val_loss: 0.0126 - val_mse: 10.9899\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0099 - mse: 8.5937 - val_loss: 0.0124 - val_mse: 10.8330\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0097 - mse: 8.4815 - val_loss: 0.0123 - val_mse: 10.7506\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0096 - mse: 8.3901 - val_loss: 0.0123 - val_mse: 10.6937\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0095 - mse: 8.2783 - val_loss: 0.0122 - val_mse: 10.6275\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0094 - mse: 8.1743 - val_loss: 0.0120 - val_mse: 10.4716\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0093 - mse: 8.0624 - val_loss: 0.0119 - val_mse: 10.3952\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0091 - mse: 7.9507 - val_loss: 0.0119 - val_mse: 10.3403\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0090 - mse: 7.8432 - val_loss: 0.0117 - val_mse: 10.2316\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0089 - mse: 7.7331 - val_loss: 0.0116 - val_mse: 10.1042\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0088 - mse: 7.6342 - val_loss: 0.0116 - val_mse: 10.0933\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0087 - mse: 7.5487 - val_loss: 0.0115 - val_mse: 10.0330\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0086 - mse: 7.5230 - val_loss: 0.0119 - val_mse: 10.3269\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0089 - mse: 7.7609 - val_loss: 0.0127 - val_mse: 11.1004\n",
      "Epoch 163/1000\n",
      "\n",
      "Epoch 00162: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0101 - mse: 8.7737 - val_loss: 0.0171 - val_mse: 14.8808\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0144 - mse: 12.5670 - val_loss: 0.0114 - val_mse: 9.9684\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0088 - mse: 7.6755 - val_loss: 0.0150 - val_mse: 13.0710\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0130 - mse: 11.2914 - val_loss: 0.0129 - val_mse: 11.1952\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0101 - mse: 8.7705 - val_loss: 0.0146 - val_mse: 12.6946\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0117 - mse: 10.2203 - val_loss: 0.0115 - val_mse: 9.9839\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0085 - mse: 7.4352 - val_loss: 0.0141 - val_mse: 12.2879\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0117 - mse: 10.1477 - val_loss: 0.0111 - val_mse: 9.6541\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0082 - mse: 7.1524 - val_loss: 0.0134 - val_mse: 11.6929\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0105 - mse: 9.1532 - val_loss: 0.0120 - val_mse: 10.4123\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0089 - mse: 7.7802 - val_loss: 0.0117 - val_mse: 10.1991\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0088 - mse: 7.6835 - val_loss: 0.0122 - val_mse: 10.6318\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0096 - mse: 8.3376 - val_loss: 0.0110 - val_mse: 9.5826\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0081 - mse: 7.0718 - val_loss: 0.0121 - val_mse: 10.5729\n",
      "Epoch 177/1000\n",
      "\n",
      "Epoch 00176: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0090 - mse: 7.8789 - val_loss: 0.0118 - val_mse: 10.3088\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0086 - mse: 7.4499 - val_loss: 0.0112 - val_mse: 9.7503\n",
      "Epoch 179/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0080 - mse: 6.9389 - val_loss: 0.0112 - val_mse: 9.7650\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0083 - mse: 7.2173 - val_loss: 0.0111 - val_mse: 9.6671\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0083 - mse: 7.2497 - val_loss: 0.0107 - val_mse: 9.3480\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0078 - mse: 6.8096 - val_loss: 0.0110 - val_mse: 9.6107\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0079 - mse: 6.8864 - val_loss: 0.0114 - val_mse: 9.8852\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0080 - mse: 7.0066 - val_loss: 0.0110 - val_mse: 9.5967\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.0077 - mse: 6.7008 - val_loss: 0.0107 - val_mse: 9.3432\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0076 - mse: 6.6178 - val_loss: 0.0107 - val_mse: 9.3511\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0078 - mse: 6.7716 - val_loss: 0.0106 - val_mse: 9.1930\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0075 - mse: 6.5607 - val_loss: 0.0106 - val_mse: 9.2439\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0074 - mse: 6.4433 - val_loss: 0.0109 - val_mse: 9.5033\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0075 - mse: 6.5688 - val_loss: 0.0107 - val_mse: 9.3527\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0074 - mse: 6.4403 - val_loss: 0.0104 - val_mse: 9.0541\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0072 - mse: 6.3004 - val_loss: 0.0104 - val_mse: 9.0458\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0074 - mse: 6.4044 - val_loss: 0.0104 - val_mse: 9.0146\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0072 - mse: 6.3054 - val_loss: 0.0104 - val_mse: 9.0849\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0071 - mse: 6.1848 - val_loss: 0.0107 - val_mse: 9.2938\n",
      "Epoch 196/1000\n",
      "\n",
      "Epoch 00195: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0072 - mse: 6.2624 - val_loss: 0.0105 - val_mse: 9.1538\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0071 - mse: 6.1712 - val_loss: 0.0103 - val_mse: 8.9596\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0070 - mse: 6.0827 - val_loss: 0.0102 - val_mse: 8.8836\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0070 - mse: 6.0986 - val_loss: 0.0102 - val_mse: 8.8746\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0070 - mse: 6.1040 - val_loss: 0.0102 - val_mse: 8.8704\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0069 - mse: 6.0271 - val_loss: 0.0103 - val_mse: 8.9373\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0069 - mse: 5.9886 - val_loss: 0.0104 - val_mse: 9.0163\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0069 - mse: 6.0039 - val_loss: 0.0103 - val_mse: 8.9734\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0069 - mse: 5.9722 - val_loss: 0.0102 - val_mse: 8.8558\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.0068 - mse: 5.9131 - val_loss: 0.0101 - val_mse: 8.7924\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0068 - mse: 5.9025 - val_loss: 0.0101 - val_mse: 8.7840\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0068 - mse: 5.8989 - val_loss: 0.0101 - val_mse: 8.7870\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0067 - mse: 5.8523 - val_loss: 0.0101 - val_mse: 8.8226\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0067 - mse: 5.8177 - val_loss: 0.0102 - val_mse: 8.8639\n",
      "Epoch 210/1000\n",
      "\n",
      "Epoch 00209: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0067 - mse: 5.8139 - val_loss: 0.0101 - val_mse: 8.8384\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0066 - mse: 5.7888 - val_loss: 0.0101 - val_mse: 8.7844\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0066 - mse: 5.7572 - val_loss: 0.0100 - val_mse: 8.7363\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0066 - mse: 5.7373 - val_loss: 0.0100 - val_mse: 8.7135\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0066 - mse: 5.7311 - val_loss: 0.0100 - val_mse: 8.7055\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0066 - mse: 5.7176 - val_loss: 0.0100 - val_mse: 8.7060\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0065 - mse: 5.6923 - val_loss: 0.0100 - val_mse: 8.7188\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0065 - mse: 5.6714 - val_loss: 0.0100 - val_mse: 8.7352\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0065 - mse: 5.6609 - val_loss: 0.0100 - val_mse: 8.7330\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0065 - mse: 5.6482 - val_loss: 0.0100 - val_mse: 8.7048\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0065 - mse: 5.6265 - val_loss: 0.0100 - val_mse: 8.6687\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0064 - mse: 5.6061 - val_loss: 0.0099 - val_mse: 8.6438\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0064 - mse: 5.5932 - val_loss: 0.0099 - val_mse: 8.6314\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0064 - mse: 5.5800 - val_loss: 0.0099 - val_mse: 8.6272\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0064 - mse: 5.5604 - val_loss: 0.0099 - val_mse: 8.6314\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0064 - mse: 5.5412 - val_loss: 0.0099 - val_mse: 8.6392\n",
      "Epoch 226/1000\n",
      "\n",
      "Epoch 00225: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0063 - mse: 5.5274 - val_loss: 0.0099 - val_mse: 8.6376\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0063 - mse: 5.5134 - val_loss: 0.0099 - val_mse: 8.6252\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0063 - mse: 5.5007 - val_loss: 0.0099 - val_mse: 8.6058\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.0063 - mse: 5.4876 - val_loss: 0.0099 - val_mse: 8.5860\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0063 - mse: 5.4766 - val_loss: 0.0098 - val_mse: 8.5693\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0063 - mse: 5.4670 - val_loss: 0.0098 - val_mse: 8.5563\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0063 - mse: 5.4563 - val_loss: 0.0098 - val_mse: 8.5473\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0063 - mse: 5.4439 - val_loss: 0.0098 - val_mse: 8.5428\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0062 - mse: 5.4318 - val_loss: 0.0098 - val_mse: 8.5414\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0062 - mse: 5.4211 - val_loss: 0.0098 - val_mse: 8.5398\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0062 - mse: 5.4110 - val_loss: 0.0098 - val_mse: 8.5343\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0062 - mse: 5.3999 - val_loss: 0.0098 - val_mse: 8.5250\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0062 - mse: 5.3881 - val_loss: 0.0098 - val_mse: 8.5135\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0062 - mse: 5.3767 - val_loss: 0.0098 - val_mse: 8.5021\n",
      "Epoch 240/1000\n",
      "\n",
      "Epoch 00239: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0062 - mse: 5.3661 - val_loss: 0.0098 - val_mse: 8.4921\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0061 - mse: 5.3556 - val_loss: 0.0097 - val_mse: 8.4863\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0061 - mse: 5.3476 - val_loss: 0.0097 - val_mse: 8.4820\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.0061 - mse: 5.3394 - val_loss: 0.0097 - val_mse: 8.4794\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0061 - mse: 5.3313 - val_loss: 0.0097 - val_mse: 8.4781\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0061 - mse: 5.3237 - val_loss: 0.0097 - val_mse: 8.4766\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0061 - mse: 5.3162 - val_loss: 0.0097 - val_mse: 8.4738\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0061 - mse: 5.3086 - val_loss: 0.0097 - val_mse: 8.4690\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0061 - mse: 5.3006 - val_loss: 0.0097 - val_mse: 8.4624\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0061 - mse: 5.2927 - val_loss: 0.0097 - val_mse: 8.4548\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0061 - mse: 5.2849 - val_loss: 0.0097 - val_mse: 8.4468\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0061 - mse: 5.2772 - val_loss: 0.0097 - val_mse: 8.4391\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0061 - mse: 5.2696 - val_loss: 0.0097 - val_mse: 8.4329\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0060 - mse: 5.2618 - val_loss: 0.0097 - val_mse: 8.4282\n",
      "Epoch 254/1000\n",
      "\n",
      "Epoch 00253: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0060 - mse: 5.2539 - val_loss: 0.0097 - val_mse: 8.4249\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0060 - mse: 5.2461 - val_loss: 0.0097 - val_mse: 8.4231\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0060 - mse: 5.2406 - val_loss: 0.0097 - val_mse: 8.4216\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.0060 - mse: 5.2353 - val_loss: 0.0097 - val_mse: 8.4195\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0060 - mse: 5.2299 - val_loss: 0.0097 - val_mse: 8.4166\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0060 - mse: 5.2246 - val_loss: 0.0097 - val_mse: 8.4130\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0060 - mse: 5.2192 - val_loss: 0.0097 - val_mse: 8.4089\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0060 - mse: 5.2137 - val_loss: 0.0097 - val_mse: 8.4046\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0060 - mse: 5.2084 - val_loss: 0.0096 - val_mse: 8.4008\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0060 - mse: 5.2032 - val_loss: 0.0096 - val_mse: 8.3977\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0060 - mse: 5.1980 - val_loss: 0.0096 - val_mse: 8.3951\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0060 - mse: 5.1928 - val_loss: 0.0096 - val_mse: 8.3930\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0060 - mse: 5.1875 - val_loss: 0.0096 - val_mse: 8.3912\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0060 - mse: 5.1824 - val_loss: 0.0096 - val_mse: 8.3893\n",
      "Epoch 268/1000\n",
      "\n",
      "Epoch 00267: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0059 - mse: 5.1772 - val_loss: 0.0096 - val_mse: 8.3869\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0059 - mse: 5.1721 - val_loss: 0.0096 - val_mse: 8.3849\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0059 - mse: 5.1685 - val_loss: 0.0096 - val_mse: 8.3828\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.0059 - mse: 5.1649 - val_loss: 0.0096 - val_mse: 8.3806\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0059 - mse: 5.1613 - val_loss: 0.0096 - val_mse: 8.3784\n",
      "Epoch 273/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0059 - mse: 5.1578 - val_loss: 0.0096 - val_mse: 8.3762\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0059 - mse: 5.1543 - val_loss: 0.0096 - val_mse: 8.3740\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0059 - mse: 5.1507 - val_loss: 0.0096 - val_mse: 8.3716\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0059 - mse: 5.1472 - val_loss: 0.0096 - val_mse: 8.3692\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0059 - mse: 5.1437 - val_loss: 0.0096 - val_mse: 8.3667\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0059 - mse: 5.1402 - val_loss: 0.0096 - val_mse: 8.3645\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0059 - mse: 5.1367 - val_loss: 0.0096 - val_mse: 8.3625\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0059 - mse: 5.1332 - val_loss: 0.0096 - val_mse: 8.3608\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0059 - mse: 5.1297 - val_loss: 0.0096 - val_mse: 8.3592\n",
      "Epoch 282/1000\n",
      "\n",
      "Epoch 00281: reducing learning rate to 0.000282475253334.\n",
      "0s - loss: 0.0059 - mse: 5.1262 - val_loss: 0.0096 - val_mse: 8.3576\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0059 - mse: 5.1227 - val_loss: 0.0096 - val_mse: 8.3564\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0059 - mse: 5.1203 - val_loss: 0.0096 - val_mse: 8.3552\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.0059 - mse: 5.1179 - val_loss: 0.0096 - val_mse: 8.3539\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0059 - mse: 5.1154 - val_loss: 0.0096 - val_mse: 8.3527\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0059 - mse: 5.1130 - val_loss: 0.0096 - val_mse: 8.3516\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0059 - mse: 5.1105 - val_loss: 0.0096 - val_mse: 8.3505\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0059 - mse: 5.1081 - val_loss: 0.0096 - val_mse: 8.3495\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0059 - mse: 5.1057 - val_loss: 0.0096 - val_mse: 8.3485\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0059 - mse: 5.1033 - val_loss: 0.0096 - val_mse: 8.3475\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0059 - mse: 5.1008 - val_loss: 0.0096 - val_mse: 8.3463\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0059 - mse: 5.0984 - val_loss: 0.0096 - val_mse: 8.3451\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0059 - mse: 5.0960 - val_loss: 0.0096 - val_mse: 8.3439\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0058 - mse: 5.0936 - val_loss: 0.0096 - val_mse: 8.3428\n",
      "Epoch 296/1000\n",
      "\n",
      "Epoch 00295: reducing learning rate to 0.000197732681409.\n",
      "0s - loss: 0.0058 - mse: 5.0912 - val_loss: 0.0096 - val_mse: 8.3417\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0058 - mse: 5.0888 - val_loss: 0.0096 - val_mse: 8.3410\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0058 - mse: 5.0871 - val_loss: 0.0096 - val_mse: 8.3403\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.0058 - mse: 5.0854 - val_loss: 0.0096 - val_mse: 8.3396\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0058 - mse: 5.0838 - val_loss: 0.0096 - val_mse: 8.3388\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0058 - mse: 5.0821 - val_loss: 0.0096 - val_mse: 8.3379\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0058 - mse: 5.0804 - val_loss: 0.0096 - val_mse: 8.3370\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0058 - mse: 5.0788 - val_loss: 0.0096 - val_mse: 8.3361\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0058 - mse: 5.0771 - val_loss: 0.0096 - val_mse: 8.3352\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0058 - mse: 5.0755 - val_loss: 0.0096 - val_mse: 8.3344\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0058 - mse: 5.0738 - val_loss: 0.0096 - val_mse: 8.3336\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0058 - mse: 5.0722 - val_loss: 0.0096 - val_mse: 8.3328\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0058 - mse: 5.0705 - val_loss: 0.0096 - val_mse: 8.3322\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0058 - mse: 5.0689 - val_loss: 0.0096 - val_mse: 8.3316\n",
      "Epoch 310/1000\n",
      "\n",
      "Epoch 00309: reducing learning rate to 0.000138412872911.\n",
      "0s - loss: 0.0058 - mse: 5.0672 - val_loss: 0.0096 - val_mse: 8.3309\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0058 - mse: 5.0656 - val_loss: 0.0096 - val_mse: 8.3304\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0058 - mse: 5.0644 - val_loss: 0.0096 - val_mse: 8.3299\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.0058 - mse: 5.0633 - val_loss: 0.0096 - val_mse: 8.3293\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0058 - mse: 5.0621 - val_loss: 0.0096 - val_mse: 8.3288\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0058 - mse: 5.0610 - val_loss: 0.0096 - val_mse: 8.3282\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0058 - mse: 5.0598 - val_loss: 0.0096 - val_mse: 8.3276\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0058 - mse: 5.0587 - val_loss: 0.0096 - val_mse: 8.3271\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0058 - mse: 5.0576 - val_loss: 0.0096 - val_mse: 8.3265\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.0058 - mse: 5.0564 - val_loss: 0.0096 - val_mse: 8.3260\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0058 - mse: 5.0553 - val_loss: 0.0096 - val_mse: 8.3254\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0058 - mse: 5.0542 - val_loss: 0.0096 - val_mse: 8.3249\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0058 - mse: 5.0530 - val_loss: 0.0096 - val_mse: 8.3243\n",
      "[{'loss': 0.0029864141251891851, 'run': 0, 'dropout': 0.0, 'val_mse': 5.508796215057373, 'label': '', 'width': 135, 'mse': 2.6006863117218018, 'val_loss': 0.0063258488662540913}, {'loss': 0.0058024991303682327, 'run': 1, 'dropout': 0.0, 'val_mse': 8.3242835998535156, 'label': '', 'width': 135, 'mse': 5.053041934967041, 'val_loss': 0.0095589244738221169}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, None, 1)           136       \n",
      "=================================================================\n",
      "Total params: 74,656\n",
      "Trainable params: 74,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 1.2004 - mse: 1045.3596 - val_loss: 0.6737 - val_mse: 586.6693\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.6215 - mse: 541.2297 - val_loss: 0.3848 - val_mse: 335.1357\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.3589 - mse: 312.5642 - val_loss: 0.4376 - val_mse: 381.0556\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.4290 - mse: 373.5837 - val_loss: 0.3176 - val_mse: 276.5996\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.3047 - mse: 265.3586 - val_loss: 0.2308 - val_mse: 200.9966\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2239 - mse: 194.9586 - val_loss: 0.2302 - val_mse: 200.4707\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.2219 - mse: 193.2511 - val_loss: 0.1662 - val_mse: 144.7220\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.1541 - mse: 134.2384 - val_loss: 0.1485 - val_mse: 129.2766\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.1357 - mse: 118.1374 - val_loss: 0.1332 - val_mse: 116.0156\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.1201 - mse: 104.6165 - val_loss: 0.1522 - val_mse: 132.5723\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.1335 - mse: 116.2844 - val_loss: 0.1519 - val_mse: 132.2557\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.1281 - mse: 111.5508 - val_loss: 0.1318 - val_mse: 114.7333\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.1107 - mse: 96.4419 - val_loss: 0.1247 - val_mse: 108.5963\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.1065 - mse: 92.7657 - val_loss: 0.1197 - val_mse: 104.2760\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.1031 - mse: 89.8166 - val_loss: 0.1120 - val_mse: 97.5540\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.0967 - mse: 84.2165 - val_loss: 0.1020 - val_mse: 88.8154\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0884 - mse: 76.9886 - val_loss: 0.0931 - val_mse: 81.1018\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0815 - mse: 70.9656 - val_loss: 0.0908 - val_mse: 79.0318\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0808 - mse: 70.3593 - val_loss: 0.0880 - val_mse: 76.6575\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0801 - mse: 69.7293 - val_loss: 0.0792 - val_mse: 68.9670\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0734 - mse: 63.9522 - val_loss: 0.0724 - val_mse: 63.0588\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0676 - mse: 58.8396 - val_loss: 0.0730 - val_mse: 63.5685\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0676 - mse: 58.8388 - val_loss: 0.0755 - val_mse: 65.7695\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0692 - mse: 60.2927 - val_loss: 0.0724 - val_mse: 63.0656\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0662 - mse: 57.6513 - val_loss: 0.0646 - val_mse: 56.2544\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0593 - mse: 51.6452 - val_loss: 0.0616 - val_mse: 53.6638\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0575 - mse: 50.0513 - val_loss: 0.0615 - val_mse: 53.5723\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0581 - mse: 50.6255 - val_loss: 0.0592 - val_mse: 51.5416\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0559 - mse: 48.6480 - val_loss: 0.0554 - val_mse: 48.2749\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0515 - mse: 44.8845 - val_loss: 0.0525 - val_mse: 45.7223\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0474 - mse: 41.3192 - val_loss: 0.0541 - val_mse: 47.1012\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0480 - mse: 41.7894 - val_loss: 0.0534 - val_mse: 46.5284\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0469 - mse: 40.8073 - val_loss: 0.0511 - val_mse: 44.4807\n",
      "Epoch 34/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0450 - mse: 39.2060 - val_loss: 0.0481 - val_mse: 41.9116\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0430 - mse: 37.4340 - val_loss: 0.0475 - val_mse: 41.3354\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0431 - mse: 37.5687 - val_loss: 0.0461 - val_mse: 40.1161\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0415 - mse: 36.1256 - val_loss: 0.0457 - val_mse: 39.7917\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0405 - mse: 35.3112 - val_loss: 0.0442 - val_mse: 38.4877\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0389 - mse: 33.9057 - val_loss: 0.0432 - val_mse: 37.5834\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0382 - mse: 33.2524 - val_loss: 0.0419 - val_mse: 36.5056\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0372 - mse: 32.4365 - val_loss: 0.0409 - val_mse: 35.5838\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0365 - mse: 31.7578 - val_loss: 0.0399 - val_mse: 34.7357\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0359 - mse: 31.2292 - val_loss: 0.0381 - val_mse: 33.2092\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0345 - mse: 30.0559 - val_loss: 0.0371 - val_mse: 32.2791\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0337 - mse: 29.3726 - val_loss: 0.0362 - val_mse: 31.5437\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0328 - mse: 28.5389 - val_loss: 0.0360 - val_mse: 31.3609\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0324 - mse: 28.2146 - val_loss: 0.0351 - val_mse: 30.5551\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0316 - mse: 27.4864 - val_loss: 0.0339 - val_mse: 29.5534\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0307 - mse: 26.7698 - val_loss: 0.0331 - val_mse: 28.8397\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0302 - mse: 26.2664 - val_loss: 0.0327 - val_mse: 28.4469\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0297 - mse: 25.8639 - val_loss: 0.0322 - val_mse: 28.0438\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0292 - mse: 25.4118 - val_loss: 0.0315 - val_mse: 27.4147\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0285 - mse: 24.8184 - val_loss: 0.0310 - val_mse: 27.0066\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0281 - mse: 24.4729 - val_loss: 0.0304 - val_mse: 26.4682\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0275 - mse: 23.9741 - val_loss: 0.0299 - val_mse: 26.0400\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0272 - mse: 23.6802 - val_loss: 0.0292 - val_mse: 25.4549\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0268 - mse: 23.3036 - val_loss: 0.0287 - val_mse: 24.9557\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0264 - mse: 22.9473 - val_loss: 0.0282 - val_mse: 24.5622\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0259 - mse: 22.5133 - val_loss: 0.0281 - val_mse: 24.4325\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0256 - mse: 22.3363 - val_loss: 0.0275 - val_mse: 23.9405\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0252 - mse: 21.9670 - val_loss: 0.0269 - val_mse: 23.4505\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0249 - mse: 21.6582 - val_loss: 0.0266 - val_mse: 23.1341\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0246 - mse: 21.3930 - val_loss: 0.0263 - val_mse: 22.9174\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0243 - mse: 21.1621 - val_loss: 0.0259 - val_mse: 22.5368\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0239 - mse: 20.7907 - val_loss: 0.0256 - val_mse: 22.3339\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0236 - mse: 20.5683 - val_loss: 0.0255 - val_mse: 22.1643\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0233 - mse: 20.2847 - val_loss: 0.0252 - val_mse: 21.9799\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0230 - mse: 20.0215 - val_loss: 0.0249 - val_mse: 21.7205\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0227 - mse: 19.7566 - val_loss: 0.0247 - val_mse: 21.5330\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0224 - mse: 19.5156 - val_loss: 0.0245 - val_mse: 21.3749\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0221 - mse: 19.2286 - val_loss: 0.0243 - val_mse: 21.1790\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0218 - mse: 18.9931 - val_loss: 0.0240 - val_mse: 20.8641\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0215 - mse: 18.7439 - val_loss: 0.0236 - val_mse: 20.5817\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.0212 - mse: 18.4651 - val_loss: 0.0234 - val_mse: 20.3734\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.0210 - mse: 18.2451 - val_loss: 0.0230 - val_mse: 20.0703\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.0207 - mse: 17.9919 - val_loss: 0.0228 - val_mse: 19.8361\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.0204 - mse: 17.7552 - val_loss: 0.0226 - val_mse: 19.6699\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.0201 - mse: 17.5330 - val_loss: 0.0223 - val_mse: 19.4028\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.0198 - mse: 17.2840 - val_loss: 0.0220 - val_mse: 19.1947\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.0196 - mse: 17.0507 - val_loss: 0.0219 - val_mse: 19.0798\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.0193 - mse: 16.8173 - val_loss: 0.0217 - val_mse: 18.8949\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.0190 - mse: 16.5799 - val_loss: 0.0215 - val_mse: 18.7415\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.0188 - mse: 16.3508 - val_loss: 0.0213 - val_mse: 18.5339\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.0185 - mse: 16.1128 - val_loss: 0.0210 - val_mse: 18.2483\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.0182 - mse: 15.8866 - val_loss: 0.0208 - val_mse: 18.0817\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.0180 - mse: 15.6553 - val_loss: 0.0204 - val_mse: 17.7824\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.0177 - mse: 15.4128 - val_loss: 0.0203 - val_mse: 17.6814\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.0175 - mse: 15.2013 - val_loss: 0.0199 - val_mse: 17.3436\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.0173 - mse: 15.0221 - val_loss: 0.0201 - val_mse: 17.5203\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.0173 - mse: 15.0268 - val_loss: 0.0200 - val_mse: 17.4194\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.0177 - mse: 15.3757 - val_loss: 0.0224 - val_mse: 19.4711\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.0195 - mse: 16.9806 - val_loss: 0.0210 - val_mse: 18.2969\n",
      "Epoch 93/1000\n",
      "\n",
      "Epoch 00092: reducing learning rate to 0.00699999984354.\n",
      "1s - loss: 0.0192 - mse: 16.6936 - val_loss: 0.0204 - val_mse: 17.8060\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.0178 - mse: 15.5356 - val_loss: 0.0184 - val_mse: 16.0460\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.0160 - mse: 13.9107 - val_loss: 0.0191 - val_mse: 16.6185\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.0173 - mse: 15.0247 - val_loss: 0.0176 - val_mse: 15.3509\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.0154 - mse: 13.3778 - val_loss: 0.0190 - val_mse: 16.5605\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.0164 - mse: 14.2779 - val_loss: 0.0176 - val_mse: 15.3623\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.0151 - mse: 13.1696 - val_loss: 0.0178 - val_mse: 15.5072\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.0157 - mse: 13.7073 - val_loss: 0.0171 - val_mse: 14.9012\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.0148 - mse: 12.8744 - val_loss: 0.0178 - val_mse: 15.4999\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.0151 - mse: 13.1519 - val_loss: 0.0172 - val_mse: 14.9755\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.0146 - mse: 12.7019 - val_loss: 0.0167 - val_mse: 14.5197\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.0145 - mse: 12.6286 - val_loss: 0.0164 - val_mse: 14.3048\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.0143 - mse: 12.4573 - val_loss: 0.0164 - val_mse: 14.3076\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.0140 - mse: 12.1999 - val_loss: 0.0164 - val_mse: 14.3201\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.0140 - mse: 12.2072 - val_loss: 0.0157 - val_mse: 13.6590\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.0135 - mse: 11.7971 - val_loss: 0.0157 - val_mse: 13.6669\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.0137 - mse: 11.9365 - val_loss: 0.0154 - val_mse: 13.4308\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.0132 - mse: 11.4846 - val_loss: 0.0157 - val_mse: 13.6775\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0133 - mse: 11.6217 - val_loss: 0.0151 - val_mse: 13.1300\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.0129 - mse: 11.1955 - val_loss: 0.0150 - val_mse: 13.1001\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.0130 - mse: 11.3085 - val_loss: 0.0148 - val_mse: 12.9008\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.0126 - mse: 10.9304 - val_loss: 0.0151 - val_mse: 13.1247\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.0126 - mse: 10.9864 - val_loss: 0.0146 - val_mse: 12.7540\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.0123 - mse: 10.6773 - val_loss: 0.0145 - val_mse: 12.6307\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.0123 - mse: 10.6944 - val_loss: 0.0143 - val_mse: 12.4749\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.0120 - mse: 10.4174 - val_loss: 0.0145 - val_mse: 12.6319\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.0120 - mse: 10.4145 - val_loss: 0.0142 - val_mse: 12.3286\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.0117 - mse: 10.1745 - val_loss: 0.0140 - val_mse: 12.1766\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.0117 - mse: 10.1575 - val_loss: 0.0138 - val_mse: 12.0492\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.0114 - mse: 9.9381 - val_loss: 0.0139 - val_mse: 12.1358\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.0114 - mse: 9.9124 - val_loss: 0.0136 - val_mse: 11.8743\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.0112 - mse: 9.7144 - val_loss: 0.0135 - val_mse: 11.7563\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.0111 - mse: 9.6820 - val_loss: 0.0134 - val_mse: 11.6738\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.0109 - mse: 9.4989 - val_loss: 0.0134 - val_mse: 11.6977\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.0109 - mse: 9.4575 - val_loss: 0.0131 - val_mse: 11.4475\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.0107 - mse: 9.2975 - val_loss: 0.0130 - val_mse: 11.3457\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.0106 - mse: 9.2441 - val_loss: 0.0130 - val_mse: 11.3165\n",
      "Epoch 130/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0105 - mse: 9.1106 - val_loss: 0.0129 - val_mse: 11.2666\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.0104 - mse: 9.0344 - val_loss: 0.0127 - val_mse: 11.0829\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.0103 - mse: 8.9342 - val_loss: 0.0126 - val_mse: 10.9886\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.0101 - mse: 8.8359 - val_loss: 0.0126 - val_mse: 10.9947\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.0101 - mse: 8.7675 - val_loss: 0.0125 - val_mse: 10.8679\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.0099 - mse: 8.6559 - val_loss: 0.0124 - val_mse: 10.7616\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.0099 - mse: 8.6023 - val_loss: 0.0123 - val_mse: 10.6737\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.0098 - mse: 8.4970 - val_loss: 0.0122 - val_mse: 10.6160\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.0097 - mse: 8.4339 - val_loss: 0.0121 - val_mse: 10.5100\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.0096 - mse: 8.3552 - val_loss: 0.0120 - val_mse: 10.4485\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.0095 - mse: 8.2723 - val_loss: 0.0120 - val_mse: 10.4102\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.0094 - mse: 8.2158 - val_loss: 0.0118 - val_mse: 10.2896\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.0093 - mse: 8.1292 - val_loss: 0.0117 - val_mse: 10.2249\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.0093 - mse: 8.0709 - val_loss: 0.0117 - val_mse: 10.1822\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.0092 - mse: 8.0016 - val_loss: 0.0116 - val_mse: 10.0773\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.0091 - mse: 7.9274 - val_loss: 0.0115 - val_mse: 9.9778\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.0090 - mse: 7.8739 - val_loss: 0.0114 - val_mse: 9.9221\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.0090 - mse: 7.8015 - val_loss: 0.0113 - val_mse: 9.8826\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.0089 - mse: 7.7395 - val_loss: 0.0113 - val_mse: 9.8187\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.0088 - mse: 7.6844 - val_loss: 0.0112 - val_mse: 9.7457\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.0087 - mse: 7.6164 - val_loss: 0.0111 - val_mse: 9.7018\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.0087 - mse: 7.5586 - val_loss: 0.0111 - val_mse: 9.6609\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.0086 - mse: 7.5046 - val_loss: 0.0110 - val_mse: 9.5844\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.0085 - mse: 7.4404 - val_loss: 0.0109 - val_mse: 9.5108\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.0085 - mse: 7.3842 - val_loss: 0.0109 - val_mse: 9.4718\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.0084 - mse: 7.3306 - val_loss: 0.0108 - val_mse: 9.4356\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.0083 - mse: 7.2706 - val_loss: 0.0108 - val_mse: 9.3768\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.0083 - mse: 7.2137 - val_loss: 0.0107 - val_mse: 9.3258\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.0082 - mse: 7.1623 - val_loss: 0.0107 - val_mse: 9.2968\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.0082 - mse: 7.1069 - val_loss: 0.0106 - val_mse: 9.2663\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.0081 - mse: 7.0523 - val_loss: 0.0106 - val_mse: 9.2121\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.0080 - mse: 7.0024 - val_loss: 0.0105 - val_mse: 9.1687\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.0080 - mse: 6.9530 - val_loss: 0.0105 - val_mse: 9.1551\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.0079 - mse: 6.9012 - val_loss: 0.0105 - val_mse: 9.1051\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.0079 - mse: 6.8504 - val_loss: 0.0104 - val_mse: 9.0411\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.0078 - mse: 6.8032 - val_loss: 0.0104 - val_mse: 9.0195\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.0078 - mse: 6.7562 - val_loss: 0.0103 - val_mse: 8.9862\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.0077 - mse: 6.7077 - val_loss: 0.0103 - val_mse: 8.9404\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.0076 - mse: 6.6590 - val_loss: 0.0102 - val_mse: 8.8998\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.0076 - mse: 6.6120 - val_loss: 0.0102 - val_mse: 8.8656\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.0075 - mse: 6.5661 - val_loss: 0.0101 - val_mse: 8.8223\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.0075 - mse: 6.5196 - val_loss: 0.0100 - val_mse: 8.7439\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.0074 - mse: 6.4725 - val_loss: 0.0100 - val_mse: 8.7244\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.0074 - mse: 6.4250 - val_loss: 0.0099 - val_mse: 8.6596\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.0073 - mse: 6.3774 - val_loss: 0.0099 - val_mse: 8.6017\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.0073 - mse: 6.3307 - val_loss: 0.0098 - val_mse: 8.5568\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.0072 - mse: 6.2843 - val_loss: 0.0098 - val_mse: 8.5016\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.0072 - mse: 6.2383 - val_loss: 0.0097 - val_mse: 8.4417\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.0071 - mse: 6.1932 - val_loss: 0.0096 - val_mse: 8.3723\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.0071 - mse: 6.1488 - val_loss: 0.0096 - val_mse: 8.3443\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.0070 - mse: 6.1057 - val_loss: 0.0095 - val_mse: 8.2518\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.0070 - mse: 6.0654 - val_loss: 0.0095 - val_mse: 8.2617\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.0069 - mse: 6.0333 - val_loss: 0.0094 - val_mse: 8.1649\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.0069 - mse: 6.0276 - val_loss: 0.0096 - val_mse: 8.3745\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.0070 - mse: 6.1046 - val_loss: 0.0097 - val_mse: 8.4365\n",
      "Epoch 185/1000\n",
      "\n",
      "Epoch 00184: reducing learning rate to 0.00489999982528.\n",
      "0s - loss: 0.0074 - mse: 6.4250 - val_loss: 0.0110 - val_mse: 9.5895\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.0083 - mse: 7.2190 - val_loss: 0.0098 - val_mse: 8.5124\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.0074 - mse: 6.4305 - val_loss: 0.0093 - val_mse: 8.0945\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.0070 - mse: 6.1042 - val_loss: 0.0102 - val_mse: 8.9174\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.0078 - mse: 6.8110 - val_loss: 0.0092 - val_mse: 7.9912\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.0066 - mse: 5.7705 - val_loss: 0.0098 - val_mse: 8.5299\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.0074 - mse: 6.4532 - val_loss: 0.0091 - val_mse: 7.9541\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.0067 - mse: 5.8593 - val_loss: 0.0094 - val_mse: 8.1705\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.0070 - mse: 6.0561 - val_loss: 0.0093 - val_mse: 8.1341\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.0069 - mse: 5.9940 - val_loss: 0.0091 - val_mse: 7.9593\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.0066 - mse: 5.7799 - val_loss: 0.0093 - val_mse: 8.1316\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.0069 - mse: 5.9989 - val_loss: 0.0088 - val_mse: 7.6774\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.0064 - mse: 5.6102 - val_loss: 0.0091 - val_mse: 7.9679\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.0068 - mse: 5.9152 - val_loss: 0.0089 - val_mse: 7.7112\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.0064 - mse: 5.5448 - val_loss: 0.0091 - val_mse: 7.9423\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.0067 - mse: 5.8051 - val_loss: 0.0087 - val_mse: 7.5496\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.0063 - mse: 5.5127 - val_loss: 0.0088 - val_mse: 7.7061\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.0065 - mse: 5.6717 - val_loss: 0.0088 - val_mse: 7.6642\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.0063 - mse: 5.4949 - val_loss: 0.0089 - val_mse: 7.7193\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.0064 - mse: 5.5469 - val_loss: 0.0087 - val_mse: 7.5403\n",
      "Epoch 205/1000\n",
      "\n",
      "Epoch 00204: reducing learning rate to 0.00343000004068.\n",
      "0s - loss: 0.0063 - mse: 5.4788 - val_loss: 0.0086 - val_mse: 7.4904\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.0062 - mse: 5.4357 - val_loss: 0.0086 - val_mse: 7.4975\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.0062 - mse: 5.3754 - val_loss: 0.0087 - val_mse: 7.5851\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.0062 - mse: 5.4195 - val_loss: 0.0086 - val_mse: 7.4591\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.0061 - mse: 5.3200 - val_loss: 0.0086 - val_mse: 7.4575\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.0062 - mse: 5.3732 - val_loss: 0.0085 - val_mse: 7.3781\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.0061 - mse: 5.2919 - val_loss: 0.0085 - val_mse: 7.4402\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.0061 - mse: 5.3111 - val_loss: 0.0085 - val_mse: 7.4346\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.0061 - mse: 5.2723 - val_loss: 0.0085 - val_mse: 7.3962\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.0060 - mse: 5.2498 - val_loss: 0.0085 - val_mse: 7.3657\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.0060 - mse: 5.2483 - val_loss: 0.0084 - val_mse: 7.3129\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.0060 - mse: 5.1985 - val_loss: 0.0084 - val_mse: 7.3409\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.0060 - mse: 5.2134 - val_loss: 0.0084 - val_mse: 7.2973\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.0059 - mse: 5.1591 - val_loss: 0.0084 - val_mse: 7.2977\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.0059 - mse: 5.1691 - val_loss: 0.0083 - val_mse: 7.2464\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.0059 - mse: 5.1266 - val_loss: 0.0083 - val_mse: 7.2328\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.0059 - mse: 5.1212 - val_loss: 0.0083 - val_mse: 7.2049\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.0059 - mse: 5.0953 - val_loss: 0.0083 - val_mse: 7.1878\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.0058 - mse: 5.0748 - val_loss: 0.0082 - val_mse: 7.1842\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.0058 - mse: 5.0614 - val_loss: 0.0082 - val_mse: 7.1589\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.0058 - mse: 5.0318 - val_loss: 0.0082 - val_mse: 7.1371\n",
      "Epoch 226/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0058 - mse: 5.0242 - val_loss: 0.0081 - val_mse: 7.0926\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.0057 - mse: 4.9922 - val_loss: 0.0081 - val_mse: 7.0943\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.0057 - mse: 4.9849 - val_loss: 0.0081 - val_mse: 7.0889\n",
      "Epoch 229/1000\n",
      "\n",
      "Epoch 00228: reducing learning rate to 0.00240100002848.\n",
      "0s - loss: 0.0057 - mse: 4.9543 - val_loss: 0.0081 - val_mse: 7.0843\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.0057 - mse: 4.9446 - val_loss: 0.0081 - val_mse: 7.0520\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.0057 - mse: 4.9242 - val_loss: 0.0081 - val_mse: 7.0266\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.0056 - mse: 4.9117 - val_loss: 0.0081 - val_mse: 7.0211\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.0056 - mse: 4.9019 - val_loss: 0.0081 - val_mse: 7.0211\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.0056 - mse: 4.8821 - val_loss: 0.0081 - val_mse: 7.0277\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.0056 - mse: 4.8732 - val_loss: 0.0081 - val_mse: 7.0114\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.0056 - mse: 4.8579 - val_loss: 0.0080 - val_mse: 6.9837\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.0056 - mse: 4.8422 - val_loss: 0.0080 - val_mse: 6.9681\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.0055 - mse: 4.8322 - val_loss: 0.0080 - val_mse: 6.9605\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.0055 - mse: 4.8149 - val_loss: 0.0080 - val_mse: 6.9626\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.0055 - mse: 4.8029 - val_loss: 0.0080 - val_mse: 6.9547\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.0055 - mse: 4.7896 - val_loss: 0.0080 - val_mse: 6.9340\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.0055 - mse: 4.7739 - val_loss: 0.0079 - val_mse: 6.9151\n",
      "Epoch 243/1000\n",
      "\n",
      "Epoch 00242: reducing learning rate to 0.00168070008513.\n",
      "0s - loss: 0.0055 - mse: 4.7627 - val_loss: 0.0079 - val_mse: 6.9011\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.0055 - mse: 4.7471 - val_loss: 0.0079 - val_mse: 6.8981\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.0054 - mse: 4.7374 - val_loss: 0.0079 - val_mse: 6.8963\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.0054 - mse: 4.7288 - val_loss: 0.0079 - val_mse: 6.8889\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.0054 - mse: 4.7184 - val_loss: 0.0079 - val_mse: 6.8774\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.0054 - mse: 4.7086 - val_loss: 0.0079 - val_mse: 6.8651\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.0054 - mse: 4.6998 - val_loss: 0.0079 - val_mse: 6.8534\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.0054 - mse: 4.6896 - val_loss: 0.0079 - val_mse: 6.8450\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.0054 - mse: 4.6796 - val_loss: 0.0079 - val_mse: 6.8399\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.0054 - mse: 4.6705 - val_loss: 0.0078 - val_mse: 6.8348\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.0054 - mse: 4.6605 - val_loss: 0.0078 - val_mse: 6.8294\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.0053 - mse: 4.6505 - val_loss: 0.0078 - val_mse: 6.8231\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.0053 - mse: 4.6412 - val_loss: 0.0078 - val_mse: 6.8148\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.0053 - mse: 4.6313 - val_loss: 0.0078 - val_mse: 6.8053\n",
      "Epoch 257/1000\n",
      "\n",
      "Epoch 00256: reducing learning rate to 0.00117649007589.\n",
      "0s - loss: 0.0053 - mse: 4.6215 - val_loss: 0.0078 - val_mse: 6.7967\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.0053 - mse: 4.6122 - val_loss: 0.0078 - val_mse: 6.7920\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.0053 - mse: 4.6053 - val_loss: 0.0078 - val_mse: 6.7890\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.0053 - mse: 4.5984 - val_loss: 0.0078 - val_mse: 6.7873\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.0053 - mse: 4.5918 - val_loss: 0.0078 - val_mse: 6.7854\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.0053 - mse: 4.5851 - val_loss: 0.0078 - val_mse: 6.7825\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.0053 - mse: 4.5782 - val_loss: 0.0078 - val_mse: 6.7789\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.0052 - mse: 4.5714 - val_loss: 0.0078 - val_mse: 6.7738\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.0052 - mse: 4.5648 - val_loss: 0.0078 - val_mse: 6.7669\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.0052 - mse: 4.5581 - val_loss: 0.0078 - val_mse: 6.7605\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.0052 - mse: 4.5513 - val_loss: 0.0078 - val_mse: 6.7563\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.0052 - mse: 4.5446 - val_loss: 0.0078 - val_mse: 6.7529\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.0052 - mse: 4.5379 - val_loss: 0.0078 - val_mse: 6.7495\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.0052 - mse: 4.5311 - val_loss: 0.0077 - val_mse: 6.7448\n",
      "Epoch 271/1000\n",
      "\n",
      "Epoch 00270: reducing learning rate to 0.000823543069419.\n",
      "0s - loss: 0.0052 - mse: 4.5243 - val_loss: 0.0077 - val_mse: 6.7389\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.0052 - mse: 4.5176 - val_loss: 0.0077 - val_mse: 6.7344\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.0052 - mse: 4.5128 - val_loss: 0.0077 - val_mse: 6.7296\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.0052 - mse: 4.5080 - val_loss: 0.0077 - val_mse: 6.7251\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.0052 - mse: 4.5032 - val_loss: 0.0077 - val_mse: 6.7220\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.0052 - mse: 4.4984 - val_loss: 0.0077 - val_mse: 6.7198\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.0052 - mse: 4.4937 - val_loss: 0.0077 - val_mse: 6.7181\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.0052 - mse: 4.4889 - val_loss: 0.0077 - val_mse: 6.7163\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.0051 - mse: 4.4841 - val_loss: 0.0077 - val_mse: 6.7136\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.0051 - mse: 4.4793 - val_loss: 0.0077 - val_mse: 6.7098\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.0051 - mse: 4.4744 - val_loss: 0.0077 - val_mse: 6.7057\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.0051 - mse: 4.4694 - val_loss: 0.0077 - val_mse: 6.7017\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.0051 - mse: 4.4645 - val_loss: 0.0077 - val_mse: 6.6981\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.0051 - mse: 4.4596 - val_loss: 0.0077 - val_mse: 6.6949\n",
      "Epoch 285/1000\n",
      "\n",
      "Epoch 00284: reducing learning rate to 0.000576480140444.\n",
      "0s - loss: 0.0051 - mse: 4.4546 - val_loss: 0.0077 - val_mse: 6.6915\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.0051 - mse: 4.4496 - val_loss: 0.0077 - val_mse: 6.6886\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.0051 - mse: 4.4461 - val_loss: 0.0077 - val_mse: 6.6852\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.0051 - mse: 4.4425 - val_loss: 0.0077 - val_mse: 6.6818\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.0051 - mse: 4.4390 - val_loss: 0.0077 - val_mse: 6.6791\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.0051 - mse: 4.4354 - val_loss: 0.0077 - val_mse: 6.6770\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.0051 - mse: 4.4318 - val_loss: 0.0077 - val_mse: 6.6749\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.0051 - mse: 4.4282 - val_loss: 0.0077 - val_mse: 6.6722\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.0051 - mse: 4.4246 - val_loss: 0.0077 - val_mse: 6.6687\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.0051 - mse: 4.4210 - val_loss: 0.0077 - val_mse: 6.6648\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.0051 - mse: 4.4174 - val_loss: 0.0076 - val_mse: 6.6613\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.0051 - mse: 4.4138 - val_loss: 0.0076 - val_mse: 6.6583\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.0051 - mse: 4.4103 - val_loss: 0.0076 - val_mse: 6.6556\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.0051 - mse: 4.4067 - val_loss: 0.0076 - val_mse: 6.6534\n",
      "Epoch 299/1000\n",
      "\n",
      "Epoch 00298: reducing learning rate to 0.000403536082013.\n",
      "0s - loss: 0.0051 - mse: 4.4031 - val_loss: 0.0076 - val_mse: 6.6515\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.0051 - mse: 4.3995 - val_loss: 0.0076 - val_mse: 6.6500\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.0050 - mse: 4.3970 - val_loss: 0.0076 - val_mse: 6.6483\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.0050 - mse: 4.3945 - val_loss: 0.0076 - val_mse: 6.6466\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.0050 - mse: 4.3920 - val_loss: 0.0076 - val_mse: 6.6449\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.0050 - mse: 4.3895 - val_loss: 0.0076 - val_mse: 6.6432\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.0050 - mse: 4.3870 - val_loss: 0.0076 - val_mse: 6.6420\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.0050 - mse: 4.3845 - val_loss: 0.0076 - val_mse: 6.6409\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.0050 - mse: 4.3819 - val_loss: 0.0076 - val_mse: 6.6396\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.0050 - mse: 4.3794 - val_loss: 0.0076 - val_mse: 6.6382\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.0050 - mse: 4.3769 - val_loss: 0.0076 - val_mse: 6.6367\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.0050 - mse: 4.3744 - val_loss: 0.0076 - val_mse: 6.6351\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.0050 - mse: 4.3719 - val_loss: 0.0076 - val_mse: 6.6333\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.0050 - mse: 4.3694 - val_loss: 0.0076 - val_mse: 6.6317\n",
      "Epoch 313/1000\n",
      "\n",
      "Epoch 00312: reducing learning rate to 0.000282475253334.\n",
      "0s - loss: 0.0050 - mse: 4.3668 - val_loss: 0.0076 - val_mse: 6.6303\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.0050 - mse: 4.3643 - val_loss: 0.0076 - val_mse: 6.6293\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.0050 - mse: 4.3626 - val_loss: 0.0076 - val_mse: 6.6281\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.0050 - mse: 4.3608 - val_loss: 0.0076 - val_mse: 6.6270\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.0050 - mse: 4.3590 - val_loss: 0.0076 - val_mse: 6.6259\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.0050 - mse: 4.3573 - val_loss: 0.0076 - val_mse: 6.6251\n",
      "Epoch 319/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.0050 - mse: 4.3555 - val_loss: 0.0076 - val_mse: 6.6242\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.0050 - mse: 4.3537 - val_loss: 0.0076 - val_mse: 6.6232\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.0050 - mse: 4.3520 - val_loss: 0.0076 - val_mse: 6.6221\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.0050 - mse: 4.3502 - val_loss: 0.0076 - val_mse: 6.6209\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.0050 - mse: 4.3485 - val_loss: 0.0076 - val_mse: 6.6199\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.0050 - mse: 4.3467 - val_loss: 0.0076 - val_mse: 6.6187\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.0050 - mse: 4.3449 - val_loss: 0.0076 - val_mse: 6.6175\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.0050 - mse: 4.3432 - val_loss: 0.0076 - val_mse: 6.6163\n",
      "Epoch 327/1000\n",
      "\n",
      "Epoch 00326: reducing learning rate to 0.000197732681409.\n",
      "0s - loss: 0.0050 - mse: 4.3414 - val_loss: 0.0076 - val_mse: 6.6151\n",
      "Epoch 328/1000\n",
      "0s - loss: 0.0050 - mse: 4.3397 - val_loss: 0.0076 - val_mse: 6.6144\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.0050 - mse: 4.3384 - val_loss: 0.0076 - val_mse: 6.6138\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.0050 - mse: 4.3372 - val_loss: 0.0076 - val_mse: 6.6130\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.0050 - mse: 4.3360 - val_loss: 0.0076 - val_mse: 6.6121\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.0050 - mse: 4.3348 - val_loss: 0.0076 - val_mse: 6.6112\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.0050 - mse: 4.3336 - val_loss: 0.0076 - val_mse: 6.6104\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.0050 - mse: 4.3324 - val_loss: 0.0076 - val_mse: 6.6097\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.0050 - mse: 4.3312 - val_loss: 0.0076 - val_mse: 6.6091\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.0050 - mse: 4.3300 - val_loss: 0.0076 - val_mse: 6.6084\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.0050 - mse: 4.3288 - val_loss: 0.0076 - val_mse: 6.6076\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.0050 - mse: 4.3276 - val_loss: 0.0076 - val_mse: 6.6069\n",
      "[{'loss': 0.0029864141251891851, 'run': 0, 'dropout': 0.0, 'val_mse': 5.508796215057373, 'label': '', 'width': 135, 'mse': 2.6006863117218018, 'val_loss': 0.0063258488662540913}, {'loss': 0.0058024991303682327, 'run': 1, 'dropout': 0.0, 'val_mse': 8.3242835998535156, 'label': '', 'width': 135, 'mse': 5.053041934967041, 'val_loss': 0.0095589244738221169}, {'loss': 0.0049694813787937164, 'run': 2, 'dropout': 0.0, 'val_mse': 6.6069202423095703, 'label': '', 'width': 135, 'mse': 4.3276176452636719, 'val_loss': 0.0075868424028158188}]\n"
     ]
    }
   ],
   "source": [
    "rerun1_perf = []\n",
    "prev_weights = []\n",
    "for run in range(3):\n",
    "    fit, results, prev_weights = run_depth1(width1, learningrates2, None, \"finalFirst\")\n",
    "    results['run']=run\n",
    "    rerun1_perf.append(results)\n",
    "    print rerun1_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5.508796\n",
      "1    8.324284\n",
      "2    6.606920\n",
      "Name: val_mse, dtype: float64\n",
      "6.81333335241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ba52e850>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAENCAYAAADwjPVkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba5450d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print pd.DataFrame(rerun1_perf)['val_mse']\n",
    "print np.mean(pd.DataFrame(rerun1_perf)['val_mse'])\n",
    "pd.DataFrame(rerun1_perf).loc[:,['run','mse','val_mse']].set_index('run').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_depthn(layers, learning_params, initial_weights=None):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(layers[0]['width'], return_sequences=True, activation='tanh', input_shape=(None, 2), dropout=layers[0]['dropout']))\n",
    "    if len(layers)>1:\n",
    "        extra_layers = layers[1:]\n",
    "        for i in range(len(extra_layers)):\n",
    "            model.add(LSTM(extra_layers[i]['width'], return_sequences=True, activation='tanh', dropout=extra_layers[i]['dropout']))\n",
    "    model.add(TimeDistributed(Dense(1, activation='linear')))\n",
    "    optimizer = keras.optimizers.Adam(lr=learning_params['lr_intial'], \n",
    "                                      beta_1=0.9, \n",
    "                                      beta_2=0.999, \n",
    "                                      epsilon=1e-08, \n",
    "                                      decay=learning_params['lr_decay'])\n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[mse])\n",
    "    if not initial_weights is None:\n",
    "        assign_weights(model, initial_weights)   \n",
    "    model.summary()\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  cooldown=learning_params['lr_plateau_cooldown'], \n",
    "                                  factor=learning_params['lr_plateau_factor'], \n",
    "                                  patience=learning_params['lr_plateau_patience'], \n",
    "                                  min_lr=learning_params['lr_minimum'],\n",
    "                                  verbose=1)\n",
    "    tensor_board = TensorBoard(log_dir=\"../tensorboardlog/depthn{}_{}\".format(\n",
    "        len(layers),\n",
    "        \"_\".join([\"width{}x{}\".format(l['width'],l['dropout']) for l in layers])))\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   min_delta=learning_params['stop_delta'],\n",
    "                                   patience=learning_params['stop_patience'], \n",
    "                                   verbose=0, \n",
    "                                   mode='auto')\n",
    "    \n",
    "    fit = model.fit(X_train_p[:,i_fit,:], \n",
    "                    y_train_p[:,i_fit,:], \n",
    "                    epochs=learning_params['epochs_max'], \n",
    "                    validation_data=(X_train_p, y_train_p),\n",
    "                    verbose=2, \n",
    "                    callbacks=[reduce_lr, early_stopping, tensor_board]\n",
    "                   )\n",
    "    result_widths = {'width{}'.format(i):y['width'] for i,y in enumerate(layers)}\n",
    "    result_dropouts = {'dropout{}'.format(i):y['dropout'] for i,y in enumerate(layers)}\n",
    "    result_values = {'val_mse':fit.history['val_mse'][-1],\n",
    "                       'val_loss':fit.history['val_loss'][-1],\n",
    "                       'mse':fit.history['mse'][-1],\n",
    "                       'loss':fit.history['loss'][-1],\n",
    "                      }\n",
    "    results = result_widths.copy()\n",
    "    results.update(result_dropouts)\n",
    "    results.update(result_values)\n",
    "    return fit, results, [layer.get_weights() for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 75)          63300     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 1)           76        \n",
      "=================================================================\n",
      "Total params: 137,896\n",
      "Trainable params: 137,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 1.0955 - mse: 954.0013 - val_loss: 0.6145 - val_mse: 535.0911\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.5657 - mse: 492.5928 - val_loss: 0.3368 - val_mse: 293.2662\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.3268 - mse: 284.6288 - val_loss: 0.4502 - val_mse: 392.0319\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.4492 - mse: 391.2126 - val_loss: 0.3925 - val_mse: 341.8412\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.3851 - mse: 335.3211 - val_loss: 0.4012 - val_mse: 349.4204\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.3784 - mse: 329.5274 - val_loss: 0.3668 - val_mse: 319.4286\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.3441 - mse: 299.6553 - val_loss: 0.3200 - val_mse: 278.6736\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.3027 - mse: 263.5776 - val_loss: 0.2233 - val_mse: 194.4197\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.2060 - mse: 179.4205 - val_loss: 0.1707 - val_mse: 148.6415\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.1548 - mse: 134.8146 - val_loss: 0.1780 - val_mse: 154.9733\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.1643 - mse: 143.0431 - val_loss: 0.1723 - val_mse: 150.0839\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.1572 - mse: 136.9104 - val_loss: 0.1397 - val_mse: 121.6849\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.1251 - mse: 108.9122 - val_loss: 0.1143 - val_mse: 99.5259\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.1019 - mse: 88.7342 - val_loss: 0.1099 - val_mse: 95.6996\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0977 - mse: 85.1153 - val_loss: 0.1184 - val_mse: 103.1136\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.1060 - mse: 92.2721 - val_loss: 0.1209 - val_mse: 105.3087\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.1085 - mse: 94.5289 - val_loss: 0.1136 - val_mse: 98.9482\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.1013 - mse: 88.2187 - val_loss: 0.1043 - val_mse: 90.8120\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0913 - mse: 79.4860 - val_loss: 0.1002 - val_mse: 87.2380\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0856 - mse: 74.5532 - val_loss: 0.1001 - val_mse: 87.2125\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0844 - mse: 73.4991 - val_loss: 0.0983 - val_mse: 85.6290\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0835 - mse: 72.7010 - val_loss: 0.0917 - val_mse: 79.8981\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0797 - mse: 69.4154 - val_loss: 0.0829 - val_mse: 72.2227\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0740 - mse: 64.4123 - val_loss: 0.0769 - val_mse: 66.9365\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0701 - mse: 61.0528 - val_loss: 0.0753 - val_mse: 65.5430\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0697 - mse: 60.7291 - val_loss: 0.0758 - val_mse: 65.9931\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0709 - mse: 61.7267 - val_loss: 0.0749 - val_mse: 65.2183\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0704 - mse: 61.3173 - val_loss: 0.0707 - val_mse: 61.5885\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0666 - mse: 57.9894 - val_loss: 0.0647 - val_mse: 56.3176\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0607 - mse: 52.8278 - val_loss: 0.0603 - val_mse: 52.5329\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0560 - mse: 48.7662 - val_loss: 0.0600 - val_mse: 52.2112\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0549 - mse: 47.8098 - val_loss: 0.0621 - val_mse: 54.0783\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0562 - mse: 48.9685 - val_loss: 0.0631 - val_mse: 54.9149\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0568 - mse: 49.4231 - val_loss: 0.0603 - val_mse: 52.5348\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0544 - mse: 47.3805 - val_loss: 0.0554 - val_mse: 48.2381\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0506 - mse: 44.0550 - val_loss: 0.0512 - val_mse: 44.5503\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0475 - mse: 41.3745 - val_loss: 0.0489 - val_mse: 42.5563\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0459 - mse: 39.9706 - val_loss: 0.0483 - val_mse: 42.0667\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0455 - mse: 39.6310 - val_loss: 0.0484 - val_mse: 42.1702\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0456 - mse: 39.6790 - val_loss: 0.0478 - val_mse: 41.6123\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0449 - mse: 39.1232 - val_loss: 0.0457 - val_mse: 39.7845\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0431 - mse: 37.5143 - val_loss: 0.0430 - val_mse: 37.4346\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0408 - mse: 35.5575 - val_loss: 0.0410 - val_mse: 35.7230\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0393 - mse: 34.2179 - val_loss: 0.0401 - val_mse: 34.9053\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0386 - mse: 33.6209 - val_loss: 0.0394 - val_mse: 34.2970\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0380 - mse: 33.0978 - val_loss: 0.0382 - val_mse: 33.2243\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0368 - mse: 32.0357 - val_loss: 0.0367 - val_mse: 32.0013\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0354 - mse: 30.8004 - val_loss: 0.0358 - val_mse: 31.1828\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0344 - mse: 29.9403 - val_loss: 0.0354 - val_mse: 30.8004\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0339 - mse: 29.4948 - val_loss: 0.0349 - val_mse: 30.4163\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0334 - mse: 29.0614 - val_loss: 0.0342 - val_mse: 29.7448\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0326 - mse: 28.3505 - val_loss: 0.0333 - val_mse: 29.0290\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0317 - mse: 27.5657 - val_loss: 0.0328 - val_mse: 28.5507\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0310 - mse: 26.9797 - val_loss: 0.0325 - val_mse: 28.2903\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0306 - mse: 26.6402 - val_loss: 0.0321 - val_mse: 27.9488\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0302 - mse: 26.3337 - val_loss: 0.0313 - val_mse: 27.2640\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0296 - mse: 25.7950 - val_loss: 0.0304 - val_mse: 26.4590\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0289 - mse: 25.1729 - val_loss: 0.0297 - val_mse: 25.8827\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0284 - mse: 24.7358 - val_loss: 0.0294 - val_mse: 25.5848\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0282 - mse: 24.5207 - val_loss: 0.0291 - val_mse: 25.3322\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0279 - mse: 24.3359 - val_loss: 0.0286 - val_mse: 24.9491\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0276 - mse: 24.0102 - val_loss: 0.0282 - val_mse: 24.5399\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0271 - mse: 23.5973 - val_loss: 0.0279 - val_mse: 24.2574\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0266 - mse: 23.1998 - val_loss: 0.0277 - val_mse: 24.1433\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0263 - mse: 22.8840 - val_loss: 0.0276 - val_mse: 24.0286\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0259 - mse: 22.5810 - val_loss: 0.0273 - val_mse: 23.7710\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0255 - mse: 22.2282 - val_loss: 0.0269 - val_mse: 23.4620\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0251 - mse: 21.8973 - val_loss: 0.0266 - val_mse: 23.1915\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0248 - mse: 21.6011 - val_loss: 0.0264 - val_mse: 22.9658\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0245 - mse: 21.3450 - val_loss: 0.0261 - val_mse: 22.7069\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0242 - mse: 21.0787 - val_loss: 0.0257 - val_mse: 22.4115\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0239 - mse: 20.7893 - val_loss: 0.0255 - val_mse: 22.1741\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0236 - mse: 20.5144 - val_loss: 0.0253 - val_mse: 22.0291\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0233 - mse: 20.2661 - val_loss: 0.0252 - val_mse: 21.9066\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0230 - mse: 20.0206 - val_loss: 0.0249 - val_mse: 21.6893\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0227 - mse: 19.7536 - val_loss: 0.0245 - val_mse: 21.3581\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0224 - mse: 19.4690 - val_loss: 0.0241 - val_mse: 21.0234\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0221 - mse: 19.2191 - val_loss: 0.0238 - val_mse: 20.7578\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0218 - mse: 18.9871 - val_loss: 0.0236 - val_mse: 20.5834\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0215 - mse: 18.7321 - val_loss: 0.0235 - val_mse: 20.4304\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0212 - mse: 18.4713 - val_loss: 0.0232 - val_mse: 20.2073\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0209 - mse: 18.2292 - val_loss: 0.0229 - val_mse: 19.9234\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0206 - mse: 17.9743 - val_loss: 0.0225 - val_mse: 19.5952\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0203 - mse: 17.6883 - val_loss: 0.0221 - val_mse: 19.2849\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0200 - mse: 17.4233 - val_loss: 0.0219 - val_mse: 19.0486\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0197 - mse: 17.1628 - val_loss: 0.0216 - val_mse: 18.8227\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0194 - mse: 16.8744 - val_loss: 0.0215 - val_mse: 18.6919\n",
      "Epoch 88/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0191 - mse: 16.6055 - val_loss: 0.0212 - val_mse: 18.4782\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0188 - mse: 16.3419 - val_loss: 0.0208 - val_mse: 18.1142\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0184 - mse: 16.0379 - val_loss: 0.0204 - val_mse: 17.7434\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0181 - mse: 15.7433 - val_loss: 0.0201 - val_mse: 17.4796\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0177 - mse: 15.4461 - val_loss: 0.0198 - val_mse: 17.2791\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0174 - mse: 15.1648 - val_loss: 0.0203 - val_mse: 17.7142\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0176 - mse: 15.3375 - val_loss: 0.0248 - val_mse: 21.5963\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0230 - mse: 20.0441 - val_loss: 0.0522 - val_mse: 45.4415\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0511 - mse: 44.4965 - val_loss: 0.0255 - val_mse: 22.2342\n",
      "Epoch 97/1000\n",
      "\n",
      "Epoch 00096: reducing learning rate to 0.00699999984354.\n",
      "1s - loss: 0.0252 - mse: 21.9572 - val_loss: 0.0291 - val_mse: 25.3249\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0287 - mse: 25.0033 - val_loss: 0.0202 - val_mse: 17.5860\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0185 - mse: 16.0939 - val_loss: 0.0250 - val_mse: 21.7631\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0231 - mse: 20.0844 - val_loss: 0.0267 - val_mse: 23.2333\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0248 - mse: 21.6209 - val_loss: 0.0221 - val_mse: 19.2617\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0203 - mse: 17.7010 - val_loss: 0.0204 - val_mse: 17.7351\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0189 - mse: 16.5016 - val_loss: 0.0224 - val_mse: 19.4930\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0215 - mse: 18.7491 - val_loss: 0.0222 - val_mse: 19.2917\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0214 - mse: 18.6354 - val_loss: 0.0202 - val_mse: 17.5633\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0190 - mse: 16.5885 - val_loss: 0.0201 - val_mse: 17.4791\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0186 - mse: 16.1578 - val_loss: 0.0212 - val_mse: 18.4411\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0194 - mse: 16.9215 - val_loss: 0.0212 - val_mse: 18.4674\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0195 - mse: 16.9383 - val_loss: 0.0201 - val_mse: 17.5153\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0185 - mse: 16.1507 - val_loss: 0.0192 - val_mse: 16.7094\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0179 - mse: 15.6056 - val_loss: 0.0188 - val_mse: 16.3557\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0178 - mse: 15.4581 - val_loss: 0.0186 - val_mse: 16.1853\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0176 - mse: 15.2848 - val_loss: 0.0186 - val_mse: 16.1913\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0173 - mse: 15.0933 - val_loss: 0.0187 - val_mse: 16.3069\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0172 - mse: 14.9934 - val_loss: 0.0186 - val_mse: 16.2230\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0170 - mse: 14.8176 - val_loss: 0.0182 - val_mse: 15.8569\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0166 - mse: 14.4802 - val_loss: 0.0178 - val_mse: 15.5040\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0163 - mse: 14.1933 - val_loss: 0.0176 - val_mse: 15.3597\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0162 - mse: 14.0728 - val_loss: 0.0176 - val_mse: 15.3176\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0160 - mse: 13.9475 - val_loss: 0.0176 - val_mse: 15.3137\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0158 - mse: 13.7698 - val_loss: 0.0176 - val_mse: 15.3251\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0157 - mse: 13.6445 - val_loss: 0.0174 - val_mse: 15.1878\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0155 - mse: 13.5213 - val_loss: 0.0170 - val_mse: 14.8381\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0153 - mse: 13.3016 - val_loss: 0.0167 - val_mse: 14.5095\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0150 - mse: 13.0913 - val_loss: 0.0166 - val_mse: 14.4212\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0150 - mse: 13.0225 - val_loss: 0.0166 - val_mse: 14.4333\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0149 - mse: 12.9703 - val_loss: 0.0164 - val_mse: 14.2994\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0146 - mse: 12.7500 - val_loss: 0.0162 - val_mse: 14.1271\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0144 - mse: 12.4988 - val_loss: 0.0162 - val_mse: 14.1432\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0143 - mse: 12.4329 - val_loss: 0.0163 - val_mse: 14.1986\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0142 - mse: 12.4081 - val_loss: 0.0161 - val_mse: 14.0596\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0140 - mse: 12.2264 - val_loss: 0.0159 - val_mse: 13.8445\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0138 - mse: 12.0192 - val_loss: 0.0158 - val_mse: 13.7278\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0137 - mse: 11.9510 - val_loss: 0.0156 - val_mse: 13.6247\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0137 - mse: 11.9094 - val_loss: 0.0154 - val_mse: 13.4489\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0135 - mse: 11.7505 - val_loss: 0.0153 - val_mse: 13.3520\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0133 - mse: 11.5765 - val_loss: 0.0154 - val_mse: 13.4381\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0132 - mse: 11.5214 - val_loss: 0.0155 - val_mse: 13.4958\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0132 - mse: 11.4848 - val_loss: 0.0153 - val_mse: 13.3557\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0130 - mse: 11.3488 - val_loss: 0.0151 - val_mse: 13.1692\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0129 - mse: 11.2016 - val_loss: 0.0151 - val_mse: 13.1200\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0128 - mse: 11.1330 - val_loss: 0.0151 - val_mse: 13.1512\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0127 - mse: 11.0787 - val_loss: 0.0151 - val_mse: 13.1296\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0126 - mse: 10.9756 - val_loss: 0.0150 - val_mse: 13.0448\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0125 - mse: 10.8652 - val_loss: 0.0149 - val_mse: 12.9498\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0124 - mse: 10.7871 - val_loss: 0.0148 - val_mse: 12.8601\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0123 - mse: 10.7133 - val_loss: 0.0147 - val_mse: 12.7754\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0122 - mse: 10.6251 - val_loss: 0.0146 - val_mse: 12.6947\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0121 - mse: 10.5401 - val_loss: 0.0145 - val_mse: 12.5995\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0120 - mse: 10.4611 - val_loss: 0.0144 - val_mse: 12.5041\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0119 - mse: 10.3820 - val_loss: 0.0143 - val_mse: 12.4461\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0118 - mse: 10.3041 - val_loss: 0.0143 - val_mse: 12.4180\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0117 - mse: 10.2262 - val_loss: 0.0142 - val_mse: 12.3749\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0117 - mse: 10.1480 - val_loss: 0.0141 - val_mse: 12.2939\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0116 - mse: 10.0748 - val_loss: 0.0140 - val_mse: 12.1970\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0115 - mse: 10.0047 - val_loss: 0.0139 - val_mse: 12.1161\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0114 - mse: 9.9286 - val_loss: 0.0138 - val_mse: 12.0540\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0113 - mse: 9.8525 - val_loss: 0.0138 - val_mse: 11.9913\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0112 - mse: 9.7856 - val_loss: 0.0137 - val_mse: 11.9092\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0112 - mse: 9.7197 - val_loss: 0.0136 - val_mse: 11.8273\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0111 - mse: 9.6479 - val_loss: 0.0135 - val_mse: 11.7831\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0110 - mse: 9.5769 - val_loss: 0.0135 - val_mse: 11.7798\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0109 - mse: 9.5115 - val_loss: 0.0135 - val_mse: 11.7738\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0108 - mse: 9.4468 - val_loss: 0.0135 - val_mse: 11.7413\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0108 - mse: 9.3799 - val_loss: 0.0134 - val_mse: 11.7001\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0107 - mse: 9.3134 - val_loss: 0.0134 - val_mse: 11.6640\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0106 - mse: 9.2493 - val_loss: 0.0134 - val_mse: 11.6258\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0105 - mse: 9.1868 - val_loss: 0.0133 - val_mse: 11.5797\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0105 - mse: 9.1229 - val_loss: 0.0132 - val_mse: 11.5372\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0104 - mse: 9.0584 - val_loss: 0.0132 - val_mse: 11.5147\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0103 - mse: 8.9963 - val_loss: 0.0132 - val_mse: 11.4995\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0103 - mse: 8.9346 - val_loss: 0.0132 - val_mse: 11.4628\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0102 - mse: 8.8708 - val_loss: 0.0131 - val_mse: 11.4072\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0101 - mse: 8.8085 - val_loss: 0.0130 - val_mse: 11.3572\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0100 - mse: 8.7487 - val_loss: 0.0130 - val_mse: 11.3231\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0100 - mse: 8.6868 - val_loss: 0.0130 - val_mse: 11.2978\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0099 - mse: 8.6244 - val_loss: 0.0129 - val_mse: 11.2712\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0098 - mse: 8.5651 - val_loss: 0.0129 - val_mse: 11.2350\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0098 - mse: 8.5060 - val_loss: 0.0128 - val_mse: 11.1852\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0097 - mse: 8.4460 - val_loss: 0.0128 - val_mse: 11.1471\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0096 - mse: 8.3871 - val_loss: 0.0128 - val_mse: 11.1321\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0096 - mse: 8.3283 - val_loss: 0.0128 - val_mse: 11.1116\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0095 - mse: 8.2690 - val_loss: 0.0127 - val_mse: 11.0754\n",
      "Epoch 184/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0094 - mse: 8.2102 - val_loss: 0.0127 - val_mse: 11.0366\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0094 - mse: 8.1518 - val_loss: 0.0126 - val_mse: 10.9969\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0093 - mse: 8.0932 - val_loss: 0.0126 - val_mse: 10.9579\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0092 - mse: 8.0353 - val_loss: 0.0125 - val_mse: 10.9130\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0092 - mse: 7.9774 - val_loss: 0.0125 - val_mse: 10.8741\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0091 - mse: 7.9197 - val_loss: 0.0125 - val_mse: 10.8585\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0090 - mse: 7.8625 - val_loss: 0.0124 - val_mse: 10.8409\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0090 - mse: 7.8057 - val_loss: 0.0124 - val_mse: 10.7952\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0089 - mse: 7.7490 - val_loss: 0.0123 - val_mse: 10.7437\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0088 - mse: 7.6934 - val_loss: 0.0123 - val_mse: 10.7169\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0088 - mse: 7.6379 - val_loss: 0.0123 - val_mse: 10.6925\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0087 - mse: 7.5824 - val_loss: 0.0122 - val_mse: 10.6514\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0086 - mse: 7.5275 - val_loss: 0.0122 - val_mse: 10.6042\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0086 - mse: 7.4728 - val_loss: 0.0121 - val_mse: 10.5782\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0085 - mse: 7.4178 - val_loss: 0.0121 - val_mse: 10.5568\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0085 - mse: 7.3634 - val_loss: 0.0121 - val_mse: 10.5220\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0084 - mse: 7.3090 - val_loss: 0.0120 - val_mse: 10.4870\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0083 - mse: 7.2546 - val_loss: 0.0120 - val_mse: 10.4678\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0083 - mse: 7.2004 - val_loss: 0.0120 - val_mse: 10.4411\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0082 - mse: 7.1461 - val_loss: 0.0119 - val_mse: 10.3898\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0081 - mse: 7.0919 - val_loss: 0.0119 - val_mse: 10.3452\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0081 - mse: 7.0374 - val_loss: 0.0119 - val_mse: 10.3277\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0080 - mse: 6.9825 - val_loss: 0.0118 - val_mse: 10.3089\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0080 - mse: 6.9278 - val_loss: 0.0118 - val_mse: 10.2647\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0079 - mse: 6.8728 - val_loss: 0.0117 - val_mse: 10.2287\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0078 - mse: 6.8178 - val_loss: 0.0117 - val_mse: 10.2118\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0078 - mse: 6.7624 - val_loss: 0.0117 - val_mse: 10.1839\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0077 - mse: 6.7069 - val_loss: 0.0116 - val_mse: 10.1274\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0076 - mse: 6.6510 - val_loss: 0.0116 - val_mse: 10.0928\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0076 - mse: 6.5955 - val_loss: 0.0116 - val_mse: 10.0661\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0075 - mse: 6.5407 - val_loss: 0.0115 - val_mse: 10.0406\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0074 - mse: 6.4862 - val_loss: 0.0115 - val_mse: 9.9928\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0074 - mse: 6.4322 - val_loss: 0.0114 - val_mse: 9.9626\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0073 - mse: 6.3783 - val_loss: 0.0114 - val_mse: 9.9546\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0073 - mse: 6.3245 - val_loss: 0.0114 - val_mse: 9.9104\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0072 - mse: 6.2706 - val_loss: 0.0113 - val_mse: 9.8822\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0071 - mse: 6.2166 - val_loss: 0.0113 - val_mse: 9.8662\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0071 - mse: 6.1626 - val_loss: 0.0113 - val_mse: 9.8159\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0070 - mse: 6.1081 - val_loss: 0.0112 - val_mse: 9.7675\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0070 - mse: 6.0538 - val_loss: 0.0112 - val_mse: 9.7500\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0069 - mse: 5.9998 - val_loss: 0.0112 - val_mse: 9.7320\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0068 - mse: 5.9453 - val_loss: 0.0111 - val_mse: 9.6906\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0068 - mse: 5.8905 - val_loss: 0.0111 - val_mse: 9.6475\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0067 - mse: 5.8358 - val_loss: 0.0111 - val_mse: 9.6275\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0066 - mse: 5.7820 - val_loss: 0.0110 - val_mse: 9.5964\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0066 - mse: 5.7286 - val_loss: 0.0110 - val_mse: 9.5776\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0065 - mse: 5.6754 - val_loss: 0.0110 - val_mse: 9.5433\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0065 - mse: 5.6223 - val_loss: 0.0109 - val_mse: 9.4990\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0064 - mse: 5.5695 - val_loss: 0.0109 - val_mse: 9.4839\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0063 - mse: 5.5171 - val_loss: 0.0109 - val_mse: 9.4486\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0063 - mse: 5.4645 - val_loss: 0.0108 - val_mse: 9.4118\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0062 - mse: 5.4121 - val_loss: 0.0108 - val_mse: 9.3906\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0062 - mse: 5.3599 - val_loss: 0.0107 - val_mse: 9.3479\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0061 - mse: 5.3079 - val_loss: 0.0107 - val_mse: 9.3089\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0060 - mse: 5.2559 - val_loss: 0.0107 - val_mse: 9.2880\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0060 - mse: 5.2040 - val_loss: 0.0106 - val_mse: 9.2370\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0059 - mse: 5.1523 - val_loss: 0.0106 - val_mse: 9.2433\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0059 - mse: 5.1012 - val_loss: 0.0105 - val_mse: 9.1695\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0058 - mse: 5.0502 - val_loss: 0.0105 - val_mse: 9.1657\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.0057 - mse: 5.0002 - val_loss: 0.0105 - val_mse: 9.1290\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0057 - mse: 4.9506 - val_loss: 0.0104 - val_mse: 9.0728\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0056 - mse: 4.9013 - val_loss: 0.0104 - val_mse: 9.0803\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0056 - mse: 4.8527 - val_loss: 0.0104 - val_mse: 9.0218\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0055 - mse: 4.8044 - val_loss: 0.0103 - val_mse: 9.0126\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0055 - mse: 4.7564 - val_loss: 0.0103 - val_mse: 8.9537\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0054 - mse: 4.7090 - val_loss: 0.0102 - val_mse: 8.9175\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0054 - mse: 4.6622 - val_loss: 0.0102 - val_mse: 8.8636\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0053 - mse: 4.6179 - val_loss: 0.0101 - val_mse: 8.8183\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0053 - mse: 4.5740 - val_loss: 0.0101 - val_mse: 8.7689\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0052 - mse: 4.5302 - val_loss: 0.0099 - val_mse: 8.6640\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.0052 - mse: 4.4873 - val_loss: 0.0100 - val_mse: 8.6862\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0051 - mse: 4.4448 - val_loss: 0.0098 - val_mse: 8.5227\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0051 - mse: 4.4028 - val_loss: 0.0099 - val_mse: 8.6232\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.0050 - mse: 4.3628 - val_loss: 0.0095 - val_mse: 8.2996\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0050 - mse: 4.3297 - val_loss: 0.0101 - val_mse: 8.7534\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.0050 - mse: 4.3186 - val_loss: 0.0090 - val_mse: 7.8719\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0050 - mse: 4.3907 - val_loss: 0.0112 - val_mse: 9.7885\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.0054 - mse: 4.6987 - val_loss: 0.0092 - val_mse: 7.9844\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.0064 - mse: 5.5398 - val_loss: 0.0133 - val_mse: 11.5846\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0072 - mse: 6.2599 - val_loss: 0.0101 - val_mse: 8.7784\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.0070 - mse: 6.1331 - val_loss: 0.0084 - val_mse: 7.3195\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0051 - mse: 4.4209 - val_loss: 0.0100 - val_mse: 8.6673\n",
      "Epoch 266/1000\n",
      "1s - loss: 0.0055 - mse: 4.8084 - val_loss: 0.0095 - val_mse: 8.2917\n",
      "Epoch 267/1000\n",
      "1s - loss: 0.0061 - mse: 5.3142 - val_loss: 0.0080 - val_mse: 6.9293\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0048 - mse: 4.1527 - val_loss: 0.0094 - val_mse: 8.1822\n",
      "Epoch 269/1000\n",
      "1s - loss: 0.0058 - mse: 5.0598 - val_loss: 0.0088 - val_mse: 7.6961\n",
      "Epoch 270/1000\n",
      "1s - loss: 0.0052 - mse: 4.5018 - val_loss: 0.0085 - val_mse: 7.4081\n",
      "Epoch 271/1000\n",
      "1s - loss: 0.0050 - mse: 4.3750 - val_loss: 0.0087 - val_mse: 7.5988\n",
      "Epoch 272/1000\n",
      "\n",
      "Epoch 00271: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0054 - mse: 4.6680 - val_loss: 0.0079 - val_mse: 6.9055\n",
      "Epoch 273/1000\n",
      "1s - loss: 0.0046 - mse: 4.0053 - val_loss: 0.0082 - val_mse: 7.1780\n",
      "Epoch 274/1000\n",
      "1s - loss: 0.0048 - mse: 4.1914 - val_loss: 0.0083 - val_mse: 7.2127\n",
      "Epoch 275/1000\n",
      "1s - loss: 0.0046 - mse: 4.0203 - val_loss: 0.0083 - val_mse: 7.2369\n",
      "Epoch 276/1000\n",
      "1s - loss: 0.0046 - mse: 4.0089 - val_loss: 0.0081 - val_mse: 7.0413\n",
      "Epoch 277/1000\n",
      "1s - loss: 0.0046 - mse: 4.0048 - val_loss: 0.0079 - val_mse: 6.8762\n",
      "Epoch 278/1000\n",
      "1s - loss: 0.0045 - mse: 3.8894 - val_loss: 0.0083 - val_mse: 7.2116\n",
      "Epoch 279/1000\n",
      "1s - loss: 0.0045 - mse: 3.9591 - val_loss: 0.0084 - val_mse: 7.3546\n",
      "Epoch 280/1000\n",
      "1s - loss: 0.0044 - mse: 3.8110 - val_loss: 0.0083 - val_mse: 7.2589\n",
      "Epoch 281/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0045 - mse: 3.8871 - val_loss: 0.0078 - val_mse: 6.8245\n",
      "Epoch 282/1000\n",
      "1s - loss: 0.0043 - mse: 3.7589 - val_loss: 0.0080 - val_mse: 6.9885\n",
      "Epoch 283/1000\n",
      "1s - loss: 0.0044 - mse: 3.8156 - val_loss: 0.0084 - val_mse: 7.3278\n",
      "Epoch 284/1000\n",
      "1s - loss: 0.0043 - mse: 3.7141 - val_loss: 0.0085 - val_mse: 7.4254\n",
      "Epoch 285/1000\n",
      "1s - loss: 0.0043 - mse: 3.7525 - val_loss: 0.0080 - val_mse: 6.9331\n",
      "Epoch 286/1000\n",
      "\n",
      "Epoch 00285: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0042 - mse: 3.6632 - val_loss: 0.0079 - val_mse: 6.8411\n",
      "Epoch 287/1000\n",
      "1s - loss: 0.0042 - mse: 3.6926 - val_loss: 0.0081 - val_mse: 7.0557\n",
      "Epoch 288/1000\n",
      "1s - loss: 0.0042 - mse: 3.6257 - val_loss: 0.0085 - val_mse: 7.3703\n",
      "Epoch 289/1000\n",
      "1s - loss: 0.0042 - mse: 3.6223 - val_loss: 0.0084 - val_mse: 7.3227\n",
      "Epoch 290/1000\n",
      "1s - loss: 0.0042 - mse: 3.6214 - val_loss: 0.0080 - val_mse: 6.9349\n",
      "Epoch 291/1000\n",
      "1s - loss: 0.0041 - mse: 3.5622 - val_loss: 0.0078 - val_mse: 6.7589\n",
      "Epoch 292/1000\n",
      "1s - loss: 0.0041 - mse: 3.5862 - val_loss: 0.0079 - val_mse: 6.8421\n",
      "Epoch 293/1000\n",
      "1s - loss: 0.0041 - mse: 3.5402 - val_loss: 0.0081 - val_mse: 7.0888\n",
      "Epoch 294/1000\n",
      "1s - loss: 0.0040 - mse: 3.5249 - val_loss: 0.0082 - val_mse: 7.1212\n",
      "Epoch 295/1000\n",
      "1s - loss: 0.0040 - mse: 3.5263 - val_loss: 0.0078 - val_mse: 6.8303\n",
      "Epoch 296/1000\n",
      "1s - loss: 0.0040 - mse: 3.4793 - val_loss: 0.0076 - val_mse: 6.6310\n",
      "Epoch 297/1000\n",
      "1s - loss: 0.0040 - mse: 3.4892 - val_loss: 0.0076 - val_mse: 6.6522\n",
      "Epoch 298/1000\n",
      "1s - loss: 0.0040 - mse: 3.4581 - val_loss: 0.0079 - val_mse: 6.8440\n",
      "Epoch 299/1000\n",
      "1s - loss: 0.0040 - mse: 3.4403 - val_loss: 0.0079 - val_mse: 6.9159\n",
      "Epoch 300/1000\n",
      "1s - loss: 0.0039 - mse: 3.4367 - val_loss: 0.0077 - val_mse: 6.7254\n",
      "Epoch 301/1000\n",
      "\n",
      "Epoch 00300: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0039 - mse: 3.4024 - val_loss: 0.0075 - val_mse: 6.5477\n",
      "Epoch 302/1000\n",
      "1s - loss: 0.0039 - mse: 3.4010 - val_loss: 0.0075 - val_mse: 6.5374\n",
      "Epoch 303/1000\n",
      "1s - loss: 0.0039 - mse: 3.3821 - val_loss: 0.0076 - val_mse: 6.6320\n",
      "Epoch 304/1000\n",
      "1s - loss: 0.0039 - mse: 3.3646 - val_loss: 0.0077 - val_mse: 6.7283\n",
      "Epoch 305/1000\n",
      "1s - loss: 0.0039 - mse: 3.3628 - val_loss: 0.0077 - val_mse: 6.7064\n",
      "Epoch 306/1000\n",
      "1s - loss: 0.0038 - mse: 3.3456 - val_loss: 0.0076 - val_mse: 6.5979\n",
      "Epoch 307/1000\n",
      "1s - loss: 0.0038 - mse: 3.3302 - val_loss: 0.0075 - val_mse: 6.5040\n",
      "Epoch 308/1000\n",
      "1s - loss: 0.0038 - mse: 3.3250 - val_loss: 0.0074 - val_mse: 6.4815\n",
      "Epoch 309/1000\n",
      "1s - loss: 0.0038 - mse: 3.3090 - val_loss: 0.0075 - val_mse: 6.5369\n",
      "Epoch 310/1000\n",
      "1s - loss: 0.0038 - mse: 3.2954 - val_loss: 0.0076 - val_mse: 6.6037\n",
      "Epoch 311/1000\n",
      "1s - loss: 0.0038 - mse: 3.2882 - val_loss: 0.0076 - val_mse: 6.5976\n",
      "Epoch 312/1000\n",
      "1s - loss: 0.0038 - mse: 3.2726 - val_loss: 0.0075 - val_mse: 6.5336\n",
      "Epoch 313/1000\n",
      "1s - loss: 0.0037 - mse: 3.2605 - val_loss: 0.0074 - val_mse: 6.4746\n",
      "Epoch 314/1000\n",
      "1s - loss: 0.0037 - mse: 3.2515 - val_loss: 0.0074 - val_mse: 6.4584\n",
      "Epoch 315/1000\n",
      "\n",
      "Epoch 00314: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0037 - mse: 3.2364 - val_loss: 0.0075 - val_mse: 6.4880\n",
      "Epoch 316/1000\n",
      "1s - loss: 0.0037 - mse: 3.2256 - val_loss: 0.0075 - val_mse: 6.5068\n",
      "Epoch 317/1000\n",
      "1s - loss: 0.0037 - mse: 3.2182 - val_loss: 0.0075 - val_mse: 6.5016\n",
      "Epoch 318/1000\n",
      "1s - loss: 0.0037 - mse: 3.2081 - val_loss: 0.0074 - val_mse: 6.4770\n",
      "Epoch 319/1000\n",
      "1s - loss: 0.0037 - mse: 3.1997 - val_loss: 0.0074 - val_mse: 6.4481\n",
      "Epoch 320/1000\n",
      "1s - loss: 0.0037 - mse: 3.1930 - val_loss: 0.0074 - val_mse: 6.4261\n",
      "Epoch 321/1000\n",
      "1s - loss: 0.0037 - mse: 3.1840 - val_loss: 0.0074 - val_mse: 6.4203\n",
      "Epoch 322/1000\n",
      "1s - loss: 0.0036 - mse: 3.1747 - val_loss: 0.0074 - val_mse: 6.4251\n",
      "Epoch 323/1000\n",
      "1s - loss: 0.0036 - mse: 3.1675 - val_loss: 0.0074 - val_mse: 6.4257\n",
      "Epoch 324/1000\n",
      "1s - loss: 0.0036 - mse: 3.1597 - val_loss: 0.0074 - val_mse: 6.4126\n",
      "Epoch 325/1000\n",
      "1s - loss: 0.0036 - mse: 3.1504 - val_loss: 0.0073 - val_mse: 6.3929\n",
      "Epoch 326/1000\n",
      "1s - loss: 0.0036 - mse: 3.1424 - val_loss: 0.0073 - val_mse: 6.3782\n",
      "Epoch 327/1000\n",
      "1s - loss: 0.0036 - mse: 3.1350 - val_loss: 0.0073 - val_mse: 6.3711\n",
      "Epoch 328/1000\n",
      "1s - loss: 0.0036 - mse: 3.1262 - val_loss: 0.0073 - val_mse: 6.3703\n",
      "Epoch 329/1000\n",
      "\n",
      "Epoch 00328: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0036 - mse: 3.1176 - val_loss: 0.0073 - val_mse: 6.3661\n",
      "Epoch 330/1000\n",
      "1s - loss: 0.0036 - mse: 3.1100 - val_loss: 0.0073 - val_mse: 6.3563\n",
      "Epoch 331/1000\n",
      "1s - loss: 0.0036 - mse: 3.1042 - val_loss: 0.0073 - val_mse: 6.3430\n",
      "Epoch 332/1000\n",
      "1s - loss: 0.0036 - mse: 3.0979 - val_loss: 0.0073 - val_mse: 6.3327\n",
      "Epoch 333/1000\n",
      "1s - loss: 0.0036 - mse: 3.0921 - val_loss: 0.0073 - val_mse: 6.3277\n",
      "Epoch 334/1000\n",
      "1s - loss: 0.0035 - mse: 3.0865 - val_loss: 0.0073 - val_mse: 6.3263\n",
      "Epoch 335/1000\n",
      "1s - loss: 0.0035 - mse: 3.0807 - val_loss: 0.0073 - val_mse: 6.3241\n",
      "Epoch 336/1000\n",
      "1s - loss: 0.0035 - mse: 3.0747 - val_loss: 0.0073 - val_mse: 6.3189\n",
      "Epoch 337/1000\n",
      "1s - loss: 0.0035 - mse: 3.0688 - val_loss: 0.0072 - val_mse: 6.3097\n",
      "Epoch 338/1000\n",
      "1s - loss: 0.0035 - mse: 3.0631 - val_loss: 0.0072 - val_mse: 6.2998\n",
      "Epoch 339/1000\n",
      "1s - loss: 0.0035 - mse: 3.0573 - val_loss: 0.0072 - val_mse: 6.2905\n",
      "Epoch 340/1000\n",
      "1s - loss: 0.0035 - mse: 3.0513 - val_loss: 0.0072 - val_mse: 6.2830\n",
      "Epoch 341/1000\n",
      "1s - loss: 0.0035 - mse: 3.0453 - val_loss: 0.0072 - val_mse: 6.2786\n",
      "Epoch 342/1000\n",
      "1s - loss: 0.0035 - mse: 3.0396 - val_loss: 0.0072 - val_mse: 6.2768\n",
      "Epoch 343/1000\n",
      "\n",
      "Epoch 00342: reducing learning rate to 0.000823543069419.\n",
      "1s - loss: 0.0035 - mse: 3.0338 - val_loss: 0.0072 - val_mse: 6.2741\n",
      "Epoch 344/1000\n",
      "1s - loss: 0.0035 - mse: 3.0278 - val_loss: 0.0072 - val_mse: 6.2709\n",
      "Epoch 345/1000\n",
      "1s - loss: 0.0035 - mse: 3.0236 - val_loss: 0.0072 - val_mse: 6.2663\n",
      "Epoch 346/1000\n",
      "1s - loss: 0.0035 - mse: 3.0195 - val_loss: 0.0072 - val_mse: 6.2605\n",
      "Epoch 347/1000\n",
      "1s - loss: 0.0035 - mse: 3.0154 - val_loss: 0.0072 - val_mse: 6.2538\n",
      "Epoch 348/1000\n",
      "1s - loss: 0.0035 - mse: 3.0112 - val_loss: 0.0072 - val_mse: 6.2487\n",
      "Epoch 349/1000\n",
      "1s - loss: 0.0035 - mse: 3.0069 - val_loss: 0.0072 - val_mse: 6.2463\n",
      "Epoch 350/1000\n",
      "1s - loss: 0.0034 - mse: 3.0026 - val_loss: 0.0072 - val_mse: 6.2460\n",
      "Epoch 351/1000\n",
      "1s - loss: 0.0034 - mse: 2.9983 - val_loss: 0.0072 - val_mse: 6.2469\n",
      "Epoch 352/1000\n",
      "1s - loss: 0.0034 - mse: 2.9941 - val_loss: 0.0072 - val_mse: 6.2476\n",
      "Epoch 353/1000\n",
      "1s - loss: 0.0034 - mse: 2.9898 - val_loss: 0.0072 - val_mse: 6.2460\n",
      "Epoch 354/1000\n",
      "1s - loss: 0.0034 - mse: 2.9855 - val_loss: 0.0072 - val_mse: 6.2417\n",
      "Epoch 355/1000\n",
      "1s - loss: 0.0034 - mse: 2.9812 - val_loss: 0.0072 - val_mse: 6.2358\n",
      "Epoch 356/1000\n",
      "1s - loss: 0.0034 - mse: 2.9769 - val_loss: 0.0072 - val_mse: 6.2302\n",
      "Epoch 357/1000\n",
      "\n",
      "Epoch 00356: reducing learning rate to 0.000576480140444.\n",
      "1s - loss: 0.0034 - mse: 2.9727 - val_loss: 0.0071 - val_mse: 6.2259\n",
      "Epoch 358/1000\n",
      "1s - loss: 0.0034 - mse: 2.9684 - val_loss: 0.0071 - val_mse: 6.2238\n",
      "Epoch 359/1000\n",
      "1s - loss: 0.0034 - mse: 2.9654 - val_loss: 0.0071 - val_mse: 6.2225\n",
      "Epoch 360/1000\n",
      "1s - loss: 0.0034 - mse: 2.9624 - val_loss: 0.0071 - val_mse: 6.2209\n",
      "Epoch 361/1000\n",
      "1s - loss: 0.0034 - mse: 2.9594 - val_loss: 0.0071 - val_mse: 6.2190\n",
      "Epoch 362/1000\n",
      "1s - loss: 0.0034 - mse: 2.9564 - val_loss: 0.0071 - val_mse: 6.2165\n",
      "Epoch 363/1000\n",
      "1s - loss: 0.0034 - mse: 2.9534 - val_loss: 0.0071 - val_mse: 6.2134\n",
      "Epoch 364/1000\n",
      "1s - loss: 0.0034 - mse: 2.9503 - val_loss: 0.0071 - val_mse: 6.2107\n",
      "Epoch 365/1000\n",
      "1s - loss: 0.0034 - mse: 2.9473 - val_loss: 0.0071 - val_mse: 6.2087\n",
      "Epoch 366/1000\n",
      "1s - loss: 0.0034 - mse: 2.9443 - val_loss: 0.0071 - val_mse: 6.2066\n",
      "Epoch 367/1000\n",
      "1s - loss: 0.0034 - mse: 2.9412 - val_loss: 0.0071 - val_mse: 6.2045\n",
      "Epoch 368/1000\n",
      "1s - loss: 0.0034 - mse: 2.9382 - val_loss: 0.0071 - val_mse: 6.2022\n",
      "Epoch 369/1000\n",
      "1s - loss: 0.0034 - mse: 2.9352 - val_loss: 0.0071 - val_mse: 6.1989\n",
      "Epoch 370/1000\n",
      "1s - loss: 0.0034 - mse: 2.9321 - val_loss: 0.0071 - val_mse: 6.1947\n",
      "Epoch 371/1000\n",
      "\n",
      "Epoch 00370: reducing learning rate to 0.000403536082013.\n",
      "1s - loss: 0.0034 - mse: 2.9291 - val_loss: 0.0071 - val_mse: 6.1916\n",
      "Epoch 372/1000\n",
      "1s - loss: 0.0034 - mse: 2.9261 - val_loss: 0.0071 - val_mse: 6.1903\n",
      "Epoch 373/1000\n",
      "1s - loss: 0.0034 - mse: 2.9239 - val_loss: 0.0071 - val_mse: 6.1889\n",
      "Epoch 374/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0034 - mse: 2.9218 - val_loss: 0.0071 - val_mse: 6.1881\n",
      "Epoch 375/1000\n",
      "1s - loss: 0.0034 - mse: 2.9197 - val_loss: 0.0071 - val_mse: 6.1879\n",
      "Epoch 376/1000\n",
      "1s - loss: 0.0034 - mse: 2.9176 - val_loss: 0.0071 - val_mse: 6.1876\n",
      "[{'loss': 0.0033503014128655195, 'width0': 135, 'width1': 75, 'val_mse': 6.1875839233398438, 'mse': 2.9175732135772705, 'val_loss': 0.007105313241481781, 'dropout1': 0, 'dropout0': 0}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, None, 100)         94400     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 1)           101       \n",
      "=================================================================\n",
      "Total params: 169,021\n",
      "Trainable params: 169,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 1.0021 - mse: 872.7087 - val_loss: 1.4950 - val_mse: 1301.9291\n",
      "Epoch 2/1000\n",
      "1s - loss: 1.5226 - mse: 1325.9220 - val_loss: 0.3037 - val_mse: 264.4788\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.3010 - mse: 262.0965 - val_loss: 0.2465 - val_mse: 214.6238\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.2499 - mse: 217.5805 - val_loss: 0.1315 - val_mse: 114.5068\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.1329 - mse: 115.7124 - val_loss: 0.2429 - val_mse: 211.5253\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.2441 - mse: 212.5704 - val_loss: 0.1675 - val_mse: 145.8399\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.1575 - mse: 137.1647 - val_loss: 0.1399 - val_mse: 121.8484\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.1305 - mse: 113.6011 - val_loss: 0.1075 - val_mse: 93.6427\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.0997 - mse: 86.8615 - val_loss: 0.1372 - val_mse: 119.4750\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.1289 - mse: 112.2385 - val_loss: 0.1140 - val_mse: 99.2988\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.1044 - mse: 90.9283 - val_loss: 0.0919 - val_mse: 80.0108\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.0832 - mse: 72.4914 - val_loss: 0.0960 - val_mse: 83.6027\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0891 - mse: 77.5638 - val_loss: 0.0931 - val_mse: 81.0446\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0874 - mse: 76.0804 - val_loss: 0.0833 - val_mse: 72.5159\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0789 - mse: 68.6863 - val_loss: 0.0719 - val_mse: 62.5918\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0682 - mse: 59.3855 - val_loss: 0.0602 - val_mse: 52.3989\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0562 - mse: 48.9190 - val_loss: 0.0587 - val_mse: 51.1060\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0545 - mse: 47.4758 - val_loss: 0.0644 - val_mse: 56.1171\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0608 - mse: 52.9065 - val_loss: 0.0653 - val_mse: 56.8832\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0627 - mse: 54.5606 - val_loss: 0.0590 - val_mse: 51.4161\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0574 - mse: 50.0231 - val_loss: 0.0535 - val_mse: 46.5533\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0525 - mse: 45.6986 - val_loss: 0.0526 - val_mse: 45.7974\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0517 - mse: 45.0409 - val_loss: 0.0530 - val_mse: 46.1630\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0519 - mse: 45.1910 - val_loss: 0.0512 - val_mse: 44.6162\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0498 - mse: 43.3292 - val_loss: 0.0481 - val_mse: 41.8687\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0464 - mse: 40.4141 - val_loss: 0.0455 - val_mse: 39.6016\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0437 - mse: 38.0929 - val_loss: 0.0441 - val_mse: 38.3861\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0422 - mse: 36.7072 - val_loss: 0.0434 - val_mse: 37.8191\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0412 - mse: 35.8491 - val_loss: 0.0424 - val_mse: 36.9226\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0399 - mse: 34.7601 - val_loss: 0.0405 - val_mse: 35.2968\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0380 - mse: 33.1186 - val_loss: 0.0387 - val_mse: 33.7052\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0362 - mse: 31.5574 - val_loss: 0.0376 - val_mse: 32.7239\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0351 - mse: 30.5491 - val_loss: 0.0365 - val_mse: 31.8213\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0340 - mse: 29.6196 - val_loss: 0.0349 - val_mse: 30.3559\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0324 - mse: 28.2283 - val_loss: 0.0329 - val_mse: 28.6829\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0307 - mse: 26.7561 - val_loss: 0.0317 - val_mse: 27.5695\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0297 - mse: 25.8585 - val_loss: 0.0310 - val_mse: 26.9605\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0291 - mse: 25.3384 - val_loss: 0.0300 - val_mse: 26.0993\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0281 - mse: 24.4273 - val_loss: 0.0285 - val_mse: 24.7875\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0264 - mse: 23.0128 - val_loss: 0.0271 - val_mse: 23.6405\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0250 - mse: 21.7818 - val_loss: 0.0265 - val_mse: 23.0595\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0243 - mse: 21.1627 - val_loss: 0.0261 - val_mse: 22.7579\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0240 - mse: 20.8614 - val_loss: 0.0255 - val_mse: 22.2198\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0234 - mse: 20.3512 - val_loss: 0.0244 - val_mse: 21.2597\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0223 - mse: 19.4516 - val_loss: 0.0232 - val_mse: 20.2156\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0213 - mse: 18.5223 - val_loss: 0.0224 - val_mse: 19.5230\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0207 - mse: 17.9857 - val_loss: 0.0221 - val_mse: 19.2396\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0205 - mse: 17.8461 - val_loss: 0.0217 - val_mse: 18.9227\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0202 - mse: 17.5998 - val_loss: 0.0211 - val_mse: 18.3934\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0196 - mse: 17.0512 - val_loss: 0.0207 - val_mse: 17.9889\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0190 - mse: 16.5723 - val_loss: 0.0205 - val_mse: 17.8230\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0188 - mse: 16.3291 - val_loss: 0.0202 - val_mse: 17.6222\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0185 - mse: 16.0744 - val_loss: 0.0198 - val_mse: 17.2789\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0180 - mse: 15.6966 - val_loss: 0.0195 - val_mse: 16.9677\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0176 - mse: 15.3596 - val_loss: 0.0193 - val_mse: 16.7982\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0174 - mse: 15.1676 - val_loss: 0.0191 - val_mse: 16.6629\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0172 - mse: 15.0153 - val_loss: 0.0189 - val_mse: 16.4431\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0170 - mse: 14.7903 - val_loss: 0.0186 - val_mse: 16.1865\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0167 - mse: 14.5400 - val_loss: 0.0184 - val_mse: 15.9900\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0165 - mse: 14.3579 - val_loss: 0.0182 - val_mse: 15.8192\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0163 - mse: 14.2133 - val_loss: 0.0179 - val_mse: 15.5743\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0161 - mse: 14.0109 - val_loss: 0.0175 - val_mse: 15.2695\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0158 - mse: 13.7562 - val_loss: 0.0173 - val_mse: 15.0283\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0156 - mse: 13.5518 - val_loss: 0.0171 - val_mse: 14.8989\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0154 - mse: 13.4312 - val_loss: 0.0170 - val_mse: 14.7853\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0153 - mse: 13.3031 - val_loss: 0.0168 - val_mse: 14.6159\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0151 - mse: 13.1135 - val_loss: 0.0166 - val_mse: 14.4320\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0148 - mse: 12.9224 - val_loss: 0.0164 - val_mse: 14.2682\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0147 - mse: 12.7781 - val_loss: 0.0162 - val_mse: 14.0906\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0145 - mse: 12.6483 - val_loss: 0.0159 - val_mse: 13.8797\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0143 - mse: 12.4963 - val_loss: 0.0157 - val_mse: 13.6794\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0142 - mse: 12.3365 - val_loss: 0.0155 - val_mse: 13.5353\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0140 - mse: 12.1960 - val_loss: 0.0154 - val_mse: 13.4347\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0139 - mse: 12.0673 - val_loss: 0.0153 - val_mse: 13.3375\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0137 - mse: 11.9321 - val_loss: 0.0152 - val_mse: 13.2276\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0135 - mse: 11.7956 - val_loss: 0.0151 - val_mse: 13.1080\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0134 - mse: 11.6715 - val_loss: 0.0149 - val_mse: 12.9700\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0133 - mse: 11.5493 - val_loss: 0.0147 - val_mse: 12.8106\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0131 - mse: 11.4163 - val_loss: 0.0145 - val_mse: 12.6514\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0130 - mse: 11.2840 - val_loss: 0.0144 - val_mse: 12.5145\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0128 - mse: 11.1662 - val_loss: 0.0142 - val_mse: 12.3998\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0127 - mse: 11.0562 - val_loss: 0.0141 - val_mse: 12.2934\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0126 - mse: 10.9400 - val_loss: 0.0140 - val_mse: 12.1887\n",
      "Epoch 84/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0124 - mse: 10.8192 - val_loss: 0.0139 - val_mse: 12.0839\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0123 - mse: 10.7056 - val_loss: 0.0137 - val_mse: 11.9700\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0122 - mse: 10.5989 - val_loss: 0.0136 - val_mse: 11.8377\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0120 - mse: 10.4891 - val_loss: 0.0134 - val_mse: 11.6988\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0119 - mse: 10.3772 - val_loss: 0.0133 - val_mse: 11.5766\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0118 - mse: 10.2726 - val_loss: 0.0132 - val_mse: 11.4754\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0117 - mse: 10.1726 - val_loss: 0.0131 - val_mse: 11.3838\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0116 - mse: 10.0693 - val_loss: 0.0130 - val_mse: 11.2947\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0114 - mse: 9.9645 - val_loss: 0.0129 - val_mse: 11.2103\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0113 - mse: 9.8650 - val_loss: 0.0128 - val_mse: 11.1293\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0112 - mse: 9.7696 - val_loss: 0.0127 - val_mse: 11.0473\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0111 - mse: 9.6725 - val_loss: 0.0126 - val_mse: 10.9649\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0110 - mse: 9.5763 - val_loss: 0.0125 - val_mse: 10.8822\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0109 - mse: 9.4849 - val_loss: 0.0124 - val_mse: 10.7937\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0108 - mse: 9.3934 - val_loss: 0.0123 - val_mse: 10.7019\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0107 - mse: 9.3000 - val_loss: 0.0122 - val_mse: 10.6158\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0106 - mse: 9.2083 - val_loss: 0.0121 - val_mse: 10.5359\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0105 - mse: 9.1192 - val_loss: 0.0120 - val_mse: 10.4535\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0104 - mse: 9.0295 - val_loss: 0.0119 - val_mse: 10.3682\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0103 - mse: 8.9395 - val_loss: 0.0118 - val_mse: 10.2856\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0102 - mse: 8.8517 - val_loss: 0.0117 - val_mse: 10.2068\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0101 - mse: 8.7637 - val_loss: 0.0116 - val_mse: 10.1286\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0100 - mse: 8.6738 - val_loss: 0.0115 - val_mse: 10.0504\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0099 - mse: 8.5840 - val_loss: 0.0115 - val_mse: 9.9729\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0098 - mse: 8.4952 - val_loss: 0.0114 - val_mse: 9.8956\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0097 - mse: 8.4057 - val_loss: 0.0113 - val_mse: 9.8176\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0095 - mse: 8.3162 - val_loss: 0.0112 - val_mse: 9.7363\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0094 - mse: 8.2275 - val_loss: 0.0111 - val_mse: 9.6516\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0093 - mse: 8.1385 - val_loss: 0.0110 - val_mse: 9.5665\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0092 - mse: 8.0494 - val_loss: 0.0109 - val_mse: 9.4830\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0091 - mse: 7.9604 - val_loss: 0.0108 - val_mse: 9.3967\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0090 - mse: 7.8714 - val_loss: 0.0107 - val_mse: 9.3064\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0089 - mse: 7.7823 - val_loss: 0.0106 - val_mse: 9.2158\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0088 - mse: 7.6939 - val_loss: 0.0105 - val_mse: 9.1274\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0087 - mse: 7.6054 - val_loss: 0.0104 - val_mse: 9.0416\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0086 - mse: 7.5165 - val_loss: 0.0103 - val_mse: 8.9571\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0085 - mse: 7.4271 - val_loss: 0.0102 - val_mse: 8.8745\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0084 - mse: 7.3375 - val_loss: 0.0101 - val_mse: 8.7932\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0083 - mse: 7.2481 - val_loss: 0.0100 - val_mse: 8.7078\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0082 - mse: 7.1587 - val_loss: 0.0099 - val_mse: 8.6193\n",
      "Epoch 124/1000\n",
      "2s - loss: 0.0081 - mse: 7.0705 - val_loss: 0.0098 - val_mse: 8.5326\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0080 - mse: 6.9824 - val_loss: 0.0097 - val_mse: 8.4491\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0079 - mse: 6.8953 - val_loss: 0.0096 - val_mse: 8.3653\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0078 - mse: 6.8086 - val_loss: 0.0095 - val_mse: 8.2834\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0077 - mse: 6.7236 - val_loss: 0.0094 - val_mse: 8.2036\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0076 - mse: 6.6400 - val_loss: 0.0093 - val_mse: 8.1238\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0075 - mse: 6.5581 - val_loss: 0.0092 - val_mse: 8.0468\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0074 - mse: 6.4782 - val_loss: 0.0092 - val_mse: 7.9740\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0073 - mse: 6.4000 - val_loss: 0.0091 - val_mse: 7.9019\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0073 - mse: 6.3238 - val_loss: 0.0090 - val_mse: 7.8341\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0072 - mse: 6.2508 - val_loss: 0.0089 - val_mse: 7.7706\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0071 - mse: 6.1819 - val_loss: 0.0089 - val_mse: 7.7128\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0070 - mse: 6.1194 - val_loss: 0.0088 - val_mse: 7.6564\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0070 - mse: 6.0584 - val_loss: 0.0087 - val_mse: 7.5978\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0069 - mse: 5.9991 - val_loss: 0.0087 - val_mse: 7.5392\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0068 - mse: 5.9416 - val_loss: 0.0086 - val_mse: 7.4784\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0068 - mse: 5.8862 - val_loss: 0.0085 - val_mse: 7.4192\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0067 - mse: 5.8319 - val_loss: 0.0085 - val_mse: 7.3640\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0066 - mse: 5.7784 - val_loss: 0.0084 - val_mse: 7.3087\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0066 - mse: 5.7258 - val_loss: 0.0083 - val_mse: 7.2557\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0065 - mse: 5.6747 - val_loss: 0.0083 - val_mse: 7.2019\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0065 - mse: 5.6246 - val_loss: 0.0082 - val_mse: 7.1485\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0064 - mse: 5.5754 - val_loss: 0.0082 - val_mse: 7.1008\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0063 - mse: 5.5275 - val_loss: 0.0081 - val_mse: 7.0551\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0063 - mse: 5.4808 - val_loss: 0.0080 - val_mse: 7.0102\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0062 - mse: 5.4348 - val_loss: 0.0080 - val_mse: 6.9666\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0062 - mse: 5.3892 - val_loss: 0.0079 - val_mse: 6.9222\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0061 - mse: 5.3439 - val_loss: 0.0079 - val_mse: 6.8813\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0061 - mse: 5.2994 - val_loss: 0.0079 - val_mse: 6.8401\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0060 - mse: 5.2552 - val_loss: 0.0078 - val_mse: 6.7980\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0060 - mse: 5.2115 - val_loss: 0.0078 - val_mse: 6.7553\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0059 - mse: 5.1678 - val_loss: 0.0077 - val_mse: 6.7097\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0059 - mse: 5.1240 - val_loss: 0.0077 - val_mse: 6.6678\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0058 - mse: 5.0803 - val_loss: 0.0076 - val_mse: 6.6252\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0058 - mse: 5.0362 - val_loss: 0.0076 - val_mse: 6.5847\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0057 - mse: 4.9918 - val_loss: 0.0075 - val_mse: 6.5400\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0057 - mse: 4.9473 - val_loss: 0.0075 - val_mse: 6.4995\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0056 - mse: 4.9032 - val_loss: 0.0074 - val_mse: 6.4562\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0056 - mse: 4.8597 - val_loss: 0.0074 - val_mse: 6.4208\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0055 - mse: 4.8162 - val_loss: 0.0073 - val_mse: 6.3763\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0055 - mse: 4.7728 - val_loss: 0.0073 - val_mse: 6.3417\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0054 - mse: 4.7295 - val_loss: 0.0072 - val_mse: 6.2881\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0054 - mse: 4.6864 - val_loss: 0.0072 - val_mse: 6.2647\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0053 - mse: 4.6443 - val_loss: 0.0071 - val_mse: 6.1997\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0053 - mse: 4.6062 - val_loss: 0.0072 - val_mse: 6.2282\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0053 - mse: 4.5835 - val_loss: 0.0071 - val_mse: 6.1559\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0053 - mse: 4.6119 - val_loss: 0.0075 - val_mse: 6.5319\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0056 - mse: 4.8421 - val_loss: 0.0079 - val_mse: 6.8837\n",
      "Epoch 172/1000\n",
      "\n",
      "Epoch 00171: reducing learning rate to 0.00699999984354.\n",
      "1s - loss: 0.0064 - mse: 5.5898 - val_loss: 0.0107 - val_mse: 9.3174\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0088 - mse: 7.6861 - val_loss: 0.0072 - val_mse: 6.2378\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0055 - mse: 4.8093 - val_loss: 0.0083 - val_mse: 7.2267\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0070 - mse: 6.0634 - val_loss: 0.0083 - val_mse: 7.2334\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0064 - mse: 5.6032 - val_loss: 0.0079 - val_mse: 6.8806\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0061 - mse: 5.2824 - val_loss: 0.0076 - val_mse: 6.5804\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0062 - mse: 5.3850 - val_loss: 0.0071 - val_mse: 6.1786\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0056 - mse: 4.8693 - val_loss: 0.0077 - val_mse: 6.7232\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0058 - mse: 5.0909 - val_loss: 0.0075 - val_mse: 6.5296\n",
      "Epoch 181/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0056 - mse: 4.8792 - val_loss: 0.0070 - val_mse: 6.0777\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0054 - mse: 4.6888 - val_loss: 0.0071 - val_mse: 6.1898\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0056 - mse: 4.8335 - val_loss: 0.0070 - val_mse: 6.1030\n",
      "Epoch 184/1000\n",
      "1s - loss: 0.0051 - mse: 4.4394 - val_loss: 0.0075 - val_mse: 6.5127\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0055 - mse: 4.8025 - val_loss: 0.0066 - val_mse: 5.7768\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0048 - mse: 4.2099 - val_loss: 0.0070 - val_mse: 6.1319\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0054 - mse: 4.7187 - val_loss: 0.0066 - val_mse: 5.7592\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0047 - mse: 4.1155 - val_loss: 0.0072 - val_mse: 6.2818\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0052 - mse: 4.5507 - val_loss: 0.0066 - val_mse: 5.7266\n",
      "Epoch 190/1000\n",
      "\n",
      "Epoch 00189: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0047 - mse: 4.0749 - val_loss: 0.0067 - val_mse: 5.8374\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0050 - mse: 4.3756 - val_loss: 0.0065 - val_mse: 5.6536\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0047 - mse: 4.1181 - val_loss: 0.0066 - val_mse: 5.7321\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0047 - mse: 4.0595 - val_loss: 0.0068 - val_mse: 5.9291\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0048 - mse: 4.2221 - val_loss: 0.0064 - val_mse: 5.5962\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0045 - mse: 3.9449 - val_loss: 0.0064 - val_mse: 5.5845\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0047 - mse: 4.0514 - val_loss: 0.0064 - val_mse: 5.5534\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0046 - mse: 4.0234 - val_loss: 0.0063 - val_mse: 5.4960\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0044 - mse: 3.8629 - val_loss: 0.0065 - val_mse: 5.6709\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0046 - mse: 3.9921 - val_loss: 0.0063 - val_mse: 5.4954\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0044 - mse: 3.8467 - val_loss: 0.0062 - val_mse: 5.3955\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0044 - mse: 3.8333 - val_loss: 0.0062 - val_mse: 5.4039\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0044 - mse: 3.8674 - val_loss: 0.0061 - val_mse: 5.3406\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0043 - mse: 3.7298 - val_loss: 0.0063 - val_mse: 5.4515\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0044 - mse: 3.7927 - val_loss: 0.0061 - val_mse: 5.3544\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0043 - mse: 3.7146 - val_loss: 0.0060 - val_mse: 5.2437\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0042 - mse: 3.6725 - val_loss: 0.0060 - val_mse: 5.2372\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0042 - mse: 3.6950 - val_loss: 0.0060 - val_mse: 5.1935\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0041 - mse: 3.5964 - val_loss: 0.0060 - val_mse: 5.2653\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0042 - mse: 3.6242 - val_loss: 0.0060 - val_mse: 5.1955\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0041 - mse: 3.5648 - val_loss: 0.0059 - val_mse: 5.1152\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0041 - mse: 3.5341 - val_loss: 0.0059 - val_mse: 5.0965\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0041 - mse: 3.5305 - val_loss: 0.0058 - val_mse: 5.0720\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0040 - mse: 3.4622 - val_loss: 0.0059 - val_mse: 5.1082\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0040 - mse: 3.4712 - val_loss: 0.0058 - val_mse: 5.0309\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0039 - mse: 3.4115 - val_loss: 0.0057 - val_mse: 4.9788\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0039 - mse: 3.3978 - val_loss: 0.0057 - val_mse: 4.9528\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0039 - mse: 3.3669 - val_loss: 0.0057 - val_mse: 4.9515\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0038 - mse: 3.3269 - val_loss: 0.0057 - val_mse: 4.9536\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0038 - mse: 3.3162 - val_loss: 0.0056 - val_mse: 4.8795\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0038 - mse: 3.2663 - val_loss: 0.0056 - val_mse: 4.8457\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0037 - mse: 3.2576 - val_loss: 0.0055 - val_mse: 4.8189\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0037 - mse: 3.2133 - val_loss: 0.0055 - val_mse: 4.8273\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0037 - mse: 3.1944 - val_loss: 0.0055 - val_mse: 4.7937\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0036 - mse: 3.1611 - val_loss: 0.0054 - val_mse: 4.7434\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0036 - mse: 3.1314 - val_loss: 0.0054 - val_mse: 4.7184\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0036 - mse: 3.1073 - val_loss: 0.0054 - val_mse: 4.7076\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0035 - mse: 3.0716 - val_loss: 0.0054 - val_mse: 4.6988\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0035 - mse: 3.0511 - val_loss: 0.0053 - val_mse: 4.6475\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0035 - mse: 3.0135 - val_loss: 0.0053 - val_mse: 4.6179\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0034 - mse: 2.9944 - val_loss: 0.0053 - val_mse: 4.6008\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0034 - mse: 2.9597 - val_loss: 0.0053 - val_mse: 4.5991\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0034 - mse: 2.9397 - val_loss: 0.0052 - val_mse: 4.5628\n",
      "Epoch 233/1000\n",
      "2s - loss: 0.0033 - mse: 2.9078 - val_loss: 0.0052 - val_mse: 4.5282\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0033 - mse: 2.8859 - val_loss: 0.0052 - val_mse: 4.5067\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0033 - mse: 2.8571 - val_loss: 0.0052 - val_mse: 4.4995\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0033 - mse: 2.8336 - val_loss: 0.0051 - val_mse: 4.4739\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0032 - mse: 2.8067 - val_loss: 0.0051 - val_mse: 4.4400\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0032 - mse: 2.7814 - val_loss: 0.0051 - val_mse: 4.4194\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0032 - mse: 2.7556 - val_loss: 0.0051 - val_mse: 4.4097\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0031 - mse: 2.7296 - val_loss: 0.0050 - val_mse: 4.3895\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0031 - mse: 2.7044 - val_loss: 0.0050 - val_mse: 4.3566\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0031 - mse: 2.6777 - val_loss: 0.0050 - val_mse: 4.3339\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.0030 - mse: 2.6525 - val_loss: 0.0050 - val_mse: 4.3217\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0030 - mse: 2.6256 - val_loss: 0.0049 - val_mse: 4.3046\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0030 - mse: 2.6011 - val_loss: 0.0049 - val_mse: 4.2753\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0030 - mse: 2.5745 - val_loss: 0.0049 - val_mse: 4.2544\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0029 - mse: 2.5502 - val_loss: 0.0049 - val_mse: 4.2418\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0029 - mse: 2.5239 - val_loss: 0.0049 - val_mse: 4.2243\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0029 - mse: 2.4990 - val_loss: 0.0048 - val_mse: 4.1962\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0028 - mse: 2.4725 - val_loss: 0.0048 - val_mse: 4.1751\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0028 - mse: 2.4474 - val_loss: 0.0048 - val_mse: 4.1620\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0028 - mse: 2.4214 - val_loss: 0.0048 - val_mse: 4.1457\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0028 - mse: 2.3966 - val_loss: 0.0047 - val_mse: 4.1210\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.0027 - mse: 2.3706 - val_loss: 0.0047 - val_mse: 4.1022\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0027 - mse: 2.3451 - val_loss: 0.0047 - val_mse: 4.0894\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0027 - mse: 2.3191 - val_loss: 0.0047 - val_mse: 4.0716\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.0026 - mse: 2.2934 - val_loss: 0.0046 - val_mse: 4.0488\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0026 - mse: 2.2676 - val_loss: 0.0046 - val_mse: 4.0323\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.0026 - mse: 2.2419 - val_loss: 0.0046 - val_mse: 4.0208\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0025 - mse: 2.2165 - val_loss: 0.0046 - val_mse: 4.0031\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.0025 - mse: 2.1906 - val_loss: 0.0046 - val_mse: 3.9833\n",
      "Epoch 262/1000\n",
      "\n",
      "Epoch 00261: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0025 - mse: 2.1650 - val_loss: 0.0046 - val_mse: 3.9686\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0025 - mse: 2.1389 - val_loss: 0.0045 - val_mse: 3.9593\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.0024 - mse: 2.1211 - val_loss: 0.0045 - val_mse: 3.9482\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0024 - mse: 2.1036 - val_loss: 0.0045 - val_mse: 3.9354\n",
      "Epoch 266/1000\n",
      "1s - loss: 0.0024 - mse: 2.0861 - val_loss: 0.0045 - val_mse: 3.9237\n",
      "Epoch 267/1000\n",
      "1s - loss: 0.0024 - mse: 2.0688 - val_loss: 0.0045 - val_mse: 3.9135\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0024 - mse: 2.0513 - val_loss: 0.0045 - val_mse: 3.9032\n",
      "Epoch 269/1000\n",
      "1s - loss: 0.0023 - mse: 2.0340 - val_loss: 0.0045 - val_mse: 3.8912\n",
      "Epoch 270/1000\n",
      "1s - loss: 0.0023 - mse: 2.0166 - val_loss: 0.0045 - val_mse: 3.8793\n",
      "Epoch 271/1000\n",
      "1s - loss: 0.0023 - mse: 1.9995 - val_loss: 0.0044 - val_mse: 3.8695\n",
      "Epoch 272/1000\n",
      "1s - loss: 0.0023 - mse: 1.9824 - val_loss: 0.0044 - val_mse: 3.8612\n",
      "Epoch 273/1000\n",
      "1s - loss: 0.0023 - mse: 1.9655 - val_loss: 0.0044 - val_mse: 3.8522\n",
      "Epoch 274/1000\n",
      "1s - loss: 0.0022 - mse: 1.9486 - val_loss: 0.0044 - val_mse: 3.8424\n",
      "Epoch 275/1000\n",
      "1s - loss: 0.0022 - mse: 1.9318 - val_loss: 0.0044 - val_mse: 3.8337\n",
      "Epoch 276/1000\n",
      "\n",
      "Epoch 00275: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0022 - mse: 1.9152 - val_loss: 0.0044 - val_mse: 3.8263\n",
      "Epoch 277/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0022 - mse: 1.8986 - val_loss: 0.0044 - val_mse: 3.8213\n",
      "Epoch 278/1000\n",
      "1s - loss: 0.0022 - mse: 1.8873 - val_loss: 0.0044 - val_mse: 3.8154\n",
      "Epoch 279/1000\n",
      "1s - loss: 0.0022 - mse: 1.8760 - val_loss: 0.0044 - val_mse: 3.8092\n",
      "Epoch 280/1000\n",
      "1s - loss: 0.0021 - mse: 1.8648 - val_loss: 0.0044 - val_mse: 3.8035\n",
      "Epoch 281/1000\n",
      "1s - loss: 0.0021 - mse: 1.8537 - val_loss: 0.0044 - val_mse: 3.7986\n",
      "Epoch 282/1000\n",
      "1s - loss: 0.0021 - mse: 1.8429 - val_loss: 0.0044 - val_mse: 3.7940\n",
      "Epoch 283/1000\n",
      "1s - loss: 0.0021 - mse: 1.8321 - val_loss: 0.0044 - val_mse: 3.7892\n",
      "Epoch 284/1000\n",
      "1s - loss: 0.0021 - mse: 1.8215 - val_loss: 0.0043 - val_mse: 3.7840\n",
      "Epoch 285/1000\n",
      "1s - loss: 0.0021 - mse: 1.8108 - val_loss: 0.0043 - val_mse: 3.7789\n",
      "Epoch 286/1000\n",
      "1s - loss: 0.0021 - mse: 1.8003 - val_loss: 0.0043 - val_mse: 3.7741\n",
      "Epoch 287/1000\n",
      "1s - loss: 0.0021 - mse: 1.7898 - val_loss: 0.0043 - val_mse: 3.7699\n",
      "Epoch 288/1000\n",
      "1s - loss: 0.0020 - mse: 1.7795 - val_loss: 0.0043 - val_mse: 3.7654\n",
      "Epoch 289/1000\n",
      "1s - loss: 0.0020 - mse: 1.7693 - val_loss: 0.0043 - val_mse: 3.7603\n",
      "Epoch 290/1000\n",
      "\n",
      "Epoch 00289: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0020 - mse: 1.7592 - val_loss: 0.0043 - val_mse: 3.7551\n",
      "Epoch 291/1000\n",
      "1s - loss: 0.0020 - mse: 1.7493 - val_loss: 0.0043 - val_mse: 3.7519\n",
      "Epoch 292/1000\n",
      "1s - loss: 0.0020 - mse: 1.7424 - val_loss: 0.0043 - val_mse: 3.7492\n",
      "Epoch 293/1000\n",
      "1s - loss: 0.0020 - mse: 1.7356 - val_loss: 0.0043 - val_mse: 3.7467\n",
      "Epoch 294/1000\n",
      "1s - loss: 0.0020 - mse: 1.7288 - val_loss: 0.0043 - val_mse: 3.7443\n",
      "Epoch 295/1000\n",
      "1s - loss: 0.0020 - mse: 1.7221 - val_loss: 0.0043 - val_mse: 3.7416\n",
      "Epoch 296/1000\n",
      "1s - loss: 0.0020 - mse: 1.7154 - val_loss: 0.0043 - val_mse: 3.7389\n",
      "Epoch 297/1000\n",
      "1s - loss: 0.0020 - mse: 1.7088 - val_loss: 0.0043 - val_mse: 3.7361\n",
      "Epoch 298/1000\n",
      "1s - loss: 0.0020 - mse: 1.7022 - val_loss: 0.0043 - val_mse: 3.7334\n",
      "Epoch 299/1000\n",
      "1s - loss: 0.0019 - mse: 1.6957 - val_loss: 0.0043 - val_mse: 3.7308\n",
      "Epoch 300/1000\n",
      "1s - loss: 0.0019 - mse: 1.6893 - val_loss: 0.0043 - val_mse: 3.7281\n",
      "Epoch 301/1000\n",
      "1s - loss: 0.0019 - mse: 1.6828 - val_loss: 0.0043 - val_mse: 3.7252\n",
      "Epoch 302/1000\n",
      "1s - loss: 0.0019 - mse: 1.6764 - val_loss: 0.0043 - val_mse: 3.7224\n",
      "Epoch 303/1000\n",
      "1s - loss: 0.0019 - mse: 1.6700 - val_loss: 0.0043 - val_mse: 3.7197\n",
      "Epoch 304/1000\n",
      "\n",
      "Epoch 00303: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0019 - mse: 1.6637 - val_loss: 0.0043 - val_mse: 3.7173\n",
      "Epoch 305/1000\n",
      "1s - loss: 0.0019 - mse: 1.6574 - val_loss: 0.0043 - val_mse: 3.7158\n",
      "Epoch 306/1000\n",
      "1s - loss: 0.0019 - mse: 1.6530 - val_loss: 0.0043 - val_mse: 3.7143\n",
      "Epoch 307/1000\n",
      "1s - loss: 0.0019 - mse: 1.6487 - val_loss: 0.0043 - val_mse: 3.7128\n",
      "Epoch 308/1000\n",
      "1s - loss: 0.0019 - mse: 1.6443 - val_loss: 0.0043 - val_mse: 3.7112\n",
      "Epoch 309/1000\n",
      "1s - loss: 0.0019 - mse: 1.6400 - val_loss: 0.0043 - val_mse: 3.7095\n",
      "Epoch 310/1000\n",
      "1s - loss: 0.0019 - mse: 1.6357 - val_loss: 0.0043 - val_mse: 3.7078\n",
      "Epoch 311/1000\n",
      "1s - loss: 0.0019 - mse: 1.6315 - val_loss: 0.0043 - val_mse: 3.7062\n",
      "Epoch 312/1000\n",
      "1s - loss: 0.0019 - mse: 1.6272 - val_loss: 0.0043 - val_mse: 3.7047\n",
      "Epoch 313/1000\n",
      "1s - loss: 0.0019 - mse: 1.6230 - val_loss: 0.0043 - val_mse: 3.7032\n",
      "Epoch 314/1000\n",
      "1s - loss: 0.0019 - mse: 1.6189 - val_loss: 0.0043 - val_mse: 3.7017\n",
      "Epoch 315/1000\n",
      "1s - loss: 0.0019 - mse: 1.6147 - val_loss: 0.0042 - val_mse: 3.7001\n",
      "Epoch 316/1000\n",
      "1s - loss: 0.0018 - mse: 1.6106 - val_loss: 0.0042 - val_mse: 3.6984\n",
      "[{'loss': 0.0033503014128655195, 'width0': 135, 'width1': 75, 'val_mse': 6.1875839233398438, 'mse': 2.9175732135772705, 'val_loss': 0.007105313241481781, 'dropout1': 0, 'dropout0': 0}, {'loss': 0.0018494370160624385, 'width0': 135, 'width1': 100, 'val_mse': 3.698413610458374, 'mse': 1.6105610132217407, 'val_loss': 0.0042469552718102932, 'dropout1': 0, 'dropout0': 0}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, None, 125)         130500    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, None, 1)           126       \n",
      "=================================================================\n",
      "Total params: 205,146\n",
      "Trainable params: 205,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 1.0762 - mse: 937.1887 - val_loss: 1.6855 - val_mse: 1467.7700\n",
      "Epoch 2/1000\n",
      "1s - loss: 1.7553 - mse: 1528.6241 - val_loss: 1.4271 - val_mse: 1242.7629\n",
      "Epoch 3/1000\n",
      "1s - loss: 1.5003 - mse: 1306.4886 - val_loss: 0.5967 - val_mse: 519.5998\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.5839 - mse: 508.5075 - val_loss: 0.5504 - val_mse: 479.3116\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.5222 - mse: 454.7095 - val_loss: 0.4274 - val_mse: 372.2094\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.4014 - mse: 349.5149 - val_loss: 0.2961 - val_mse: 257.8587\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.2767 - mse: 240.9353 - val_loss: 0.2318 - val_mse: 201.8172\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.2203 - mse: 191.8293 - val_loss: 0.1561 - val_mse: 135.9707\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.1495 - mse: 130.1976 - val_loss: 0.1079 - val_mse: 93.9905\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.1035 - mse: 90.0980 - val_loss: 0.1301 - val_mse: 113.2728\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.1231 - mse: 107.2229 - val_loss: 0.1343 - val_mse: 116.9952\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.1255 - mse: 109.2778 - val_loss: 0.1048 - val_mse: 91.2279\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0958 - mse: 83.3917 - val_loss: 0.1003 - val_mse: 87.3745\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0922 - mse: 80.3176 - val_loss: 0.1077 - val_mse: 93.8020\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.1010 - mse: 87.9541 - val_loss: 0.0921 - val_mse: 80.1679\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0851 - mse: 74.1334 - val_loss: 0.0771 - val_mse: 67.1316\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0686 - mse: 59.7697 - val_loss: 0.0740 - val_mse: 64.4681\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0641 - mse: 55.8296 - val_loss: 0.0725 - val_mse: 63.1272\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0624 - mse: 54.3707 - val_loss: 0.0666 - val_mse: 57.9827\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0582 - mse: 50.7148 - val_loss: 0.0608 - val_mse: 52.9148\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0553 - mse: 48.1411 - val_loss: 0.0610 - val_mse: 53.1113\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0579 - mse: 50.4321 - val_loss: 0.0637 - val_mse: 55.4816\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0615 - mse: 53.5354 - val_loss: 0.0604 - val_mse: 52.6408\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0579 - mse: 50.4010 - val_loss: 0.0530 - val_mse: 46.1399\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0496 - mse: 43.2270 - val_loss: 0.0485 - val_mse: 42.2249\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0443 - mse: 38.5489 - val_loss: 0.0482 - val_mse: 41.9644\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0432 - mse: 37.6451 - val_loss: 0.0486 - val_mse: 42.3583\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0433 - mse: 37.7058 - val_loss: 0.0473 - val_mse: 41.1564\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0419 - mse: 36.5301 - val_loss: 0.0443 - val_mse: 38.6197\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0394 - mse: 34.2898 - val_loss: 0.0418 - val_mse: 36.3708\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0372 - mse: 32.4340 - val_loss: 0.0405 - val_mse: 35.3105\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0364 - mse: 31.6927 - val_loss: 0.0398 - val_mse: 34.6704\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0358 - mse: 31.2086 - val_loss: 0.0383 - val_mse: 33.3298\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0343 - mse: 29.8547 - val_loss: 0.0363 - val_mse: 31.6201\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0321 - mse: 27.9965 - val_loss: 0.0353 - val_mse: 30.7376\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0309 - mse: 26.9195 - val_loss: 0.0353 - val_mse: 30.7114\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0308 - mse: 26.7835 - val_loss: 0.0348 - val_mse: 30.2971\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0304 - mse: 26.4430 - val_loss: 0.0330 - val_mse: 28.7390\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0289 - mse: 25.1253 - val_loss: 0.0307 - val_mse: 26.7179\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0269 - mse: 23.3837 - val_loss: 0.0291 - val_mse: 25.3516\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0255 - mse: 22.1993 - val_loss: 0.0285 - val_mse: 24.7963\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0249 - mse: 21.6877 - val_loss: 0.0279 - val_mse: 24.3136\n",
      "Epoch 43/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0243 - mse: 21.1778 - val_loss: 0.0269 - val_mse: 23.4660\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0233 - mse: 20.3313 - val_loss: 0.0259 - val_mse: 22.5641\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0224 - mse: 19.5024 - val_loss: 0.0252 - val_mse: 21.9371\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0218 - mse: 18.9891 - val_loss: 0.0246 - val_mse: 21.4416\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0213 - mse: 18.5825 - val_loss: 0.0239 - val_mse: 20.8044\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0206 - mse: 17.9585 - val_loss: 0.0231 - val_mse: 20.1063\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0197 - mse: 17.1882 - val_loss: 0.0226 - val_mse: 19.6486\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0191 - mse: 16.6200 - val_loss: 0.0223 - val_mse: 19.4194\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0188 - mse: 16.3294 - val_loss: 0.0218 - val_mse: 19.0276\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0184 - mse: 15.9959 - val_loss: 0.0210 - val_mse: 18.2989\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0177 - mse: 15.4392 - val_loss: 0.0202 - val_mse: 17.5655\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0171 - mse: 14.9102 - val_loss: 0.0197 - val_mse: 17.1560\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0168 - mse: 14.6486 - val_loss: 0.0195 - val_mse: 16.9812\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0167 - mse: 14.5195 - val_loss: 0.0193 - val_mse: 16.7903\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0164 - mse: 14.2698 - val_loss: 0.0190 - val_mse: 16.5608\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0160 - mse: 13.8963 - val_loss: 0.0189 - val_mse: 16.4550\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0156 - mse: 13.5961 - val_loss: 0.0190 - val_mse: 16.5054\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0155 - mse: 13.4592 - val_loss: 0.0190 - val_mse: 16.5272\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0153 - mse: 13.3666 - val_loss: 0.0188 - val_mse: 16.3552\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0152 - mse: 13.1963 - val_loss: 0.0184 - val_mse: 16.0302\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0149 - mse: 12.9824 - val_loss: 0.0180 - val_mse: 15.6852\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0147 - mse: 12.8134 - val_loss: 0.0177 - val_mse: 15.3735\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0146 - mse: 12.6892 - val_loss: 0.0173 - val_mse: 15.0692\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0144 - mse: 12.5501 - val_loss: 0.0170 - val_mse: 14.7818\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0142 - mse: 12.3862 - val_loss: 0.0167 - val_mse: 14.5583\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0141 - mse: 12.2417 - val_loss: 0.0165 - val_mse: 14.4089\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0139 - mse: 12.1325 - val_loss: 0.0164 - val_mse: 14.2836\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0138 - mse: 12.0215 - val_loss: 0.0162 - val_mse: 14.1367\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0136 - mse: 11.8788 - val_loss: 0.0160 - val_mse: 13.9768\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0135 - mse: 11.7270 - val_loss: 0.0159 - val_mse: 13.8308\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0133 - mse: 11.5996 - val_loss: 0.0157 - val_mse: 13.6956\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0132 - mse: 11.4927 - val_loss: 0.0156 - val_mse: 13.5551\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0131 - mse: 11.3827 - val_loss: 0.0154 - val_mse: 13.4117\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0129 - mse: 11.2659 - val_loss: 0.0153 - val_mse: 13.2826\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0128 - mse: 11.1554 - val_loss: 0.0151 - val_mse: 13.1698\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0127 - mse: 11.0533 - val_loss: 0.0150 - val_mse: 13.0607\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0126 - mse: 10.9487 - val_loss: 0.0149 - val_mse: 12.9497\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0124 - mse: 10.8377 - val_loss: 0.0147 - val_mse: 12.8430\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0123 - mse: 10.7290 - val_loss: 0.0146 - val_mse: 12.7409\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0122 - mse: 10.6264 - val_loss: 0.0145 - val_mse: 12.6360\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0121 - mse: 10.5249 - val_loss: 0.0144 - val_mse: 12.5280\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0120 - mse: 10.4244 - val_loss: 0.0143 - val_mse: 12.4255\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0119 - mse: 10.3314 - val_loss: 0.0142 - val_mse: 12.3309\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0118 - mse: 10.2451 - val_loss: 0.0141 - val_mse: 12.2371\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0117 - mse: 10.1556 - val_loss: 0.0139 - val_mse: 12.1426\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0116 - mse: 10.0592 - val_loss: 0.0138 - val_mse: 12.0574\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0114 - mse: 9.9651 - val_loss: 0.0138 - val_mse: 11.9855\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0113 - mse: 9.8797 - val_loss: 0.0137 - val_mse: 11.9181\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0113 - mse: 9.7981 - val_loss: 0.0136 - val_mse: 11.8448\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0112 - mse: 9.7131 - val_loss: 0.0135 - val_mse: 11.7666\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0111 - mse: 9.6266 - val_loss: 0.0134 - val_mse: 11.6906\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0110 - mse: 9.5443 - val_loss: 0.0133 - val_mse: 11.6189\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0109 - mse: 9.4664 - val_loss: 0.0133 - val_mse: 11.5470\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0108 - mse: 9.3875 - val_loss: 0.0132 - val_mse: 11.4732\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0107 - mse: 9.3063 - val_loss: 0.0131 - val_mse: 11.3988\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0106 - mse: 9.2258 - val_loss: 0.0130 - val_mse: 11.3235\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0105 - mse: 9.1475 - val_loss: 0.0129 - val_mse: 11.2446\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0104 - mse: 9.0701 - val_loss: 0.0128 - val_mse: 11.1626\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0103 - mse: 8.9932 - val_loss: 0.0127 - val_mse: 11.0812\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0102 - mse: 8.9173 - val_loss: 0.0126 - val_mse: 11.0045\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0102 - mse: 8.8422 - val_loss: 0.0126 - val_mse: 10.9350\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0101 - mse: 8.7671 - val_loss: 0.0125 - val_mse: 10.8741\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0100 - mse: 8.6920 - val_loss: 0.0124 - val_mse: 10.8211\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0099 - mse: 8.6178 - val_loss: 0.0124 - val_mse: 10.7725\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0098 - mse: 8.5443 - val_loss: 0.0123 - val_mse: 10.7237\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0097 - mse: 8.4708 - val_loss: 0.0123 - val_mse: 10.6721\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0096 - mse: 8.3980 - val_loss: 0.0122 - val_mse: 10.6169\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0096 - mse: 8.3263 - val_loss: 0.0121 - val_mse: 10.5588\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0095 - mse: 8.2556 - val_loss: 0.0121 - val_mse: 10.4986\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0094 - mse: 8.1850 - val_loss: 0.0120 - val_mse: 10.4382\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0093 - mse: 8.1143 - val_loss: 0.0119 - val_mse: 10.3799\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0092 - mse: 8.0442 - val_loss: 0.0119 - val_mse: 10.3243\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0092 - mse: 7.9748 - val_loss: 0.0118 - val_mse: 10.2696\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0091 - mse: 7.9059 - val_loss: 0.0117 - val_mse: 10.2146\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0090 - mse: 7.8372 - val_loss: 0.0117 - val_mse: 10.1592\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0089 - mse: 7.7692 - val_loss: 0.0116 - val_mse: 10.1037\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0088 - mse: 7.7018 - val_loss: 0.0115 - val_mse: 10.0482\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0088 - mse: 7.6347 - val_loss: 0.0115 - val_mse: 9.9939\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0087 - mse: 7.5681 - val_loss: 0.0114 - val_mse: 9.9420\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0086 - mse: 7.5025 - val_loss: 0.0114 - val_mse: 9.8923\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0085 - mse: 7.4377 - val_loss: 0.0113 - val_mse: 9.8443\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0085 - mse: 7.3734 - val_loss: 0.0113 - val_mse: 9.7972\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0084 - mse: 7.3096 - val_loss: 0.0112 - val_mse: 9.7506\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0083 - mse: 7.2466 - val_loss: 0.0111 - val_mse: 9.7033\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0082 - mse: 7.1843 - val_loss: 0.0111 - val_mse: 9.6548\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0082 - mse: 7.1226 - val_loss: 0.0110 - val_mse: 9.6050\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0081 - mse: 7.0620 - val_loss: 0.0110 - val_mse: 9.5541\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0080 - mse: 7.0023 - val_loss: 0.0109 - val_mse: 9.5030\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0080 - mse: 6.9436 - val_loss: 0.0109 - val_mse: 9.4527\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0079 - mse: 6.8856 - val_loss: 0.0108 - val_mse: 9.4044\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0078 - mse: 6.8285 - val_loss: 0.0107 - val_mse: 9.3588\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0078 - mse: 6.7723 - val_loss: 0.0107 - val_mse: 9.3160\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0077 - mse: 6.7170 - val_loss: 0.0107 - val_mse: 9.2749\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0077 - mse: 6.6624 - val_loss: 0.0106 - val_mse: 9.2343\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0076 - mse: 6.6086 - val_loss: 0.0106 - val_mse: 9.1933\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0075 - mse: 6.5556 - val_loss: 0.0105 - val_mse: 9.1515\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0075 - mse: 6.5034 - val_loss: 0.0105 - val_mse: 9.1091\n",
      "Epoch 140/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0074 - mse: 6.4522 - val_loss: 0.0104 - val_mse: 9.0667\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0074 - mse: 6.4018 - val_loss: 0.0104 - val_mse: 9.0256\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0073 - mse: 6.3527 - val_loss: 0.0103 - val_mse: 8.9866\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0072 - mse: 6.3047 - val_loss: 0.0103 - val_mse: 8.9501\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0072 - mse: 6.2579 - val_loss: 0.0102 - val_mse: 8.9150\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0071 - mse: 6.2121 - val_loss: 0.0102 - val_mse: 8.8802\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0071 - mse: 6.1678 - val_loss: 0.0102 - val_mse: 8.8451\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0070 - mse: 6.1244 - val_loss: 0.0101 - val_mse: 8.8095\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0070 - mse: 6.0818 - val_loss: 0.0101 - val_mse: 8.7740\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0069 - mse: 6.0400 - val_loss: 0.0100 - val_mse: 8.7391\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0069 - mse: 5.9991 - val_loss: 0.0100 - val_mse: 8.7051\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0068 - mse: 5.9587 - val_loss: 0.0100 - val_mse: 8.6718\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0068 - mse: 5.9189 - val_loss: 0.0099 - val_mse: 8.6392\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0068 - mse: 5.8803 - val_loss: 0.0099 - val_mse: 8.6061\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0067 - mse: 5.8428 - val_loss: 0.0098 - val_mse: 8.5716\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0067 - mse: 5.8057 - val_loss: 0.0098 - val_mse: 8.5361\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0066 - mse: 5.7692 - val_loss: 0.0098 - val_mse: 8.5005\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0066 - mse: 5.7334 - val_loss: 0.0097 - val_mse: 8.4655\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0065 - mse: 5.6982 - val_loss: 0.0097 - val_mse: 8.4316\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0065 - mse: 5.6634 - val_loss: 0.0096 - val_mse: 8.3989\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0065 - mse: 5.6290 - val_loss: 0.0096 - val_mse: 8.3671\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0064 - mse: 5.5950 - val_loss: 0.0096 - val_mse: 8.3361\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0064 - mse: 5.5615 - val_loss: 0.0095 - val_mse: 8.3055\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0063 - mse: 5.5286 - val_loss: 0.0095 - val_mse: 8.2750\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0063 - mse: 5.4963 - val_loss: 0.0095 - val_mse: 8.2453\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0063 - mse: 5.4646 - val_loss: 0.0094 - val_mse: 8.2161\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0062 - mse: 5.4334 - val_loss: 0.0094 - val_mse: 8.1869\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0062 - mse: 5.4024 - val_loss: 0.0094 - val_mse: 8.1577\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0062 - mse: 5.3718 - val_loss: 0.0093 - val_mse: 8.1288\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0061 - mse: 5.3414 - val_loss: 0.0093 - val_mse: 8.1000\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0061 - mse: 5.3114 - val_loss: 0.0093 - val_mse: 8.0711\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0061 - mse: 5.2820 - val_loss: 0.0092 - val_mse: 8.0421\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0060 - mse: 5.2531 - val_loss: 0.0092 - val_mse: 8.0126\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0060 - mse: 5.2245 - val_loss: 0.0092 - val_mse: 7.9828\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0060 - mse: 5.1961 - val_loss: 0.0091 - val_mse: 7.9536\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0059 - mse: 5.1681 - val_loss: 0.0091 - val_mse: 7.9251\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0059 - mse: 5.1403 - val_loss: 0.0091 - val_mse: 7.8972\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0059 - mse: 5.1128 - val_loss: 0.0090 - val_mse: 7.8700\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0058 - mse: 5.0856 - val_loss: 0.0090 - val_mse: 7.8435\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0058 - mse: 5.0588 - val_loss: 0.0090 - val_mse: 7.8169\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0058 - mse: 5.0319 - val_loss: 0.0089 - val_mse: 7.7902\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0057 - mse: 5.0053 - val_loss: 0.0089 - val_mse: 7.7632\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0057 - mse: 4.9788 - val_loss: 0.0089 - val_mse: 7.7366\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0057 - mse: 4.9524 - val_loss: 0.0089 - val_mse: 7.7098\n",
      "Epoch 184/1000\n",
      "1s - loss: 0.0057 - mse: 4.9263 - val_loss: 0.0088 - val_mse: 7.6833\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0056 - mse: 4.9004 - val_loss: 0.0088 - val_mse: 7.6569\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0056 - mse: 4.8746 - val_loss: 0.0088 - val_mse: 7.6305\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0056 - mse: 4.8491 - val_loss: 0.0087 - val_mse: 7.6040\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0055 - mse: 4.8237 - val_loss: 0.0087 - val_mse: 7.5770\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0055 - mse: 4.7986 - val_loss: 0.0087 - val_mse: 7.5497\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0055 - mse: 4.7734 - val_loss: 0.0086 - val_mse: 7.5228\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0055 - mse: 4.7484 - val_loss: 0.0086 - val_mse: 7.4965\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0054 - mse: 4.7236 - val_loss: 0.0086 - val_mse: 7.4704\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0054 - mse: 4.6990 - val_loss: 0.0085 - val_mse: 7.4445\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0054 - mse: 4.6744 - val_loss: 0.0085 - val_mse: 7.4186\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0053 - mse: 4.6498 - val_loss: 0.0085 - val_mse: 7.3922\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0053 - mse: 4.6252 - val_loss: 0.0085 - val_mse: 7.3660\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0053 - mse: 4.6007 - val_loss: 0.0084 - val_mse: 7.3407\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0053 - mse: 4.5771 - val_loss: 0.0084 - val_mse: 7.3158\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0052 - mse: 4.5535 - val_loss: 0.0084 - val_mse: 7.2912\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0052 - mse: 4.5306 - val_loss: 0.0083 - val_mse: 7.2659\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0052 - mse: 4.5076 - val_loss: 0.0083 - val_mse: 7.2398\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0051 - mse: 4.4844 - val_loss: 0.0083 - val_mse: 7.2135\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0051 - mse: 4.4613 - val_loss: 0.0083 - val_mse: 7.1875\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0051 - mse: 4.4384 - val_loss: 0.0082 - val_mse: 7.1620\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0051 - mse: 4.4157 - val_loss: 0.0082 - val_mse: 7.1369\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0050 - mse: 4.3930 - val_loss: 0.0082 - val_mse: 7.1118\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0050 - mse: 4.3703 - val_loss: 0.0081 - val_mse: 7.0865\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0050 - mse: 4.3476 - val_loss: 0.0081 - val_mse: 7.0607\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0050 - mse: 4.3250 - val_loss: 0.0081 - val_mse: 7.0347\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0049 - mse: 4.3024 - val_loss: 0.0080 - val_mse: 7.0088\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0049 - mse: 4.2800 - val_loss: 0.0080 - val_mse: 6.9835\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0049 - mse: 4.2577 - val_loss: 0.0080 - val_mse: 6.9587\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0049 - mse: 4.2356 - val_loss: 0.0080 - val_mse: 6.9336\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0048 - mse: 4.2136 - val_loss: 0.0079 - val_mse: 6.9082\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0048 - mse: 4.1916 - val_loss: 0.0079 - val_mse: 6.8826\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0048 - mse: 4.1695 - val_loss: 0.0079 - val_mse: 6.8574\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0048 - mse: 4.1475 - val_loss: 0.0078 - val_mse: 6.8324\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0047 - mse: 4.1254 - val_loss: 0.0078 - val_mse: 6.8082\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0047 - mse: 4.1032 - val_loss: 0.0078 - val_mse: 6.7844\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0047 - mse: 4.0810 - val_loss: 0.0078 - val_mse: 6.7610\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0047 - mse: 4.0588 - val_loss: 0.0077 - val_mse: 6.7374\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0046 - mse: 4.0365 - val_loss: 0.0077 - val_mse: 6.7136\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0046 - mse: 4.0143 - val_loss: 0.0077 - val_mse: 6.6890\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0046 - mse: 3.9921 - val_loss: 0.0077 - val_mse: 6.6640\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0046 - mse: 3.9699 - val_loss: 0.0076 - val_mse: 6.6379\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0045 - mse: 3.9471 - val_loss: 0.0076 - val_mse: 6.6105\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0045 - mse: 3.9239 - val_loss: 0.0076 - val_mse: 6.5830\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0045 - mse: 3.9009 - val_loss: 0.0075 - val_mse: 6.5559\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0045 - mse: 3.8779 - val_loss: 0.0075 - val_mse: 6.5301\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0044 - mse: 3.8555 - val_loss: 0.0075 - val_mse: 6.5053\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0044 - mse: 3.8337 - val_loss: 0.0074 - val_mse: 6.4807\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0044 - mse: 3.8124 - val_loss: 0.0074 - val_mse: 6.4561\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0044 - mse: 3.7911 - val_loss: 0.0074 - val_mse: 6.4316\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0043 - mse: 3.7699 - val_loss: 0.0074 - val_mse: 6.4078\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0043 - mse: 3.7491 - val_loss: 0.0073 - val_mse: 6.3842\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0043 - mse: 3.7283 - val_loss: 0.0073 - val_mse: 6.3607\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0043 - mse: 3.7077 - val_loss: 0.0073 - val_mse: 6.3371\n",
      "Epoch 238/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0042 - mse: 3.6872 - val_loss: 0.0072 - val_mse: 6.3132\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0042 - mse: 3.6672 - val_loss: 0.0072 - val_mse: 6.2890\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0042 - mse: 3.6472 - val_loss: 0.0072 - val_mse: 6.2653\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0042 - mse: 3.6275 - val_loss: 0.0072 - val_mse: 6.2424\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0041 - mse: 3.6077 - val_loss: 0.0071 - val_mse: 6.2200\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.0041 - mse: 3.5879 - val_loss: 0.0071 - val_mse: 6.1981\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0041 - mse: 3.5684 - val_loss: 0.0071 - val_mse: 6.1763\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0041 - mse: 3.5491 - val_loss: 0.0071 - val_mse: 6.1546\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0041 - mse: 3.5299 - val_loss: 0.0070 - val_mse: 6.1328\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0040 - mse: 3.5103 - val_loss: 0.0070 - val_mse: 6.1115\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0040 - mse: 3.4911 - val_loss: 0.0070 - val_mse: 6.0903\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0040 - mse: 3.4724 - val_loss: 0.0070 - val_mse: 6.0693\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0040 - mse: 3.4537 - val_loss: 0.0069 - val_mse: 6.0482\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0039 - mse: 3.4349 - val_loss: 0.0069 - val_mse: 6.0266\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0039 - mse: 3.4161 - val_loss: 0.0069 - val_mse: 6.0048\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0039 - mse: 3.3974 - val_loss: 0.0069 - val_mse: 5.9827\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.0039 - mse: 3.3786 - val_loss: 0.0068 - val_mse: 5.9610\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0039 - mse: 3.3599 - val_loss: 0.0068 - val_mse: 5.9395\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0038 - mse: 3.3414 - val_loss: 0.0068 - val_mse: 5.9176\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.0038 - mse: 3.3230 - val_loss: 0.0068 - val_mse: 5.8955\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0038 - mse: 3.3046 - val_loss: 0.0067 - val_mse: 5.8739\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.0038 - mse: 3.2865 - val_loss: 0.0067 - val_mse: 5.8523\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0038 - mse: 3.2686 - val_loss: 0.0067 - val_mse: 5.8309\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.0037 - mse: 3.2506 - val_loss: 0.0067 - val_mse: 5.8101\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.0037 - mse: 3.2326 - val_loss: 0.0066 - val_mse: 5.7893\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0037 - mse: 3.2146 - val_loss: 0.0066 - val_mse: 5.7685\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.0037 - mse: 3.1965 - val_loss: 0.0066 - val_mse: 5.7476\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0036 - mse: 3.1783 - val_loss: 0.0066 - val_mse: 5.7270\n",
      "Epoch 266/1000\n",
      "1s - loss: 0.0036 - mse: 3.1601 - val_loss: 0.0066 - val_mse: 5.7068\n",
      "Epoch 267/1000\n",
      "1s - loss: 0.0036 - mse: 3.1419 - val_loss: 0.0065 - val_mse: 5.6867\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0036 - mse: 3.1238 - val_loss: 0.0065 - val_mse: 5.6660\n",
      "Epoch 269/1000\n",
      "1s - loss: 0.0036 - mse: 3.1057 - val_loss: 0.0065 - val_mse: 5.6447\n",
      "Epoch 270/1000\n",
      "1s - loss: 0.0035 - mse: 3.0876 - val_loss: 0.0065 - val_mse: 5.6227\n",
      "Epoch 271/1000\n",
      "1s - loss: 0.0035 - mse: 3.0695 - val_loss: 0.0064 - val_mse: 5.6007\n",
      "Epoch 272/1000\n",
      "1s - loss: 0.0035 - mse: 3.0514 - val_loss: 0.0064 - val_mse: 5.5791\n",
      "Epoch 273/1000\n",
      "1s - loss: 0.0035 - mse: 3.0333 - val_loss: 0.0064 - val_mse: 5.5573\n",
      "Epoch 274/1000\n",
      "1s - loss: 0.0035 - mse: 3.0150 - val_loss: 0.0064 - val_mse: 5.5356\n",
      "Epoch 275/1000\n",
      "1s - loss: 0.0034 - mse: 2.9965 - val_loss: 0.0063 - val_mse: 5.5134\n",
      "Epoch 276/1000\n",
      "1s - loss: 0.0034 - mse: 2.9781 - val_loss: 0.0063 - val_mse: 5.4914\n",
      "Epoch 277/1000\n",
      "1s - loss: 0.0034 - mse: 2.9597 - val_loss: 0.0063 - val_mse: 5.4698\n",
      "Epoch 278/1000\n",
      "1s - loss: 0.0034 - mse: 2.9411 - val_loss: 0.0063 - val_mse: 5.4484\n",
      "Epoch 279/1000\n",
      "1s - loss: 0.0034 - mse: 2.9225 - val_loss: 0.0062 - val_mse: 5.4263\n",
      "Epoch 280/1000\n",
      "1s - loss: 0.0033 - mse: 2.9038 - val_loss: 0.0062 - val_mse: 5.4035\n",
      "Epoch 281/1000\n",
      "1s - loss: 0.0033 - mse: 2.8849 - val_loss: 0.0062 - val_mse: 5.3807\n",
      "Epoch 282/1000\n",
      "1s - loss: 0.0033 - mse: 2.8660 - val_loss: 0.0062 - val_mse: 5.3580\n",
      "Epoch 283/1000\n",
      "1s - loss: 0.0033 - mse: 2.8469 - val_loss: 0.0061 - val_mse: 5.3352\n",
      "Epoch 284/1000\n",
      "1s - loss: 0.0032 - mse: 2.8278 - val_loss: 0.0061 - val_mse: 5.3126\n",
      "Epoch 285/1000\n",
      "1s - loss: 0.0032 - mse: 2.8086 - val_loss: 0.0061 - val_mse: 5.2897\n",
      "Epoch 286/1000\n",
      "1s - loss: 0.0032 - mse: 2.7892 - val_loss: 0.0060 - val_mse: 5.2668\n",
      "Epoch 287/1000\n",
      "1s - loss: 0.0032 - mse: 2.7698 - val_loss: 0.0060 - val_mse: 5.2437\n",
      "Epoch 288/1000\n",
      "1s - loss: 0.0032 - mse: 2.7503 - val_loss: 0.0060 - val_mse: 5.2204\n",
      "Epoch 289/1000\n",
      "1s - loss: 0.0031 - mse: 2.7307 - val_loss: 0.0060 - val_mse: 5.1966\n",
      "Epoch 290/1000\n",
      "1s - loss: 0.0031 - mse: 2.7110 - val_loss: 0.0059 - val_mse: 5.1726\n",
      "Epoch 291/1000\n",
      "1s - loss: 0.0031 - mse: 2.6910 - val_loss: 0.0059 - val_mse: 5.1485\n",
      "Epoch 292/1000\n",
      "1s - loss: 0.0031 - mse: 2.6711 - val_loss: 0.0059 - val_mse: 5.1243\n",
      "Epoch 293/1000\n",
      "1s - loss: 0.0030 - mse: 2.6510 - val_loss: 0.0059 - val_mse: 5.0997\n",
      "Epoch 294/1000\n",
      "1s - loss: 0.0030 - mse: 2.6308 - val_loss: 0.0058 - val_mse: 5.0756\n",
      "Epoch 295/1000\n",
      "1s - loss: 0.0030 - mse: 2.6104 - val_loss: 0.0058 - val_mse: 5.0519\n",
      "Epoch 296/1000\n",
      "1s - loss: 0.0030 - mse: 2.5899 - val_loss: 0.0058 - val_mse: 5.0279\n",
      "Epoch 297/1000\n",
      "1s - loss: 0.0030 - mse: 2.5692 - val_loss: 0.0057 - val_mse: 5.0035\n",
      "Epoch 298/1000\n",
      "1s - loss: 0.0029 - mse: 2.5483 - val_loss: 0.0057 - val_mse: 4.9780\n",
      "Epoch 299/1000\n",
      "1s - loss: 0.0029 - mse: 2.5272 - val_loss: 0.0057 - val_mse: 4.9522\n",
      "Epoch 300/1000\n",
      "1s - loss: 0.0029 - mse: 2.5060 - val_loss: 0.0057 - val_mse: 4.9263\n",
      "Epoch 301/1000\n",
      "1s - loss: 0.0029 - mse: 2.4845 - val_loss: 0.0056 - val_mse: 4.9007\n",
      "Epoch 302/1000\n",
      "1s - loss: 0.0028 - mse: 2.4629 - val_loss: 0.0056 - val_mse: 4.8750\n",
      "Epoch 303/1000\n",
      "1s - loss: 0.0028 - mse: 2.4411 - val_loss: 0.0056 - val_mse: 4.8493\n",
      "Epoch 304/1000\n",
      "1s - loss: 0.0028 - mse: 2.4194 - val_loss: 0.0055 - val_mse: 4.8240\n",
      "Epoch 305/1000\n",
      "1s - loss: 0.0028 - mse: 2.3976 - val_loss: 0.0055 - val_mse: 4.7991\n",
      "Epoch 306/1000\n",
      "1s - loss: 0.0027 - mse: 2.3756 - val_loss: 0.0055 - val_mse: 4.7742\n",
      "Epoch 307/1000\n",
      "1s - loss: 0.0027 - mse: 2.3535 - val_loss: 0.0055 - val_mse: 4.7490\n",
      "Epoch 308/1000\n",
      "1s - loss: 0.0027 - mse: 2.3312 - val_loss: 0.0054 - val_mse: 4.7235\n",
      "Epoch 309/1000\n",
      "1s - loss: 0.0027 - mse: 2.3090 - val_loss: 0.0054 - val_mse: 4.6977\n",
      "Epoch 310/1000\n",
      "1s - loss: 0.0026 - mse: 2.2866 - val_loss: 0.0054 - val_mse: 4.6717\n",
      "Epoch 311/1000\n",
      "1s - loss: 0.0026 - mse: 2.2640 - val_loss: 0.0053 - val_mse: 4.6457\n",
      "Epoch 312/1000\n",
      "1s - loss: 0.0026 - mse: 2.2414 - val_loss: 0.0053 - val_mse: 4.6199\n",
      "Epoch 313/1000\n",
      "1s - loss: 0.0025 - mse: 2.2187 - val_loss: 0.0053 - val_mse: 4.5945\n",
      "Epoch 314/1000\n",
      "1s - loss: 0.0025 - mse: 2.1960 - val_loss: 0.0052 - val_mse: 4.5690\n",
      "Epoch 315/1000\n",
      "1s - loss: 0.0025 - mse: 2.1733 - val_loss: 0.0052 - val_mse: 4.5432\n",
      "Epoch 316/1000\n",
      "1s - loss: 0.0025 - mse: 2.1503 - val_loss: 0.0052 - val_mse: 4.5179\n",
      "Epoch 317/1000\n",
      "1s - loss: 0.0024 - mse: 2.1274 - val_loss: 0.0052 - val_mse: 4.4941\n",
      "Epoch 318/1000\n",
      "1s - loss: 0.0024 - mse: 2.1046 - val_loss: 0.0051 - val_mse: 4.4713\n",
      "Epoch 319/1000\n",
      "1s - loss: 0.0024 - mse: 2.0817 - val_loss: 0.0051 - val_mse: 4.4480\n",
      "Epoch 320/1000\n",
      "1s - loss: 0.0024 - mse: 2.0587 - val_loss: 0.0051 - val_mse: 4.4232\n",
      "Epoch 321/1000\n",
      "1s - loss: 0.0023 - mse: 2.0356 - val_loss: 0.0050 - val_mse: 4.3969\n",
      "Epoch 322/1000\n",
      "1s - loss: 0.0023 - mse: 2.0123 - val_loss: 0.0050 - val_mse: 4.3702\n",
      "Epoch 323/1000\n",
      "1s - loss: 0.0023 - mse: 1.9889 - val_loss: 0.0050 - val_mse: 4.3443\n",
      "Epoch 324/1000\n",
      "1s - loss: 0.0023 - mse: 1.9654 - val_loss: 0.0050 - val_mse: 4.3201\n",
      "Epoch 325/1000\n",
      "1s - loss: 0.0022 - mse: 1.9422 - val_loss: 0.0049 - val_mse: 4.2975\n",
      "Epoch 326/1000\n",
      "1s - loss: 0.0022 - mse: 1.9194 - val_loss: 0.0049 - val_mse: 4.2754\n",
      "Epoch 327/1000\n",
      "1s - loss: 0.0022 - mse: 1.8968 - val_loss: 0.0049 - val_mse: 4.2533\n",
      "Epoch 328/1000\n",
      "1s - loss: 0.0022 - mse: 1.8741 - val_loss: 0.0049 - val_mse: 4.2307\n",
      "Epoch 329/1000\n",
      "1s - loss: 0.0021 - mse: 1.8514 - val_loss: 0.0048 - val_mse: 4.2073\n",
      "Epoch 330/1000\n",
      "1s - loss: 0.0021 - mse: 1.8288 - val_loss: 0.0048 - val_mse: 4.1833\n",
      "Epoch 331/1000\n",
      "1s - loss: 0.0021 - mse: 1.8063 - val_loss: 0.0048 - val_mse: 4.1596\n",
      "Epoch 332/1000\n",
      "1s - loss: 0.0020 - mse: 1.7838 - val_loss: 0.0048 - val_mse: 4.1370\n",
      "Epoch 333/1000\n",
      "1s - loss: 0.0020 - mse: 1.7614 - val_loss: 0.0047 - val_mse: 4.1152\n",
      "Epoch 334/1000\n",
      "1s - loss: 0.0020 - mse: 1.7390 - val_loss: 0.0047 - val_mse: 4.0934\n",
      "Epoch 335/1000\n",
      "1s - loss: 0.0020 - mse: 1.7170 - val_loss: 0.0047 - val_mse: 4.0713\n",
      "Epoch 336/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0019 - mse: 1.6951 - val_loss: 0.0046 - val_mse: 4.0489\n",
      "Epoch 337/1000\n",
      "1s - loss: 0.0019 - mse: 1.6733 - val_loss: 0.0046 - val_mse: 4.0264\n",
      "Epoch 338/1000\n",
      "1s - loss: 0.0019 - mse: 1.6517 - val_loss: 0.0046 - val_mse: 4.0042\n",
      "Epoch 339/1000\n",
      "1s - loss: 0.0019 - mse: 1.6302 - val_loss: 0.0046 - val_mse: 3.9830\n",
      "Epoch 340/1000\n",
      "1s - loss: 0.0018 - mse: 1.6091 - val_loss: 0.0045 - val_mse: 3.9622\n",
      "Epoch 341/1000\n",
      "1s - loss: 0.0018 - mse: 1.5882 - val_loss: 0.0045 - val_mse: 3.9414\n",
      "Epoch 342/1000\n",
      "1s - loss: 0.0018 - mse: 1.5675 - val_loss: 0.0045 - val_mse: 3.9201\n",
      "Epoch 343/1000\n",
      "1s - loss: 0.0018 - mse: 1.5471 - val_loss: 0.0045 - val_mse: 3.8987\n",
      "Epoch 344/1000\n",
      "1s - loss: 0.0018 - mse: 1.5271 - val_loss: 0.0045 - val_mse: 3.8776\n",
      "Epoch 345/1000\n",
      "1s - loss: 0.0017 - mse: 1.5073 - val_loss: 0.0044 - val_mse: 3.8573\n",
      "Epoch 346/1000\n",
      "1s - loss: 0.0017 - mse: 1.4879 - val_loss: 0.0044 - val_mse: 3.8380\n",
      "Epoch 347/1000\n",
      "1s - loss: 0.0017 - mse: 1.4687 - val_loss: 0.0044 - val_mse: 3.8192\n",
      "Epoch 348/1000\n",
      "1s - loss: 0.0017 - mse: 1.4498 - val_loss: 0.0044 - val_mse: 3.8004\n",
      "Epoch 349/1000\n",
      "1s - loss: 0.0016 - mse: 1.4311 - val_loss: 0.0043 - val_mse: 3.7817\n",
      "Epoch 350/1000\n",
      "1s - loss: 0.0016 - mse: 1.4126 - val_loss: 0.0043 - val_mse: 3.7632\n",
      "Epoch 351/1000\n",
      "1s - loss: 0.0016 - mse: 1.3944 - val_loss: 0.0043 - val_mse: 3.7453\n",
      "Epoch 352/1000\n",
      "1s - loss: 0.0016 - mse: 1.3765 - val_loss: 0.0043 - val_mse: 3.7279\n",
      "Epoch 353/1000\n",
      "1s - loss: 0.0016 - mse: 1.3587 - val_loss: 0.0043 - val_mse: 3.7104\n",
      "Epoch 354/1000\n",
      "1s - loss: 0.0015 - mse: 1.3413 - val_loss: 0.0042 - val_mse: 3.6924\n",
      "Epoch 355/1000\n",
      "1s - loss: 0.0015 - mse: 1.3242 - val_loss: 0.0042 - val_mse: 3.6743\n",
      "Epoch 356/1000\n",
      "1s - loss: 0.0015 - mse: 1.3071 - val_loss: 0.0042 - val_mse: 3.6564\n",
      "Epoch 357/1000\n",
      "1s - loss: 0.0015 - mse: 1.2903 - val_loss: 0.0042 - val_mse: 3.6386\n",
      "Epoch 358/1000\n",
      "1s - loss: 0.0015 - mse: 1.2736 - val_loss: 0.0042 - val_mse: 3.6216\n",
      "Epoch 359/1000\n",
      "1s - loss: 0.0014 - mse: 1.2571 - val_loss: 0.0041 - val_mse: 3.6047\n",
      "Epoch 360/1000\n",
      "1s - loss: 0.0014 - mse: 1.2408 - val_loss: 0.0041 - val_mse: 3.5880\n",
      "Epoch 361/1000\n",
      "1s - loss: 0.0014 - mse: 1.2246 - val_loss: 0.0041 - val_mse: 3.5715\n",
      "Epoch 362/1000\n",
      "\n",
      "Epoch 00361: reducing learning rate to 0.00699999984354.\n",
      "1s - loss: 0.0014 - mse: 1.2087 - val_loss: 0.0041 - val_mse: 3.5550\n",
      "Epoch 363/1000\n",
      "1s - loss: 0.0014 - mse: 1.1930 - val_loss: 0.0041 - val_mse: 3.5428\n",
      "Epoch 364/1000\n",
      "1s - loss: 0.0014 - mse: 1.1822 - val_loss: 0.0041 - val_mse: 3.5300\n",
      "Epoch 365/1000\n",
      "1s - loss: 0.0013 - mse: 1.1714 - val_loss: 0.0040 - val_mse: 3.5172\n",
      "Epoch 366/1000\n",
      "1s - loss: 0.0013 - mse: 1.1607 - val_loss: 0.0040 - val_mse: 3.5052\n",
      "Epoch 367/1000\n",
      "1s - loss: 0.0013 - mse: 1.1502 - val_loss: 0.0040 - val_mse: 3.4939\n",
      "Epoch 368/1000\n",
      "1s - loss: 0.0013 - mse: 1.1399 - val_loss: 0.0040 - val_mse: 3.4825\n",
      "Epoch 369/1000\n",
      "1s - loss: 0.0013 - mse: 1.1296 - val_loss: 0.0040 - val_mse: 3.4710\n",
      "Epoch 370/1000\n",
      "1s - loss: 0.0013 - mse: 1.1194 - val_loss: 0.0040 - val_mse: 3.4595\n",
      "Epoch 371/1000\n",
      "1s - loss: 0.0013 - mse: 1.1094 - val_loss: 0.0040 - val_mse: 3.4487\n",
      "Epoch 372/1000\n",
      "1s - loss: 0.0013 - mse: 1.0996 - val_loss: 0.0039 - val_mse: 3.4382\n",
      "Epoch 373/1000\n",
      "1s - loss: 0.0013 - mse: 1.0900 - val_loss: 0.0039 - val_mse: 3.4271\n",
      "Epoch 374/1000\n",
      "1s - loss: 0.0012 - mse: 1.0804 - val_loss: 0.0039 - val_mse: 3.4158\n",
      "Epoch 375/1000\n",
      "1s - loss: 0.0012 - mse: 1.0708 - val_loss: 0.0039 - val_mse: 3.4046\n",
      "Epoch 376/1000\n",
      "\n",
      "Epoch 00375: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0012 - mse: 1.0614 - val_loss: 0.0039 - val_mse: 3.3942\n",
      "Epoch 377/1000\n",
      "1s - loss: 0.0012 - mse: 1.0520 - val_loss: 0.0039 - val_mse: 3.3876\n",
      "Epoch 378/1000\n",
      "1s - loss: 0.0012 - mse: 1.0456 - val_loss: 0.0039 - val_mse: 3.3813\n",
      "Epoch 379/1000\n",
      "1s - loss: 0.0012 - mse: 1.0393 - val_loss: 0.0039 - val_mse: 3.3750\n",
      "Epoch 380/1000\n",
      "1s - loss: 0.0012 - mse: 1.0329 - val_loss: 0.0039 - val_mse: 3.3687\n",
      "Epoch 381/1000\n",
      "1s - loss: 0.0012 - mse: 1.0266 - val_loss: 0.0039 - val_mse: 3.3622\n",
      "Epoch 382/1000\n",
      "1s - loss: 0.0012 - mse: 1.0203 - val_loss: 0.0039 - val_mse: 3.3556\n",
      "Epoch 383/1000\n",
      "1s - loss: 0.0012 - mse: 1.0140 - val_loss: 0.0038 - val_mse: 3.3491\n",
      "Epoch 384/1000\n",
      "1s - loss: 0.0012 - mse: 1.0079 - val_loss: 0.0038 - val_mse: 3.3425\n",
      "Epoch 385/1000\n",
      "1s - loss: 0.0012 - mse: 1.0017 - val_loss: 0.0038 - val_mse: 3.3357\n",
      "Epoch 386/1000\n",
      "1s - loss: 0.0011 - mse: 0.9957 - val_loss: 0.0038 - val_mse: 3.3288\n",
      "Epoch 387/1000\n",
      "1s - loss: 0.0011 - mse: 0.9897 - val_loss: 0.0038 - val_mse: 3.3219\n",
      "Epoch 388/1000\n",
      "1s - loss: 0.0011 - mse: 0.9837 - val_loss: 0.0038 - val_mse: 3.3152\n",
      "Epoch 389/1000\n",
      "1s - loss: 0.0011 - mse: 0.9777 - val_loss: 0.0038 - val_mse: 3.3085\n",
      "Epoch 390/1000\n",
      "\n",
      "Epoch 00389: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0011 - mse: 0.9719 - val_loss: 0.0038 - val_mse: 3.3019\n",
      "Epoch 391/1000\n",
      "1s - loss: 0.0011 - mse: 0.9661 - val_loss: 0.0038 - val_mse: 3.2974\n",
      "Epoch 392/1000\n",
      "1s - loss: 0.0011 - mse: 0.9620 - val_loss: 0.0038 - val_mse: 3.2931\n",
      "Epoch 393/1000\n",
      "1s - loss: 0.0011 - mse: 0.9580 - val_loss: 0.0038 - val_mse: 3.2889\n",
      "Epoch 394/1000\n",
      "1s - loss: 0.0011 - mse: 0.9540 - val_loss: 0.0038 - val_mse: 3.2847\n",
      "Epoch 395/1000\n",
      "1s - loss: 0.0011 - mse: 0.9501 - val_loss: 0.0038 - val_mse: 3.2804\n",
      "Epoch 396/1000\n",
      "1s - loss: 0.0011 - mse: 0.9462 - val_loss: 0.0038 - val_mse: 3.2762\n",
      "Epoch 397/1000\n",
      "1s - loss: 0.0011 - mse: 0.9422 - val_loss: 0.0038 - val_mse: 3.2722\n",
      "Epoch 398/1000\n",
      "1s - loss: 0.0011 - mse: 0.9384 - val_loss: 0.0038 - val_mse: 3.2684\n",
      "Epoch 399/1000\n",
      "1s - loss: 0.0011 - mse: 0.9345 - val_loss: 0.0037 - val_mse: 3.2647\n",
      "Epoch 400/1000\n",
      "1s - loss: 0.0011 - mse: 0.9306 - val_loss: 0.0037 - val_mse: 3.2609\n",
      "Epoch 401/1000\n",
      "1s - loss: 0.0011 - mse: 0.9268 - val_loss: 0.0037 - val_mse: 3.2569\n",
      "Epoch 402/1000\n",
      "1s - loss: 0.0011 - mse: 0.9230 - val_loss: 0.0037 - val_mse: 3.2529\n",
      "Epoch 403/1000\n",
      "1s - loss: 0.0011 - mse: 0.9193 - val_loss: 0.0037 - val_mse: 3.2487\n",
      "Epoch 404/1000\n",
      "\n",
      "Epoch 00403: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0011 - mse: 0.9156 - val_loss: 0.0037 - val_mse: 3.2446\n",
      "Epoch 405/1000\n",
      "1s - loss: 0.0010 - mse: 0.9118 - val_loss: 0.0037 - val_mse: 3.2416\n",
      "Epoch 406/1000\n",
      "1s - loss: 0.0010 - mse: 0.9093 - val_loss: 0.0037 - val_mse: 3.2386\n",
      "Epoch 407/1000\n",
      "1s - loss: 0.0010 - mse: 0.9067 - val_loss: 0.0037 - val_mse: 3.2358\n",
      "Epoch 408/1000\n",
      "1s - loss: 0.0010 - mse: 0.9043 - val_loss: 0.0037 - val_mse: 3.2330\n",
      "Epoch 409/1000\n",
      "1s - loss: 0.0010 - mse: 0.9018 - val_loss: 0.0037 - val_mse: 3.2304\n",
      "Epoch 410/1000\n",
      "1s - loss: 0.0010 - mse: 0.8993 - val_loss: 0.0037 - val_mse: 3.2280\n",
      "Epoch 411/1000\n",
      "1s - loss: 0.0010 - mse: 0.8969 - val_loss: 0.0037 - val_mse: 3.2256\n",
      "Epoch 412/1000\n",
      "1s - loss: 0.0010 - mse: 0.8945 - val_loss: 0.0037 - val_mse: 3.2233\n",
      "Epoch 413/1000\n",
      "1s - loss: 0.0010 - mse: 0.8920 - val_loss: 0.0037 - val_mse: 3.2207\n",
      "Epoch 414/1000\n",
      "1s - loss: 0.0010 - mse: 0.8896 - val_loss: 0.0037 - val_mse: 3.2179\n",
      "Epoch 415/1000\n",
      "1s - loss: 0.0010 - mse: 0.8873 - val_loss: 0.0037 - val_mse: 3.2150\n",
      "Epoch 416/1000\n",
      "1s - loss: 0.0010 - mse: 0.8849 - val_loss: 0.0037 - val_mse: 3.2120\n",
      "Epoch 417/1000\n",
      "1s - loss: 0.0010 - mse: 0.8825 - val_loss: 0.0037 - val_mse: 3.2092\n",
      "Epoch 418/1000\n",
      "\n",
      "Epoch 00417: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0010 - mse: 0.8802 - val_loss: 0.0037 - val_mse: 3.2066\n",
      "Epoch 419/1000\n",
      "1s - loss: 0.0010 - mse: 0.8779 - val_loss: 0.0037 - val_mse: 3.2049\n",
      "Epoch 420/1000\n",
      "1s - loss: 0.0010 - mse: 0.8763 - val_loss: 0.0037 - val_mse: 3.2032\n",
      "Epoch 421/1000\n",
      "1s - loss: 0.0010 - mse: 0.8747 - val_loss: 0.0037 - val_mse: 3.2015\n",
      "Epoch 422/1000\n",
      "1s - loss: 0.0010 - mse: 0.8731 - val_loss: 0.0037 - val_mse: 3.1998\n",
      "Epoch 423/1000\n",
      "1s - loss: 0.0010 - mse: 0.8715 - val_loss: 0.0037 - val_mse: 3.1981\n",
      "Epoch 424/1000\n",
      "1s - loss: 9.9891e-04 - mse: 0.8699 - val_loss: 0.0037 - val_mse: 3.1964\n",
      "Epoch 425/1000\n",
      "1s - loss: 9.9710e-04 - mse: 0.8683 - val_loss: 0.0037 - val_mse: 3.1946\n",
      "Epoch 426/1000\n",
      "1s - loss: 9.9529e-04 - mse: 0.8667 - val_loss: 0.0037 - val_mse: 3.1928\n",
      "Epoch 427/1000\n",
      "1s - loss: 9.9348e-04 - mse: 0.8652 - val_loss: 0.0037 - val_mse: 3.1910\n",
      "Epoch 428/1000\n",
      "1s - loss: 9.9168e-04 - mse: 0.8636 - val_loss: 0.0037 - val_mse: 3.1893\n",
      "Epoch 429/1000\n",
      "1s - loss: 9.8989e-04 - mse: 0.8620 - val_loss: 0.0037 - val_mse: 3.1877\n",
      "[{'loss': 0.0033503014128655195, 'width0': 135, 'width1': 75, 'val_mse': 6.1875839233398438, 'mse': 2.9175732135772705, 'val_loss': 0.007105313241481781, 'dropout1': 0, 'dropout0': 0}, {'loss': 0.0018494370160624385, 'width0': 135, 'width1': 100, 'val_mse': 3.698413610458374, 'mse': 1.6105610132217407, 'val_loss': 0.0042469552718102932, 'dropout1': 0, 'dropout0': 0}, {'loss': 0.00098988576792180538, 'width0': 135, 'width1': 125, 'val_mse': 3.1876635551452637, 'mse': 0.8620307445526123, 'val_loss': 0.0036604509223252535, 'dropout1': 0, 'dropout0': 0}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 1.0436 - mse: 908.8286 - val_loss: 1.9815 - val_mse: 1725.5627\n",
      "Epoch 2/1000\n",
      "1s - loss: 2.0687 - mse: 1801.5411 - val_loss: 0.5695 - val_mse: 495.9612\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.5700 - mse: 496.3499 - val_loss: 0.6486 - val_mse: 564.8232\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.6345 - mse: 552.5880 - val_loss: 0.5247 - val_mse: 456.9244\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.5093 - mse: 443.5373 - val_loss: 0.2861 - val_mse: 249.1307\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.2793 - mse: 243.1973 - val_loss: 0.2019 - val_mse: 175.8373\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.1946 - mse: 169.5047 - val_loss: 0.2279 - val_mse: 198.4827\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.2131 - mse: 185.5636 - val_loss: 0.1711 - val_mse: 149.0104\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.1611 - mse: 140.3083 - val_loss: 0.1295 - val_mse: 112.7766\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.1213 - mse: 105.6003 - val_loss: 0.1037 - val_mse: 90.3174\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.0956 - mse: 83.2729 - val_loss: 0.1180 - val_mse: 102.7740\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.1112 - mse: 96.8807 - val_loss: 0.1364 - val_mse: 118.8166\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.1301 - mse: 113.3171 - val_loss: 0.1236 - val_mse: 107.6166\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.1168 - mse: 101.6825 - val_loss: 0.1031 - val_mse: 89.7495\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0956 - mse: 83.2824 - val_loss: 0.0882 - val_mse: 76.7907\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0808 - mse: 70.3285 - val_loss: 0.0761 - val_mse: 66.2292\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0695 - mse: 60.5128 - val_loss: 0.0666 - val_mse: 58.0176\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0615 - mse: 53.5166 - val_loss: 0.0628 - val_mse: 54.6526\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0589 - mse: 51.3269 - val_loss: 0.0637 - val_mse: 55.4800\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0607 - mse: 52.8630 - val_loss: 0.0631 - val_mse: 54.9202\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0604 - mse: 52.5804 - val_loss: 0.0567 - val_mse: 49.3401\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0541 - mse: 47.1440 - val_loss: 0.0489 - val_mse: 42.5914\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0465 - mse: 40.4728 - val_loss: 0.0458 - val_mse: 39.8452\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0432 - mse: 37.6384 - val_loss: 0.0469 - val_mse: 40.8601\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0442 - mse: 38.4805 - val_loss: 0.0482 - val_mse: 41.9577\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0454 - mse: 39.5259 - val_loss: 0.0469 - val_mse: 40.8038\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0442 - mse: 38.5264 - val_loss: 0.0436 - val_mse: 37.9715\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0413 - mse: 35.9571 - val_loss: 0.0404 - val_mse: 35.1813\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0383 - mse: 33.3792 - val_loss: 0.0378 - val_mse: 32.9422\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0359 - mse: 31.2499 - val_loss: 0.0353 - val_mse: 30.7132\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0334 - mse: 29.0848 - val_loss: 0.0332 - val_mse: 28.9383\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0314 - mse: 27.3373 - val_loss: 0.0328 - val_mse: 28.5310\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0309 - mse: 26.9089 - val_loss: 0.0332 - val_mse: 28.9069\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0313 - mse: 27.2706 - val_loss: 0.0328 - val_mse: 28.5251\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0309 - mse: 26.8926 - val_loss: 0.0309 - val_mse: 26.9117\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0289 - mse: 25.1643 - val_loss: 0.0287 - val_mse: 25.0136\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0263 - mse: 22.9096 - val_loss: 0.0274 - val_mse: 23.8547\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0244 - mse: 21.2436 - val_loss: 0.0269 - val_mse: 23.3986\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0234 - mse: 20.3856 - val_loss: 0.0263 - val_mse: 22.8892\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0227 - mse: 19.7836 - val_loss: 0.0253 - val_mse: 22.0012\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0219 - mse: 19.0905 - val_loss: 0.0242 - val_mse: 21.0905\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0212 - mse: 18.4883 - val_loss: 0.0235 - val_mse: 20.4320\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0208 - mse: 18.1045 - val_loss: 0.0228 - val_mse: 19.8118\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0203 - mse: 17.6851 - val_loss: 0.0218 - val_mse: 18.9901\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0195 - mse: 17.0062 - val_loss: 0.0208 - val_mse: 18.1106\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0186 - mse: 16.1989 - val_loss: 0.0201 - val_mse: 17.4617\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0178 - mse: 15.5208 - val_loss: 0.0196 - val_mse: 17.0404\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0172 - mse: 14.9892 - val_loss: 0.0191 - val_mse: 16.6102\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0166 - mse: 14.4517 - val_loss: 0.0185 - val_mse: 16.0997\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0160 - mse: 13.9118 - val_loss: 0.0180 - val_mse: 15.6415\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0155 - mse: 13.5052 - val_loss: 0.0175 - val_mse: 15.2699\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0152 - mse: 13.2230 - val_loss: 0.0171 - val_mse: 14.8682\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0148 - mse: 12.9062 - val_loss: 0.0165 - val_mse: 14.3797\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0143 - mse: 12.4842 - val_loss: 0.0160 - val_mse: 13.9186\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0139 - mse: 12.0753 - val_loss: 0.0156 - val_mse: 13.6069\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0136 - mse: 11.8163 - val_loss: 0.0154 - val_mse: 13.4019\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0134 - mse: 11.6782 - val_loss: 0.0151 - val_mse: 13.1683\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0132 - mse: 11.5251 - val_loss: 0.0148 - val_mse: 12.8607\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0130 - mse: 11.2952 - val_loss: 0.0144 - val_mse: 12.5282\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0127 - mse: 11.0195 - val_loss: 0.0140 - val_mse: 12.2049\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0123 - mse: 10.7254 - val_loss: 0.0137 - val_mse: 11.9137\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0120 - mse: 10.4328 - val_loss: 0.0134 - val_mse: 11.7055\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0117 - mse: 10.1926 - val_loss: 0.0133 - val_mse: 11.6047\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0115 - mse: 10.0408 - val_loss: 0.0132 - val_mse: 11.5315\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0114 - mse: 9.9263 - val_loss: 0.0131 - val_mse: 11.3723\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0112 - mse: 9.7592 - val_loss: 0.0128 - val_mse: 11.1111\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0109 - mse: 9.5210 - val_loss: 0.0124 - val_mse: 10.8345\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0106 - mse: 9.2735 - val_loss: 0.0122 - val_mse: 10.6203\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0104 - mse: 9.0702 - val_loss: 0.0120 - val_mse: 10.4670\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0102 - mse: 8.9056 - val_loss: 0.0119 - val_mse: 10.3406\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0101 - mse: 8.7529 - val_loss: 0.0117 - val_mse: 10.2241\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0099 - mse: 8.6034 - val_loss: 0.0116 - val_mse: 10.1059\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0097 - mse: 8.4518 - val_loss: 0.0114 - val_mse: 9.9664\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0095 - mse: 8.2870 - val_loss: 0.0113 - val_mse: 9.7985\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0093 - mse: 8.1117 - val_loss: 0.0110 - val_mse: 9.6197\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0091 - mse: 7.9482 - val_loss: 0.0108 - val_mse: 9.4422\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0090 - mse: 7.8063 - val_loss: 0.0106 - val_mse: 9.2596\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0088 - mse: 7.6718 - val_loss: 0.0104 - val_mse: 9.0676\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0086 - mse: 7.5313 - val_loss: 0.0102 - val_mse: 8.8818\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0085 - mse: 7.3905 - val_loss: 0.0100 - val_mse: 8.7203\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0083 - mse: 7.2607 - val_loss: 0.0099 - val_mse: 8.5833\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0082 - mse: 7.1386 - val_loss: 0.0097 - val_mse: 8.4655\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0081 - mse: 7.0181 - val_loss: 0.0096 - val_mse: 8.3669\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0079 - mse: 6.9010 - val_loss: 0.0095 - val_mse: 8.2875\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0078 - mse: 6.7927 - val_loss: 0.0094 - val_mse: 8.2189\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0077 - mse: 6.6933 - val_loss: 0.0094 - val_mse: 8.1459\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0076 - mse: 6.5958 - val_loss: 0.0093 - val_mse: 8.0644\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0075 - mse: 6.5009 - val_loss: 0.0092 - val_mse: 7.9746\n",
      "Epoch 89/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0074 - mse: 6.4106 - val_loss: 0.0090 - val_mse: 7.8775\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0073 - mse: 6.3233 - val_loss: 0.0089 - val_mse: 7.7785\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0072 - mse: 6.2383 - val_loss: 0.0088 - val_mse: 7.6886\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0071 - mse: 6.1582 - val_loss: 0.0087 - val_mse: 7.6146\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0070 - mse: 6.0832 - val_loss: 0.0087 - val_mse: 7.5516\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0069 - mse: 6.0099 - val_loss: 0.0086 - val_mse: 7.4951\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0068 - mse: 5.9361 - val_loss: 0.0085 - val_mse: 7.4423\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0067 - mse: 5.8641 - val_loss: 0.0085 - val_mse: 7.3908\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0067 - mse: 5.7967 - val_loss: 0.0084 - val_mse: 7.3360\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0066 - mse: 5.7334 - val_loss: 0.0084 - val_mse: 7.2746\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0065 - mse: 5.6722 - val_loss: 0.0083 - val_mse: 7.2080\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0064 - mse: 5.6123 - val_loss: 0.0082 - val_mse: 7.1398\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0064 - mse: 5.5533 - val_loss: 0.0081 - val_mse: 7.0734\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0063 - mse: 5.4947 - val_loss: 0.0081 - val_mse: 7.0126\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0062 - mse: 5.4369 - val_loss: 0.0080 - val_mse: 6.9603\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0062 - mse: 5.3814 - val_loss: 0.0079 - val_mse: 6.9155\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0061 - mse: 5.3278 - val_loss: 0.0079 - val_mse: 6.8750\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0061 - mse: 5.2752 - val_loss: 0.0078 - val_mse: 6.8347\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0060 - mse: 5.2228 - val_loss: 0.0078 - val_mse: 6.7926\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0059 - mse: 5.1710 - val_loss: 0.0077 - val_mse: 6.7473\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0059 - mse: 5.1201 - val_loss: 0.0077 - val_mse: 6.6988\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0058 - mse: 5.0702 - val_loss: 0.0076 - val_mse: 6.6481\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0058 - mse: 5.0214 - val_loss: 0.0076 - val_mse: 6.5975\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0057 - mse: 4.9739 - val_loss: 0.0075 - val_mse: 6.5485\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0057 - mse: 4.9271 - val_loss: 0.0075 - val_mse: 6.5024\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0056 - mse: 4.8806 - val_loss: 0.0074 - val_mse: 6.4602\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0056 - mse: 4.8347 - val_loss: 0.0074 - val_mse: 6.4214\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0055 - mse: 4.7901 - val_loss: 0.0073 - val_mse: 6.3838\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0055 - mse: 4.7463 - val_loss: 0.0073 - val_mse: 6.3449\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0054 - mse: 4.7030 - val_loss: 0.0072 - val_mse: 6.3038\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0054 - mse: 4.6600 - val_loss: 0.0072 - val_mse: 6.2608\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0053 - mse: 4.6173 - val_loss: 0.0071 - val_mse: 6.2173\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0053 - mse: 4.5750 - val_loss: 0.0071 - val_mse: 6.1752\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0052 - mse: 4.5333 - val_loss: 0.0070 - val_mse: 6.1356\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0052 - mse: 4.4921 - val_loss: 0.0070 - val_mse: 6.0984\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0051 - mse: 4.4514 - val_loss: 0.0070 - val_mse: 6.0623\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0051 - mse: 4.4109 - val_loss: 0.0069 - val_mse: 6.0259\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0050 - mse: 4.3708 - val_loss: 0.0069 - val_mse: 5.9884\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0050 - mse: 4.3310 - val_loss: 0.0068 - val_mse: 5.9495\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0049 - mse: 4.2915 - val_loss: 0.0068 - val_mse: 5.9093\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0049 - mse: 4.2523 - val_loss: 0.0067 - val_mse: 5.8688\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0048 - mse: 4.2136 - val_loss: 0.0067 - val_mse: 5.8292\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0048 - mse: 4.1752 - val_loss: 0.0066 - val_mse: 5.7910\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0048 - mse: 4.1370 - val_loss: 0.0066 - val_mse: 5.7544\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0047 - mse: 4.0990 - val_loss: 0.0066 - val_mse: 5.7194\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0047 - mse: 4.0612 - val_loss: 0.0065 - val_mse: 5.6860\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0046 - mse: 4.0237 - val_loss: 0.0065 - val_mse: 5.6532\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0046 - mse: 3.9863 - val_loss: 0.0065 - val_mse: 5.6207\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0045 - mse: 3.9488 - val_loss: 0.0064 - val_mse: 5.5879\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0045 - mse: 3.9116 - val_loss: 0.0064 - val_mse: 5.5545\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0044 - mse: 3.8743 - val_loss: 0.0063 - val_mse: 5.5208\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0044 - mse: 3.8373 - val_loss: 0.0063 - val_mse: 5.4867\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0044 - mse: 3.8003 - val_loss: 0.0063 - val_mse: 5.4525\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0043 - mse: 3.7633 - val_loss: 0.0062 - val_mse: 5.4187\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0043 - mse: 3.7262 - val_loss: 0.0062 - val_mse: 5.3852\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0042 - mse: 3.6889 - val_loss: 0.0061 - val_mse: 5.3520\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0042 - mse: 3.6514 - val_loss: 0.0061 - val_mse: 5.3187\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0041 - mse: 3.6137 - val_loss: 0.0061 - val_mse: 5.2849\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0041 - mse: 3.5758 - val_loss: 0.0060 - val_mse: 5.2507\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0041 - mse: 3.5376 - val_loss: 0.0060 - val_mse: 5.2162\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0040 - mse: 3.4995 - val_loss: 0.0060 - val_mse: 5.1821\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0040 - mse: 3.4614 - val_loss: 0.0059 - val_mse: 5.1481\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0039 - mse: 3.4231 - val_loss: 0.0059 - val_mse: 5.1142\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0039 - mse: 3.3846 - val_loss: 0.0058 - val_mse: 5.0801\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0038 - mse: 3.3459 - val_loss: 0.0058 - val_mse: 5.0458\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0038 - mse: 3.3069 - val_loss: 0.0058 - val_mse: 5.0115\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0038 - mse: 3.2677 - val_loss: 0.0057 - val_mse: 4.9774\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0037 - mse: 3.2282 - val_loss: 0.0057 - val_mse: 4.9432\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0037 - mse: 3.1884 - val_loss: 0.0056 - val_mse: 4.9091\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0036 - mse: 3.1482 - val_loss: 0.0056 - val_mse: 4.8748\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0036 - mse: 3.1078 - val_loss: 0.0056 - val_mse: 4.8403\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0035 - mse: 3.0670 - val_loss: 0.0055 - val_mse: 4.8058\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0035 - mse: 3.0258 - val_loss: 0.0055 - val_mse: 4.7713\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0034 - mse: 2.9845 - val_loss: 0.0054 - val_mse: 4.7370\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0034 - mse: 2.9434 - val_loss: 0.0054 - val_mse: 4.7024\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0033 - mse: 2.9023 - val_loss: 0.0054 - val_mse: 4.6671\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0033 - mse: 2.8609 - val_loss: 0.0053 - val_mse: 4.6314\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0032 - mse: 2.8194 - val_loss: 0.0053 - val_mse: 4.5959\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0032 - mse: 2.7778 - val_loss: 0.0052 - val_mse: 4.5611\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0031 - mse: 2.7363 - val_loss: 0.0052 - val_mse: 4.5269\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0031 - mse: 2.6948 - val_loss: 0.0052 - val_mse: 4.4934\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0030 - mse: 2.6537 - val_loss: 0.0051 - val_mse: 4.4601\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0030 - mse: 2.6131 - val_loss: 0.0051 - val_mse: 4.4267\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0030 - mse: 2.5728 - val_loss: 0.0050 - val_mse: 4.3931\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0029 - mse: 2.5328 - val_loss: 0.0050 - val_mse: 4.3594\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0029 - mse: 2.4932 - val_loss: 0.0050 - val_mse: 4.3259\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0028 - mse: 2.4539 - val_loss: 0.0049 - val_mse: 4.2927\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0028 - mse: 2.4150 - val_loss: 0.0049 - val_mse: 4.2600\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0027 - mse: 2.3765 - val_loss: 0.0049 - val_mse: 4.2277\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0027 - mse: 2.3383 - val_loss: 0.0048 - val_mse: 4.1961\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0026 - mse: 2.3006 - val_loss: 0.0048 - val_mse: 4.1661\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0026 - mse: 2.2638 - val_loss: 0.0048 - val_mse: 4.1366\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0026 - mse: 2.2277 - val_loss: 0.0047 - val_mse: 4.1072\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0025 - mse: 2.1923 - val_loss: 0.0047 - val_mse: 4.0778\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0025 - mse: 2.1576 - val_loss: 0.0046 - val_mse: 4.0487\n",
      "Epoch 184/1000\n",
      "1s - loss: 0.0024 - mse: 2.1237 - val_loss: 0.0046 - val_mse: 4.0197\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0024 - mse: 2.0906 - val_loss: 0.0046 - val_mse: 3.9915\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0024 - mse: 2.0582 - val_loss: 0.0046 - val_mse: 3.9641\n",
      "Epoch 187/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0023 - mse: 2.0267 - val_loss: 0.0045 - val_mse: 3.9372\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0023 - mse: 1.9960 - val_loss: 0.0045 - val_mse: 3.9109\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0023 - mse: 1.9661 - val_loss: 0.0045 - val_mse: 3.8850\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0022 - mse: 1.9369 - val_loss: 0.0044 - val_mse: 3.8593\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0022 - mse: 1.9084 - val_loss: 0.0044 - val_mse: 3.8342\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0022 - mse: 1.8806 - val_loss: 0.0044 - val_mse: 3.8093\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0021 - mse: 1.8535 - val_loss: 0.0043 - val_mse: 3.7847\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0021 - mse: 1.8270 - val_loss: 0.0043 - val_mse: 3.7605\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0021 - mse: 1.8012 - val_loss: 0.0043 - val_mse: 3.7367\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0020 - mse: 1.7762 - val_loss: 0.0043 - val_mse: 3.7131\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0020 - mse: 1.7518 - val_loss: 0.0042 - val_mse: 3.6898\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0020 - mse: 1.7280 - val_loss: 0.0042 - val_mse: 3.6669\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0020 - mse: 1.7048 - val_loss: 0.0042 - val_mse: 3.6442\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0019 - mse: 1.6823 - val_loss: 0.0042 - val_mse: 3.6216\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0019 - mse: 1.6604 - val_loss: 0.0041 - val_mse: 3.5994\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0019 - mse: 1.6389 - val_loss: 0.0041 - val_mse: 3.5777\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0019 - mse: 1.6181 - val_loss: 0.0041 - val_mse: 3.5566\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0018 - mse: 1.5977 - val_loss: 0.0041 - val_mse: 3.5358\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0018 - mse: 1.5777 - val_loss: 0.0040 - val_mse: 3.5156\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0018 - mse: 1.5581 - val_loss: 0.0040 - val_mse: 3.4960\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0018 - mse: 1.5391 - val_loss: 0.0040 - val_mse: 3.4767\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0017 - mse: 1.5200 - val_loss: 0.0040 - val_mse: 3.4578\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0017 - mse: 1.5013 - val_loss: 0.0039 - val_mse: 3.4390\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0017 - mse: 1.4829 - val_loss: 0.0039 - val_mse: 3.4204\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0017 - mse: 1.4649 - val_loss: 0.0039 - val_mse: 3.4020\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0017 - mse: 1.4472 - val_loss: 0.0039 - val_mse: 3.3838\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0016 - mse: 1.4298 - val_loss: 0.0039 - val_mse: 3.3659\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0016 - mse: 1.4128 - val_loss: 0.0038 - val_mse: 3.3483\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0016 - mse: 1.3961 - val_loss: 0.0038 - val_mse: 3.3310\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0016 - mse: 1.3797 - val_loss: 0.0038 - val_mse: 3.3139\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0016 - mse: 1.3636 - val_loss: 0.0038 - val_mse: 3.2973\n",
      "Epoch 218/1000\n",
      "\n",
      "Epoch 00217: reducing learning rate to 0.00699999984354.\n",
      "1s - loss: 0.0015 - mse: 1.3477 - val_loss: 0.0038 - val_mse: 3.2812\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0015 - mse: 1.3321 - val_loss: 0.0038 - val_mse: 3.2702\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0015 - mse: 1.3214 - val_loss: 0.0037 - val_mse: 3.2594\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0015 - mse: 1.3108 - val_loss: 0.0037 - val_mse: 3.2487\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0015 - mse: 1.3004 - val_loss: 0.0037 - val_mse: 3.2381\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0015 - mse: 1.2900 - val_loss: 0.0037 - val_mse: 3.2276\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0015 - mse: 1.2798 - val_loss: 0.0037 - val_mse: 3.2172\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0015 - mse: 1.2697 - val_loss: 0.0037 - val_mse: 3.2067\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0014 - mse: 1.2597 - val_loss: 0.0037 - val_mse: 3.1963\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0014 - mse: 1.2496 - val_loss: 0.0037 - val_mse: 3.1861\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0014 - mse: 1.2396 - val_loss: 0.0036 - val_mse: 3.1759\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0014 - mse: 1.2301 - val_loss: 0.0036 - val_mse: 3.1657\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0014 - mse: 1.2207 - val_loss: 0.0036 - val_mse: 3.1554\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0014 - mse: 1.2114 - val_loss: 0.0036 - val_mse: 3.1451\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0014 - mse: 1.2022 - val_loss: 0.0036 - val_mse: 3.1348\n",
      "Epoch 233/1000\n",
      "\n",
      "Epoch 00232: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0014 - mse: 1.1932 - val_loss: 0.0036 - val_mse: 3.1245\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0014 - mse: 1.1843 - val_loss: 0.0036 - val_mse: 3.1173\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0014 - mse: 1.1782 - val_loss: 0.0036 - val_mse: 3.1100\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0013 - mse: 1.1721 - val_loss: 0.0036 - val_mse: 3.1028\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0013 - mse: 1.1662 - val_loss: 0.0036 - val_mse: 3.0956\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0013 - mse: 1.1603 - val_loss: 0.0035 - val_mse: 3.0885\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0013 - mse: 1.1545 - val_loss: 0.0035 - val_mse: 3.0815\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0013 - mse: 1.1488 - val_loss: 0.0035 - val_mse: 3.0745\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0013 - mse: 1.1432 - val_loss: 0.0035 - val_mse: 3.0677\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0013 - mse: 1.1378 - val_loss: 0.0035 - val_mse: 3.0610\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.0013 - mse: 1.1325 - val_loss: 0.0035 - val_mse: 3.0544\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0013 - mse: 1.1272 - val_loss: 0.0035 - val_mse: 3.0480\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0013 - mse: 1.1219 - val_loss: 0.0035 - val_mse: 3.0415\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0013 - mse: 1.1167 - val_loss: 0.0035 - val_mse: 3.0352\n",
      "Epoch 247/1000\n",
      "\n",
      "Epoch 00246: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0013 - mse: 1.1115 - val_loss: 0.0035 - val_mse: 3.0290\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0013 - mse: 1.1064 - val_loss: 0.0035 - val_mse: 3.0248\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0013 - mse: 1.1029 - val_loss: 0.0035 - val_mse: 3.0205\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0013 - mse: 1.0994 - val_loss: 0.0035 - val_mse: 3.0163\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0013 - mse: 1.0959 - val_loss: 0.0035 - val_mse: 3.0121\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0013 - mse: 1.0925 - val_loss: 0.0035 - val_mse: 3.0080\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0013 - mse: 1.0891 - val_loss: 0.0034 - val_mse: 3.0039\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.0012 - mse: 1.0857 - val_loss: 0.0034 - val_mse: 2.9999\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0012 - mse: 1.0823 - val_loss: 0.0034 - val_mse: 2.9958\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0012 - mse: 1.0790 - val_loss: 0.0034 - val_mse: 2.9918\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.0012 - mse: 1.0756 - val_loss: 0.0034 - val_mse: 2.9877\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0012 - mse: 1.0723 - val_loss: 0.0034 - val_mse: 2.9837\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.0012 - mse: 1.0690 - val_loss: 0.0034 - val_mse: 2.9796\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0012 - mse: 1.0658 - val_loss: 0.0034 - val_mse: 2.9755\n",
      "Epoch 261/1000\n",
      "\n",
      "Epoch 00260: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0012 - mse: 1.0625 - val_loss: 0.0034 - val_mse: 2.9715\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.0012 - mse: 1.0593 - val_loss: 0.0034 - val_mse: 2.9687\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0012 - mse: 1.0571 - val_loss: 0.0034 - val_mse: 2.9658\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.0012 - mse: 1.0548 - val_loss: 0.0034 - val_mse: 2.9630\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0012 - mse: 1.0526 - val_loss: 0.0034 - val_mse: 2.9601\n",
      "Epoch 266/1000\n",
      "1s - loss: 0.0012 - mse: 1.0504 - val_loss: 0.0034 - val_mse: 2.9573\n",
      "Epoch 267/1000\n",
      "1s - loss: 0.0012 - mse: 1.0482 - val_loss: 0.0034 - val_mse: 2.9545\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0012 - mse: 1.0460 - val_loss: 0.0034 - val_mse: 2.9518\n",
      "Epoch 269/1000\n",
      "1s - loss: 0.0012 - mse: 1.0439 - val_loss: 0.0034 - val_mse: 2.9490\n",
      "Epoch 270/1000\n",
      "1s - loss: 0.0012 - mse: 1.0417 - val_loss: 0.0034 - val_mse: 2.9462\n",
      "Epoch 271/1000\n",
      "1s - loss: 0.0012 - mse: 1.0396 - val_loss: 0.0034 - val_mse: 2.9435\n",
      "Epoch 272/1000\n",
      "1s - loss: 0.0012 - mse: 1.0374 - val_loss: 0.0034 - val_mse: 2.9407\n",
      "Epoch 273/1000\n",
      "1s - loss: 0.0012 - mse: 1.0353 - val_loss: 0.0034 - val_mse: 2.9379\n",
      "Epoch 274/1000\n",
      "1s - loss: 0.0012 - mse: 1.0332 - val_loss: 0.0034 - val_mse: 2.9351\n",
      "Epoch 275/1000\n",
      "\n",
      "Epoch 00274: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0012 - mse: 1.0311 - val_loss: 0.0034 - val_mse: 2.9324\n",
      "Epoch 276/1000\n",
      "1s - loss: 0.0012 - mse: 1.0290 - val_loss: 0.0034 - val_mse: 2.9305\n",
      "Epoch 277/1000\n",
      "1s - loss: 0.0012 - mse: 1.0276 - val_loss: 0.0034 - val_mse: 2.9286\n",
      "Epoch 278/1000\n",
      "1s - loss: 0.0012 - mse: 1.0261 - val_loss: 0.0034 - val_mse: 2.9267\n",
      "Epoch 279/1000\n",
      "1s - loss: 0.0012 - mse: 1.0247 - val_loss: 0.0034 - val_mse: 2.9248\n",
      "Epoch 280/1000\n",
      "1s - loss: 0.0012 - mse: 1.0232 - val_loss: 0.0034 - val_mse: 2.9230\n",
      "Epoch 281/1000\n",
      "1s - loss: 0.0012 - mse: 1.0218 - val_loss: 0.0034 - val_mse: 2.9212\n",
      "Epoch 282/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0012 - mse: 1.0204 - val_loss: 0.0034 - val_mse: 2.9193\n",
      "Epoch 283/1000\n",
      "1s - loss: 0.0012 - mse: 1.0190 - val_loss: 0.0034 - val_mse: 2.9176\n",
      "Epoch 284/1000\n",
      "1s - loss: 0.0012 - mse: 1.0176 - val_loss: 0.0033 - val_mse: 2.9158\n",
      "Epoch 285/1000\n",
      "1s - loss: 0.0012 - mse: 1.0162 - val_loss: 0.0033 - val_mse: 2.9140\n",
      "[{'loss': 0.0033503014128655195, 'width0': 135, 'width1': 75, 'val_mse': 6.1875839233398438, 'mse': 2.9175732135772705, 'val_loss': 0.007105313241481781, 'dropout1': 0, 'dropout0': 0}, {'loss': 0.0018494370160624385, 'width0': 135, 'width1': 100, 'val_mse': 3.698413610458374, 'mse': 1.6105610132217407, 'val_loss': 0.0042469552718102932, 'dropout1': 0, 'dropout0': 0}, {'loss': 0.00098988576792180538, 'width0': 135, 'width1': 125, 'val_mse': 3.1876635551452637, 'mse': 0.8620307445526123, 'val_loss': 0.0036604509223252535, 'dropout1': 0, 'dropout0': 0}, {'loss': 0.0011669626692309976, 'width0': 135, 'width1': 150, 'val_mse': 2.914015531539917, 'mse': 1.0162359476089478, 'val_loss': 0.0033462168648838997, 'dropout1': 0, 'dropout0': 0}]\n"
     ]
    }
   ],
   "source": [
    "width1 = 135\n",
    "dropout_perf = []\n",
    "prev_weights = []\n",
    "for width2 in range(75, 151, 25):\n",
    "        fit, results, prev_weights = run_depthn([{'width':width1, 'dropout':0},\n",
    "                                                 {'width':width2, 'dropout':0}\n",
    "                                                ], learningrates2, prev_weights, 'secondlayer')\n",
    "        dropout_perf.append(results)\n",
    "        print dropout_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x136bf2d50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAENCAYAAAAfTp5aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noAshRICQQBdCiADx/wGUenmlD6NRywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13617dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(dropout_perf).loc[:,['width1','mse','val_mse']].set_index('width1').plot()#logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final run for 2 layers (no dropout)\n",
    "If we add a second layer, 150 seems a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learningrates3 = {\n",
    "    'lr_intial' : 1e-2, # higher than the default\n",
    "    'lr_decay' : 0.01, # for adam it is 1 / (1 + decay * t) , thus with decay 0.001 and at t==1000, lr is divided by 2\n",
    "                    # note that the effect of this decay is not visible in tensorboard\n",
    "    'lr_plateau_factor' : 0.7, # if no convergence (possibly by too high lr), we boost the lr decay\n",
    "    'lr_plateau_patience' : 4, # nr of consequetive epochs without improvement before we boost the lr decay\n",
    "    'lr_plateau_cooldown' : 10, # first do this nr of iterations at new lr before detecting plateau\n",
    "    'lr_minimum' : 1e-6, # the minimum lr too which we decay (for plateau detection)\n",
    "\n",
    "    'stop_patience' : 50, # if no extra improvement after this nr of steps , we terminate learning\n",
    "    'stop_delta' : 0.0001, # the epsilon, changes below this threshold are 'no improvement'\n",
    "\n",
    "    'epochs_max' : 1000 # limit the total nr op epochs, very high, we will stop based on plateau\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.9382 - mse: 817.0059 - val_loss: 0.6523 - val_mse: 568.0666\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.6038 - mse: 525.8548 - val_loss: 0.6912 - val_mse: 601.9639\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.6882 - mse: 599.2872 - val_loss: 0.4110 - val_mse: 357.9218\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.3912 - mse: 340.7106 - val_loss: 0.9569 - val_mse: 833.3126\n",
      "Epoch 5/1000\n",
      "1s - loss: 1.0005 - mse: 871.2885 - val_loss: 0.3710 - val_mse: 323.0926\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.3626 - mse: 315.8009 - val_loss: 0.4728 - val_mse: 411.7694\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.4562 - mse: 397.3150 - val_loss: 0.3979 - val_mse: 346.5014\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.3752 - mse: 326.6953 - val_loss: 0.3119 - val_mse: 271.5948\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.2915 - mse: 253.8087 - val_loss: 0.2562 - val_mse: 223.1440\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.2376 - mse: 206.9051 - val_loss: 0.1800 - val_mse: 156.7583\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.1673 - mse: 145.7078 - val_loss: 0.1359 - val_mse: 118.3245\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.1236 - mse: 107.6317 - val_loss: 0.1324 - val_mse: 115.2921\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.1186 - mse: 103.2725 - val_loss: 0.1415 - val_mse: 123.1972\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.1264 - mse: 110.0724 - val_loss: 0.1476 - val_mse: 128.4958\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.1320 - mse: 114.9937 - val_loss: 0.1442 - val_mse: 125.6111\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.1298 - mse: 113.0412 - val_loss: 0.1291 - val_mse: 112.4657\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.1167 - mse: 101.6656 - val_loss: 0.1106 - val_mse: 96.2962\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0999 - mse: 87.0118 - val_loss: 0.0990 - val_mse: 86.2417\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0887 - mse: 77.2059 - val_loss: 0.0973 - val_mse: 84.7423\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0856 - mse: 74.5574 - val_loss: 0.1006 - val_mse: 87.6084\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0872 - mse: 75.9094 - val_loss: 0.1031 - val_mse: 89.7902\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0889 - mse: 77.3770 - val_loss: 0.1024 - val_mse: 89.1465\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0885 - mse: 77.0842 - val_loss: 0.0981 - val_mse: 85.4486\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0853 - mse: 74.3239 - val_loss: 0.0912 - val_mse: 79.3855\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0796 - mse: 69.3375 - val_loss: 0.0834 - val_mse: 72.5917\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0732 - mse: 63.7077 - val_loss: 0.0765 - val_mse: 66.6197\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0679 - mse: 59.1568 - val_loss: 0.0712 - val_mse: 61.9654\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0645 - mse: 56.1301 - val_loss: 0.0673 - val_mse: 58.6460\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0623 - mse: 54.2590 - val_loss: 0.0650 - val_mse: 56.5911\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0610 - mse: 53.1203 - val_loss: 0.0634 - val_mse: 55.2272\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0598 - mse: 52.0721 - val_loss: 0.0621 - val_mse: 54.0372\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0582 - mse: 50.7059 - val_loss: 0.0605 - val_mse: 52.6949\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0561 - mse: 48.8539 - val_loss: 0.0581 - val_mse: 50.6032\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0529 - mse: 46.0799 - val_loss: 0.0554 - val_mse: 48.2800\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0494 - mse: 43.0043 - val_loss: 0.0540 - val_mse: 46.9847\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0470 - mse: 40.9582 - val_loss: 0.0535 - val_mse: 46.6296\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0460 - mse: 40.0181 - val_loss: 0.0530 - val_mse: 46.1602\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0450 - mse: 39.1858 - val_loss: 0.0521 - val_mse: 45.3587\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0439 - mse: 38.2327 - val_loss: 0.0504 - val_mse: 43.8618\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0423 - mse: 36.7972 - val_loss: 0.0479 - val_mse: 41.7481\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0402 - mse: 34.9844 - val_loss: 0.0456 - val_mse: 39.7002\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0384 - mse: 33.4662 - val_loss: 0.0437 - val_mse: 38.0676\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0373 - mse: 32.4596 - val_loss: 0.0421 - val_mse: 36.7047\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0363 - mse: 31.6362 - val_loss: 0.0407 - val_mse: 35.4054\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0352 - mse: 30.6815 - val_loss: 0.0393 - val_mse: 34.1897\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0341 - mse: 29.6811 - val_loss: 0.0380 - val_mse: 33.0913\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0330 - mse: 28.7788 - val_loss: 0.0370 - val_mse: 32.2349\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0323 - mse: 28.1273 - val_loss: 0.0363 - val_mse: 31.5741\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0317 - mse: 27.6437 - val_loss: 0.0354 - val_mse: 30.8150\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0310 - mse: 26.9798 - val_loss: 0.0344 - val_mse: 29.9677\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0300 - mse: 26.1139 - val_loss: 0.0335 - val_mse: 29.2166\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0289 - mse: 25.2020 - val_loss: 0.0330 - val_mse: 28.7399\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0281 - mse: 24.4486 - val_loss: 0.0328 - val_mse: 28.5602\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0275 - mse: 23.9898 - val_loss: 0.0326 - val_mse: 28.3600\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0271 - mse: 23.6274 - val_loss: 0.0320 - val_mse: 27.8273\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0265 - mse: 23.1193 - val_loss: 0.0310 - val_mse: 26.9902\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0258 - mse: 22.4649 - val_loss: 0.0300 - val_mse: 26.1106\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0251 - mse: 21.8452 - val_loss: 0.0292 - val_mse: 25.4145\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0246 - mse: 21.3973 - val_loss: 0.0285 - val_mse: 24.7922\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0241 - mse: 20.9566 - val_loss: 0.0278 - val_mse: 24.2318\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0235 - mse: 20.4736 - val_loss: 0.0273 - val_mse: 23.7600\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0229 - mse: 19.9646 - val_loss: 0.0269 - val_mse: 23.4396\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0224 - mse: 19.5321 - val_loss: 0.0266 - val_mse: 23.1565\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0220 - mse: 19.1617 - val_loss: 0.0262 - val_mse: 22.7744\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0216 - mse: 18.7775 - val_loss: 0.0256 - val_mse: 22.2926\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0211 - mse: 18.3549 - val_loss: 0.0251 - val_mse: 21.8207\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0206 - mse: 17.9594 - val_loss: 0.0246 - val_mse: 21.4312\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0203 - mse: 17.6428 - val_loss: 0.0243 - val_mse: 21.1232\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0199 - mse: 17.3671 - val_loss: 0.0240 - val_mse: 20.8864\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0196 - mse: 17.0740 - val_loss: 0.0237 - val_mse: 20.6605\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0192 - mse: 16.7544 - val_loss: 0.0235 - val_mse: 20.4314\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0189 - mse: 16.4484 - val_loss: 0.0232 - val_mse: 20.2429\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0186 - mse: 16.1806 - val_loss: 0.0231 - val_mse: 20.0782\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0183 - mse: 15.9246 - val_loss: 0.0229 - val_mse: 19.9448\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0180 - mse: 15.6720 - val_loss: 0.0227 - val_mse: 19.7888\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0177 - mse: 15.4266 - val_loss: 0.0226 - val_mse: 19.6402\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0175 - mse: 15.2034 - val_loss: 0.0224 - val_mse: 19.4997\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0172 - mse: 14.9771 - val_loss: 0.0222 - val_mse: 19.3299\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0169 - mse: 14.7472 - val_loss: 0.0220 - val_mse: 19.1888\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0167 - mse: 14.5205 - val_loss: 0.0219 - val_mse: 19.0332\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0165 - mse: 14.3548 - val_loss: 0.0221 - val_mse: 19.2839\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0166 - mse: 14.4900 - val_loss: 0.0231 - val_mse: 20.1392\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0181 - mse: 15.7363 - val_loss: 0.0280 - val_mse: 24.3580\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0229 - mse: 19.9293 - val_loss: 0.0225 - val_mse: 19.6258\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0176 - mse: 15.3625 - val_loss: 0.0212 - val_mse: 18.4378\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0162 - mse: 14.1475 - val_loss: 0.0233 - val_mse: 20.3148\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0182 - mse: 15.8227 - val_loss: 0.0206 - val_mse: 17.9053\n",
      "Epoch 88/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0153 - mse: 13.3589 - val_loss: 0.0223 - val_mse: 19.3825\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0176 - mse: 15.2995 - val_loss: 0.0203 - val_mse: 17.7148\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0151 - mse: 13.1099 - val_loss: 0.0218 - val_mse: 18.9584\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0164 - mse: 14.2835 - val_loss: 0.0199 - val_mse: 17.3314\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0147 - mse: 12.8227 - val_loss: 0.0207 - val_mse: 18.0048\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0158 - mse: 13.7808 - val_loss: 0.0195 - val_mse: 17.0052\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0144 - mse: 12.5543 - val_loss: 0.0202 - val_mse: 17.5737\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0152 - mse: 13.2150 - val_loss: 0.0188 - val_mse: 16.4061\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0140 - mse: 12.1929 - val_loss: 0.0194 - val_mse: 16.8516\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0147 - mse: 12.7790 - val_loss: 0.0185 - val_mse: 16.0857\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0137 - mse: 11.9114 - val_loss: 0.0189 - val_mse: 16.4951\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0142 - mse: 12.3293 - val_loss: 0.0182 - val_mse: 15.8138\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0133 - mse: 11.6038 - val_loss: 0.0186 - val_mse: 16.2240\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0137 - mse: 11.9277 - val_loss: 0.0182 - val_mse: 15.8480\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0130 - mse: 11.3415 - val_loss: 0.0185 - val_mse: 16.0903\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0132 - mse: 11.5297 - val_loss: 0.0180 - val_mse: 15.6928\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0127 - mse: 11.0971 - val_loss: 0.0182 - val_mse: 15.8845\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0128 - mse: 11.1759 - val_loss: 0.0181 - val_mse: 15.7393\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0124 - mse: 10.8279 - val_loss: 0.0180 - val_mse: 15.7003\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0124 - mse: 10.8088 - val_loss: 0.0177 - val_mse: 15.3809\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0121 - mse: 10.5720 - val_loss: 0.0176 - val_mse: 15.3612\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0120 - mse: 10.4773 - val_loss: 0.0176 - val_mse: 15.3236\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0118 - mse: 10.3106 - val_loss: 0.0174 - val_mse: 15.1098\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0117 - mse: 10.1539 - val_loss: 0.0172 - val_mse: 14.9627\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0115 - mse: 10.0537 - val_loss: 0.0171 - val_mse: 14.8987\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0113 - mse: 9.8501 - val_loss: 0.0172 - val_mse: 14.9566\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0112 - mse: 9.7883 - val_loss: 0.0168 - val_mse: 14.6718\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0110 - mse: 9.5621 - val_loss: 0.0168 - val_mse: 14.6193\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0109 - mse: 9.5295 - val_loss: 0.0167 - val_mse: 14.5659\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0107 - mse: 9.2998 - val_loss: 0.0168 - val_mse: 14.6160\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0106 - mse: 9.2500 - val_loss: 0.0164 - val_mse: 14.3193\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0104 - mse: 9.0423 - val_loss: 0.0164 - val_mse: 14.2401\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0103 - mse: 8.9717 - val_loss: 0.0163 - val_mse: 14.2267\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0101 - mse: 8.8013 - val_loss: 0.0163 - val_mse: 14.1818\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0100 - mse: 8.6948 - val_loss: 0.0160 - val_mse: 13.9589\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0098 - mse: 8.5470 - val_loss: 0.0159 - val_mse: 13.8627\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0097 - mse: 8.4467 - val_loss: 0.0158 - val_mse: 13.7857\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0095 - mse: 8.2844 - val_loss: 0.0158 - val_mse: 13.7783\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0094 - mse: 8.2095 - val_loss: 0.0157 - val_mse: 13.6488\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0092 - mse: 8.0487 - val_loss: 0.0155 - val_mse: 13.5375\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0091 - mse: 7.9536 - val_loss: 0.0155 - val_mse: 13.4771\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0090 - mse: 7.8329 - val_loss: 0.0155 - val_mse: 13.5210\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0089 - mse: 7.7412 - val_loss: 0.0153 - val_mse: 13.3619\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0087 - mse: 7.6030 - val_loss: 0.0152 - val_mse: 13.2452\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0086 - mse: 7.5211 - val_loss: 0.0152 - val_mse: 13.2209\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0085 - mse: 7.4197 - val_loss: 0.0151 - val_mse: 13.1298\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0084 - mse: 7.3398 - val_loss: 0.0150 - val_mse: 13.0336\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0083 - mse: 7.2287 - val_loss: 0.0148 - val_mse: 12.8667\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0082 - mse: 7.1397 - val_loss: 0.0147 - val_mse: 12.7997\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0081 - mse: 7.0394 - val_loss: 0.0147 - val_mse: 12.7918\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0080 - mse: 6.9657 - val_loss: 0.0146 - val_mse: 12.6903\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0079 - mse: 6.8844 - val_loss: 0.0145 - val_mse: 12.6691\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0078 - mse: 6.8219 - val_loss: 0.0144 - val_mse: 12.5403\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0078 - mse: 6.7589 - val_loss: 0.0145 - val_mse: 12.6380\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0077 - mse: 6.7321 - val_loss: 0.0144 - val_mse: 12.4986\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0078 - mse: 6.7706 - val_loss: 0.0149 - val_mse: 13.0130\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0081 - mse: 7.0622 - val_loss: 0.0151 - val_mse: 13.1602\n",
      "Epoch 145/1000\n",
      "\n",
      "Epoch 00144: reducing learning rate to 0.00699999984354.\n",
      "2s - loss: 0.0089 - mse: 7.7493 - val_loss: 0.0179 - val_mse: 15.5546\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0112 - mse: 9.7149 - val_loss: 0.0141 - val_mse: 12.2727\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0077 - mse: 6.7453 - val_loss: 0.0151 - val_mse: 13.1771\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0091 - mse: 7.9550 - val_loss: 0.0156 - val_mse: 13.5679\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0090 - mse: 7.8065 - val_loss: 0.0145 - val_mse: 12.6449\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0079 - mse: 6.9203 - val_loss: 0.0148 - val_mse: 12.9293\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0090 - mse: 7.7964 - val_loss: 0.0136 - val_mse: 11.8089\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0073 - mse: 6.3139 - val_loss: 0.0151 - val_mse: 13.1403\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0085 - mse: 7.3855 - val_loss: 0.0137 - val_mse: 11.9304\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0072 - mse: 6.2437 - val_loss: 0.0141 - val_mse: 12.3129\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0081 - mse: 7.0755 - val_loss: 0.0133 - val_mse: 11.5776\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0070 - mse: 6.0934 - val_loss: 0.0143 - val_mse: 12.4236\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0077 - mse: 6.7205 - val_loss: 0.0135 - val_mse: 11.7375\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0070 - mse: 6.0630 - val_loss: 0.0136 - val_mse: 11.8236\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0074 - mse: 6.4522 - val_loss: 0.0132 - val_mse: 11.4728\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0068 - mse: 5.9645 - val_loss: 0.0137 - val_mse: 11.9320\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0071 - mse: 6.2087 - val_loss: 0.0133 - val_mse: 11.6009\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0068 - mse: 5.9171 - val_loss: 0.0131 - val_mse: 11.4241\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0069 - mse: 6.0059 - val_loss: 0.0129 - val_mse: 11.2692\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0067 - mse: 5.8250 - val_loss: 0.0132 - val_mse: 11.4608\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0067 - mse: 5.8447 - val_loss: 0.0130 - val_mse: 11.3089\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0066 - mse: 5.7534 - val_loss: 0.0126 - val_mse: 11.0148\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0065 - mse: 5.6896 - val_loss: 0.0126 - val_mse: 10.9844\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0065 - mse: 5.6604 - val_loss: 0.0127 - val_mse: 11.0830\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0064 - mse: 5.5723 - val_loss: 0.0127 - val_mse: 11.0542\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0064 - mse: 5.5711 - val_loss: 0.0123 - val_mse: 10.7270\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0063 - mse: 5.4551 - val_loss: 0.0123 - val_mse: 10.6942\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0063 - mse: 5.4753 - val_loss: 0.0123 - val_mse: 10.7474\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0062 - mse: 5.3619 - val_loss: 0.0124 - val_mse: 10.7699\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0062 - mse: 5.3765 - val_loss: 0.0121 - val_mse: 10.4982\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0060 - mse: 5.2658 - val_loss: 0.0120 - val_mse: 10.4553\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0061 - mse: 5.2790 - val_loss: 0.0120 - val_mse: 10.4906\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0059 - mse: 5.1812 - val_loss: 0.0121 - val_mse: 10.5136\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0060 - mse: 5.1819 - val_loss: 0.0118 - val_mse: 10.3001\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0059 - mse: 5.0965 - val_loss: 0.0118 - val_mse: 10.2416\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0058 - mse: 5.0843 - val_loss: 0.0118 - val_mse: 10.2778\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0058 - mse: 5.0161 - val_loss: 0.0118 - val_mse: 10.2661\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0057 - mse: 4.9870 - val_loss: 0.0116 - val_mse: 10.1104\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0057 - mse: 4.9358 - val_loss: 0.0115 - val_mse: 10.0503\n",
      "Epoch 184/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0056 - mse: 4.8906 - val_loss: 0.0116 - val_mse: 10.0859\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0056 - mse: 4.8548 - val_loss: 0.0115 - val_mse: 9.9929\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0055 - mse: 4.7953 - val_loss: 0.0113 - val_mse: 9.8772\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0055 - mse: 4.7734 - val_loss: 0.0113 - val_mse: 9.8409\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0054 - mse: 4.7007 - val_loss: 0.0113 - val_mse: 9.8779\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0054 - mse: 4.6889 - val_loss: 0.0112 - val_mse: 9.7318\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0053 - mse: 4.6109 - val_loss: 0.0111 - val_mse: 9.6692\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0053 - mse: 4.5991 - val_loss: 0.0111 - val_mse: 9.6752\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0052 - mse: 4.5273 - val_loss: 0.0111 - val_mse: 9.6498\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0052 - mse: 4.4992 - val_loss: 0.0109 - val_mse: 9.5083\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0051 - mse: 4.4486 - val_loss: 0.0109 - val_mse: 9.4766\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0050 - mse: 4.3969 - val_loss: 0.0109 - val_mse: 9.5077\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0050 - mse: 4.3659 - val_loss: 0.0108 - val_mse: 9.3790\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0049 - mse: 4.3022 - val_loss: 0.0107 - val_mse: 9.3055\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0049 - mse: 4.2712 - val_loss: 0.0107 - val_mse: 9.3365\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0048 - mse: 4.2190 - val_loss: 0.0106 - val_mse: 9.2566\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0048 - mse: 4.1676 - val_loss: 0.0105 - val_mse: 9.1517\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0047 - mse: 4.1326 - val_loss: 0.0105 - val_mse: 9.1579\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0047 - mse: 4.0742 - val_loss: 0.0105 - val_mse: 9.1177\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0046 - mse: 4.0336 - val_loss: 0.0103 - val_mse: 8.9998\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0046 - mse: 3.9935 - val_loss: 0.0103 - val_mse: 9.0057\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0045 - mse: 3.9401 - val_loss: 0.0103 - val_mse: 8.9833\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0045 - mse: 3.8999 - val_loss: 0.0102 - val_mse: 8.8779\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0044 - mse: 3.8575 - val_loss: 0.0102 - val_mse: 8.8737\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0044 - mse: 3.8050 - val_loss: 0.0102 - val_mse: 8.8409\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0043 - mse: 3.7618 - val_loss: 0.0100 - val_mse: 8.7381\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0043 - mse: 3.7174 - val_loss: 0.0100 - val_mse: 8.7261\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0042 - mse: 3.6664 - val_loss: 0.0100 - val_mse: 8.6865\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0042 - mse: 3.6219 - val_loss: 0.0099 - val_mse: 8.6046\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0041 - mse: 3.5827 - val_loss: 0.0099 - val_mse: 8.6242\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0041 - mse: 3.5388 - val_loss: 0.0098 - val_mse: 8.5459\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0040 - mse: 3.4923 - val_loss: 0.0098 - val_mse: 8.5217\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0040 - mse: 3.4491 - val_loss: 0.0098 - val_mse: 8.5181\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0039 - mse: 3.4096 - val_loss: 0.0097 - val_mse: 8.4315\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0039 - mse: 3.3709 - val_loss: 0.0097 - val_mse: 8.4636\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0038 - mse: 3.3301 - val_loss: 0.0096 - val_mse: 8.3721\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0038 - mse: 3.2872 - val_loss: 0.0096 - val_mse: 8.3902\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0037 - mse: 3.2461 - val_loss: 0.0096 - val_mse: 8.3406\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0037 - mse: 3.2055 - val_loss: 0.0096 - val_mse: 8.3400\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0036 - mse: 3.1654 - val_loss: 0.0095 - val_mse: 8.3007\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0036 - mse: 3.1262 - val_loss: 0.0095 - val_mse: 8.3053\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0035 - mse: 3.0882 - val_loss: 0.0095 - val_mse: 8.2683\n",
      "Epoch 226/1000\n",
      "\n",
      "Epoch 00225: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0035 - mse: 3.0510 - val_loss: 0.0095 - val_mse: 8.3072\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0035 - mse: 3.0163 - val_loss: 0.0095 - val_mse: 8.2361\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0034 - mse: 2.9878 - val_loss: 0.0095 - val_mse: 8.2532\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0034 - mse: 2.9553 - val_loss: 0.0095 - val_mse: 8.2666\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0034 - mse: 2.9313 - val_loss: 0.0094 - val_mse: 8.2105\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0033 - mse: 2.9072 - val_loss: 0.0095 - val_mse: 8.2330\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0033 - mse: 2.8774 - val_loss: 0.0094 - val_mse: 8.2239\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0033 - mse: 2.8511 - val_loss: 0.0094 - val_mse: 8.1793\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0032 - mse: 2.8271 - val_loss: 0.0094 - val_mse: 8.2070\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0032 - mse: 2.8005 - val_loss: 0.0094 - val_mse: 8.1724\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0032 - mse: 2.7720 - val_loss: 0.0094 - val_mse: 8.1516\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0032 - mse: 2.7480 - val_loss: 0.0094 - val_mse: 8.1868\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0031 - mse: 2.7253 - val_loss: 0.0093 - val_mse: 8.1245\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0031 - mse: 2.7005 - val_loss: 0.0094 - val_mse: 8.1521\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0031 - mse: 2.6716 - val_loss: 0.0093 - val_mse: 8.1235\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0030 - mse: 2.6448 - val_loss: 0.0093 - val_mse: 8.1005\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0030 - mse: 2.6203 - val_loss: 0.0093 - val_mse: 8.1264\n",
      "Epoch 243/1000\n",
      "\n",
      "Epoch 00242: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0030 - mse: 2.5964 - val_loss: 0.0093 - val_mse: 8.0701\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0030 - mse: 2.5720 - val_loss: 0.0093 - val_mse: 8.0838\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0029 - mse: 2.5511 - val_loss: 0.0093 - val_mse: 8.0874\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0029 - mse: 2.5353 - val_loss: 0.0092 - val_mse: 8.0482\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0029 - mse: 2.5177 - val_loss: 0.0092 - val_mse: 8.0533\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0029 - mse: 2.4984 - val_loss: 0.0093 - val_mse: 8.0744\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0029 - mse: 2.4837 - val_loss: 0.0092 - val_mse: 8.0362\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0028 - mse: 2.4643 - val_loss: 0.0092 - val_mse: 8.0236\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0028 - mse: 2.4475 - val_loss: 0.0092 - val_mse: 8.0394\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0028 - mse: 2.4311 - val_loss: 0.0092 - val_mse: 8.0122\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0028 - mse: 2.4121 - val_loss: 0.0092 - val_mse: 7.9931\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.0028 - mse: 2.3961 - val_loss: 0.0092 - val_mse: 8.0074\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0027 - mse: 2.3785 - val_loss: 0.0092 - val_mse: 7.9906\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0027 - mse: 2.3606 - val_loss: 0.0091 - val_mse: 7.9682\n",
      "Epoch 257/1000\n",
      "\n",
      "Epoch 00256: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0027 - mse: 2.3444 - val_loss: 0.0092 - val_mse: 7.9805\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0027 - mse: 2.3262 - val_loss: 0.0092 - val_mse: 7.9730\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.0027 - mse: 2.3140 - val_loss: 0.0091 - val_mse: 7.9559\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0026 - mse: 2.3025 - val_loss: 0.0091 - val_mse: 7.9555\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.0026 - mse: 2.2901 - val_loss: 0.0091 - val_mse: 7.9614\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.0026 - mse: 2.2786 - val_loss: 0.0091 - val_mse: 7.9487\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0026 - mse: 2.2665 - val_loss: 0.0091 - val_mse: 7.9281\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.0026 - mse: 2.2547 - val_loss: 0.0091 - val_mse: 7.9255\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0026 - mse: 2.2424 - val_loss: 0.0091 - val_mse: 7.9296\n",
      "Epoch 266/1000\n",
      "1s - loss: 0.0026 - mse: 2.2311 - val_loss: 0.0091 - val_mse: 7.9202\n",
      "Epoch 267/1000\n",
      "1s - loss: 0.0025 - mse: 2.2187 - val_loss: 0.0091 - val_mse: 7.9142\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0025 - mse: 2.2069 - val_loss: 0.0091 - val_mse: 7.9204\n",
      "Epoch 269/1000\n",
      "1s - loss: 0.0025 - mse: 2.1945 - val_loss: 0.0091 - val_mse: 7.9206\n",
      "Epoch 270/1000\n",
      "1s - loss: 0.0025 - mse: 2.1832 - val_loss: 0.0091 - val_mse: 7.9074\n",
      "Epoch 271/1000\n",
      "\n",
      "Epoch 00270: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0025 - mse: 2.1711 - val_loss: 0.0091 - val_mse: 7.9019\n",
      "Epoch 272/1000\n",
      "1s - loss: 0.0025 - mse: 2.1592 - val_loss: 0.0091 - val_mse: 7.9066\n",
      "Epoch 273/1000\n",
      "1s - loss: 0.0025 - mse: 2.1507 - val_loss: 0.0091 - val_mse: 7.9100\n",
      "Epoch 274/1000\n",
      "1s - loss: 0.0025 - mse: 2.1424 - val_loss: 0.0091 - val_mse: 7.9068\n",
      "Epoch 275/1000\n",
      "1s - loss: 0.0025 - mse: 2.1340 - val_loss: 0.0091 - val_mse: 7.9038\n",
      "Epoch 276/1000\n",
      "1s - loss: 0.0024 - mse: 2.1257 - val_loss: 0.0091 - val_mse: 7.9054\n",
      "Epoch 277/1000\n",
      "1s - loss: 0.0024 - mse: 2.1172 - val_loss: 0.0091 - val_mse: 7.9046\n",
      "Epoch 278/1000\n",
      "1s - loss: 0.0024 - mse: 2.1089 - val_loss: 0.0091 - val_mse: 7.8969\n",
      "Epoch 279/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0024 - mse: 2.1003 - val_loss: 0.0091 - val_mse: 7.8916\n",
      "Epoch 280/1000\n",
      "1s - loss: 0.0024 - mse: 2.0921 - val_loss: 0.0091 - val_mse: 7.8940\n",
      "Epoch 281/1000\n",
      "1s - loss: 0.0024 - mse: 2.0837 - val_loss: 0.0091 - val_mse: 7.8957\n",
      "Epoch 282/1000\n",
      "1s - loss: 0.0024 - mse: 2.0756 - val_loss: 0.0091 - val_mse: 7.8924\n",
      "Epoch 283/1000\n",
      "1s - loss: 0.0024 - mse: 2.0671 - val_loss: 0.0091 - val_mse: 7.8896\n",
      "Epoch 284/1000\n",
      "1s - loss: 0.0024 - mse: 2.0594 - val_loss: 0.0091 - val_mse: 7.8893\n",
      "Epoch 285/1000\n",
      "\n",
      "Epoch 00284: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0024 - mse: 2.0515 - val_loss: 0.0091 - val_mse: 7.8871\n",
      "Epoch 286/1000\n",
      "1s - loss: 0.0023 - mse: 2.0437 - val_loss: 0.0091 - val_mse: 7.8844\n",
      "Epoch 287/1000\n",
      "1s - loss: 0.0023 - mse: 2.0380 - val_loss: 0.0091 - val_mse: 7.8816\n",
      "Epoch 288/1000\n",
      "1s - loss: 0.0023 - mse: 2.0322 - val_loss: 0.0090 - val_mse: 7.8802\n",
      "Epoch 289/1000\n",
      "1s - loss: 0.0023 - mse: 2.0268 - val_loss: 0.0090 - val_mse: 7.8801\n",
      "Epoch 290/1000\n",
      "1s - loss: 0.0023 - mse: 2.0210 - val_loss: 0.0090 - val_mse: 7.8797\n",
      "Epoch 291/1000\n",
      "1s - loss: 0.0023 - mse: 2.0153 - val_loss: 0.0090 - val_mse: 7.8776\n",
      "Epoch 292/1000\n",
      "1s - loss: 0.0023 - mse: 2.0095 - val_loss: 0.0090 - val_mse: 7.8743\n",
      "Epoch 293/1000\n",
      "1s - loss: 0.0023 - mse: 2.0037 - val_loss: 0.0090 - val_mse: 7.8703\n",
      "Epoch 294/1000\n",
      "1s - loss: 0.0023 - mse: 1.9980 - val_loss: 0.0090 - val_mse: 7.8684\n",
      "Epoch 295/1000\n",
      "1s - loss: 0.0023 - mse: 1.9922 - val_loss: 0.0090 - val_mse: 7.8680\n",
      "Epoch 296/1000\n",
      "1s - loss: 0.0023 - mse: 1.9864 - val_loss: 0.0090 - val_mse: 7.8671\n",
      "Epoch 297/1000\n",
      "1s - loss: 0.0023 - mse: 1.9807 - val_loss: 0.0090 - val_mse: 7.8635\n",
      "Epoch 298/1000\n",
      "1s - loss: 0.0023 - mse: 1.9747 - val_loss: 0.0090 - val_mse: 7.8600\n",
      "Epoch 299/1000\n",
      "1s - loss: 0.0023 - mse: 1.9690 - val_loss: 0.0090 - val_mse: 7.8581\n",
      "Epoch 300/1000\n",
      "1s - loss: 0.0023 - mse: 1.9632 - val_loss: 0.0090 - val_mse: 7.8545\n",
      "Epoch 301/1000\n",
      "1s - loss: 0.0022 - mse: 1.9574 - val_loss: 0.0090 - val_mse: 7.8511\n",
      "Epoch 302/1000\n",
      "1s - loss: 0.0022 - mse: 1.9517 - val_loss: 0.0090 - val_mse: 7.8491\n",
      "Epoch 303/1000\n",
      "\n",
      "Epoch 00302: reducing learning rate to 0.000823543069419.\n",
      "1s - loss: 0.0022 - mse: 1.9462 - val_loss: 0.0090 - val_mse: 7.8458\n",
      "Epoch 304/1000\n",
      "1s - loss: 0.0022 - mse: 1.9407 - val_loss: 0.0090 - val_mse: 7.8436\n",
      "Epoch 305/1000\n",
      "1s - loss: 0.0022 - mse: 1.9367 - val_loss: 0.0090 - val_mse: 7.8417\n",
      "Epoch 306/1000\n",
      "1s - loss: 0.0022 - mse: 1.9328 - val_loss: 0.0090 - val_mse: 7.8398\n",
      "Epoch 307/1000\n",
      "1s - loss: 0.0022 - mse: 1.9290 - val_loss: 0.0090 - val_mse: 7.8373\n",
      "Epoch 308/1000\n",
      "1s - loss: 0.0022 - mse: 1.9250 - val_loss: 0.0090 - val_mse: 7.8352\n",
      "Epoch 309/1000\n",
      "1s - loss: 0.0022 - mse: 1.9211 - val_loss: 0.0090 - val_mse: 7.8348\n",
      "Epoch 310/1000\n",
      "1s - loss: 0.0022 - mse: 1.9172 - val_loss: 0.0090 - val_mse: 7.8351\n",
      "Epoch 311/1000\n",
      "1s - loss: 0.0022 - mse: 1.9133 - val_loss: 0.0090 - val_mse: 7.8335\n",
      "Epoch 312/1000\n",
      "1s - loss: 0.0022 - mse: 1.9094 - val_loss: 0.0090 - val_mse: 7.8301\n",
      "Epoch 313/1000\n",
      "1s - loss: 0.0022 - mse: 1.9055 - val_loss: 0.0090 - val_mse: 7.8267\n",
      "Epoch 314/1000\n",
      "1s - loss: 0.0022 - mse: 1.9016 - val_loss: 0.0090 - val_mse: 7.8251\n",
      "Epoch 315/1000\n",
      "1s - loss: 0.0022 - mse: 1.8978 - val_loss: 0.0090 - val_mse: 7.8241\n",
      "Epoch 316/1000\n",
      "1s - loss: 0.0022 - mse: 1.8939 - val_loss: 0.0090 - val_mse: 7.8228\n",
      "Epoch 317/1000\n",
      "\n",
      "Epoch 00316: reducing learning rate to 0.000576480140444.\n",
      "1s - loss: 0.0022 - mse: 1.8900 - val_loss: 0.0090 - val_mse: 7.8208\n",
      "Epoch 318/1000\n",
      "1s - loss: 0.0022 - mse: 1.8862 - val_loss: 0.0090 - val_mse: 7.8194\n",
      "Epoch 319/1000\n",
      "1s - loss: 0.0022 - mse: 1.8835 - val_loss: 0.0090 - val_mse: 7.8186\n",
      "Epoch 320/1000\n",
      "1s - loss: 0.0022 - mse: 1.8808 - val_loss: 0.0090 - val_mse: 7.8176\n",
      "Epoch 321/1000\n",
      "1s - loss: 0.0022 - mse: 1.8782 - val_loss: 0.0090 - val_mse: 7.8157\n",
      "Epoch 322/1000\n",
      "1s - loss: 0.0022 - mse: 1.8755 - val_loss: 0.0090 - val_mse: 7.8133\n",
      "Epoch 323/1000\n",
      "1s - loss: 0.0022 - mse: 1.8729 - val_loss: 0.0090 - val_mse: 7.8116\n",
      "Epoch 324/1000\n",
      "1s - loss: 0.0021 - mse: 1.8702 - val_loss: 0.0090 - val_mse: 7.8111\n",
      "Epoch 325/1000\n",
      "1s - loss: 0.0021 - mse: 1.8676 - val_loss: 0.0090 - val_mse: 7.8114\n",
      "Epoch 326/1000\n",
      "1s - loss: 0.0021 - mse: 1.8649 - val_loss: 0.0090 - val_mse: 7.8109\n",
      "Epoch 327/1000\n",
      "1s - loss: 0.0021 - mse: 1.8623 - val_loss: 0.0090 - val_mse: 7.8090\n",
      "Epoch 328/1000\n",
      "1s - loss: 0.0021 - mse: 1.8597 - val_loss: 0.0090 - val_mse: 7.8062\n",
      "Epoch 329/1000\n",
      "1s - loss: 0.0021 - mse: 1.8570 - val_loss: 0.0090 - val_mse: 7.8037\n",
      "Epoch 330/1000\n",
      "1s - loss: 0.0021 - mse: 1.8544 - val_loss: 0.0090 - val_mse: 7.8019\n",
      "Epoch 331/1000\n",
      "\n",
      "Epoch 00330: reducing learning rate to 0.000403536082013.\n",
      "1s - loss: 0.0021 - mse: 1.8518 - val_loss: 0.0090 - val_mse: 7.8013\n",
      "Epoch 332/1000\n",
      "1s - loss: 0.0021 - mse: 1.8492 - val_loss: 0.0090 - val_mse: 7.8017\n",
      "Epoch 333/1000\n",
      "1s - loss: 0.0021 - mse: 1.8473 - val_loss: 0.0090 - val_mse: 7.8021\n",
      "Epoch 334/1000\n",
      "1s - loss: 0.0021 - mse: 1.8454 - val_loss: 0.0090 - val_mse: 7.8020\n",
      "Epoch 335/1000\n",
      "1s - loss: 0.0021 - mse: 1.8437 - val_loss: 0.0090 - val_mse: 7.8009\n",
      "Epoch 336/1000\n",
      "1s - loss: 0.0021 - mse: 1.8418 - val_loss: 0.0090 - val_mse: 7.7991\n",
      "Epoch 337/1000\n",
      "1s - loss: 0.0021 - mse: 1.8399 - val_loss: 0.0090 - val_mse: 7.7976\n",
      "Epoch 338/1000\n",
      "1s - loss: 0.0021 - mse: 1.8381 - val_loss: 0.0090 - val_mse: 7.7968\n",
      "Epoch 339/1000\n",
      "1s - loss: 0.0021 - mse: 1.8363 - val_loss: 0.0090 - val_mse: 7.7965\n",
      "Epoch 340/1000\n",
      "1s - loss: 0.0021 - mse: 1.8345 - val_loss: 0.0090 - val_mse: 7.7961\n",
      "Epoch 341/1000\n",
      "1s - loss: 0.0021 - mse: 1.8327 - val_loss: 0.0090 - val_mse: 7.7951\n",
      "Epoch 342/1000\n",
      "1s - loss: 0.0021 - mse: 1.8308 - val_loss: 0.0090 - val_mse: 7.7943\n",
      "Epoch 343/1000\n",
      "1s - loss: 0.0021 - mse: 1.8290 - val_loss: 0.0089 - val_mse: 7.7937\n",
      "Epoch 344/1000\n",
      "1s - loss: 0.0021 - mse: 1.8272 - val_loss: 0.0089 - val_mse: 7.7934\n",
      "Epoch 345/1000\n",
      "\n",
      "Epoch 00344: reducing learning rate to 0.000282475253334.\n",
      "1s - loss: 0.0021 - mse: 1.8254 - val_loss: 0.0089 - val_mse: 7.7932\n",
      "Epoch 346/1000\n",
      "1s - loss: 0.0021 - mse: 1.8236 - val_loss: 0.0089 - val_mse: 7.7924\n",
      "Epoch 347/1000\n",
      "1s - loss: 0.0021 - mse: 1.8223 - val_loss: 0.0089 - val_mse: 7.7911\n",
      "Epoch 348/1000\n",
      "1s - loss: 0.0021 - mse: 1.8210 - val_loss: 0.0089 - val_mse: 7.7898\n",
      "Epoch 349/1000\n",
      "1s - loss: 0.0021 - mse: 1.8198 - val_loss: 0.0089 - val_mse: 7.7888\n",
      "[{'loss': 0.002089665038511157, 'run': 0, 'width0': 135, 'width1': 150, 'val_mse': 7.7888374328613281, 'mse': 1.8197623491287231, 'val_loss': 0.0089440615847706795, 'dropout1': 0, 'dropout0': 0}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.9529 - mse: 829.8391 - val_loss: 0.6082 - val_mse: 529.6533\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.5713 - mse: 497.5490 - val_loss: 0.7971 - val_mse: 694.1743\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.7851 - mse: 683.7131 - val_loss: 0.5636 - val_mse: 490.8225\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.5409 - mse: 471.0020 - val_loss: 0.3225 - val_mse: 280.8627\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.3012 - mse: 262.2697 - val_loss: 0.3761 - val_mse: 327.5312\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.3545 - mse: 308.6709 - val_loss: 0.2079 - val_mse: 181.0416\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.2044 - mse: 177.9571 - val_loss: 0.2297 - val_mse: 199.9892\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.2224 - mse: 193.6732 - val_loss: 0.1919 - val_mse: 167.1216\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.1831 - mse: 159.4766 - val_loss: 0.1804 - val_mse: 157.1223\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.1746 - mse: 152.0082 - val_loss: 0.1194 - val_mse: 103.9589\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.1062 - mse: 92.5015 - val_loss: 0.1428 - val_mse: 124.3518\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.1203 - mse: 104.7795 - val_loss: 0.1463 - val_mse: 127.3636\n",
      "Epoch 13/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.1222 - mse: 106.4340 - val_loss: 0.1148 - val_mse: 99.9956\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0983 - mse: 85.5897 - val_loss: 0.1039 - val_mse: 90.4887\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0954 - mse: 83.0882 - val_loss: 0.1047 - val_mse: 91.1579\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0990 - mse: 86.2505 - val_loss: 0.0993 - val_mse: 86.4580\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0926 - mse: 80.6096 - val_loss: 0.0882 - val_mse: 76.8133\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0807 - mse: 70.3054 - val_loss: 0.0762 - val_mse: 66.3200\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0690 - mse: 60.1153 - val_loss: 0.0720 - val_mse: 62.6655\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0646 - mse: 56.2418 - val_loss: 0.0791 - val_mse: 68.9014\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0706 - mse: 61.4843 - val_loss: 0.0785 - val_mse: 68.3698\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0707 - mse: 61.6054 - val_loss: 0.0661 - val_mse: 57.5930\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0613 - mse: 53.4242 - val_loss: 0.0595 - val_mse: 51.8261\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0570 - mse: 49.6461 - val_loss: 0.0581 - val_mse: 50.5757\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0565 - mse: 49.1770 - val_loss: 0.0571 - val_mse: 49.7340\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0560 - mse: 48.7865 - val_loss: 0.0575 - val_mse: 50.0868\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0569 - mse: 49.5325 - val_loss: 0.0556 - val_mse: 48.4558\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0544 - mse: 47.3760 - val_loss: 0.0530 - val_mse: 46.1359\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0507 - mse: 44.1894 - val_loss: 0.0525 - val_mse: 45.7088\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0497 - mse: 43.3146 - val_loss: 0.0517 - val_mse: 45.0265\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0488 - mse: 42.5237 - val_loss: 0.0489 - val_mse: 42.5688\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0462 - mse: 40.2099 - val_loss: 0.0480 - val_mse: 41.8384\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0453 - mse: 39.4567 - val_loss: 0.0474 - val_mse: 41.3144\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0445 - mse: 38.7867 - val_loss: 0.0456 - val_mse: 39.7491\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0424 - mse: 36.9502 - val_loss: 0.0441 - val_mse: 38.3666\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0407 - mse: 35.4528 - val_loss: 0.0428 - val_mse: 37.2940\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0398 - mse: 34.6492 - val_loss: 0.0416 - val_mse: 36.2127\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0390 - mse: 33.9817 - val_loss: 0.0410 - val_mse: 35.6693\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0388 - mse: 33.7626 - val_loss: 0.0404 - val_mse: 35.2013\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0384 - mse: 33.4330 - val_loss: 0.0392 - val_mse: 34.1645\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0371 - mse: 32.3130 - val_loss: 0.0383 - val_mse: 33.3852\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0359 - mse: 31.2413 - val_loss: 0.0379 - val_mse: 33.0458\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0350 - mse: 30.4531 - val_loss: 0.0378 - val_mse: 32.9218\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0342 - mse: 29.7414 - val_loss: 0.0380 - val_mse: 33.1133\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0337 - mse: 29.3498 - val_loss: 0.0377 - val_mse: 32.8346\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0331 - mse: 28.8640 - val_loss: 0.0363 - val_mse: 31.5956\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0322 - mse: 28.0544 - val_loss: 0.0343 - val_mse: 29.8748\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0312 - mse: 27.1792 - val_loss: 0.0332 - val_mse: 28.8754\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0309 - mse: 26.9147 - val_loss: 0.0324 - val_mse: 28.2188\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0307 - mse: 26.7027 - val_loss: 0.0317 - val_mse: 27.6436\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0302 - mse: 26.3241 - val_loss: 0.0310 - val_mse: 27.0148\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0296 - mse: 25.7537 - val_loss: 0.0306 - val_mse: 26.6334\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0289 - mse: 25.1706 - val_loss: 0.0304 - val_mse: 26.4678\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0282 - mse: 24.5773 - val_loss: 0.0305 - val_mse: 26.5265\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0278 - mse: 24.2433 - val_loss: 0.0303 - val_mse: 26.3438\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0274 - mse: 23.8883 - val_loss: 0.0296 - val_mse: 25.8023\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0269 - mse: 23.4421 - val_loss: 0.0288 - val_mse: 25.0888\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0264 - mse: 23.0011 - val_loss: 0.0281 - val_mse: 24.4895\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0260 - mse: 22.6662 - val_loss: 0.0275 - val_mse: 23.9366\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0256 - mse: 22.2670 - val_loss: 0.0271 - val_mse: 23.5998\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0251 - mse: 21.8930 - val_loss: 0.0268 - val_mse: 23.3739\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0247 - mse: 21.5007 - val_loss: 0.0266 - val_mse: 23.1482\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0242 - mse: 21.1140 - val_loss: 0.0263 - val_mse: 22.9085\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0239 - mse: 20.8118 - val_loss: 0.0259 - val_mse: 22.5655\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0236 - mse: 20.5348 - val_loss: 0.0254 - val_mse: 22.0955\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0232 - mse: 20.2127 - val_loss: 0.0248 - val_mse: 21.6378\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0229 - mse: 19.9085 - val_loss: 0.0243 - val_mse: 21.1935\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0225 - mse: 19.5791 - val_loss: 0.0239 - val_mse: 20.7948\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0221 - mse: 19.2359 - val_loss: 0.0236 - val_mse: 20.5469\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0218 - mse: 18.9729 - val_loss: 0.0233 - val_mse: 20.3340\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0215 - mse: 18.6927 - val_loss: 0.0230 - val_mse: 20.0519\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0211 - mse: 18.3778 - val_loss: 0.0226 - val_mse: 19.6847\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0208 - mse: 18.0730 - val_loss: 0.0222 - val_mse: 19.3073\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0204 - mse: 17.7977 - val_loss: 0.0218 - val_mse: 19.0073\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0201 - mse: 17.5260 - val_loss: 0.0216 - val_mse: 18.8038\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0198 - mse: 17.2241 - val_loss: 0.0215 - val_mse: 18.6918\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0195 - mse: 16.9841 - val_loss: 0.0211 - val_mse: 18.4163\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0192 - mse: 16.7162 - val_loss: 0.0207 - val_mse: 18.0277\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0189 - mse: 16.4296 - val_loss: 0.0204 - val_mse: 17.7337\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0186 - mse: 16.1754 - val_loss: 0.0202 - val_mse: 17.5637\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0183 - mse: 15.9544 - val_loss: 0.0199 - val_mse: 17.3480\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0180 - mse: 15.6972 - val_loss: 0.0196 - val_mse: 17.0771\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0177 - mse: 15.4413 - val_loss: 0.0193 - val_mse: 16.8338\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0175 - mse: 15.2014 - val_loss: 0.0191 - val_mse: 16.6061\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0172 - mse: 14.9738 - val_loss: 0.0187 - val_mse: 16.2988\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0169 - mse: 14.7365 - val_loss: 0.0184 - val_mse: 16.0544\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0167 - mse: 14.5061 - val_loss: 0.0183 - val_mse: 15.9440\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0164 - mse: 14.2636 - val_loss: 0.0181 - val_mse: 15.7543\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0161 - mse: 14.0262 - val_loss: 0.0177 - val_mse: 15.4499\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0158 - mse: 13.7902 - val_loss: 0.0175 - val_mse: 15.2396\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0156 - mse: 13.5685 - val_loss: 0.0172 - val_mse: 15.0013\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0153 - mse: 13.3531 - val_loss: 0.0170 - val_mse: 14.8246\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0151 - mse: 13.1442 - val_loss: 0.0169 - val_mse: 14.7484\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0149 - mse: 12.9358 - val_loss: 0.0167 - val_mse: 14.5383\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0146 - mse: 12.7177 - val_loss: 0.0165 - val_mse: 14.3517\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0144 - mse: 12.5253 - val_loss: 0.0165 - val_mse: 14.3446\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0142 - mse: 12.3815 - val_loss: 0.0164 - val_mse: 14.2852\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0143 - mse: 12.4785 - val_loss: 0.0187 - val_mse: 16.3216\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0160 - mse: 13.9150 - val_loss: 0.0223 - val_mse: 19.3969\n",
      "Epoch 100/1000\n",
      "\n",
      "Epoch 00099: reducing learning rate to 0.00699999984354.\n",
      "2s - loss: 0.0205 - mse: 17.8807 - val_loss: 0.0330 - val_mse: 28.7463\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0294 - mse: 25.5826 - val_loss: 0.0181 - val_mse: 15.7926\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0155 - mse: 13.4967 - val_loss: 0.0248 - val_mse: 21.5727\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0229 - mse: 19.9619 - val_loss: 0.0178 - val_mse: 15.5086\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0164 - mse: 14.2742 - val_loss: 0.0203 - val_mse: 17.6633\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0173 - mse: 15.0841 - val_loss: 0.0212 - val_mse: 18.4388\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0182 - mse: 15.8620 - val_loss: 0.0196 - val_mse: 17.0856\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0165 - mse: 14.3312 - val_loss: 0.0182 - val_mse: 15.8498\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0154 - mse: 13.3807 - val_loss: 0.0186 - val_mse: 16.2066\n",
      "Epoch 109/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0168 - mse: 14.6101 - val_loss: 0.0177 - val_mse: 15.3800\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0157 - mse: 13.6562 - val_loss: 0.0164 - val_mse: 14.3213\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0140 - mse: 12.1778 - val_loss: 0.0187 - val_mse: 16.3168\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0159 - mse: 13.8586 - val_loss: 0.0178 - val_mse: 15.5315\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0149 - mse: 12.9494 - val_loss: 0.0160 - val_mse: 13.9111\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0132 - mse: 11.4882 - val_loss: 0.0170 - val_mse: 14.8402\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0146 - mse: 12.7350 - val_loss: 0.0163 - val_mse: 14.1539\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0139 - mse: 12.1022 - val_loss: 0.0155 - val_mse: 13.4886\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0128 - mse: 11.1259 - val_loss: 0.0164 - val_mse: 14.3227\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0135 - mse: 11.7685 - val_loss: 0.0160 - val_mse: 13.9109\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0131 - mse: 11.3800 - val_loss: 0.0152 - val_mse: 13.2628\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0124 - mse: 10.8067 - val_loss: 0.0153 - val_mse: 13.3013\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0126 - mse: 11.0023 - val_loss: 0.0151 - val_mse: 13.1117\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0124 - mse: 10.8303 - val_loss: 0.0149 - val_mse: 12.9609\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0120 - mse: 10.4715 - val_loss: 0.0149 - val_mse: 12.9917\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0119 - mse: 10.3906 - val_loss: 0.0149 - val_mse: 13.0007\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0120 - mse: 10.4096 - val_loss: 0.0146 - val_mse: 12.6858\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0117 - mse: 10.1578 - val_loss: 0.0141 - val_mse: 12.2902\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0113 - mse: 9.8593 - val_loss: 0.0143 - val_mse: 12.4615\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0115 - mse: 10.0115 - val_loss: 0.0141 - val_mse: 12.3000\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0112 - mse: 9.7830 - val_loss: 0.0138 - val_mse: 11.9936\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0108 - mse: 9.4480 - val_loss: 0.0140 - val_mse: 12.1715\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0110 - mse: 9.5952 - val_loss: 0.0137 - val_mse: 11.9475\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0108 - mse: 9.3792 - val_loss: 0.0134 - val_mse: 11.6639\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0104 - mse: 9.0953 - val_loss: 0.0136 - val_mse: 11.8216\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0106 - mse: 9.1971 - val_loss: 0.0134 - val_mse: 11.6260\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0103 - mse: 8.9690 - val_loss: 0.0132 - val_mse: 11.4579\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0101 - mse: 8.7712 - val_loss: 0.0133 - val_mse: 11.5457\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0101 - mse: 8.8110 - val_loss: 0.0130 - val_mse: 11.3594\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0099 - mse: 8.5948 - val_loss: 0.0129 - val_mse: 11.2430\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0097 - mse: 8.4478 - val_loss: 0.0129 - val_mse: 11.2648\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0097 - mse: 8.4431 - val_loss: 0.0128 - val_mse: 11.1163\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0095 - mse: 8.2610 - val_loss: 0.0127 - val_mse: 11.0730\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0094 - mse: 8.1474 - val_loss: 0.0127 - val_mse: 11.0908\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0093 - mse: 8.1037 - val_loss: 0.0126 - val_mse: 10.9477\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0091 - mse: 7.9480 - val_loss: 0.0124 - val_mse: 10.8261\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0090 - mse: 7.8457 - val_loss: 0.0124 - val_mse: 10.7562\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0089 - mse: 7.7732 - val_loss: 0.0123 - val_mse: 10.6752\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0088 - mse: 7.6344 - val_loss: 0.0122 - val_mse: 10.6570\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0087 - mse: 7.5495 - val_loss: 0.0122 - val_mse: 10.5821\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0086 - mse: 7.4573 - val_loss: 0.0120 - val_mse: 10.4447\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0084 - mse: 7.3431 - val_loss: 0.0119 - val_mse: 10.3636\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0083 - mse: 7.2641 - val_loss: 0.0118 - val_mse: 10.3098\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0082 - mse: 7.1664 - val_loss: 0.0118 - val_mse: 10.2584\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0081 - mse: 7.0739 - val_loss: 0.0117 - val_mse: 10.1843\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0080 - mse: 6.9915 - val_loss: 0.0116 - val_mse: 10.0991\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0079 - mse: 6.9002 - val_loss: 0.0116 - val_mse: 10.0599\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0078 - mse: 6.8174 - val_loss: 0.0115 - val_mse: 10.0379\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0077 - mse: 6.7381 - val_loss: 0.0115 - val_mse: 9.9779\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0076 - mse: 6.6543 - val_loss: 0.0114 - val_mse: 9.9071\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0076 - mse: 6.5806 - val_loss: 0.0113 - val_mse: 9.8579\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0075 - mse: 6.5037 - val_loss: 0.0113 - val_mse: 9.8464\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0074 - mse: 6.4324 - val_loss: 0.0113 - val_mse: 9.8250\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0073 - mse: 6.3620 - val_loss: 0.0112 - val_mse: 9.7734\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0072 - mse: 6.2910 - val_loss: 0.0112 - val_mse: 9.7235\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0071 - mse: 6.2252 - val_loss: 0.0111 - val_mse: 9.6767\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0071 - mse: 6.1537 - val_loss: 0.0111 - val_mse: 9.6330\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0070 - mse: 6.0909 - val_loss: 0.0110 - val_mse: 9.5757\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0069 - mse: 6.0248 - val_loss: 0.0109 - val_mse: 9.5344\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0068 - mse: 5.9600 - val_loss: 0.0109 - val_mse: 9.5195\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0068 - mse: 5.8991 - val_loss: 0.0109 - val_mse: 9.4915\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0067 - mse: 5.8353 - val_loss: 0.0108 - val_mse: 9.4409\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0066 - mse: 5.7761 - val_loss: 0.0108 - val_mse: 9.3847\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0066 - mse: 5.7169 - val_loss: 0.0107 - val_mse: 9.3434\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0065 - mse: 5.6571 - val_loss: 0.0107 - val_mse: 9.3109\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0064 - mse: 5.6025 - val_loss: 0.0106 - val_mse: 9.2638\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0064 - mse: 5.5440 - val_loss: 0.0106 - val_mse: 9.2214\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0063 - mse: 5.4909 - val_loss: 0.0105 - val_mse: 9.1861\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0062 - mse: 5.4358 - val_loss: 0.0105 - val_mse: 9.1464\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0062 - mse: 5.3811 - val_loss: 0.0105 - val_mse: 9.1003\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0061 - mse: 5.3301 - val_loss: 0.0104 - val_mse: 9.0540\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0061 - mse: 5.2766 - val_loss: 0.0104 - val_mse: 9.0204\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0060 - mse: 5.2269 - val_loss: 0.0103 - val_mse: 8.9828\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0059 - mse: 5.1760 - val_loss: 0.0103 - val_mse: 8.9399\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0059 - mse: 5.1264 - val_loss: 0.0102 - val_mse: 8.9020\n",
      "Epoch 184/1000\n",
      "1s - loss: 0.0058 - mse: 5.0786 - val_loss: 0.0102 - val_mse: 8.8691\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0058 - mse: 5.0292 - val_loss: 0.0101 - val_mse: 8.8389\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0057 - mse: 4.9826 - val_loss: 0.0101 - val_mse: 8.8072\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0057 - mse: 4.9349 - val_loss: 0.0101 - val_mse: 8.7787\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0056 - mse: 4.8888 - val_loss: 0.0100 - val_mse: 8.7505\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0056 - mse: 4.8424 - val_loss: 0.0100 - val_mse: 8.7228\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0055 - mse: 4.7956 - val_loss: 0.0100 - val_mse: 8.6984\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0055 - mse: 4.7497 - val_loss: 0.0100 - val_mse: 8.6713\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0054 - mse: 4.7030 - val_loss: 0.0099 - val_mse: 8.6424\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0053 - mse: 4.6583 - val_loss: 0.0099 - val_mse: 8.6112\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0053 - mse: 4.6131 - val_loss: 0.0099 - val_mse: 8.5782\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0052 - mse: 4.5688 - val_loss: 0.0098 - val_mse: 8.5428\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0052 - mse: 4.5244 - val_loss: 0.0098 - val_mse: 8.5076\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0051 - mse: 4.4801 - val_loss: 0.0097 - val_mse: 8.4748\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0051 - mse: 4.4362 - val_loss: 0.0097 - val_mse: 8.4470\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0050 - mse: 4.3945 - val_loss: 0.0097 - val_mse: 8.4266\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0050 - mse: 4.3540 - val_loss: 0.0097 - val_mse: 8.4082\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0050 - mse: 4.3131 - val_loss: 0.0096 - val_mse: 8.3867\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0049 - mse: 4.2725 - val_loss: 0.0096 - val_mse: 8.3651\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0049 - mse: 4.2328 - val_loss: 0.0096 - val_mse: 8.3447\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0048 - mse: 4.1929 - val_loss: 0.0096 - val_mse: 8.3220\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0048 - mse: 4.1522 - val_loss: 0.0095 - val_mse: 8.2992\n",
      "Epoch 206/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0047 - mse: 4.1117 - val_loss: 0.0095 - val_mse: 8.2790\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0047 - mse: 4.0718 - val_loss: 0.0095 - val_mse: 8.2558\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0046 - mse: 4.0322 - val_loss: 0.0095 - val_mse: 8.2308\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0046 - mse: 3.9928 - val_loss: 0.0094 - val_mse: 8.2062\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0045 - mse: 3.9529 - val_loss: 0.0094 - val_mse: 8.1815\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0045 - mse: 3.9132 - val_loss: 0.0094 - val_mse: 8.1563\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0044 - mse: 3.8737 - val_loss: 0.0093 - val_mse: 8.1211\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0044 - mse: 3.8339 - val_loss: 0.0093 - val_mse: 8.0898\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0044 - mse: 3.7942 - val_loss: 0.0093 - val_mse: 8.0645\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0043 - mse: 3.7547 - val_loss: 0.0092 - val_mse: 8.0421\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0043 - mse: 3.7152 - val_loss: 0.0092 - val_mse: 8.0242\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0042 - mse: 3.6757 - val_loss: 0.0092 - val_mse: 7.9917\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0042 - mse: 3.6373 - val_loss: 0.0091 - val_mse: 7.9640\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0041 - mse: 3.5987 - val_loss: 0.0091 - val_mse: 7.9361\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0041 - mse: 3.5603 - val_loss: 0.0091 - val_mse: 7.9172\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0040 - mse: 3.5216 - val_loss: 0.0091 - val_mse: 7.8950\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0040 - mse: 3.4827 - val_loss: 0.0090 - val_mse: 7.8734\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0040 - mse: 3.4437 - val_loss: 0.0090 - val_mse: 7.8418\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0039 - mse: 3.4046 - val_loss: 0.0090 - val_mse: 7.8184\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0039 - mse: 3.3651 - val_loss: 0.0089 - val_mse: 7.7894\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0038 - mse: 3.3256 - val_loss: 0.0089 - val_mse: 7.7751\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0038 - mse: 3.2852 - val_loss: 0.0089 - val_mse: 7.7639\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0037 - mse: 3.2435 - val_loss: 0.0089 - val_mse: 7.7405\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0037 - mse: 3.2006 - val_loss: 0.0089 - val_mse: 7.7236\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0036 - mse: 3.1573 - val_loss: 0.0088 - val_mse: 7.6750\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0036 - mse: 3.1144 - val_loss: 0.0088 - val_mse: 7.6884\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0035 - mse: 3.0730 - val_loss: 0.0088 - val_mse: 7.6300\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0035 - mse: 3.0373 - val_loss: 0.0089 - val_mse: 7.7443\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0035 - mse: 3.0198 - val_loss: 0.0087 - val_mse: 7.6179\n",
      "Epoch 235/1000\n",
      "\n",
      "Epoch 00234: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0035 - mse: 3.0677 - val_loss: 0.0094 - val_mse: 8.2147\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0038 - mse: 3.3403 - val_loss: 0.0088 - val_mse: 7.6250\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0036 - mse: 3.1487 - val_loss: 0.0086 - val_mse: 7.4784\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0033 - mse: 2.8325 - val_loss: 0.0091 - val_mse: 7.9404\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0036 - mse: 3.0948 - val_loss: 0.0085 - val_mse: 7.4218\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0033 - mse: 2.8447 - val_loss: 0.0085 - val_mse: 7.3876\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0032 - mse: 2.8054 - val_loss: 0.0089 - val_mse: 7.7345\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0033 - mse: 2.9046 - val_loss: 0.0084 - val_mse: 7.3466\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.0031 - mse: 2.6612 - val_loss: 0.0084 - val_mse: 7.3536\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0032 - mse: 2.7838 - val_loss: 0.0086 - val_mse: 7.5204\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0031 - mse: 2.6877 - val_loss: 0.0085 - val_mse: 7.3704\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0030 - mse: 2.5790 - val_loss: 0.0083 - val_mse: 7.2670\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0031 - mse: 2.6695 - val_loss: 0.0084 - val_mse: 7.2850\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0029 - mse: 2.4981 - val_loss: 0.0084 - val_mse: 7.3504\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0029 - mse: 2.5138 - val_loss: 0.0082 - val_mse: 7.1492\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0029 - mse: 2.4894 - val_loss: 0.0082 - val_mse: 7.1301\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0027 - mse: 2.3714 - val_loss: 0.0084 - val_mse: 7.2801\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0028 - mse: 2.4106 - val_loss: 0.0081 - val_mse: 7.0441\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0027 - mse: 2.3122 - val_loss: 0.0080 - val_mse: 7.0082\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.0026 - mse: 2.2736 - val_loss: 0.0082 - val_mse: 7.1403\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0026 - mse: 2.2666 - val_loss: 0.0080 - val_mse: 6.9558\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0025 - mse: 2.1658 - val_loss: 0.0079 - val_mse: 6.8916\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.0025 - mse: 2.1623 - val_loss: 0.0080 - val_mse: 6.9760\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0024 - mse: 2.1080 - val_loss: 0.0079 - val_mse: 6.8676\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.0023 - mse: 2.0328 - val_loss: 0.0078 - val_mse: 6.7767\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0023 - mse: 2.0256 - val_loss: 0.0078 - val_mse: 6.8182\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.0022 - mse: 1.9511 - val_loss: 0.0077 - val_mse: 6.7419\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.0022 - mse: 1.8938 - val_loss: 0.0076 - val_mse: 6.6183\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0022 - mse: 1.8733 - val_loss: 0.0076 - val_mse: 6.6356\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.0021 - mse: 1.7983 - val_loss: 0.0075 - val_mse: 6.5637\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0020 - mse: 1.7422 - val_loss: 0.0074 - val_mse: 6.4444\n",
      "Epoch 266/1000\n",
      "1s - loss: 0.0020 - mse: 1.7134 - val_loss: 0.0074 - val_mse: 6.4578\n",
      "Epoch 267/1000\n",
      "1s - loss: 0.0019 - mse: 1.6437 - val_loss: 0.0073 - val_mse: 6.3688\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0018 - mse: 1.5837 - val_loss: 0.0072 - val_mse: 6.2547\n",
      "Epoch 269/1000\n",
      "1s - loss: 0.0018 - mse: 1.5500 - val_loss: 0.0072 - val_mse: 6.2668\n",
      "Epoch 270/1000\n",
      "1s - loss: 0.0017 - mse: 1.4882 - val_loss: 0.0071 - val_mse: 6.1682\n",
      "Epoch 271/1000\n",
      "1s - loss: 0.0016 - mse: 1.4236 - val_loss: 0.0070 - val_mse: 6.0806\n",
      "Epoch 272/1000\n",
      "1s - loss: 0.0016 - mse: 1.3847 - val_loss: 0.0070 - val_mse: 6.0999\n",
      "Epoch 273/1000\n",
      "1s - loss: 0.0015 - mse: 1.3330 - val_loss: 0.0069 - val_mse: 5.9816\n",
      "Epoch 274/1000\n",
      "1s - loss: 0.0015 - mse: 1.2682 - val_loss: 0.0068 - val_mse: 5.9139\n",
      "Epoch 275/1000\n",
      "1s - loss: 0.0014 - mse: 1.2224 - val_loss: 0.0068 - val_mse: 5.9314\n",
      "Epoch 276/1000\n",
      "1s - loss: 0.0014 - mse: 1.1798 - val_loss: 0.0067 - val_mse: 5.8151\n",
      "Epoch 277/1000\n",
      "1s - loss: 0.0013 - mse: 1.1222 - val_loss: 0.0066 - val_mse: 5.7802\n",
      "Epoch 278/1000\n",
      "1s - loss: 0.0012 - mse: 1.0695 - val_loss: 0.0066 - val_mse: 5.7754\n",
      "Epoch 279/1000\n",
      "1s - loss: 0.0012 - mse: 1.0294 - val_loss: 0.0065 - val_mse: 5.6704\n",
      "Epoch 280/1000\n",
      "1s - loss: 0.0011 - mse: 0.9848 - val_loss: 0.0065 - val_mse: 5.6666\n",
      "Epoch 281/1000\n",
      "1s - loss: 0.0011 - mse: 0.9352 - val_loss: 0.0064 - val_mse: 5.6166\n",
      "Epoch 282/1000\n",
      "1s - loss: 0.0010 - mse: 0.8930 - val_loss: 0.0064 - val_mse: 5.5505\n",
      "Epoch 283/1000\n",
      "1s - loss: 9.8696e-04 - mse: 0.8595 - val_loss: 0.0064 - val_mse: 5.5671\n",
      "Epoch 284/1000\n",
      "1s - loss: 9.4712e-04 - mse: 0.8248 - val_loss: 0.0063 - val_mse: 5.4929\n",
      "Epoch 285/1000\n",
      "1s - loss: 9.0323e-04 - mse: 0.7866 - val_loss: 0.0063 - val_mse: 5.4767\n",
      "Epoch 286/1000\n",
      "1s - loss: 8.6484e-04 - mse: 0.7531 - val_loss: 0.0063 - val_mse: 5.4725\n",
      "Epoch 287/1000\n",
      "1s - loss: 8.3443e-04 - mse: 0.7267 - val_loss: 0.0062 - val_mse: 5.4158\n",
      "Epoch 288/1000\n",
      "1s - loss: 8.0639e-04 - mse: 0.7022 - val_loss: 0.0063 - val_mse: 5.4432\n",
      "Epoch 289/1000\n",
      "1s - loss: 7.7650e-04 - mse: 0.6762 - val_loss: 0.0062 - val_mse: 5.4013\n",
      "Epoch 290/1000\n",
      "1s - loss: 7.4768e-04 - mse: 0.6511 - val_loss: 0.0062 - val_mse: 5.3897\n",
      "Epoch 291/1000\n",
      "1s - loss: 7.2386e-04 - mse: 0.6304 - val_loss: 0.0062 - val_mse: 5.3977\n",
      "Epoch 292/1000\n",
      "\n",
      "Epoch 00291: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 7.0466e-04 - mse: 0.6136 - val_loss: 0.0061 - val_mse: 5.3537\n",
      "Epoch 293/1000\n",
      "1s - loss: 6.8673e-04 - mse: 0.5980 - val_loss: 0.0062 - val_mse: 5.3626\n",
      "Epoch 294/1000\n",
      "1s - loss: 6.7047e-04 - mse: 0.5839 - val_loss: 0.0062 - val_mse: 5.3581\n",
      "Epoch 295/1000\n",
      "1s - loss: 6.5959e-04 - mse: 0.5744 - val_loss: 0.0061 - val_mse: 5.3223\n",
      "Epoch 296/1000\n",
      "1s - loss: 6.4753e-04 - mse: 0.5639 - val_loss: 0.0061 - val_mse: 5.3199\n",
      "Epoch 297/1000\n",
      "1s - loss: 6.3437e-04 - mse: 0.5524 - val_loss: 0.0061 - val_mse: 5.3313\n",
      "Epoch 298/1000\n",
      "1s - loss: 6.2540e-04 - mse: 0.5446 - val_loss: 0.0061 - val_mse: 5.3027\n",
      "Epoch 299/1000\n",
      "1s - loss: 6.1274e-04 - mse: 0.5336 - val_loss: 0.0061 - val_mse: 5.2885\n",
      "Epoch 300/1000\n",
      "1s - loss: 6.0360e-04 - mse: 0.5256 - val_loss: 0.0061 - val_mse: 5.2985\n",
      "Epoch 301/1000\n",
      "1s - loss: 5.9395e-04 - mse: 0.5172 - val_loss: 0.0061 - val_mse: 5.2837\n",
      "Epoch 302/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 5.8381e-04 - mse: 0.5084 - val_loss: 0.0060 - val_mse: 5.2665\n",
      "Epoch 303/1000\n",
      "1s - loss: 5.7635e-04 - mse: 0.5019 - val_loss: 0.0061 - val_mse: 5.2777\n",
      "Epoch 304/1000\n",
      "1s - loss: 5.6665e-04 - mse: 0.4935 - val_loss: 0.0061 - val_mse: 5.2783\n",
      "Epoch 305/1000\n",
      "1s - loss: 5.5929e-04 - mse: 0.4870 - val_loss: 0.0060 - val_mse: 5.2575\n",
      "Epoch 306/1000\n",
      "\n",
      "Epoch 00305: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 5.5183e-04 - mse: 0.4806 - val_loss: 0.0060 - val_mse: 5.2576\n",
      "Epoch 307/1000\n",
      "1s - loss: 5.4385e-04 - mse: 0.4736 - val_loss: 0.0060 - val_mse: 5.2612\n",
      "Epoch 308/1000\n",
      "1s - loss: 5.3925e-04 - mse: 0.4696 - val_loss: 0.0060 - val_mse: 5.2555\n",
      "Epoch 309/1000\n",
      "1s - loss: 5.3420e-04 - mse: 0.4652 - val_loss: 0.0060 - val_mse: 5.2477\n",
      "Epoch 310/1000\n",
      "1s - loss: 5.2989e-04 - mse: 0.4614 - val_loss: 0.0060 - val_mse: 5.2475\n",
      "Epoch 311/1000\n",
      "1s - loss: 5.2522e-04 - mse: 0.4574 - val_loss: 0.0060 - val_mse: 5.2512\n",
      "Epoch 312/1000\n",
      "1s - loss: 5.2095e-04 - mse: 0.4537 - val_loss: 0.0060 - val_mse: 5.2490\n",
      "Epoch 313/1000\n",
      "1s - loss: 5.1656e-04 - mse: 0.4498 - val_loss: 0.0060 - val_mse: 5.2459\n",
      "Epoch 314/1000\n",
      "1s - loss: 5.1250e-04 - mse: 0.4463 - val_loss: 0.0060 - val_mse: 5.2497\n",
      "Epoch 315/1000\n",
      "1s - loss: 5.0838e-04 - mse: 0.4427 - val_loss: 0.0060 - val_mse: 5.2574\n",
      "Epoch 316/1000\n",
      "1s - loss: 5.0448e-04 - mse: 0.4393 - val_loss: 0.0060 - val_mse: 5.2584\n",
      "Epoch 317/1000\n",
      "1s - loss: 5.0062e-04 - mse: 0.4360 - val_loss: 0.0060 - val_mse: 5.2540\n",
      "Epoch 318/1000\n",
      "1s - loss: 4.9682e-04 - mse: 0.4326 - val_loss: 0.0060 - val_mse: 5.2542\n",
      "Epoch 319/1000\n",
      "1s - loss: 4.9318e-04 - mse: 0.4295 - val_loss: 0.0060 - val_mse: 5.2591\n",
      "Epoch 320/1000\n",
      "\n",
      "Epoch 00319: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 4.8954e-04 - mse: 0.4263 - val_loss: 0.0060 - val_mse: 5.2620\n",
      "Epoch 321/1000\n",
      "1s - loss: 4.8604e-04 - mse: 0.4233 - val_loss: 0.0060 - val_mse: 5.2620\n",
      "Epoch 322/1000\n",
      "1s - loss: 4.8355e-04 - mse: 0.4211 - val_loss: 0.0060 - val_mse: 5.2628\n",
      "Epoch 323/1000\n",
      "1s - loss: 4.8123e-04 - mse: 0.4191 - val_loss: 0.0060 - val_mse: 5.2657\n",
      "Epoch 324/1000\n",
      "1s - loss: 4.7883e-04 - mse: 0.4170 - val_loss: 0.0061 - val_mse: 5.2692\n",
      "Epoch 325/1000\n",
      "1s - loss: 4.7651e-04 - mse: 0.4150 - val_loss: 0.0061 - val_mse: 5.2703\n",
      "Epoch 326/1000\n",
      "1s - loss: 4.7426e-04 - mse: 0.4130 - val_loss: 0.0060 - val_mse: 5.2683\n",
      "Epoch 327/1000\n",
      "1s - loss: 4.7197e-04 - mse: 0.4110 - val_loss: 0.0060 - val_mse: 5.2662\n",
      "Epoch 328/1000\n",
      "1s - loss: 4.6979e-04 - mse: 0.4091 - val_loss: 0.0060 - val_mse: 5.2663\n",
      "Epoch 329/1000\n",
      "1s - loss: 4.6759e-04 - mse: 0.4072 - val_loss: 0.0060 - val_mse: 5.2683\n",
      "Epoch 330/1000\n",
      "1s - loss: 4.6542e-04 - mse: 0.4053 - val_loss: 0.0061 - val_mse: 5.2696\n",
      "Epoch 331/1000\n",
      "1s - loss: 4.6335e-04 - mse: 0.4035 - val_loss: 0.0060 - val_mse: 5.2685\n",
      "Epoch 332/1000\n",
      "1s - loss: 4.6124e-04 - mse: 0.4017 - val_loss: 0.0060 - val_mse: 5.2671\n",
      "Epoch 333/1000\n",
      "1s - loss: 4.5920e-04 - mse: 0.3999 - val_loss: 0.0060 - val_mse: 5.2670\n",
      "Epoch 334/1000\n",
      "\n",
      "Epoch 00333: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 4.5719e-04 - mse: 0.3981 - val_loss: 0.0060 - val_mse: 5.2685\n",
      "Epoch 335/1000\n",
      "1s - loss: 4.5519e-04 - mse: 0.3964 - val_loss: 0.0061 - val_mse: 5.2692\n",
      "Epoch 336/1000\n",
      "1s - loss: 4.5382e-04 - mse: 0.3952 - val_loss: 0.0061 - val_mse: 5.2694\n",
      "Epoch 337/1000\n",
      "1s - loss: 4.5245e-04 - mse: 0.3940 - val_loss: 0.0061 - val_mse: 5.2690\n",
      "Epoch 338/1000\n",
      "1s - loss: 4.5109e-04 - mse: 0.3928 - val_loss: 0.0061 - val_mse: 5.2687\n",
      "Epoch 339/1000\n",
      "1s - loss: 4.4974e-04 - mse: 0.3917 - val_loss: 0.0061 - val_mse: 5.2686\n",
      "Epoch 340/1000\n",
      "1s - loss: 4.4842e-04 - mse: 0.3905 - val_loss: 0.0060 - val_mse: 5.2683\n",
      "Epoch 341/1000\n",
      "1s - loss: 4.4709e-04 - mse: 0.3893 - val_loss: 0.0060 - val_mse: 5.2677\n",
      "Epoch 342/1000\n",
      "1s - loss: 4.4579e-04 - mse: 0.3882 - val_loss: 0.0060 - val_mse: 5.2667\n",
      "Epoch 343/1000\n",
      "1s - loss: 4.4449e-04 - mse: 0.3871 - val_loss: 0.0060 - val_mse: 5.2655\n",
      "Epoch 344/1000\n",
      "1s - loss: 4.4319e-04 - mse: 0.3859 - val_loss: 0.0060 - val_mse: 5.2646\n",
      "Epoch 345/1000\n",
      "1s - loss: 4.4191e-04 - mse: 0.3848 - val_loss: 0.0060 - val_mse: 5.2642\n",
      "Epoch 346/1000\n",
      "1s - loss: 4.4064e-04 - mse: 0.3837 - val_loss: 0.0060 - val_mse: 5.2644\n",
      "[{'loss': 0.002089665038511157, 'run': 0, 'width0': 135, 'width1': 150, 'val_mse': 7.7888374328613281, 'mse': 1.8197623491287231, 'val_loss': 0.0089440615847706795, 'dropout1': 0, 'dropout0': 0}, {'loss': 0.00044063656241632998, 'run': 1, 'width0': 135, 'width1': 150, 'val_mse': 5.2644405364990234, 'mse': 0.38372343778610229, 'val_loss': 0.0060452516190707684, 'dropout1': 0, 'dropout0': 0}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_25 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.9263 - mse: 806.6520 - val_loss: 0.5977 - val_mse: 520.4668\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.5609 - mse: 488.4483 - val_loss: 0.8422 - val_mse: 733.4261\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.8390 - mse: 730.6136 - val_loss: 0.6236 - val_mse: 543.0341\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.6009 - mse: 523.3284 - val_loss: 0.4052 - val_mse: 352.8575\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.3811 - mse: 331.8678 - val_loss: 0.2928 - val_mse: 254.9869\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.2785 - mse: 242.5585 - val_loss: 0.2464 - val_mse: 214.5857\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.2380 - mse: 207.2917 - val_loss: 0.2246 - val_mse: 195.5958\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.2237 - mse: 194.8149 - val_loss: 0.1767 - val_mse: 153.8427\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.1690 - mse: 147.2031 - val_loss: 0.1272 - val_mse: 110.7352\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.1195 - mse: 104.0712 - val_loss: 0.1553 - val_mse: 135.2010\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.1514 - mse: 131.8340 - val_loss: 0.1342 - val_mse: 116.8792\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.1225 - mse: 106.7116 - val_loss: 0.1308 - val_mse: 113.9396\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.1089 - mse: 94.8448 - val_loss: 0.1471 - val_mse: 128.1169\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.00699999984354.\n",
      "2s - loss: 0.1251 - mse: 108.9470 - val_loss: 0.1298 - val_mse: 113.0097\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.1129 - mse: 98.2804 - val_loss: 0.1123 - val_mse: 97.7675\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0980 - mse: 85.3384 - val_loss: 0.1035 - val_mse: 90.1395\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0904 - mse: 78.7289 - val_loss: 0.1030 - val_mse: 89.6809\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0908 - mse: 79.0437 - val_loss: 0.0998 - val_mse: 86.9159\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0894 - mse: 77.8199 - val_loss: 0.0902 - val_mse: 78.5555\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0819 - mse: 71.3426 - val_loss: 0.0817 - val_mse: 71.1419\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0750 - mse: 65.2706 - val_loss: 0.0799 - val_mse: 69.5726\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0741 - mse: 64.5119 - val_loss: 0.0807 - val_mse: 70.2410\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0757 - mse: 65.8795 - val_loss: 0.0776 - val_mse: 67.5870\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0733 - mse: 63.8668 - val_loss: 0.0715 - val_mse: 62.2657\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0676 - mse: 58.8493 - val_loss: 0.0679 - val_mse: 59.1541\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0638 - mse: 55.5265 - val_loss: 0.0683 - val_mse: 59.4614\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0636 - mse: 55.3933 - val_loss: 0.0675 - val_mse: 58.7561\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0627 - mse: 54.5648 - val_loss: 0.0630 - val_mse: 54.8393\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0585 - mse: 50.9636 - val_loss: 0.0589 - val_mse: 51.2634\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0548 - mse: 47.6826 - val_loss: 0.0578 - val_mse: 50.3008\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0537 - mse: 46.7466 - val_loss: 0.0570 - val_mse: 49.6273\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0527 - mse: 45.9360 - val_loss: 0.0554 - val_mse: 48.2460\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0507 - mse: 44.1808 - val_loss: 0.0554 - val_mse: 48.2236\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0498 - mse: 43.3968 - val_loss: 0.0561 - val_mse: 48.8506\n",
      "Epoch 35/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0498 - mse: 43.3686 - val_loss: 0.0535 - val_mse: 46.6037\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0476 - mse: 41.4196 - val_loss: 0.0505 - val_mse: 43.9972\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0456 - mse: 39.6868 - val_loss: 0.0499 - val_mse: 43.4200\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0456 - mse: 39.7409 - val_loss: 0.0486 - val_mse: 42.2850\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0447 - mse: 38.9673 - val_loss: 0.0461 - val_mse: 40.1202\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0425 - mse: 36.9789 - val_loss: 0.0447 - val_mse: 38.9285\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0410 - mse: 35.7250 - val_loss: 0.0445 - val_mse: 38.7271\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0407 - mse: 35.4242 - val_loss: 0.0431 - val_mse: 37.5181\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0396 - mse: 34.4503 - val_loss: 0.0405 - val_mse: 35.2524\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0375 - mse: 32.6942 - val_loss: 0.0388 - val_mse: 33.7867\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0363 - mse: 31.5892 - val_loss: 0.0384 - val_mse: 33.4142\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0359 - mse: 31.2520 - val_loss: 0.0378 - val_mse: 32.9133\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0351 - mse: 30.5933 - val_loss: 0.0367 - val_mse: 31.9569\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0338 - mse: 29.4293 - val_loss: 0.0360 - val_mse: 31.3478\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0329 - mse: 28.6162 - val_loss: 0.0359 - val_mse: 31.2199\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0326 - mse: 28.3568 - val_loss: 0.0352 - val_mse: 30.6685\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0320 - mse: 27.8514 - val_loss: 0.0340 - val_mse: 29.5845\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0309 - mse: 26.9368 - val_loss: 0.0331 - val_mse: 28.7956\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0302 - mse: 26.2758 - val_loss: 0.0327 - val_mse: 28.5123\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0299 - mse: 26.0183 - val_loss: 0.0324 - val_mse: 28.2327\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0295 - mse: 25.6886 - val_loss: 0.0319 - val_mse: 27.7630\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0288 - mse: 25.0864 - val_loss: 0.0316 - val_mse: 27.4844\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0282 - mse: 24.5731 - val_loss: 0.0316 - val_mse: 27.4824\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0279 - mse: 24.3108 - val_loss: 0.0313 - val_mse: 27.2868\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0275 - mse: 23.9586 - val_loss: 0.0308 - val_mse: 26.8580\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0270 - mse: 23.4913 - val_loss: 0.0304 - val_mse: 26.4322\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0265 - mse: 23.1010 - val_loss: 0.0298 - val_mse: 25.9101\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0261 - mse: 22.6929 - val_loss: 0.0291 - val_mse: 25.3075\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0255 - mse: 22.2404 - val_loss: 0.0285 - val_mse: 24.8433\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0251 - mse: 21.8535 - val_loss: 0.0281 - val_mse: 24.4915\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0247 - mse: 21.4784 - val_loss: 0.0277 - val_mse: 24.1560\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0242 - mse: 21.0723 - val_loss: 0.0275 - val_mse: 23.9232\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0238 - mse: 20.7358 - val_loss: 0.0273 - val_mse: 23.7437\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0234 - mse: 20.4192 - val_loss: 0.0270 - val_mse: 23.4944\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0230 - mse: 20.0360 - val_loss: 0.0267 - val_mse: 23.2404\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0226 - mse: 19.6869 - val_loss: 0.0264 - val_mse: 23.0325\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0223 - mse: 19.4137 - val_loss: 0.0261 - val_mse: 22.7314\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0219 - mse: 19.0813 - val_loss: 0.0257 - val_mse: 22.3664\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0215 - mse: 18.7239 - val_loss: 0.0254 - val_mse: 22.0901\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0212 - mse: 18.4693 - val_loss: 0.0250 - val_mse: 21.7984\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0209 - mse: 18.1869 - val_loss: 0.0247 - val_mse: 21.4670\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0205 - mse: 17.8468 - val_loss: 0.0244 - val_mse: 21.2430\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0202 - mse: 17.5968 - val_loss: 0.0242 - val_mse: 21.0384\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0199 - mse: 17.3596 - val_loss: 0.0239 - val_mse: 20.7716\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0196 - mse: 17.0666 - val_loss: 0.0236 - val_mse: 20.5696\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0193 - mse: 16.8435 - val_loss: 0.0234 - val_mse: 20.3759\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0191 - mse: 16.6276 - val_loss: 0.0231 - val_mse: 20.1523\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0188 - mse: 16.3683 - val_loss: 0.0229 - val_mse: 19.9833\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0185 - mse: 16.1525 - val_loss: 0.0227 - val_mse: 19.7965\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0183 - mse: 15.9260 - val_loss: 0.0225 - val_mse: 19.5936\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0180 - mse: 15.7001 - val_loss: 0.0223 - val_mse: 19.3959\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0178 - mse: 15.4997 - val_loss: 0.0220 - val_mse: 19.1491\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0175 - mse: 15.2631 - val_loss: 0.0217 - val_mse: 18.9278\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0173 - mse: 15.0580 - val_loss: 0.0215 - val_mse: 18.6900\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0170 - mse: 14.8436 - val_loss: 0.0212 - val_mse: 18.4577\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0168 - mse: 14.6159 - val_loss: 0.0210 - val_mse: 18.2875\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0166 - mse: 14.4173 - val_loss: 0.0208 - val_mse: 18.1061\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0163 - mse: 14.1983 - val_loss: 0.0206 - val_mse: 17.9426\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0161 - mse: 14.0069 - val_loss: 0.0204 - val_mse: 17.7496\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0158 - mse: 13.7976 - val_loss: 0.0202 - val_mse: 17.5858\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0156 - mse: 13.6085 - val_loss: 0.0200 - val_mse: 17.4347\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0154 - mse: 13.4207 - val_loss: 0.0198 - val_mse: 17.2842\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0152 - mse: 13.2333 - val_loss: 0.0197 - val_mse: 17.1304\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0150 - mse: 13.0526 - val_loss: 0.0195 - val_mse: 16.9440\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0148 - mse: 12.8688 - val_loss: 0.0192 - val_mse: 16.7382\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0146 - mse: 12.6950 - val_loss: 0.0190 - val_mse: 16.5379\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0144 - mse: 12.5275 - val_loss: 0.0188 - val_mse: 16.3759\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0142 - mse: 12.3562 - val_loss: 0.0186 - val_mse: 16.2388\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0140 - mse: 12.2019 - val_loss: 0.0184 - val_mse: 16.0507\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0138 - mse: 12.0352 - val_loss: 0.0182 - val_mse: 15.8860\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0136 - mse: 11.8806 - val_loss: 0.0181 - val_mse: 15.7764\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0135 - mse: 11.7188 - val_loss: 0.0180 - val_mse: 15.6865\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0133 - mse: 11.5606 - val_loss: 0.0179 - val_mse: 15.5865\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0131 - mse: 11.4123 - val_loss: 0.0178 - val_mse: 15.4674\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0129 - mse: 11.2565 - val_loss: 0.0176 - val_mse: 15.3334\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0128 - mse: 11.1070 - val_loss: 0.0175 - val_mse: 15.2218\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0126 - mse: 10.9611 - val_loss: 0.0174 - val_mse: 15.1443\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0124 - mse: 10.8131 - val_loss: 0.0173 - val_mse: 15.0383\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0122 - mse: 10.6677 - val_loss: 0.0171 - val_mse: 14.9284\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0121 - mse: 10.5324 - val_loss: 0.0170 - val_mse: 14.8197\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0119 - mse: 10.3981 - val_loss: 0.0169 - val_mse: 14.7091\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0118 - mse: 10.2649 - val_loss: 0.0168 - val_mse: 14.6132\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0116 - mse: 10.1326 - val_loss: 0.0166 - val_mse: 14.4851\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0115 - mse: 9.9972 - val_loss: 0.0166 - val_mse: 14.4169\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0113 - mse: 9.8773 - val_loss: 0.0165 - val_mse: 14.3406\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0112 - mse: 9.7847 - val_loss: 0.0166 - val_mse: 14.4792\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0113 - mse: 9.8462 - val_loss: 0.0169 - val_mse: 14.6938\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0117 - mse: 10.2144 - val_loss: 0.0185 - val_mse: 16.1433\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0133 - mse: 11.5790 - val_loss: 0.0171 - val_mse: 14.9323\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0122 - mse: 10.6014 - val_loss: 0.0160 - val_mse: 13.8961\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0107 - mse: 9.2967 - val_loss: 0.0161 - val_mse: 14.0184\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0108 - mse: 9.4482 - val_loss: 0.0159 - val_mse: 13.8626\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0110 - mse: 9.5414 - val_loss: 0.0153 - val_mse: 13.3669\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0102 - mse: 8.8521 - val_loss: 0.0158 - val_mse: 13.7576\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0105 - mse: 9.1068 - val_loss: 0.0153 - val_mse: 13.3197\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0101 - mse: 8.7955 - val_loss: 0.0151 - val_mse: 13.1246\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0099 - mse: 8.6040 - val_loss: 0.0153 - val_mse: 13.3587\n",
      "Epoch 132/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0100 - mse: 8.6981 - val_loss: 0.0148 - val_mse: 12.9040\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0096 - mse: 8.3177 - val_loss: 0.0149 - val_mse: 12.9654\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0097 - mse: 8.4349 - val_loss: 0.0150 - val_mse: 13.0240\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0094 - mse: 8.2211 - val_loss: 0.0148 - val_mse: 12.8529\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0093 - mse: 8.0568 - val_loss: 0.0146 - val_mse: 12.6748\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0093 - mse: 8.1097 - val_loss: 0.0143 - val_mse: 12.4651\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0090 - mse: 7.8093 - val_loss: 0.0143 - val_mse: 12.4481\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0089 - mse: 7.7921 - val_loss: 0.0140 - val_mse: 12.1869\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0089 - mse: 7.7218 - val_loss: 0.0138 - val_mse: 12.0280\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0086 - mse: 7.5010 - val_loss: 0.0138 - val_mse: 12.0531\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0086 - mse: 7.5088 - val_loss: 0.0135 - val_mse: 11.7341\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0085 - mse: 7.3852 - val_loss: 0.0134 - val_mse: 11.6503\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0083 - mse: 7.2330 - val_loss: 0.0135 - val_mse: 11.7161\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0083 - mse: 7.2130 - val_loss: 0.0131 - val_mse: 11.4118\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0081 - mse: 7.0895 - val_loss: 0.0130 - val_mse: 11.2951\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0080 - mse: 6.9503 - val_loss: 0.0130 - val_mse: 11.3090\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0079 - mse: 6.9075 - val_loss: 0.0127 - val_mse: 11.0813\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0078 - mse: 6.8079 - val_loss: 0.0127 - val_mse: 11.0380\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0077 - mse: 6.6685 - val_loss: 0.0126 - val_mse: 10.9716\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0076 - mse: 6.5943 - val_loss: 0.0123 - val_mse: 10.7401\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0075 - mse: 6.5257 - val_loss: 0.0123 - val_mse: 10.7047\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0074 - mse: 6.4119 - val_loss: 0.0121 - val_mse: 10.5328\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0072 - mse: 6.2999 - val_loss: 0.0119 - val_mse: 10.3706\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0071 - mse: 6.2239 - val_loss: 0.0119 - val_mse: 10.3391\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0071 - mse: 6.1523 - val_loss: 0.0116 - val_mse: 10.0814\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0069 - mse: 6.0510 - val_loss: 0.0115 - val_mse: 10.0028\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0068 - mse: 5.9393 - val_loss: 0.0113 - val_mse: 9.8429\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0067 - mse: 5.8439 - val_loss: 0.0111 - val_mse: 9.6654\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0066 - mse: 5.7621 - val_loss: 0.0111 - val_mse: 9.6253\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0065 - mse: 5.6863 - val_loss: 0.0108 - val_mse: 9.4003\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0064 - mse: 5.6124 - val_loss: 0.0109 - val_mse: 9.4623\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0064 - mse: 5.5540 - val_loss: 0.0106 - val_mse: 9.1943\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0064 - mse: 5.5301 - val_loss: 0.0109 - val_mse: 9.5021\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0064 - mse: 5.6089 - val_loss: 0.0107 - val_mse: 9.3339\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0068 - mse: 5.9093 - val_loss: 0.0124 - val_mse: 10.7726\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0078 - mse: 6.8220 - val_loss: 0.0120 - val_mse: 10.4724\n",
      "Epoch 168/1000\n",
      "\n",
      "Epoch 00167: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0086 - mse: 7.5013 - val_loss: 0.0129 - val_mse: 11.2467\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0085 - mse: 7.3978 - val_loss: 0.0101 - val_mse: 8.7670\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0059 - mse: 5.1148 - val_loss: 0.0115 - val_mse: 9.9787\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0080 - mse: 6.9941 - val_loss: 0.0099 - val_mse: 8.5960\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0057 - mse: 4.9972 - val_loss: 0.0116 - val_mse: 10.1390\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0073 - mse: 6.3573 - val_loss: 0.0097 - val_mse: 8.4099\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0056 - mse: 4.8912 - val_loss: 0.0104 - val_mse: 9.0509\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0069 - mse: 5.9915 - val_loss: 0.0095 - val_mse: 8.3027\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0055 - mse: 4.7967 - val_loss: 0.0108 - val_mse: 9.3667\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0064 - mse: 5.6031 - val_loss: 0.0096 - val_mse: 8.3610\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0055 - mse: 4.7564 - val_loss: 0.0098 - val_mse: 8.5130\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0061 - mse: 5.2787 - val_loss: 0.0094 - val_mse: 8.2054\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0054 - mse: 4.7047 - val_loss: 0.0101 - val_mse: 8.8226\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0057 - mse: 4.9806 - val_loss: 0.0097 - val_mse: 8.4632\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0054 - mse: 4.6923 - val_loss: 0.0094 - val_mse: 8.1601\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0054 - mse: 4.7150 - val_loss: 0.0093 - val_mse: 8.1281\n",
      "Epoch 184/1000\n",
      "\n",
      "Epoch 00183: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0053 - mse: 4.6418 - val_loss: 0.0096 - val_mse: 8.3466\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0052 - mse: 4.5180 - val_loss: 0.0096 - val_mse: 8.3947\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0052 - mse: 4.5271 - val_loss: 0.0093 - val_mse: 8.0936\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0050 - mse: 4.3498 - val_loss: 0.0092 - val_mse: 8.0308\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0051 - mse: 4.4232 - val_loss: 0.0092 - val_mse: 8.0181\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0050 - mse: 4.3435 - val_loss: 0.0093 - val_mse: 8.1071\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0049 - mse: 4.2707 - val_loss: 0.0094 - val_mse: 8.1869\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0049 - mse: 4.2968 - val_loss: 0.0092 - val_mse: 8.0329\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0048 - mse: 4.2144 - val_loss: 0.0090 - val_mse: 7.8794\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0048 - mse: 4.1791 - val_loss: 0.0090 - val_mse: 7.8717\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0048 - mse: 4.1789 - val_loss: 0.0091 - val_mse: 7.8836\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0047 - mse: 4.1018 - val_loss: 0.0091 - val_mse: 7.9222\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0047 - mse: 4.0937 - val_loss: 0.0090 - val_mse: 7.8686\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0047 - mse: 4.0708 - val_loss: 0.0089 - val_mse: 7.7282\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0046 - mse: 4.0021 - val_loss: 0.0089 - val_mse: 7.7230\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0046 - mse: 4.0151 - val_loss: 0.0089 - val_mse: 7.7123\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0045 - mse: 3.9536 - val_loss: 0.0089 - val_mse: 7.7316\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0045 - mse: 3.9275 - val_loss: 0.0089 - val_mse: 7.7227\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0045 - mse: 3.9185 - val_loss: 0.0088 - val_mse: 7.6199\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0044 - mse: 3.8524 - val_loss: 0.0087 - val_mse: 7.6091\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0044 - mse: 3.8607 - val_loss: 0.0087 - val_mse: 7.5820\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0044 - mse: 3.8042 - val_loss: 0.0087 - val_mse: 7.6100\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0043 - mse: 3.7841 - val_loss: 0.0087 - val_mse: 7.6153\n",
      "Epoch 207/1000\n",
      "\n",
      "Epoch 00206: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0043 - mse: 3.7632 - val_loss: 0.0087 - val_mse: 7.5582\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0043 - mse: 3.7150 - val_loss: 0.0087 - val_mse: 7.5374\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0043 - mse: 3.7071 - val_loss: 0.0086 - val_mse: 7.5116\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0042 - mse: 3.6804 - val_loss: 0.0086 - val_mse: 7.5096\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0042 - mse: 3.6573 - val_loss: 0.0086 - val_mse: 7.5225\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0042 - mse: 3.6440 - val_loss: 0.0086 - val_mse: 7.5110\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0042 - mse: 3.6207 - val_loss: 0.0086 - val_mse: 7.4798\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0041 - mse: 3.5986 - val_loss: 0.0086 - val_mse: 7.4482\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0041 - mse: 3.5822 - val_loss: 0.0085 - val_mse: 7.4315\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0041 - mse: 3.5619 - val_loss: 0.0085 - val_mse: 7.4376\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0041 - mse: 3.5398 - val_loss: 0.0086 - val_mse: 7.4519\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0040 - mse: 3.5224 - val_loss: 0.0085 - val_mse: 7.4453\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0040 - mse: 3.5039 - val_loss: 0.0085 - val_mse: 7.4151\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0040 - mse: 3.4816 - val_loss: 0.0085 - val_mse: 7.3939\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0040 - mse: 3.4660 - val_loss: 0.0085 - val_mse: 7.3923\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0040 - mse: 3.4466 - val_loss: 0.0085 - val_mse: 7.4054\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0039 - mse: 3.4258 - val_loss: 0.0085 - val_mse: 7.4143\n",
      "Epoch 224/1000\n",
      "\n",
      "Epoch 00223: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0039 - mse: 3.4101 - val_loss: 0.0085 - val_mse: 7.3954\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0039 - mse: 3.3897 - val_loss: 0.0085 - val_mse: 7.3746\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0039 - mse: 3.3758 - val_loss: 0.0084 - val_mse: 7.3586\n",
      "Epoch 227/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0039 - mse: 3.3639 - val_loss: 0.0084 - val_mse: 7.3523\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0038 - mse: 3.3508 - val_loss: 0.0084 - val_mse: 7.3547\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0038 - mse: 3.3370 - val_loss: 0.0085 - val_mse: 7.3588\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0038 - mse: 3.3244 - val_loss: 0.0084 - val_mse: 7.3563\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0038 - mse: 3.3120 - val_loss: 0.0084 - val_mse: 7.3451\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0038 - mse: 3.2989 - val_loss: 0.0084 - val_mse: 7.3309\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0038 - mse: 3.2859 - val_loss: 0.0084 - val_mse: 7.3206\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0038 - mse: 3.2734 - val_loss: 0.0084 - val_mse: 7.3172\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0037 - mse: 3.2608 - val_loss: 0.0084 - val_mse: 7.3179\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0037 - mse: 3.2477 - val_loss: 0.0084 - val_mse: 7.3182\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0037 - mse: 3.2349 - val_loss: 0.0084 - val_mse: 7.3144\n",
      "Epoch 238/1000\n",
      "\n",
      "Epoch 00237: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0037 - mse: 3.2226 - val_loss: 0.0084 - val_mse: 7.3053\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0037 - mse: 3.2100 - val_loss: 0.0084 - val_mse: 7.2971\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0037 - mse: 3.2010 - val_loss: 0.0084 - val_mse: 7.2897\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0037 - mse: 3.1925 - val_loss: 0.0084 - val_mse: 7.2846\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0037 - mse: 3.1841 - val_loss: 0.0084 - val_mse: 7.2821\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.0036 - mse: 3.1754 - val_loss: 0.0084 - val_mse: 7.2811\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0036 - mse: 3.1667 - val_loss: 0.0084 - val_mse: 7.2803\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0036 - mse: 3.1582 - val_loss: 0.0084 - val_mse: 7.2782\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0036 - mse: 3.1497 - val_loss: 0.0084 - val_mse: 7.2738\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0036 - mse: 3.1411 - val_loss: 0.0083 - val_mse: 7.2676\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0036 - mse: 3.1325 - val_loss: 0.0083 - val_mse: 7.2609\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0036 - mse: 3.1241 - val_loss: 0.0083 - val_mse: 7.2549\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0036 - mse: 3.1155 - val_loss: 0.0083 - val_mse: 7.2509\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0036 - mse: 3.1069 - val_loss: 0.0083 - val_mse: 7.2490\n",
      "Epoch 252/1000\n",
      "\n",
      "Epoch 00251: reducing learning rate to 0.000823543069419.\n",
      "1s - loss: 0.0036 - mse: 3.0985 - val_loss: 0.0083 - val_mse: 7.2479\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0035 - mse: 3.0901 - val_loss: 0.0083 - val_mse: 7.2469\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.0035 - mse: 3.0842 - val_loss: 0.0083 - val_mse: 7.2449\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0035 - mse: 3.0783 - val_loss: 0.0083 - val_mse: 7.2419\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0035 - mse: 3.0723 - val_loss: 0.0083 - val_mse: 7.2378\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.0035 - mse: 3.0664 - val_loss: 0.0083 - val_mse: 7.2333\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0035 - mse: 3.0604 - val_loss: 0.0083 - val_mse: 7.2294\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.0035 - mse: 3.0545 - val_loss: 0.0083 - val_mse: 7.2265\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0035 - mse: 3.0486 - val_loss: 0.0083 - val_mse: 7.2244\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.0035 - mse: 3.0428 - val_loss: 0.0083 - val_mse: 7.2224\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.0035 - mse: 3.0369 - val_loss: 0.0083 - val_mse: 7.2201\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0035 - mse: 3.0311 - val_loss: 0.0083 - val_mse: 7.2168\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.0035 - mse: 3.0253 - val_loss: 0.0083 - val_mse: 7.2128\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0035 - mse: 3.0195 - val_loss: 0.0083 - val_mse: 7.2083\n",
      "Epoch 266/1000\n",
      "\n",
      "Epoch 00265: reducing learning rate to 0.000576480140444.\n",
      "1s - loss: 0.0035 - mse: 3.0137 - val_loss: 0.0083 - val_mse: 7.2038\n",
      "Epoch 267/1000\n",
      "1s - loss: 0.0035 - mse: 3.0078 - val_loss: 0.0083 - val_mse: 7.2010\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0034 - mse: 3.0038 - val_loss: 0.0083 - val_mse: 7.1987\n",
      "Epoch 269/1000\n",
      "1s - loss: 0.0034 - mse: 2.9996 - val_loss: 0.0083 - val_mse: 7.1970\n",
      "Epoch 270/1000\n",
      "1s - loss: 0.0034 - mse: 2.9956 - val_loss: 0.0083 - val_mse: 7.1955\n",
      "Epoch 271/1000\n",
      "1s - loss: 0.0034 - mse: 2.9915 - val_loss: 0.0083 - val_mse: 7.1940\n",
      "Epoch 272/1000\n",
      "1s - loss: 0.0034 - mse: 2.9874 - val_loss: 0.0083 - val_mse: 7.1921\n",
      "Epoch 273/1000\n",
      "1s - loss: 0.0034 - mse: 2.9832 - val_loss: 0.0083 - val_mse: 7.1900\n",
      "Epoch 274/1000\n",
      "1s - loss: 0.0034 - mse: 2.9791 - val_loss: 0.0083 - val_mse: 7.1879\n",
      "Epoch 275/1000\n",
      "1s - loss: 0.0034 - mse: 2.9750 - val_loss: 0.0083 - val_mse: 7.1857\n",
      "Epoch 276/1000\n",
      "1s - loss: 0.0034 - mse: 2.9709 - val_loss: 0.0082 - val_mse: 7.1835\n",
      "Epoch 277/1000\n",
      "1s - loss: 0.0034 - mse: 2.9668 - val_loss: 0.0082 - val_mse: 7.1811\n",
      "Epoch 278/1000\n",
      "1s - loss: 0.0034 - mse: 2.9627 - val_loss: 0.0082 - val_mse: 7.1787\n",
      "Epoch 279/1000\n",
      "1s - loss: 0.0034 - mse: 2.9585 - val_loss: 0.0082 - val_mse: 7.1763\n",
      "Epoch 280/1000\n",
      "\n",
      "Epoch 00279: reducing learning rate to 0.000403536082013.\n",
      "1s - loss: 0.0034 - mse: 2.9544 - val_loss: 0.0082 - val_mse: 7.1740\n",
      "Epoch 281/1000\n",
      "1s - loss: 0.0034 - mse: 2.9503 - val_loss: 0.0082 - val_mse: 7.1724\n",
      "Epoch 282/1000\n",
      "1s - loss: 0.0034 - mse: 2.9474 - val_loss: 0.0082 - val_mse: 7.1710\n",
      "Epoch 283/1000\n",
      "1s - loss: 0.0034 - mse: 2.9445 - val_loss: 0.0082 - val_mse: 7.1696\n",
      "Epoch 284/1000\n",
      "1s - loss: 0.0034 - mse: 2.9416 - val_loss: 0.0082 - val_mse: 7.1680\n",
      "Epoch 285/1000\n",
      "1s - loss: 0.0034 - mse: 2.9387 - val_loss: 0.0082 - val_mse: 7.1666\n",
      "Epoch 286/1000\n",
      "1s - loss: 0.0034 - mse: 2.9359 - val_loss: 0.0082 - val_mse: 7.1652\n",
      "Epoch 287/1000\n",
      "1s - loss: 0.0034 - mse: 2.9330 - val_loss: 0.0082 - val_mse: 7.1638\n",
      "Epoch 288/1000\n",
      "1s - loss: 0.0034 - mse: 2.9301 - val_loss: 0.0082 - val_mse: 7.1624\n",
      "Epoch 289/1000\n",
      "1s - loss: 0.0034 - mse: 2.9272 - val_loss: 0.0082 - val_mse: 7.1611\n",
      "Epoch 290/1000\n",
      "1s - loss: 0.0034 - mse: 2.9243 - val_loss: 0.0082 - val_mse: 7.1598\n",
      "Epoch 291/1000\n",
      "1s - loss: 0.0034 - mse: 2.9214 - val_loss: 0.0082 - val_mse: 7.1587\n",
      "Epoch 292/1000\n",
      "1s - loss: 0.0034 - mse: 2.9186 - val_loss: 0.0082 - val_mse: 7.1577\n",
      "Epoch 293/1000\n",
      "1s - loss: 0.0033 - mse: 2.9157 - val_loss: 0.0082 - val_mse: 7.1565\n",
      "Epoch 294/1000\n",
      "\n",
      "Epoch 00293: reducing learning rate to 0.000282475253334.\n",
      "1s - loss: 0.0033 - mse: 2.9128 - val_loss: 0.0082 - val_mse: 7.1551\n",
      "Epoch 295/1000\n",
      "1s - loss: 0.0033 - mse: 2.9100 - val_loss: 0.0082 - val_mse: 7.1542\n",
      "Epoch 296/1000\n",
      "1s - loss: 0.0033 - mse: 2.9080 - val_loss: 0.0082 - val_mse: 7.1533\n",
      "Epoch 297/1000\n",
      "1s - loss: 0.0033 - mse: 2.9060 - val_loss: 0.0082 - val_mse: 7.1526\n",
      "Epoch 298/1000\n",
      "1s - loss: 0.0033 - mse: 2.9040 - val_loss: 0.0082 - val_mse: 7.1519\n",
      "Epoch 299/1000\n",
      "1s - loss: 0.0033 - mse: 2.9020 - val_loss: 0.0082 - val_mse: 7.1512\n",
      "Epoch 300/1000\n",
      "1s - loss: 0.0033 - mse: 2.9001 - val_loss: 0.0082 - val_mse: 7.1505\n",
      "Epoch 301/1000\n",
      "1s - loss: 0.0033 - mse: 2.8981 - val_loss: 0.0082 - val_mse: 7.1497\n",
      "Epoch 302/1000\n",
      "1s - loss: 0.0033 - mse: 2.8962 - val_loss: 0.0082 - val_mse: 7.1489\n",
      "Epoch 303/1000\n",
      "1s - loss: 0.0033 - mse: 2.8942 - val_loss: 0.0082 - val_mse: 7.1480\n",
      "Epoch 304/1000\n",
      "1s - loss: 0.0033 - mse: 2.8923 - val_loss: 0.0082 - val_mse: 7.1473\n",
      "Epoch 305/1000\n",
      "1s - loss: 0.0033 - mse: 2.8903 - val_loss: 0.0082 - val_mse: 7.1465\n",
      "Epoch 306/1000\n",
      "1s - loss: 0.0033 - mse: 2.8884 - val_loss: 0.0082 - val_mse: 7.1458\n",
      "Epoch 307/1000\n",
      "1s - loss: 0.0033 - mse: 2.8865 - val_loss: 0.0082 - val_mse: 7.1450\n",
      "Epoch 308/1000\n",
      "1s - loss: 0.0033 - mse: 2.8845 - val_loss: 0.0082 - val_mse: 7.1441\n",
      "Epoch 309/1000\n",
      "1s - loss: 0.0033 - mse: 2.8826 - val_loss: 0.0082 - val_mse: 7.1431\n",
      "Epoch 310/1000\n",
      "1s - loss: 0.0033 - mse: 2.8806 - val_loss: 0.0082 - val_mse: 7.1423\n",
      "Epoch 311/1000\n",
      "\n",
      "Epoch 00310: reducing learning rate to 0.000197732681409.\n",
      "1s - loss: 0.0033 - mse: 2.8787 - val_loss: 0.0082 - val_mse: 7.1415\n",
      "Epoch 312/1000\n",
      "1s - loss: 0.0033 - mse: 2.8768 - val_loss: 0.0082 - val_mse: 7.1409\n",
      "Epoch 313/1000\n",
      "1s - loss: 0.0033 - mse: 2.8754 - val_loss: 0.0082 - val_mse: 7.1403\n",
      "Epoch 314/1000\n",
      "1s - loss: 0.0033 - mse: 2.8740 - val_loss: 0.0082 - val_mse: 7.1397\n",
      "Epoch 315/1000\n",
      "1s - loss: 0.0033 - mse: 2.8727 - val_loss: 0.0082 - val_mse: 7.1391\n",
      "Epoch 316/1000\n",
      "1s - loss: 0.0033 - mse: 2.8713 - val_loss: 0.0082 - val_mse: 7.1385\n",
      "Epoch 317/1000\n",
      "1s - loss: 0.0033 - mse: 2.8700 - val_loss: 0.0082 - val_mse: 7.1380\n",
      "Epoch 318/1000\n",
      "1s - loss: 0.0033 - mse: 2.8686 - val_loss: 0.0082 - val_mse: 7.1374\n",
      "Epoch 319/1000\n",
      "1s - loss: 0.0033 - mse: 2.8672 - val_loss: 0.0082 - val_mse: 7.1368\n",
      "Epoch 320/1000\n",
      "1s - loss: 0.0033 - mse: 2.8659 - val_loss: 0.0082 - val_mse: 7.1361\n",
      "Epoch 321/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0033 - mse: 2.8645 - val_loss: 0.0082 - val_mse: 7.1356\n",
      "Epoch 322/1000\n",
      "1s - loss: 0.0033 - mse: 2.8631 - val_loss: 0.0082 - val_mse: 7.1351\n",
      "Epoch 323/1000\n",
      "1s - loss: 0.0033 - mse: 2.8617 - val_loss: 0.0082 - val_mse: 7.1347\n",
      "Epoch 324/1000\n",
      "1s - loss: 0.0033 - mse: 2.8604 - val_loss: 0.0082 - val_mse: 7.1343\n",
      "Epoch 325/1000\n",
      "\n",
      "Epoch 00324: reducing learning rate to 0.000138412872911.\n",
      "1s - loss: 0.0033 - mse: 2.8590 - val_loss: 0.0082 - val_mse: 7.1338\n",
      "Epoch 326/1000\n",
      "1s - loss: 0.0033 - mse: 2.8576 - val_loss: 0.0082 - val_mse: 7.1335\n",
      "Epoch 327/1000\n",
      "1s - loss: 0.0033 - mse: 2.8567 - val_loss: 0.0082 - val_mse: 7.1331\n",
      "Epoch 328/1000\n",
      "1s - loss: 0.0033 - mse: 2.8557 - val_loss: 0.0082 - val_mse: 7.1326\n",
      "Epoch 329/1000\n",
      "1s - loss: 0.0033 - mse: 2.8547 - val_loss: 0.0082 - val_mse: 7.1322\n",
      "Epoch 330/1000\n",
      "1s - loss: 0.0033 - mse: 2.8538 - val_loss: 0.0082 - val_mse: 7.1319\n",
      "Epoch 331/1000\n",
      "1s - loss: 0.0033 - mse: 2.8528 - val_loss: 0.0082 - val_mse: 7.1315\n",
      "Epoch 332/1000\n",
      "1s - loss: 0.0033 - mse: 2.8519 - val_loss: 0.0082 - val_mse: 7.1311\n",
      "Epoch 333/1000\n",
      "1s - loss: 0.0033 - mse: 2.8509 - val_loss: 0.0082 - val_mse: 7.1308\n",
      "Epoch 334/1000\n",
      "1s - loss: 0.0033 - mse: 2.8500 - val_loss: 0.0082 - val_mse: 7.1305\n",
      "Epoch 335/1000\n",
      "1s - loss: 0.0033 - mse: 2.8490 - val_loss: 0.0082 - val_mse: 7.1302\n",
      "Epoch 336/1000\n",
      "1s - loss: 0.0033 - mse: 2.8480 - val_loss: 0.0082 - val_mse: 7.1299\n",
      "Epoch 337/1000\n",
      "1s - loss: 0.0033 - mse: 2.8471 - val_loss: 0.0082 - val_mse: 7.1295\n",
      "Epoch 338/1000\n",
      "1s - loss: 0.0033 - mse: 2.8461 - val_loss: 0.0082 - val_mse: 7.1291\n",
      "Epoch 339/1000\n",
      "\n",
      "Epoch 00338: reducing learning rate to 9.68890090007e-05.\n",
      "1s - loss: 0.0033 - mse: 2.8452 - val_loss: 0.0082 - val_mse: 7.1287\n",
      "Epoch 340/1000\n",
      "1s - loss: 0.0033 - mse: 2.8442 - val_loss: 0.0082 - val_mse: 7.1284\n",
      "Epoch 341/1000\n",
      "1s - loss: 0.0033 - mse: 2.8436 - val_loss: 0.0082 - val_mse: 7.1282\n",
      "Epoch 342/1000\n",
      "1s - loss: 0.0033 - mse: 2.8429 - val_loss: 0.0082 - val_mse: 7.1279\n",
      "Epoch 343/1000\n",
      "1s - loss: 0.0033 - mse: 2.8422 - val_loss: 0.0082 - val_mse: 7.1278\n",
      "Epoch 344/1000\n",
      "1s - loss: 0.0033 - mse: 2.8416 - val_loss: 0.0082 - val_mse: 7.1275\n",
      "Epoch 345/1000\n",
      "1s - loss: 0.0033 - mse: 2.8409 - val_loss: 0.0082 - val_mse: 7.1273\n",
      "Epoch 346/1000\n",
      "1s - loss: 0.0033 - mse: 2.8402 - val_loss: 0.0082 - val_mse: 7.1270\n",
      "Epoch 347/1000\n",
      "1s - loss: 0.0033 - mse: 2.8396 - val_loss: 0.0082 - val_mse: 7.1268\n",
      "Epoch 348/1000\n",
      "1s - loss: 0.0033 - mse: 2.8389 - val_loss: 0.0082 - val_mse: 7.1266\n",
      "Epoch 349/1000\n",
      "1s - loss: 0.0033 - mse: 2.8382 - val_loss: 0.0082 - val_mse: 7.1264\n",
      "Epoch 350/1000\n",
      "1s - loss: 0.0033 - mse: 2.8376 - val_loss: 0.0082 - val_mse: 7.1262\n",
      "Epoch 351/1000\n",
      "1s - loss: 0.0033 - mse: 2.8369 - val_loss: 0.0082 - val_mse: 7.1259\n",
      "Epoch 352/1000\n",
      "1s - loss: 0.0033 - mse: 2.8363 - val_loss: 0.0082 - val_mse: 7.1256\n",
      "Epoch 353/1000\n",
      "\n",
      "Epoch 00352: reducing learning rate to 6.78223063005e-05.\n",
      "1s - loss: 0.0033 - mse: 2.8356 - val_loss: 0.0082 - val_mse: 7.1253\n",
      "Epoch 354/1000\n",
      "1s - loss: 0.0033 - mse: 2.8350 - val_loss: 0.0082 - val_mse: 7.1251\n",
      "Epoch 355/1000\n",
      "1s - loss: 0.0033 - mse: 2.8345 - val_loss: 0.0082 - val_mse: 7.1250\n",
      "Epoch 356/1000\n",
      "1s - loss: 0.0033 - mse: 2.8340 - val_loss: 0.0082 - val_mse: 7.1248\n",
      "Epoch 357/1000\n",
      "1s - loss: 0.0033 - mse: 2.8336 - val_loss: 0.0082 - val_mse: 7.1246\n",
      "[{'loss': 0.002089665038511157, 'run': 0, 'width0': 135, 'width1': 150, 'val_mse': 7.7888374328613281, 'mse': 1.8197623491287231, 'val_loss': 0.0089440615847706795, 'dropout1': 0, 'dropout0': 0}, {'loss': 0.00044063656241632998, 'run': 1, 'width0': 135, 'width1': 150, 'val_mse': 5.2644405364990234, 'mse': 0.38372343778610229, 'val_loss': 0.0060452516190707684, 'dropout1': 0, 'dropout0': 0}, {'loss': 0.0032538396771997213, 'run': 2, 'width0': 135, 'width1': 150, 'val_mse': 7.1246442794799805, 'mse': 2.8335700035095215, 'val_loss': 0.0081813549622893333, 'dropout1': 0, 'dropout0': 0}]\n"
     ]
    }
   ],
   "source": [
    "width1 = 135\n",
    "width2 = 150\n",
    "rerun_perf = []\n",
    "prev_weights = []\n",
    "for run in range(3):\n",
    "    fit, results, prev_weights = run_depthn([{'width':width1, 'dropout':0},\n",
    "                                             {'width':width2, 'dropout':0}\n",
    "                                            ], learningrates3, None, 'final2nd') # No prev_weights\n",
    "    results['run']=run\n",
    "    rerun_perf.append(results)\n",
    "    print rerun_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7.788837\n",
      "1    5.264441\n",
      "2    7.124644\n",
      "Name: val_mse, dtype: float64\n",
      "6.72597408295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1669520d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAENCAYAAADwjPVkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl81OW58P/Pd2aSyTJJJslkIYFAyEIStgBhkUVAwiIq\nWmqpa0XrOW3Vpy97TrX29Hmqp9WeVKW2tljbinpOf21/2qdoixsY2VdJwk7AJAQEAoQsk32bmfv5\n4xsGqEsGmCWZXO/Xy5dk5jszV76ZXLnm/t73dWtKKYUQQoigYAh0AEIIIbxHkroQQgQRSepCCBFE\nJKkLIUQQkaQuhBBBRJK6EEIEEUnqQggRRCSpCyFEEJGkLoQQQUSSuhBCBBGTJwe98847rF+/Hk3T\nGDZsGA899BChoaFf+piamhqvBOgrNpuNurq6QIfRJ4nTuyRO75I4vSclJcUrz9Nnpd7Q0MD7779P\nUVERK1aswOVysX37dq+8uBBCCO/yaPjF5XLR3d2N0+mku7ub2NhYX8clhBDiKvQ5/BIXF8ctt9zC\nd77zHUJDQxk/fjzjx4/3R2xCCCGukNZX693W1lZWrFjB9773PSIiIvjFL37BtGnTuP766y87rri4\nmOLiYgCKioro7u72XdReYDKZcDgcgQ6jTxKnd0mc3iVxek9f1yk91WelfuDAARITE4mOjgZg6tSp\nfPLJJ59J6oWFhRQWFrq/7u8XJQbChROQOL1N4vQuidN7/Hah1GazUVFRQVdXF0opDhw4QGpqqlde\nXAghhHf1WalnZWUxbdo0fvCDH2A0GhkxYsRlFbkQQoj+w6N56suWLWPZsmW+jkUIIcQ18tmK0n9/\n/zjrKu109Lh89RJCCCH+ic+SerfTxcpdZ7l/dSUvf3yW6sZOX72UEEKIXh4Nv1yNF29K58j5Dj6o\ntFNc1cT7FXay48NYmGVl1vBozCZpOyOEEN7ms6SuaRq5iRHkJkbw4CQnG6qbWFth59c7z/JqaS1z\n0qNZmBXLcKvZVyEIIcSg47Okfqkos5ElOXHcMiqWw7V69b62sol3P7GTYwtnYZaVGWlRUr0LIcQ1\n8ktSv0DTNEYnRTA6KYJ/meRgfXUTayua+NWOM6wqPcfckTEszLQyLEaqdyGEuBp+TeqXig4zcVtu\nPLfmxHHgXDtrK+28/0kja440MjoxnIWZVqanRRFilOpdCCE8FbCkfoGmaYxLjmRcciT2Tgfrq5pY\nW2nnF9vP8IfSWuaNjGFBppXUaO/0RRBCiGAW8KR+KWuYiaWj47ktL479Z/Xqfc2RBt4ub2BsUgQL\nM61MGxZFiFELdKhCCNEv9aukfoFB08gfEkn+kEgaOxwUV9lZV9nE89tqiDEbmZehV+9DoqR6F0KI\nS/XLpH6p2HATXxtj46uj49l7po0PKuy8Xd7A6sMNjE+OYGGWlalDozAZpHoXQoh+n9QvMGgaE1Ms\nTEyxUN/eQ3FVE+sq7Ty7pQZrmJHCDCsLMmNIskj1LoQYvAZMUr9UfEQIXx9r4/bR8ezprd5XH67n\nb4fqyR8SycIsK1NSLRilehdCDDIDMqlfYDRoFKRaKEi1cL6th+IqOx9WNlG0+TSx4Sbm9469J0SG\nBDpUIYTwiwGd1C+VEBnCneMSWDbGRklNK2sr7Pz1YD1/PVjPpBS9ep+UItW7ECK4BU1Sv8Bo0Jg6\nNIqpQ6M419rNh5VNFFfZeWbTaeIjTCzIsFKYGYMt0IEKIYQPBF1Sv1SSJZR78hO4Y5yN3ada+aDS\nzl8O1PHGwTqmpzcyNy2CCUMipXoXQgSNoE7qF5gMGtelRXFdWhRnW7pZV2lnfXULW481kBhpYn6G\nlcJMK3Hhg+J0CCH6iR6novx8O7tPt/J/vLTxdJ9ZrKamhhdeeMH9dW1tLcuWLeOmm27ySgD+lhwV\nyjcmJPK/bsjhvb3H+aDSzp/21/GXA3VMGWphYaaV/CGRGDSp3oUQ3tfU6aC0po2S063sOdNGe48L\nk0Hj/3jp+ftM6ikpKTz33HMAuFwuvvWtbzFlyhQvvXzghBgNzBgezYzh0dQ069X7R8ea2HmylSRL\niD72nhGDVap3IcQ1UEpxwt7F7tOt7D7dxid1HSggNszIjLQoJqdaGJcc6bXXu6KMdeDAAZKTk0lI\nSPBaAP1BSnQoyycmcvd4GztOtrK20s4f953nz/vPM21YFAuzrIxNipDqXQjhkS6HiwPn2ik53cru\n063UtTsAyIwL446xNgpSLYyMM/skp2hKKeXpwS+99BIjR45k0aJFn7mvuLiY4uJiAIqKiuju7vZe\nlD5gMplwOBxfeP+Jxnb+ceAs75XX0tzpYGhMGEvGJrM4N4nYCP/Ne+8rzv5C4vQuidO7/BFnbUsX\n2483sL26gZKTTXQ5XISHGJicZmX6iDiuS4/DFvnFK95DQ72zGt7jpO5wOPjWt77FihUrsFqtfR5f\nU1NzzcH5ks1mo66urs/jup0utn/awtoKO4fPd+gXXYdZWJhlZUxiBJqPq3dP4ww0idO7JE7v8kWc\nLqWoqO90V+PVjV0AJEaGMDk1koJUC2OSIgj1cE+IFH9dKL1gz549pKene5TQg0mo0cCc9BjmpMfw\naVMX6yrsrK9uYsuJFlKjQ1mYaWXuyBiizcZAhyqE8LH2Hid7z7Sx+3QbpTWtNHU6MWiQYwvnvvwE\nClItDIsJ9Xmx92U8Turbtm1jxowZvoyl30uLMfNgQRL35iew7dMWPqiw82pZLX/ce57pafrYe15C\neEB/oEII7zrT0u2uxg/VtuNwQWSogUlDLBSkRjIhxdKvijqPknpXVxf79+/nX//1X30dz4BgNhm4\nYWQMN4yM4XhjJ2sr7WysbmbT8WaGxfRW7+kxWPrRD1oI4RmHS3HkfAe7T7dScrqVU8369cGh0aHc\nMiqOyakWchLC++2ixSu6UHolgmVM3VOdDhdbTzTzQYWdivpOQo0aM3qr9xzb1Vfvg3nM0hckTu8K\nljibu5yU1ejV+J4zbbR1uzAZYExihLtpoK835fH7mLr4cmEmA4UZVgozrBxr0Kv3TdXNbKhuZrjV\nzMJMK3PSo4kMlepdiEBTSvFpU7e7Gj9a14FLgTXMyHXDoihItTA+OYKIkIH3+ypJ3QdGxoXxnSnJ\nLJ+QyJbe6v33Jed4fU8ts4ZHszDLSnZ8mIy9C+FH3U4XB8+1uxN5bZs+xTEjzszXxsQzOdVCRlzY\ngF+PIkndh8JDDCzItLIg00plfSdrKxvZfLyZj441kR6rV++z06MHZDUgxEBQ395DaU0b+3fU8vGJ\nRrqcilCjvgfy18ZYmJQSSbwf1534gyR1P8mMDyMzfgj3T0xkU3UzayvtvLxbr96vHxHNwsxYMuPD\nAh2mEAOaSymqGi7MHW+jqqETgKQoMzeMjGFy79xxs8mzueMDkSR1P4sIMXJjdiyLsqxU1F+cObOu\nsomMuDAWZVmZNTya8JDgfdMJ4U0dPS72ntUbZJWcbsXeO3d8lC2ce8cnUJAayaTMVOrr6wMdql9I\nUg8QTdPItoWTbQvngYmJbOyt3lfuOsurpbXMTo9mYaYVm+zmIcRnnGvtdjfIOniuHYdLERliYEJK\nJJNTLUwcEkl02MX0NpiuX0lS7wciQ43cNCqWxdlWjtR1sLbCzvpjTXxQYSevrI556RZmDo8mLIg/\nMgrxZZwuxZG6DvcioJNN+tzx1OhQbh4VS0FqJLkJEZj66dxxf5Kk3o9omkZuQgS5CRE8OMnJhuom\niqtb+PXOs6wqrWVOb/U+IlbG3kXwa+lysudMG7tPt1JW00prtwujBqOTIliQaaUgxUJKtG/njg9E\nktT7KYvZyC05cSyfkcXm8pOsrbDzYWUT731iZ5QtnEVZVmakRQX1BR8xuCilONV8ce54+Xl97niM\n2ciUofoCoPzkSFnr0QdJ6v2cpmmMToxgdGIEDxY42dA7LPOrHWdYVXqOuekxLMyyMizGHOhQhbhi\nPU4XB2svLsk/19oDQHqsma/mxTN5qIXMuLB+uyS/P5KkPoBEm43cmhvHkpxYDta2s7bCzvsVjaw5\n2kheQjgLs6xMT4vyuNWnEIHQ2OGgtHdJ/t4zbXQ69Lnj45MjWJoXx6QUCwmRwTV33J8kqQ9AmqYx\nNimSsUmR2DsdrK9qYm2lnRe2n+GV0lpuSI9mQZaVodFSvYvAU0pxrLHLXY1X1Otzx+MjTMxJ1+eO\njw3yueP+JEl9gLOGmVg6Op7b8uLYf7adtZV23jnayN+PNDImKYKFmVauG2YhRKp34UedDhf73HPH\n22jocKAB2bYw7h5vY3KqhRFW86CaaugvktSDhEHTlz7nD4mkscPBR1VNrKuys2JbDdFmI/NG6mPv\nvu40Jwav2tYeSmr0anz/2XZ6XIpw0yVzx1MisYZJyvE1OcNBKDbcxO1j4lk6Oo69Z9pYW2nn70ca\neKu8gXHJESzKtDJlaBQhRqmSxNVzuhT7a5opPlRLyek2TjTp27kNiQphUbaVyakW8hIi5H3mZ5LU\ng5hB05iYYmFiioX69h69eq+08+zWGqxhevW+INNKslTvwkOt3U721OjDKqVn2mjpcmLUIC8xggcy\nEilItZAqc8cDSpL6IBEfEcKysTa+OjqePb3V+1vlDaw+3MD4IZEsytIrK1mRJy6llOK0ezu3Ng7X\ntuNSEGU2MiklkhtyhpBhcWGRueP9hkdJva2tjZdffpmTJ0+iaRrf+c53yM7O9nVswgeMBs29k0td\new/FlfrYe9Hm08SGm5ifEcP8DCuJFplSNlj1OBWHz1/sO36mRZ87PtxqZmlePAWpkWTH69u5DZSd\njwYTj5L6a6+9Rn5+Pv/+7/+Ow+Ggq6vL13EJP7BFhHDHOBtfGxNPaU0rayvs/PVgPX89WM/ElEgW\nZulLsWXhR/Czdzooq9GX5O+paaPD4SLEoDEuOYIlOXEUpFjkD/0A0WdSb29vp7y8nIcfflh/gMmE\nySSjNsHEaNCYMjSKKUOjON/Ww7pKOx9WNfGzTaeJDzcxPzOG+ZlWbEG2mcBgppSiurHL3SCror4T\nBcSFm7h+RDSTUiMZnxwpTeQGoD6zc21tLdHR0bz00kucOHGCkSNHsnz5csLCpKlUMEqIDOHu8Qnc\nMdbG7tOtfFBh540D9bx5sJ5JKRYWZVmZMCRSqvcBqMvhYv/Z3mGVmlbq2/Xt3LLiw7hznD53PD1W\n5o4PdJpSSn3ZAVVVVfzoRz/ipz/9KVlZWbz22muEh4dzxx13XHZccXExxcXFABQVFdHd3e27qL3A\nZDLhcDgCHUaf+kOcNU2d/OPgWd49fI6G9h6SoszcMjqJm0cnkWAx95s4PTHY4jzX0sX26ga2VzdQ\ncrKJbqeL8BAjU9KszEiPY9qIWOIjr362ymA7n74UGuqdWUN9JnW73c6PfvQjVq5cCUB5eTlvv/02\nP/zhD7/0iWtqarwSoK8MlAs8/SlOh0ux61QLayvs7DvbjkGDyal69V44djgNA2Bnmf50Pr/M1cbp\ndCkqGzrZfUqvxqsb9etfyZYQClItTE61MDox3GsrjIP9fPpTSkqKV56nz+EXq9VKfHw8NTU1pKSk\ncODAAYYOHeqVFxcDi8mgMSMtmhlp0Zxp6WZdpZ2PqprYdaqV35eeZ156FIUZVmLD5ZqLP7X36H3H\nS063Unq6jaYufTu3vIRwlk9IYHLv3HEZVhkcPPrte+CBB3jxxRdxOBwkJiby0EMP+Tou0c8NiQrl\nvgmJ3DXOxs6Traw/0cb/t6+Ov+yvY8rQKBZlWRmXHIFBEolP1FzSd/xQbTtOBZZQA5NS9OmqE4dE\nYjHL3PHByKOkPmLECIqKinwdixiAQowGZo2I5isFI9l3rEav3o81seNkC8mWEBZkWpmXESM9P66R\nw6U4XNvuXgRU06Jfs0qLCeXW3Dgmp1oYZQuXC9hCVpQK70mNDuX+iYncPd7Gjk9bWFtp53/2nufP\n+88zbVgUCzOtjE2KkGEADzV3Oii9MHf8TBvtPS5MBo2xSRHufTmTLLIkX1xOkrrwulCjgdnpMcxO\nj+FkUxdrK+1sONbE1hMtpESFsjArhhvSYy7b7V3oc8dP2LsoOd3G3trTHDzTggJiw4xMT4ticqqF\n8cmRhIfI3HHxxeS3SvjUsBgzD05K4t7xCWzvrd5fKzvPH/fWMT0tikWZVvISwwdt9d7lcHHgXHtv\n3/FWzvfOHc9JtHDHWBsFqRZGxpnl2oTwmCR14Rdmk4G5I2OYOzKGE3a9et94rInNx5sZGh3Kwiwr\nc9NjiBoEF/fq23soOa0Pq+w720a3UxFm0hifHMnXx1qYlGohe1hyv5+CJ/onSerC74ZbzfxrQRL3\n5Sew5UQzayvsrCqt5Y97z7ur95yE4KneXUpRWd/pnq1yrHfueGJkCPMzYihItTAmKUL2lhVeIUld\nBIzZZKAww0phhpXqxk7WVtjZWN3MxupmhseYWZhlZXZ69IBs69re42TfmYtL8ps69bnjObZwvpGv\nzx0fFiNzx4X3SVIX/UJ6bBjfnpLMfRMS3dX770vO8fqeWmYNj2ZhlpXs+LB+nQTP9PYdLzndysHa\ndhwuiAw1MHFIpD53PMVC9CAYXhKBJUld9CvhIQYWZFpZkGmlqkGv3jcdb+ajY02kx5pZmKlX7xEh\ngU+OTpei/HyHu9PhqWZ97vjQ6FBuGaXPHc9JkLnjwr8kqYt+KyMujIemJrN8YgKbj+vV+8u7L6/e\ns+LD/RpTc5eTst7NlcvOtNHW7cJkgDGJESzKslKQapHNvUVASVIX/V5EiJFFWbEszLRS2dDJBxV2\nNh9v5sOqJjLizCzMjGXWiCifVO9KKU42XVySf6SuA5eCmDAj04b2zh0fEtEvPjkIAZLUxQCiaRpZ\n8eFkxYfzwMRENh1v5oMKOy99fJZXy2qZPSKaRVlWRsZdW6//bqeLg+cuLsmvbdO3cxsZa+b20fFM\nTrWQGR8mc8dFvyRJXQxIkaFGFmfHcmOWlaN1naytbGRDdRNrK+1kxYexMNPKrBHRHu/c09DhoLR3\nbHzf2TY6HYpQoz53/PbR+r6c8bLzkxgAJKmLAU3TNHISwslJCOebE53uxP6bXZdX7yNiL6/eXUpR\n1dBJae8ioMqGTgBsESbmpscwuXfuuFm2cxMDjCR1ETQsZiO35MRx86hYys93sLbCTnFVE+9X2Bll\n06v3IU0aH5WfofR0K42dTjRglC2ce8cnUJAayXCrbOcmBjZJ6iLoaJpGXmIEeYkRfLPAyYZjevX+\n4s6zwFkiQgxMGBLJ5FQLE1MiiZHGYiKIyLtZBLVos5Fbc+NYkhPLkfMdREZHkxLag0nmjosgJQOG\nYlDQNI3cxAgmDrVKQhdBTZK6EEIEEY+GXx5++GHCwsIwGAwYjUbZ2k4IIfopj8fUn3zySaKjo30Z\nixBCiGskwy9CCBFEPK7Un3nmGQDmz59PYWGhzwISQghx9TSllOrroIaGBuLi4mhqauLpp5/m/vvv\nJy8v77JjiouLKS4uBqCoqIju7m7fROwlJpMJh8MR6DD6JHF6l8TpXRKn94SGeqe7p0dJ/VJvvvkm\nYWFhLFmy5EuPq6mpuabAfM1msw2IPSAlTu+SOL1L4vSelJQUrzxPn2PqnZ2ddHR0uP+9f/9+0tLS\nvPLiQgghvKvPMfWmpiaef/55AJxOJzNnziQ/P9/ngQkhhLhyfSb1pKQknnvuOX/EIoQQ4hrJlEYh\nhAgiktSFECKISFIXQoggIkldCCGCiCR1IYQIIpLUhRAiiEhSF0KIICJJXQghgogkdSGECCKS1IUQ\nIohIUhdCiCAiSV0IIYKIJHUhhAgiktSFECKISFIXQoggIkldCCGCiCR1IYQIIpLUhRAiiHic1F0u\nF48//jhFRUW+jEcIIcQ18Dipv/fee6SmpvoyFiGEENfIo6ReX19PWVkZ8+bN83U8QgghroHJk4Ne\nf/117rnnHjo6Or7wmOLiYoqLiwEoKirCZrN5J0IfMZlM/T5GkDi9TeL0Lomz/+kzqZeWlhITE8PI\nkSM5dOjQFx5XWFhIYWGh++u6ujrvROgjNput38cIEqe3SZzeJXF6T0pKileep8+kfvToUUpKStiz\nZw/d3d10dHTw4osv8t3vftcrAQghhPCePpP6XXfdxV133QXAoUOHWLNmjSR0IYTop2SeuhBCBBGP\nLpReMHr0aEaPHu2rWIQQQlwjqdSFECKISFIXQoggIkldCCGCiCR1IYQIIpLUhRAiiEhSF0KIICJJ\nXQghgogkdSGECCKS1IUQIohIUhdCiCAiSV0IIYKIJHUhhAgiktSFECKISFIXQoggIkldCCGCiCR1\nIYQIIpLUhRAiiPS581F3dzdPPvkkDocDp9PJtGnTWLZsmT9iE0IIcYX6TOohISE8+eSThIWF4XA4\n+PGPf0x+fj7Z2dn+iE8IIcQV6HP4RdM0wsLCAHA6nTidTjRN83lgQgghrpxHG0+7XC5+8IMfcPbs\nWRYuXEhWVpav4xJCCHEVNKWU8vTgtrY2nn/+ee6//37S0tIuu6+4uJji4mIAioqK6O7u9m6kXmYy\nmXA4HIEOo08Sp3dJnN4lcXpPaGioV57nipI6wF//+lfMZjNLliz50uNqamquKTBfs9ls1NXVBTqM\nPkmc3iVxepfE6T0pKSleeZ4+x9Sbm5tpa2sD9JkwBw4cIDU11SsvLoQQwrv6HFNvbGxk5cqVuFwu\nlFJcd911TJo0yR+xCSHEoKB6erz2XH0m9eHDh/Pss8967QWFEELo1OlPUVvXoXZugDc2eOU5PZr9\nIoQQwjtUZwdq9xbU1g/h2FEwmiB/iteeX5K6EEL4mFIKqj9Bbf0Q9fEW6OqA5KFoX3sA7bq5aFEx\nXnstSepCCOEjqrUZtXOjXpWfPgGhZrTJM9FmLoCMHJ8s5JSkLoQQXqRcLjh6ALVlHWrPDnA4YEQW\n2r0PoU2+Hi08wqevL0ldCCG8QNnrUds+Qm0rhvNnISIS7fpFaDPnow1L91scktSFEOIqKYcDDpbg\n2loM+0tAuWDUWLRb70abMA0t1Oz3mCSpCyHEFVK1NfpFz+3roakRYmLRFi1Fm1mIluidlaFXS5K6\nEEJ4QPV0o8p2oLasg6MHQDPA2EkYZi2AMZPQTP0jnfaPKIQQop9SJ6v1qnznRmhvBVsS2m33oE2f\nhxYbH+jwPkOSuhBC/BPV0Y76eLNelZ+oBJMJbeJ0tJnz9TFzQ//dCVSSuhBC0LtAqOqIvmx/91bo\n7oLU4WhffxBt2hw0S3SgQ/SIJHUhxKCmWppQOzboC4TOnARzONrU2XpVnp494HZ6k6QuhBh0lMsF\n5fv0BUJ7d4HToa/wvO9/oRXMRAsLD3SIV02SuhBi0FAN5y8uEKqvhcgotLmL9QVCqcMDHZ5XSFIX\nQgQ15eihc8dGnO/9DQ6VgVKQl4/21fvQ8qehhYQEOkSvkqQuhAhK6uwp9wKhppYmsMaj3bRMn4qY\nkBzo8HxGkroQImiori5U6TbU1nVQcRgMBhg3BetNX6V5WCaa0RjoEH1OkroQYsBTJ6r0qYi7NkFH\nOySmoC29D236DWgxsZhtNrR+vvG0t/SZ1Ovq6li5ciV2ux1N0ygsLGTx4sX+iE0IIb6Qam9F7dqs\nV+WfHoOQULRJ09FmLYCs0QNuKqK39JnUjUYj9957LyNHjqSjo4MnnniCcePGMXToUH/EJ4QQbkop\nqDiE2vIhqnQb9HTD0HS0u76FNmU2WqQl0CEGXJ9JPTY2ltjYWADCw8NJTU2loaFBkroQwm9UcyNq\n+3rUlg+htgbCI/ShlVkLIC1j0Fbln+eKxtRra2uprq4mMzPTV/EIIQQAyuWEQ3twbVkH+3eD0wmZ\neWg3fQ1t0kw0s/97lQ8EmlJKeXJgZ2cnTz75JEuXLmXq1Kmfub+4uJji4mIAioqK6O7u9m6kXmYy\nmXA4HIEOo08Sp3dJnN7lizidtWfoKH6HjvXv4qqvRYu2Ej53MeGFN2MaOqLfxOltoaGhXnkej5K6\nw+Hg5z//OePHj+fmm2/26IlramquOThfstls1A2Aq+ESp3dJnN7lrThVTw9q7y79omf5Pv3G0RMw\nzFwA4yejma5tgdBAOJ8pKd7ZXKPP4RelFC+//DKpqakeJ3QhhPCEOv1pb6/y9dDaAnEJaDffgTaj\nEC0+IdDhDUh9JvWjR4+yefNm0tLSeOyxxwC48847mThxos+DE0IEH9XZgSrZqndFrDoCRhPkT9Gr\n8rzxaIbgXyDkS30m9ZycHN58880rfmLnc/+BlpmLlpGjdz+LjLqqAIUQA59SCo5X6F0RP94CXR2Q\nPBTta/ejTZuLFm0NdIhBw3crSrs6UR/8TW9xCTBkGFpmLmTk6v9PHCLTkIQIcqqtBbVzo76D0OkT\nEGrWW9vOmq/nAskBXuezpG78379AdXXqf50ry1FVR/TFAlvWoQCiYvQKPiNHT/LDM9FCvHP1VwgR\nOMrlgqMH9LHysh3g6NF/v+95CG3yLLSIyECHGNR82vtFM4fp+/mNGgv0/rDPnEJVlcOFRL93l57k\nTSb9B3+hks/IkY9kQgwgyl5/sVf5+bMQEYk2a4HeqzxtZKDDGzT82tBLMxggNQ0tNQ2uXwiAarbr\n+wJWlesV/fo1qHVv6Q9IHKKPyWfmomXkwZCh/XrDVyEGG+V0oPbuwrX1Q9hfAsqlF3JL7kKbeB1a\nqCwQ8reAd2nUoq0wYRrahGkAqJ5uOFHVm+SPoA6WwY4NejUfEQkjLxmySc/WPw0IIfxK1Z5Bbf2Q\nup0bcTXWQUws2qKvoM2Yj5bknfnW4uoEPKn/My0kVK/MM3NhYe9V89ozF4dsKstRB0v1JG8wwLCR\nFy/AZuSgxdkC/S0IEZRUTzeqbId+0fPoAdAMhEy6DsfUb8GYSWimfpdOBqV+/1PQNA2SUvS//tPn\nAaDaWuHYEb2SrypHbVkLH63RE31cwsUhm8xcSB0xKBrjC+Er6lS13hVx50ZobwVbEtpt96BNn0ds\n1qh+v1JzsOn3Sf3zaJEWGFuANrYAAOVwwKlqVGW5Pj5fcRh2b9GTvDkcRmbrF2AzcmDkKLn6LkQf\nVEc7avcWbzwKAAAYPElEQVRmvSvi8QowmdAmXKd3RRw1Vq5t9WMDMqn/M81kghFZaCOyoHCJPmTT\ncL43yfcO2bz7Jkq5QNMgdTjNoyfgGjoCLSNXrzxkvqwY5JRSelG0dR1q91bo7oLU4WhffxBt2hw0\nS3SgQxQeCIqk/s80TYP4RLT4RJg6GwDV2Q7HPnHPme/cvFavRgBiYi+OyWfmQtrIa24gJMRAoVqa\nUDs26Mv2z5wEcxja1NloM+frkxE8KHiUUnR2duJyufplgXTu3Dm6uroCHQZKKQwGA2FhYT47T0GZ\n1D+PFhYBefloefkAxMfGUre/7PI582Xb9SQfEgojMnvbHPTOmZcqRQQR5XJB+T592f7eXeB06EOT\n33gEbfJM/fflCnR2dhISEoKpn14sNZlMGPvJtTWHw0FnZyfh4eE+ef7++RPwA81oRBuWjjYsHebo\ne64qe73+8fPCBdh1b6Ocf9MfkDz0kjnzuZCc2i8rEiG+jGo4f3GBUH0tREahzV2sLxBKHX7Vz+ty\nufptQu9vTCaTTz81yE/hEpo1HibNQJs0AwDV3XV5m4O9u2BbsV7NW6IuDtlk5OqVvSy0EP2Qcjhg\n/8e4tnwIh/boC4Ryx6N99T60/GloIdc+1CgFzpXx5fmSpP4ltFAzZI9Byx4D9H5kPXf64gXYqiOo\nfR/rSd5o0sfiL1TymbloMbEBjV8MbursKb3/yvb10NIE1ni0xbfrvcoTkgMdnvARSepXQDMY9G6T\nQ4bBrAUAqJbmiwm+shy14T3Uh3/XH5CQ3Nt6uHfOfMow6RUtfEp1daFKt+k7CFUc1hfojZuCYdZ8\nGD1R1mwMApLUr5EWFQ35U9Hy9X1bVU8PfNrb5qDqCOrwXti5Ua/mwyP0i1EXknx6NlqYby6WiMFF\nfVqlLxDatQk62vS+SUvvQ5t+w6D4xHjy5EnuvvtupkyZQllZGXl5eSxbtowVK1ZQV1fHb3/7W9ra\n2vjxj38M6MMfq1evxmKx8Nvf/pY1a9bQ3d3NokWL+P73vx/g7+baSFL3Mi0kxN1SGHrn/p4/i6o6\n0jvLphy15i/67ZoBho24OMMmM0+28BIeU+2tqF2b9amIn1ZBSCjapOloMxdA9uiAjXO7/v8/oE5W\ne/U5tWHpGO74ly895vjx4/zud7/j2WefZfHixbz99tu8/fbbrFu3jl/96lc4HA5+9rOfMXnyZNra\n2jCbzWzatInq6mreffddlFIsX76cnTt3Mm3aNK/G70+S1H1M0zS9akocAtfNBfRfRo4dvThks/0j\n2PCuXs3H2i5rc6Bipf2wuEgpBRWH9amIpdugpxuGpqPd9S20KbP11daD1LBhw8jNzQUgOzubmTNn\nomkaOTk5nDx5kiVLlvCf//mffOUrX+HGG28kJSWFTZs2sWnTJhYs0IdT29vbqa6uDu6k/tJLL1FW\nVkZMTAwrVqzwR0xBT4uw6A2QxkwCQDmdcOr4JRdgy6FkKwqoNYfpq2XdfeZH6Y8Xg4pqbtQXCG35\nEM6dhvAIfWhl1gJIy+hXs0/6qqh9xWy+OPvMYDAQGhrq/rfD4eCRRx5h3rx5rF+/nltuuYU33ngD\npRSPPPII9957b0Bi9oU+k/qcOXNYtGgRK1eu9Ec8g5JmNMLwDLThGTDvZqB3PnFlOWE1x+k4sAf1\nwf/VZ99o2j9tDZgDCbI1YDBSLicc2qP3Kt/3MTidkJmnz2CZNEPaTl+h48ePk5ubS25uLqWlpVRW\nVjJnzhyee+45li5dSmRkJGfOnCEkJASbbeB2e+0zqefl5VFbW+uPWMQltLgEtCkJRNuW0l1Xh+rs\ngOpPLl6A3b0VNq+9ZGtAvZLX2xxkeGXusQgMVXeO1nVv4SpeA411EBWDNm+JvkBoyNBAhzdgvfLK\nK2zfvh2DwUB2djZz587FbDZTUVHBkiVLAIiIiODXv/71gE7qmlJK9XVQbW0tP//5z790+KW4uJji\n4mIAioqK6O7u9l6UPmAymXA4HIEOo09fFKdyuXCerKb7yAF6juyn58gBnGdP63eGhBKSkUNIzlhC\ncsYSmjMWg49nQAz08xloqqebro+30lH8D7r37QYgNH8q4fNvwVwws9/+kb5wPs+dO3fZ8If4cl1d\nXSQlJV1224XhomvltaT+z2pqaq4pMF+z2WwDog/0lcSpmhovmzPPiSq9pwdAYopexWf2tiBO9u7W\ngMF4Pv1B1XyqLxDasQFam/X9AGYUEn/L12jU+v88hgvns729nYiIK+sX40/97Y/5552vlBTv7BjV\n/981wmNaTCxMnI42cTrQuzXg8creNgflqP0fw/aPercGtLinXmqZefrFWKm0/EJ1dugLhLasg6oj\n+mrk/CkYZi6AvPFoBiNGmw360R8fMXBIUg9iWkgoZOWhZeUBvdPhzp2+ZM78EdSBkt42B0b31oDu\nKZXW+IDGH0yUUvof2K3rUB9vhs4O/dPS1+5HmzZX36tXCC/oM6n/8pe/5PDhw7S0tPDtb3+bZcuW\nccMNN/gjNuFlmqbpiSR5KMwoBEC1NkPV0d4LsOWoTR+giv+hPyA+8WIfm4wcGDpc2hxcIdXWgtq5\nSV+2f+o4hJrRCmaizZqvX9yWWUvCy/pM6o8++qg/4hABolmiYfxktPGTAVCOHjhZfXHI5ugB+HjT\n5VsDXphlkz4KLbz/jqMGinK54JOD+rL9su3g6IHhmWj3PIQ2eZZspyh8SoZfxGU0U4jekyY9G+bf\nqg8b1J3Th2yqyvVe8++8cbHNQepwtMxcOvIno5KG6tX9IK0+lb0etX29vmz//FmIiESbtUCfipg2\nMtDhiUFCkrr4Upqm6d0mE5Jh2hxA35RYb3PQu//rjg00b3xPf4A1rrePTS5aRh4MS9f3kA1SyumE\ng6W4tqyDAyXgcukbMy+5C23iddJjX/hd8P62CZ/RwiNg9AS00RMAPbFZ25toLNlx8QJsae/WgKGh\nMCL74v6vGTlokVEBjd8bVO0Z1LZi1LaPoKkBYmLRFn4FbcZ8tCTvTE0TvpOVlUVFRUWgw/AJSeri\nmmlGIyHp2Rii4mDuTQCoxvre4Zre/9auRr3v0h/gbnPQu2tUUsqAGLJRPd2osh368MqR/frw09hJ\nGGZ9G8YUBPUnEjFwyLtQ+IQWGw8FM9EKZgKgujov3xqwdBtsWde7NWD0xSGbzFz9omKId1bXeYM6\ndfziAqH2VrAlod12D9r0efr3KS7zSsk5qhs7vfqc6bFhPFiQ9IX3P/PMM6SmprJ8+XIAVqxYgaZp\n7Ny5k6amJhwOB48//jgLFy7s87W2b9/OihUrsNlsHDp0iMWLF5OTk8OqVavo7Oxk1apVjBgxgjVr\n1vDCCy9gMBiIjo5m9erVOJ1Ofvazn7Fjxw66u7u57777/N4sTJK68AvNHKaPNY8aC/TOEDl7Sl/5\nWvlPWwOaTHpiv1DJZ+agRft3owfV2Y76eItelVd/AiYT2oTr9K6Io8Z6dTWuuHa33norTz75pDup\nr1mzhj/96U/8y7/8C1FRUTQ1NbF48WIWLFjg0afCw4cPs3HjRqxWK9OnT+fOO+/k3Xff5ZVXXuHV\nV1/lJz/5Cb/85S/505/+xJAhQ2hqagLgL3/5C1FRUbz33nt0dXVx2223MXv2bNLS0nz57V9GkroI\nCM1ggJQ0tJQ0uF6vnlSzHY4duThks/4d1Lq39QckJF/S5iBXH8LxcmJVSukXgLesQ5Vsha5OPcav\nP4g2bY4+/VP06csqal8ZM2YMdXV1nD17lvr6emJiYkhMTOSpp55i165dGAwGzp49y/nz50lMTOzz\n+caPH+/uzTJ8+HBmz54NQE5ODtu3bwegoKCA733ve9xyyy3ceOONAGzatIny8nLeffddAFpaWqiu\nrpakLgYnLdoK+dPQ8vUNClRPD5yo7J1lcwR1sAx2bOjdGjBS7y1/6daAV9mKVrU0o3Zu0JftnzkJ\n5jC0KdejzZyvP+8AGO8XcNNNN/Huu+9SW1vLrbfeyurVq6mvr+f9998nPDycSZMm0dXV5dFzXdpc\n6/N6swP8/Oc/p6ysjI8++ogFCxawbt06AJ5++mnmzJnj3W/uCkhSF/2WFhLi3gGKhRe2Bjxz+ZDN\nwT/pSd5g0NscXGhxkJGLFvfF7VOVywVH9ukLhPbs1BufjRyF9o1H0CbPRAuTRVUDza233spjjz1G\nQ0MDf/vb31izZg02m42QkBC2bt3KqVOnvPp6x48fZ+LEiUycOJEPP/yQmpoaZs+ezf/8z/8wY8YM\nQkJCqKqqYsiQIX5tdiZJXQwY+taAKWiJKTB9HgCqrbV3yOaIXtFvXQfr39ETfZztkjYHuTB0BM66\nWlzvvInaWgz1tRAZhTZ3sb5AKHV4QL8/cW1GjRpFW1sbycnJJCUlsXTpUu677z5uvPFGxowZQ2Zm\npldf7+mnn6a6uhqlFDNnzmT06NHk5eVx8uRJFi1ahFKKuLg4Xn31Va++bl88ar17NaT1rndInFdG\nORxwqrp3a8DeFsT2ev1Oc5i+p6fLBbnj9dWe+dP6Za/y/nI++yKtd6+OtN4VwkOayaS3ER6RBYVL\n9CGbhjpU5WGoOkJEQhId+dP0FbJCBCFJ6iKoaZoG8Qlo8bNh6mwsNhudA6ACFr5XXl7Od7/73ctu\nM5vNvPPOOwGKyDskqQshBqXc3Fw+/PDDQIfhdbKCQghxzXx0aS5o+fJ8SVIXQlyzS+dviy/ncDgw\n+HBFsgy/CCGuWVhYGJ2dnXR1dfXLxVpms9njhUe+pJTCYDAQFnZ1C+U84VFS37t3L6+99houl4t5\n8+Zx2223+SwgIcTAo2ka4eHhgQ7jCw2UKaLe0OdnAJfLxapVq/iP//gPXnjhBbZt2+b1lVlCCCG8\no8+kXllZ6V6hZTKZmD59Ort37/ZHbEIIIa5Qn0m9oaGB+PiLPaPj4+NpaGjwaVBCCCGuTp9j6p83\n9ebzLoQUFxdTXFwMQFFRkdeWvPrSQIgRJE5vkzi9S+LsX/qs1OPj46mvr3d/XV9fT2zsZzcsKCws\npKioiKKiIp544gnvRukDAyFGkDi9TeL0LonTe7wVY59JPSMjgzNnzlBbW4vD4WD79u0UFBR45cWF\nEEJ4V5/DL0ajkQceeIBnnnkGl8vF3LlzGTZsmD9iE0IIcYWMTz311FN9HTRkyBBuvPFGFi9eTG5u\nrkdPPHLkyGuNzecGQowgcXqbxOldEqf3eCNGn/VTF0II4X/S+0UIIYLIFfV+6atdQE9PD7/5zW84\nduwYUVFRPProo+6du9966y3Wr1+PwWDg/vvvJz8/33vfxRXG+c477/DRRx9hNBqJjo7mO9/5DgkJ\nCQB8/etfd+/8bbPZ+MEPfhCwODdu3Mgf//hH4uLiAFi0aBHz5s1z37d69WoAli5d6tONbvuK8/XX\nX+fQoUMAdHd309TUxOuvvw7473y+9NJLlJWVERMTw4oVKz5zv1KK1157jT179mA2m3nooYfcH3X9\neS77inPLli38/e9/B/R+Kg8++CAjRowA4OGHHyYsLAyDwYDRaKSoqChgcR46dIhnn33W/fs9depU\nbr/9dsB/bUX6ivEf//gHW7ZsAfSV8adOnWLVqlVYLBa/nsu6ujpWrlyJ3W5H0zQKCwtZvHjxZcd4\n9f2pPOR0OtUjjzyizp49q3p6etT3v/99dfLkycuO+eCDD9Tvfvc7pZRSW7duVb/4xS+UUkqdPHlS\nff/731fd3d3q3Llz6pFHHlFOp9PTl74insR54MAB1dnZqZRSau3ate44lVLqnnvu8UlcVxPnhg0b\n1CuvvPKZx7a0tKiHH35YtbS0XPbvQMV5qffee0+tXLnS/bW/zuehQ4dUVVWV+rd/+7fPvb+0tFQ9\n88wzyuVyqaNHj6of/vCHSin/nktP4jxy5Ij79cvKytxxKqXUQw89pJqamnwW25XEefDgQfVf//Vf\nn7n9St8vvozxUrt371ZPPfWU+2t/nsuGhgZVVVWllFKqvb1dffe73/3MOfHm+9Pj4RdP2gWUlJS4\n/4pMmzaNgwcPopRi9+7dTJ8+nZCQEBITE0lOTqaystLTl74insQ5ZswYzGYzAFlZWQFZIXst7Rf2\n7t3LuHHjsFgsWCwWxo0bx969e/tFnNu2bWPmzJk+ieXL5OXlYbFYvvD+kpISrr/+ejRNIzs7m7a2\nNhobG/16Lj2Jc9SoUe77s7KyLlsj4k99xflF/NlW5Epi3LZtGzNmzPBJHH2JjY11V93h4eGkpqZ+\nJud48/3p8fDL57ULqKio+MJjjEYjERERtLS00NDQQFZWlvu4uLg4nyVST+K81Pr16y8bCurp6eGJ\nJ57AaDRy6623MmXKlIDGuWvXLsrLyxkyZAj33XcfNpvtM4/tL+fz/Pnz1NbWMmbMGPdt/jqffWlo\naMBms7m/vtDuwp/n8kqtX7+eCRMmXHbbM888A8D8+fMpLCwMRFhun3zyCY899hixsbHce++9DBs2\n7Ip///yhq6uLvXv38s1vfvOy2wNxLmtra6muriYzM/Oy2735/vQ4qSsP2gV80TGfd7uveBLnBZs3\nb+bYsWNcOqvzpZdeIi4ujnPnzvGTn/yEtLQ0kpO9v0mxJ3FOmjSJGTNmEBISwrp161i5ciVPPvnk\n5z6fr3pYX8n53LZtG9OmTbtsAwB/nc++XMn30R/6gR88eJANGzbwk5/8xH3bT3/6U+Li4mhqauLp\np58mJSWFvLy8gMSXnp7OSy+9RFhYGGVlZTz33HO8+OKLV3Se/aW0tPSyT0AQmHPZ2dnJihUrWL58\nOREREZfd5833p8fDL560C7j0GKfTSXt7OxaL5TOPbWhocF/88zZP2xrs37+ft956i8cff5yQkBD3\n7RfiSkpKIi8vj+PHjwcszqioKHdshYWFHDt2zB3jP5/Pz/se/RXnBdu3b//MR1x/nc++xMfHX9ZP\n+8L34c9z6akTJ07wu9/9jscee4yoqCj37RfOZUxMDJMnT/bZEKYnIiIi3Bs9TJw4EafTSXNz8xW9\nX/zl84YE/X0uHQ4HK1asYNasWUydOvUz93vz/elxUvekXcCkSZPYuHEjADt37mT06NFomkZBQQHb\nt2+np6eH2tpazpw585mPH97iSZzV1dX84Q9/4PHHHycmJsZ9e2trKz09PQA0Nzdz9OhRhg4dGrA4\nGxsb3f8uKSlxx5Kfn8++fftobW2ltbWVffv2+Ww2kadtImpqamhrayM7O9t9mz/PZ18KCgrYvHkz\nSik++eQTIiIiiI2N9eu59ERdXR3PP/88jzzyyGUNqDo7O+no6HD/e//+/e5ZRYFgt9vd1WVlZSUu\nl4uoqKh+11akvb2dw4cPXxaDv8+lUoqXX36Z1NRUbr755s89xpvvzytafFRWVsZ///d/u9sFLF26\nlDfeeIOMjAwKCgro7u7mN7/5DdXV1VgsFh599FGSkpIAWL16NRs2bMBgMLB8+fLPjBV6U19x/vSn\nP+XTTz/FarUCF6faHT16lN///vcYDAZcLhc33XQTN9xwQ8Di/POf/0xJSQlGoxGLxcKDDz5Iamoq\noI+3vvXWW4A+zWnu3LkBixPgzTffpKenh7vvvtv9OH+ez1/+8pccPnyYlpYWYmJiWLZsmXvPzAUL\nFqCUYtWqVezbt4/Q0FAeeughMjIyAP+ey77ifPnll9m1a5d7fPXCdLtz587x/PPPA/qn4JkzZ7J0\n6dKAxfnBBx+wbt06jEYjoaGhfOMb32DUqFHA579fAhEj6NMB9+7dy6OPPup+nL/P5ZEjR/jxj39M\nWlqae+jkzjvvdFfm3n5/yopSIYQIIrKiVAghgogkdSGECCKS1IUQIohIUhdCiCAiSV0IIYKIJHUh\nhAgiktSFECKISFIXQcnpdAY6BCEC4oo2yRCiP3v44YeZP38+W7dupaamBofDwYsvvuhuILZy5Uri\n4+O54447OHToEL/+9a+56aab+Pvf/47BYODOO+/06WpSIfxBKnURVLZt28YTTzzh3nnpy9jtdtrb\n23n55Zf59re/zapVq2htbfV9kEL4kCR1EVRuvPFGbDYboaGhfR5rNBq5/fbbMZlMTJw4kbCwMGpq\navwQpRC+I0ldBJVLNxroS1RUFEaj0f212Wyms7PTF2EJ4TeS1EXQMpvNdHV1ub+22+0BjEYI/5Ck\nLoLWiBEj2Lp1Ky6Xi71793L48OFAhySEz0lSF0Fr+fLllJaWsnz5crZs2cLkyZMDHZIQPif91IUQ\nIohIpS6EEEFEkroQQgQRSepCCBFEJKkLIUQQkaQuhBBBRJK6EEIEEUnqQggRRCSpCyFEEJGkLoQQ\nQeT/AQ8RD1BryLXDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16695c910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print pd.DataFrame(rerun_perf)['val_mse']\n",
    "print np.mean(pd.DataFrame(rerun_perf)['val_mse'])\n",
    "pd.DataFrame(rerun_perf).loc[:,['run','mse','val_mse']].set_index('run').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout on 2nd layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 0.9826 - mse: 855.6521 - val_loss: 0.7567 - val_mse: 658.9590\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.7259 - mse: 632.1243 - val_loss: 0.8886 - val_mse: 773.8506\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.8643 - mse: 752.7092 - val_loss: 0.7393 - val_mse: 643.8422\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.7118 - mse: 619.9037 - val_loss: 0.4595 - val_mse: 400.1246\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.4344 - mse: 378.2612 - val_loss: 0.2200 - val_mse: 191.5490\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.2038 - mse: 177.4739 - val_loss: 0.5566 - val_mse: 484.7417\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.5259 - mse: 457.9442 - val_loss: 0.2449 - val_mse: 213.3027\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.2415 - mse: 210.3280 - val_loss: 0.2643 - val_mse: 230.1663\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.2522 - mse: 219.6137 - val_loss: 0.2595 - val_mse: 226.0228\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.00699999984354.\n",
      "2s - loss: 0.2496 - mse: 217.3465 - val_loss: 0.2443 - val_mse: 212.7811\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.2402 - mse: 209.1695 - val_loss: 0.1779 - val_mse: 154.9556\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.1721 - mse: 149.8468 - val_loss: 0.1254 - val_mse: 109.1878\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.1165 - mse: 101.4894 - val_loss: 0.1245 - val_mse: 108.4243\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.1145 - mse: 99.7028 - val_loss: 0.1299 - val_mse: 113.1565\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.1188 - mse: 103.4623 - val_loss: 0.1313 - val_mse: 114.3843\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.1171 - mse: 101.9899 - val_loss: 0.1425 - val_mse: 124.1223\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.1261 - mse: 109.7882 - val_loss: 0.1156 - val_mse: 100.7072\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.1017 - mse: 88.5761 - val_loss: 0.0918 - val_mse: 79.9160\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0809 - mse: 70.4579 - val_loss: 0.0924 - val_mse: 80.4505\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0833 - mse: 72.5652 - val_loss: 0.0987 - val_mse: 85.9109\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0909 - mse: 79.1810 - val_loss: 0.0974 - val_mse: 84.8512\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0909 - mse: 79.1204 - val_loss: 0.0873 - val_mse: 76.0664\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0815 - mse: 70.9810 - val_loss: 0.0746 - val_mse: 64.9431\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0690 - mse: 60.0739 - val_loss: 0.0657 - val_mse: 57.1860\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0600 - mse: 52.2270 - val_loss: 0.0628 - val_mse: 54.6848\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0569 - mse: 49.5442 - val_loss: 0.0638 - val_mse: 55.5259\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0578 - mse: 50.3541 - val_loss: 0.0646 - val_mse: 56.2962\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0590 - mse: 51.4001 - val_loss: 0.0624 - val_mse: 54.3742\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0575 - mse: 50.0343 - val_loss: 0.0567 - val_mse: 49.3752\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0525 - mse: 45.6947 - val_loss: 0.0499 - val_mse: 43.4580\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0463 - mse: 40.3388 - val_loss: 0.0456 - val_mse: 39.6808\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0424 - mse: 36.9266 - val_loss: 0.0446 - val_mse: 38.8193\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0417 - mse: 36.3463 - val_loss: 0.0444 - val_mse: 38.6238\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0420 - mse: 36.6091 - val_loss: 0.0440 - val_mse: 38.3166\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0422 - mse: 36.7804 - val_loss: 0.0439 - val_mse: 38.2641\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0422 - mse: 36.7508 - val_loss: 0.0417 - val_mse: 36.3408\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0395 - mse: 34.4272 - val_loss: 0.0379 - val_mse: 33.0282\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0351 - mse: 30.5989 - val_loss: 0.0364 - val_mse: 31.7259\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0329 - mse: 28.6796 - val_loss: 0.0374 - val_mse: 32.5828\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0332 - mse: 28.9097 - val_loss: 0.0381 - val_mse: 33.2032\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0334 - mse: 29.1082 - val_loss: 0.0376 - val_mse: 32.7477\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0328 - mse: 28.5396 - val_loss: 0.0364 - val_mse: 31.6792\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0318 - mse: 27.6555 - val_loss: 0.0345 - val_mse: 30.0820\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0305 - mse: 26.5387 - val_loss: 0.0322 - val_mse: 28.0219\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0289 - mse: 25.1744 - val_loss: 0.0302 - val_mse: 26.2591\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0276 - mse: 24.0757 - val_loss: 0.0293 - val_mse: 25.4814\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0272 - mse: 23.6880 - val_loss: 0.0292 - val_mse: 25.4255\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0272 - mse: 23.6694 - val_loss: 0.0290 - val_mse: 25.2608\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0267 - mse: 23.2805 - val_loss: 0.0286 - val_mse: 24.9183\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0259 - mse: 22.5313 - val_loss: 0.0286 - val_mse: 24.8838\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0252 - mse: 21.9839 - val_loss: 0.0288 - val_mse: 25.0420\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0249 - mse: 21.6954 - val_loss: 0.0286 - val_mse: 24.9148\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0245 - mse: 21.3501 - val_loss: 0.0281 - val_mse: 24.4588\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0240 - mse: 20.9241 - val_loss: 0.0275 - val_mse: 23.9327\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0237 - mse: 20.6047 - val_loss: 0.0269 - val_mse: 23.4394\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0234 - mse: 20.4095 - val_loss: 0.0263 - val_mse: 22.9066\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0232 - mse: 20.1907 - val_loss: 0.0256 - val_mse: 22.3360\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0228 - mse: 19.8824 - val_loss: 0.0251 - val_mse: 21.8405\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0224 - mse: 19.5452 - val_loss: 0.0247 - val_mse: 21.4853\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0221 - mse: 19.2205 - val_loss: 0.0244 - val_mse: 21.2409\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0217 - mse: 18.8928 - val_loss: 0.0242 - val_mse: 21.0930\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0213 - mse: 18.5878 - val_loss: 0.0242 - val_mse: 21.0660\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0211 - mse: 18.3814 - val_loss: 0.0242 - val_mse: 21.0882\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0210 - mse: 18.2550 - val_loss: 0.0241 - val_mse: 20.9792\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0208 - mse: 18.0708 - val_loss: 0.0237 - val_mse: 20.6624\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0204 - mse: 17.7692 - val_loss: 0.0233 - val_mse: 20.2473\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0200 - mse: 17.4501 - val_loss: 0.0228 - val_mse: 19.8535\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0198 - mse: 17.2133 - val_loss: 0.0224 - val_mse: 19.5052\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0196 - mse: 17.0498 - val_loss: 0.0220 - val_mse: 19.1905\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0194 - mse: 16.9034 - val_loss: 0.0217 - val_mse: 18.9095\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0192 - mse: 16.7302 - val_loss: 0.0214 - val_mse: 18.6744\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0190 - mse: 16.5212 - val_loss: 0.0212 - val_mse: 18.4988\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0187 - mse: 16.2890 - val_loss: 0.0211 - val_mse: 18.3980\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0185 - mse: 16.0739 - val_loss: 0.0211 - val_mse: 18.3764\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0183 - mse: 15.9191 - val_loss: 0.0211 - val_mse: 18.3690\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0182 - mse: 15.8140 - val_loss: 0.0210 - val_mse: 18.2657\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0180 - mse: 15.6947 - val_loss: 0.0207 - val_mse: 18.0235\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0178 - mse: 15.5275 - val_loss: 0.0203 - val_mse: 17.7085\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0176 - mse: 15.3454 - val_loss: 0.0200 - val_mse: 17.4107\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0174 - mse: 15.1903 - val_loss: 0.0197 - val_mse: 17.1664\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0173 - mse: 15.0634 - val_loss: 0.0195 - val_mse: 16.9710\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0172 - mse: 14.9441 - val_loss: 0.0193 - val_mse: 16.8098\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0170 - mse: 14.8167 - val_loss: 0.0191 - val_mse: 16.6693\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0169 - mse: 14.6768 - val_loss: 0.0190 - val_mse: 16.5400\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0167 - mse: 14.5309 - val_loss: 0.0189 - val_mse: 16.4234\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0165 - mse: 14.3959 - val_loss: 0.0187 - val_mse: 16.3136\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0164 - mse: 14.2784 - val_loss: 0.0186 - val_mse: 16.1928\n",
      "Epoch 88/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0163 - mse: 14.1683 - val_loss: 0.0184 - val_mse: 16.0455\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0161 - mse: 14.0540 - val_loss: 0.0182 - val_mse: 15.8750\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0160 - mse: 13.9364 - val_loss: 0.0180 - val_mse: 15.6989\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0159 - mse: 13.8234 - val_loss: 0.0178 - val_mse: 15.5342\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0158 - mse: 13.7173 - val_loss: 0.0177 - val_mse: 15.3911\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0156 - mse: 13.6133 - val_loss: 0.0175 - val_mse: 15.2761\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0155 - mse: 13.5107 - val_loss: 0.0174 - val_mse: 15.1882\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0154 - mse: 13.4118 - val_loss: 0.0174 - val_mse: 15.1146\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0153 - mse: 13.3146 - val_loss: 0.0173 - val_mse: 15.0380\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0152 - mse: 13.2165 - val_loss: 0.0172 - val_mse: 14.9465\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0151 - mse: 13.1168 - val_loss: 0.0170 - val_mse: 14.8388\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0149 - mse: 13.0183 - val_loss: 0.0169 - val_mse: 14.7242\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0148 - mse: 12.9246 - val_loss: 0.0168 - val_mse: 14.6109\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0147 - mse: 12.8340 - val_loss: 0.0167 - val_mse: 14.5062\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0146 - mse: 12.7426 - val_loss: 0.0166 - val_mse: 14.4133\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0145 - mse: 12.6493 - val_loss: 0.0165 - val_mse: 14.3322\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0144 - mse: 12.5578 - val_loss: 0.0164 - val_mse: 14.2568\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0143 - mse: 12.4702 - val_loss: 0.0163 - val_mse: 14.1789\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0142 - mse: 12.3850 - val_loss: 0.0162 - val_mse: 14.0930\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0141 - mse: 12.2984 - val_loss: 0.0161 - val_mse: 14.0022\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0140 - mse: 12.2105 - val_loss: 0.0160 - val_mse: 13.9159\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0139 - mse: 12.1244 - val_loss: 0.0159 - val_mse: 13.8383\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0138 - mse: 12.0403 - val_loss: 0.0158 - val_mse: 13.7681\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0137 - mse: 11.9574 - val_loss: 0.0157 - val_mse: 13.6989\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0136 - mse: 11.8743 - val_loss: 0.0156 - val_mse: 13.6265\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0135 - mse: 11.7910 - val_loss: 0.0156 - val_mse: 13.5502\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0134 - mse: 11.7092 - val_loss: 0.0155 - val_mse: 13.4713\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0134 - mse: 11.6280 - val_loss: 0.0154 - val_mse: 13.3928\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0133 - mse: 11.5470 - val_loss: 0.0153 - val_mse: 13.3182\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0132 - mse: 11.4670 - val_loss: 0.0152 - val_mse: 13.2487\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0131 - mse: 11.3881 - val_loss: 0.0151 - val_mse: 13.1836\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0130 - mse: 11.3107 - val_loss: 0.0151 - val_mse: 13.1206\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0129 - mse: 11.2335 - val_loss: 0.0150 - val_mse: 13.0589\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0128 - mse: 11.1567 - val_loss: 0.0149 - val_mse: 12.9986\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0127 - mse: 11.0806 - val_loss: 0.0149 - val_mse: 12.9393\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0126 - mse: 11.0049 - val_loss: 0.0148 - val_mse: 12.8780\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0126 - mse: 10.9299 - val_loss: 0.0147 - val_mse: 12.8129\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0125 - mse: 10.8552 - val_loss: 0.0146 - val_mse: 12.7453\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0124 - mse: 10.7816 - val_loss: 0.0146 - val_mse: 12.6785\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0123 - mse: 10.7093 - val_loss: 0.0145 - val_mse: 12.6156\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0122 - mse: 10.6380 - val_loss: 0.0144 - val_mse: 12.5584\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0121 - mse: 10.5672 - val_loss: 0.0144 - val_mse: 12.5059\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0121 - mse: 10.4971 - val_loss: 0.0143 - val_mse: 12.4540\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0120 - mse: 10.4282 - val_loss: 0.0142 - val_mse: 12.3979\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0119 - mse: 10.3601 - val_loss: 0.0142 - val_mse: 12.3358\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0118 - mse: 10.2926 - val_loss: 0.0141 - val_mse: 12.2702\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0117 - mse: 10.2260 - val_loss: 0.0140 - val_mse: 12.2058\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0117 - mse: 10.1603 - val_loss: 0.0139 - val_mse: 12.1459\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0116 - mse: 10.0951 - val_loss: 0.0139 - val_mse: 12.0919\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0115 - mse: 10.0304 - val_loss: 0.0138 - val_mse: 12.0424\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0114 - mse: 9.9662 - val_loss: 0.0138 - val_mse: 11.9945\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0114 - mse: 9.9029 - val_loss: 0.0137 - val_mse: 11.9462\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0113 - mse: 9.8407 - val_loss: 0.0137 - val_mse: 11.8968\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0112 - mse: 9.7788 - val_loss: 0.0136 - val_mse: 11.8473\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0112 - mse: 9.7174 - val_loss: 0.0135 - val_mse: 11.7983\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0111 - mse: 9.6564 - val_loss: 0.0135 - val_mse: 11.7508\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0110 - mse: 9.5961 - val_loss: 0.0134 - val_mse: 11.7052\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0110 - mse: 9.5367 - val_loss: 0.0134 - val_mse: 11.6618\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0109 - mse: 9.4781 - val_loss: 0.0133 - val_mse: 11.6207\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0108 - mse: 9.4203 - val_loss: 0.0133 - val_mse: 11.5816\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0108 - mse: 9.3633 - val_loss: 0.0133 - val_mse: 11.5438\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0107 - mse: 9.3072 - val_loss: 0.0132 - val_mse: 11.5063\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0106 - mse: 9.2519 - val_loss: 0.0132 - val_mse: 11.4684\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0106 - mse: 9.1975 - val_loss: 0.0131 - val_mse: 11.4300\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0105 - mse: 9.1439 - val_loss: 0.0131 - val_mse: 11.3926\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0104 - mse: 9.0916 - val_loss: 0.0130 - val_mse: 11.3575\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0104 - mse: 9.0402 - val_loss: 0.0130 - val_mse: 11.3234\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0103 - mse: 8.9895 - val_loss: 0.0130 - val_mse: 11.2890\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0103 - mse: 8.9392 - val_loss: 0.0129 - val_mse: 11.2535\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0102 - mse: 8.8894 - val_loss: 0.0129 - val_mse: 11.2173\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0102 - mse: 8.8404 - val_loss: 0.0128 - val_mse: 11.1808\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0101 - mse: 8.7918 - val_loss: 0.0128 - val_mse: 11.1445\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0100 - mse: 8.7436 - val_loss: 0.0128 - val_mse: 11.1092\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0100 - mse: 8.6958 - val_loss: 0.0127 - val_mse: 11.0750\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0099 - mse: 8.6485 - val_loss: 0.0127 - val_mse: 11.0417\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0099 - mse: 8.6015 - val_loss: 0.0126 - val_mse: 11.0093\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0098 - mse: 8.5552 - val_loss: 0.0126 - val_mse: 10.9776\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0098 - mse: 8.5094 - val_loss: 0.0126 - val_mse: 10.9463\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0097 - mse: 8.4641 - val_loss: 0.0125 - val_mse: 10.9152\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0097 - mse: 8.4194 - val_loss: 0.0125 - val_mse: 10.8834\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0096 - mse: 8.3750 - val_loss: 0.0125 - val_mse: 10.8510\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0096 - mse: 8.3311 - val_loss: 0.0124 - val_mse: 10.8187\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0095 - mse: 8.2877 - val_loss: 0.0124 - val_mse: 10.7873\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0095 - mse: 8.2446 - val_loss: 0.0124 - val_mse: 10.7573\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0094 - mse: 8.2019 - val_loss: 0.0123 - val_mse: 10.7282\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0094 - mse: 8.1598 - val_loss: 0.0123 - val_mse: 10.6996\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0093 - mse: 8.1183 - val_loss: 0.0123 - val_mse: 10.6711\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0093 - mse: 8.0773 - val_loss: 0.0122 - val_mse: 10.6419\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0092 - mse: 8.0366 - val_loss: 0.0122 - val_mse: 10.6122\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0092 - mse: 7.9961 - val_loss: 0.0122 - val_mse: 10.5829\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0091 - mse: 7.9561 - val_loss: 0.0121 - val_mse: 10.5538\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0091 - mse: 7.9163 - val_loss: 0.0121 - val_mse: 10.5258\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0090 - mse: 7.8770 - val_loss: 0.0121 - val_mse: 10.4993\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0090 - mse: 7.8380 - val_loss: 0.0120 - val_mse: 10.4743\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0090 - mse: 7.7993 - val_loss: 0.0120 - val_mse: 10.4497\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0089 - mse: 7.7610 - val_loss: 0.0120 - val_mse: 10.4250\n",
      "Epoch 184/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0089 - mse: 7.7229 - val_loss: 0.0119 - val_mse: 10.4000\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0088 - mse: 7.6851 - val_loss: 0.0119 - val_mse: 10.3750\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0088 - mse: 7.6473 - val_loss: 0.0119 - val_mse: 10.3507\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0087 - mse: 7.6099 - val_loss: 0.0119 - val_mse: 10.3278\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0087 - mse: 7.5728 - val_loss: 0.0118 - val_mse: 10.3058\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0087 - mse: 7.5358 - val_loss: 0.0118 - val_mse: 10.2840\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0086 - mse: 7.4990 - val_loss: 0.0118 - val_mse: 10.2634\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0086 - mse: 7.4625 - val_loss: 0.0118 - val_mse: 10.2436\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0085 - mse: 7.4263 - val_loss: 0.0117 - val_mse: 10.2245\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0085 - mse: 7.3909 - val_loss: 0.0117 - val_mse: 10.2050\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0084 - mse: 7.3561 - val_loss: 0.0117 - val_mse: 10.1848\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0084 - mse: 7.3217 - val_loss: 0.0117 - val_mse: 10.1652\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0084 - mse: 7.2879 - val_loss: 0.0117 - val_mse: 10.1455\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0083 - mse: 7.2542 - val_loss: 0.0116 - val_mse: 10.1261\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0083 - mse: 7.2205 - val_loss: 0.0116 - val_mse: 10.1076\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0083 - mse: 7.1867 - val_loss: 0.0116 - val_mse: 10.0896\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0082 - mse: 7.1532 - val_loss: 0.0116 - val_mse: 10.0733\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0082 - mse: 7.1203 - val_loss: 0.0115 - val_mse: 10.0571\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0081 - mse: 7.0874 - val_loss: 0.0115 - val_mse: 10.0400\n",
      "Epoch 203/1000\n",
      "\n",
      "Epoch 00202: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0081 - mse: 7.0545 - val_loss: 0.0115 - val_mse: 10.0227\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0081 - mse: 7.0219 - val_loss: 0.0115 - val_mse: 10.0115\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0080 - mse: 6.9991 - val_loss: 0.0115 - val_mse: 10.0009\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0080 - mse: 6.9762 - val_loss: 0.0115 - val_mse: 9.9901\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0080 - mse: 6.9534 - val_loss: 0.0115 - val_mse: 9.9787\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0080 - mse: 6.9306 - val_loss: 0.0114 - val_mse: 9.9668\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0079 - mse: 6.9079 - val_loss: 0.0114 - val_mse: 9.9554\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0079 - mse: 6.8852 - val_loss: 0.0114 - val_mse: 9.9447\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0079 - mse: 6.8627 - val_loss: 0.0114 - val_mse: 9.9344\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0079 - mse: 6.8405 - val_loss: 0.0114 - val_mse: 9.9241\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0078 - mse: 6.8186 - val_loss: 0.0114 - val_mse: 9.9130\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0078 - mse: 6.7966 - val_loss: 0.0114 - val_mse: 9.9011\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0078 - mse: 6.7748 - val_loss: 0.0114 - val_mse: 9.8888\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0078 - mse: 6.7530 - val_loss: 0.0113 - val_mse: 9.8766\n",
      "Epoch 217/1000\n",
      "\n",
      "Epoch 00216: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0077 - mse: 6.7314 - val_loss: 0.0113 - val_mse: 9.8643\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0077 - mse: 6.7096 - val_loss: 0.0113 - val_mse: 9.8556\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0077 - mse: 6.6944 - val_loss: 0.0113 - val_mse: 9.8470\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0077 - mse: 6.6791 - val_loss: 0.0113 - val_mse: 9.8384\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0077 - mse: 6.6639 - val_loss: 0.0113 - val_mse: 9.8300\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0076 - mse: 6.6487 - val_loss: 0.0113 - val_mse: 9.8217\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0076 - mse: 6.6335 - val_loss: 0.0113 - val_mse: 9.8134\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0076 - mse: 6.6183 - val_loss: 0.0113 - val_mse: 9.8050\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0076 - mse: 6.6031 - val_loss: 0.0112 - val_mse: 9.7966\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0076 - mse: 6.5879 - val_loss: 0.0112 - val_mse: 9.7881\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0075 - mse: 6.5726 - val_loss: 0.0112 - val_mse: 9.7795\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0075 - mse: 6.5573 - val_loss: 0.0112 - val_mse: 9.7710\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0075 - mse: 6.5421 - val_loss: 0.0112 - val_mse: 9.7624\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0075 - mse: 6.5269 - val_loss: 0.0112 - val_mse: 9.7542\n",
      "Epoch 231/1000\n",
      "\n",
      "Epoch 00230: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0075 - mse: 6.5118 - val_loss: 0.0112 - val_mse: 9.7460\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0075 - mse: 6.4967 - val_loss: 0.0112 - val_mse: 9.7404\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0074 - mse: 6.4860 - val_loss: 0.0112 - val_mse: 9.7352\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0074 - mse: 6.4754 - val_loss: 0.0112 - val_mse: 9.7302\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0074 - mse: 6.4648 - val_loss: 0.0112 - val_mse: 9.7255\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0074 - mse: 6.4542 - val_loss: 0.0112 - val_mse: 9.7210\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0074 - mse: 6.4436 - val_loss: 0.0112 - val_mse: 9.7165\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0074 - mse: 6.4331 - val_loss: 0.0112 - val_mse: 9.7117\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0074 - mse: 6.4227 - val_loss: 0.0111 - val_mse: 9.7066\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0074 - mse: 6.4123 - val_loss: 0.0111 - val_mse: 9.7014\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0074 - mse: 6.4018 - val_loss: 0.0111 - val_mse: 9.6962\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0073 - mse: 6.3914 - val_loss: 0.0111 - val_mse: 9.6909\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.0073 - mse: 6.3810 - val_loss: 0.0111 - val_mse: 9.6857\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0073 - mse: 6.3706 - val_loss: 0.0111 - val_mse: 9.6805\n",
      "Epoch 245/1000\n",
      "\n",
      "Epoch 00244: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0073 - mse: 6.3603 - val_loss: 0.0111 - val_mse: 9.6753\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0073 - mse: 6.3499 - val_loss: 0.0111 - val_mse: 9.6716\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0073 - mse: 6.3426 - val_loss: 0.0111 - val_mse: 9.6678\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0073 - mse: 6.3354 - val_loss: 0.0111 - val_mse: 9.6640\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0073 - mse: 6.3282 - val_loss: 0.0111 - val_mse: 9.6603\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0073 - mse: 6.3210 - val_loss: 0.0111 - val_mse: 9.6566\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0073 - mse: 6.3138 - val_loss: 0.0111 - val_mse: 9.6530\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0072 - mse: 6.3066 - val_loss: 0.0111 - val_mse: 9.6495\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0072 - mse: 6.2994 - val_loss: 0.0111 - val_mse: 9.6460\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.0072 - mse: 6.2923 - val_loss: 0.0111 - val_mse: 9.6427\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0072 - mse: 6.2852 - val_loss: 0.0111 - val_mse: 9.6395\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0072 - mse: 6.2781 - val_loss: 0.0111 - val_mse: 9.6364\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.0072 - mse: 6.2710 - val_loss: 0.0111 - val_mse: 9.6334\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0072 - mse: 6.2639 - val_loss: 0.0111 - val_mse: 9.6305\n",
      "Epoch 259/1000\n",
      "\n",
      "Epoch 00258: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0072 - mse: 6.2568 - val_loss: 0.0111 - val_mse: 9.6277\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0072 - mse: 6.2497 - val_loss: 0.0111 - val_mse: 9.6257\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.0072 - mse: 6.2447 - val_loss: 0.0111 - val_mse: 9.6238\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.0072 - mse: 6.2398 - val_loss: 0.0110 - val_mse: 9.6219\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0072 - mse: 6.2348 - val_loss: 0.0110 - val_mse: 9.6199\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.0072 - mse: 6.2299 - val_loss: 0.0110 - val_mse: 9.6179\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0071 - mse: 6.2249 - val_loss: 0.0110 - val_mse: 9.6160\n",
      "Epoch 266/1000\n",
      "1s - loss: 0.0071 - mse: 6.2200 - val_loss: 0.0110 - val_mse: 9.6142\n",
      "Epoch 267/1000\n",
      "1s - loss: 0.0071 - mse: 6.2150 - val_loss: 0.0110 - val_mse: 9.6125\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0071 - mse: 6.2101 - val_loss: 0.0110 - val_mse: 9.6107\n",
      "Epoch 269/1000\n",
      "1s - loss: 0.0071 - mse: 6.2053 - val_loss: 0.0110 - val_mse: 9.6088\n",
      "Epoch 270/1000\n",
      "1s - loss: 0.0071 - mse: 6.2004 - val_loss: 0.0110 - val_mse: 9.6069\n",
      "Epoch 271/1000\n",
      "1s - loss: 0.0071 - mse: 6.1956 - val_loss: 0.0110 - val_mse: 9.6049\n",
      "Epoch 272/1000\n",
      "1s - loss: 0.0071 - mse: 6.1907 - val_loss: 0.0110 - val_mse: 9.6029\n",
      "Epoch 273/1000\n",
      "\n",
      "Epoch 00272: reducing learning rate to 0.000823543069419.\n",
      "1s - loss: 0.0071 - mse: 6.1859 - val_loss: 0.0110 - val_mse: 9.6009\n",
      "Epoch 274/1000\n",
      "1s - loss: 0.0071 - mse: 6.1810 - val_loss: 0.0110 - val_mse: 9.5995\n",
      "Epoch 275/1000\n",
      "1s - loss: 0.0071 - mse: 6.1777 - val_loss: 0.0110 - val_mse: 9.5982\n",
      "Epoch 276/1000\n",
      "1s - loss: 0.0071 - mse: 6.1743 - val_loss: 0.0110 - val_mse: 9.5969\n",
      "Epoch 277/1000\n",
      "1s - loss: 0.0071 - mse: 6.1710 - val_loss: 0.0110 - val_mse: 9.5955\n",
      "Epoch 278/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0071 - mse: 6.1676 - val_loss: 0.0110 - val_mse: 9.5942\n",
      "Epoch 279/1000\n",
      "1s - loss: 0.0071 - mse: 6.1643 - val_loss: 0.0110 - val_mse: 9.5928\n",
      "Epoch 280/1000\n",
      "1s - loss: 0.0071 - mse: 6.1609 - val_loss: 0.0110 - val_mse: 9.5915\n",
      "Epoch 281/1000\n",
      "1s - loss: 0.0071 - mse: 6.1576 - val_loss: 0.0110 - val_mse: 9.5902\n",
      "Epoch 282/1000\n",
      "1s - loss: 0.0071 - mse: 6.1543 - val_loss: 0.0110 - val_mse: 9.5889\n",
      "Epoch 283/1000\n",
      "1s - loss: 0.0071 - mse: 6.1509 - val_loss: 0.0110 - val_mse: 9.5874\n",
      "Epoch 284/1000\n",
      "1s - loss: 0.0071 - mse: 6.1476 - val_loss: 0.0110 - val_mse: 9.5859\n",
      "Epoch 285/1000\n",
      "1s - loss: 0.0071 - mse: 6.1442 - val_loss: 0.0110 - val_mse: 9.5842\n",
      "Epoch 286/1000\n",
      "1s - loss: 0.0071 - mse: 6.1409 - val_loss: 0.0110 - val_mse: 9.5825\n",
      "Epoch 287/1000\n",
      "\n",
      "Epoch 00286: reducing learning rate to 0.000576480140444.\n",
      "1s - loss: 0.0070 - mse: 6.1375 - val_loss: 0.0110 - val_mse: 9.5807\n",
      "Epoch 288/1000\n",
      "1s - loss: 0.0070 - mse: 6.1342 - val_loss: 0.0110 - val_mse: 9.5795\n",
      "Epoch 289/1000\n",
      "1s - loss: 0.0070 - mse: 6.1319 - val_loss: 0.0110 - val_mse: 9.5784\n",
      "Epoch 290/1000\n",
      "1s - loss: 0.0070 - mse: 6.1295 - val_loss: 0.0110 - val_mse: 9.5774\n",
      "Epoch 291/1000\n",
      "1s - loss: 0.0070 - mse: 6.1272 - val_loss: 0.0110 - val_mse: 9.5764\n",
      "Epoch 292/1000\n",
      "1s - loss: 0.0070 - mse: 6.1249 - val_loss: 0.0110 - val_mse: 9.5755\n",
      "Epoch 293/1000\n",
      "1s - loss: 0.0070 - mse: 6.1226 - val_loss: 0.0110 - val_mse: 9.5745\n",
      "Epoch 294/1000\n",
      "1s - loss: 0.0070 - mse: 6.1202 - val_loss: 0.0110 - val_mse: 9.5736\n",
      "Epoch 295/1000\n",
      "1s - loss: 0.0070 - mse: 6.1179 - val_loss: 0.0110 - val_mse: 9.5727\n",
      "Epoch 296/1000\n",
      "1s - loss: 0.0070 - mse: 6.1156 - val_loss: 0.0110 - val_mse: 9.5718\n",
      "Epoch 297/1000\n",
      "1s - loss: 0.0070 - mse: 6.1133 - val_loss: 0.0110 - val_mse: 9.5710\n",
      "Epoch 298/1000\n",
      "1s - loss: 0.0070 - mse: 6.1110 - val_loss: 0.0110 - val_mse: 9.5700\n",
      "Epoch 299/1000\n",
      "1s - loss: 0.0070 - mse: 6.1086 - val_loss: 0.0110 - val_mse: 9.5690\n",
      "Epoch 300/1000\n",
      "1s - loss: 0.0070 - mse: 6.1063 - val_loss: 0.0110 - val_mse: 9.5679\n",
      "Epoch 301/1000\n",
      "\n",
      "Epoch 00300: reducing learning rate to 0.000403536082013.\n",
      "1s - loss: 0.0070 - mse: 6.1040 - val_loss: 0.0110 - val_mse: 9.5668\n",
      "Epoch 302/1000\n",
      "1s - loss: 0.0070 - mse: 6.1017 - val_loss: 0.0110 - val_mse: 9.5660\n",
      "[{'loss': 0.0070067006163299084, 'width0': 135, 'width1': 150, 'val_mse': 9.5660200119018555, 'mse': 6.101707935333252, 'val_loss': 0.010984830558300018, 'dropout1': 0.0, 'dropout0': 0}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.0130 - mse: 11.3188 - val_loss: 1.6522 - val_mse: 1438.8083\n",
      "Epoch 2/1000\n",
      "1s - loss: 1.5405 - mse: 1341.5188 - val_loss: 0.3892 - val_mse: 338.8870\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.3898 - mse: 339.4636 - val_loss: 0.3059 - val_mse: 266.4151\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.2760 - mse: 240.3912 - val_loss: 0.2583 - val_mse: 224.9667\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.2478 - mse: 215.7588 - val_loss: 0.2123 - val_mse: 184.8978\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.2034 - mse: 177.0938 - val_loss: 0.1894 - val_mse: 164.9477\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.1796 - mse: 156.3925 - val_loss: 0.1701 - val_mse: 148.1096\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.1638 - mse: 142.6672 - val_loss: 0.1500 - val_mse: 130.5898\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.1429 - mse: 124.4588 - val_loss: 0.1143 - val_mse: 99.5282\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.1176 - mse: 102.4143 - val_loss: 0.1044 - val_mse: 90.9178\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.1034 - mse: 90.0794 - val_loss: 0.0958 - val_mse: 83.4513\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.0969 - mse: 84.3532 - val_loss: 0.0961 - val_mse: 83.7203\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0983 - mse: 85.5954 - val_loss: 0.0804 - val_mse: 70.0301\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0738 - mse: 64.2771 - val_loss: 0.0951 - val_mse: 82.8295\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0936 - mse: 81.5533 - val_loss: 0.0718 - val_mse: 62.4914\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0668 - mse: 58.1810 - val_loss: 0.0593 - val_mse: 51.6495\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0551 - mse: 47.9855 - val_loss: 0.0832 - val_mse: 72.4181\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0656 - mse: 57.1345 - val_loss: 0.0685 - val_mse: 59.6674\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0665 - mse: 57.8707 - val_loss: 0.0628 - val_mse: 54.6707\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0608 - mse: 52.9495 - val_loss: 0.0742 - val_mse: 64.6180\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.00699999984354.\n",
      "2s - loss: 0.0649 - mse: 56.5427 - val_loss: 0.0596 - val_mse: 51.8981\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0609 - mse: 53.0595 - val_loss: 0.0527 - val_mse: 45.8678\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0460 - mse: 40.0229 - val_loss: 0.0462 - val_mse: 40.2645\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0445 - mse: 38.7643 - val_loss: 0.0426 - val_mse: 37.1081\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0412 - mse: 35.9198 - val_loss: 0.0412 - val_mse: 35.8451\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0390 - mse: 33.9715 - val_loss: 0.0432 - val_mse: 37.5970\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0397 - mse: 34.5847 - val_loss: 0.0464 - val_mse: 40.4346\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0458 - mse: 39.8410 - val_loss: 0.0441 - val_mse: 38.4347\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0378 - mse: 32.9001 - val_loss: 0.0383 - val_mse: 33.3354\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0347 - mse: 30.1976 - val_loss: 0.0358 - val_mse: 31.1582\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0328 - mse: 28.5428 - val_loss: 0.0348 - val_mse: 30.3170\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0325 - mse: 28.3133 - val_loss: 0.0331 - val_mse: 28.8422\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0381 - mse: 33.1873 - val_loss: 0.0301 - val_mse: 26.2470\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0299 - mse: 26.0718 - val_loss: 0.0295 - val_mse: 25.6688\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0288 - mse: 25.1231 - val_loss: 0.0289 - val_mse: 25.1589\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0292 - mse: 25.4515 - val_loss: 0.0280 - val_mse: 24.3879\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0263 - mse: 22.8617 - val_loss: 0.0280 - val_mse: 24.3926\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0257 - mse: 22.4186 - val_loss: 0.0275 - val_mse: 23.9910\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0279 - mse: 24.2905 - val_loss: 0.0265 - val_mse: 23.1127\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0279 - mse: 24.2856 - val_loss: 0.0287 - val_mse: 24.9992\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0294 - mse: 25.6152 - val_loss: 0.0280 - val_mse: 24.4231\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0264 - mse: 23.0030 - val_loss: 0.0271 - val_mse: 23.6000\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0263 - mse: 22.9259 - val_loss: 0.0262 - val_mse: 22.8119\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0240 - mse: 20.9092 - val_loss: 0.0244 - val_mse: 21.2381\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0215 - mse: 18.7063 - val_loss: 0.0243 - val_mse: 21.1857\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0233 - mse: 20.2766 - val_loss: 0.0239 - val_mse: 20.8094\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0216 - mse: 18.8167 - val_loss: 0.0235 - val_mse: 20.4595\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0212 - mse: 18.4345 - val_loss: 0.0232 - val_mse: 20.1665\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0212 - mse: 18.4936 - val_loss: 0.0233 - val_mse: 20.2502\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0220 - mse: 19.1437 - val_loss: 0.0231 - val_mse: 20.1008\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0225 - mse: 19.6325 - val_loss: 0.0218 - val_mse: 19.0260\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0231 - mse: 20.1418 - val_loss: 0.0275 - val_mse: 23.9586\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0213 - mse: 18.5554 - val_loss: 0.0341 - val_mse: 29.7009\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0281 - mse: 24.4835 - val_loss: 0.0289 - val_mse: 25.1351\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0322 - mse: 28.0203 - val_loss: 0.0220 - val_mse: 19.1580\n",
      "Epoch 56/1000\n",
      "\n",
      "Epoch 00055: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0215 - mse: 18.7319 - val_loss: 0.0316 - val_mse: 27.5592\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0256 - mse: 22.3336 - val_loss: 0.0352 - val_mse: 30.6689\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0288 - mse: 25.1009 - val_loss: 0.0302 - val_mse: 26.2991\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0334 - mse: 29.0623 - val_loss: 0.0247 - val_mse: 21.5209\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0266 - mse: 23.1455 - val_loss: 0.0271 - val_mse: 23.6389\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0219 - mse: 19.1003 - val_loss: 0.0307 - val_mse: 26.7575\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0294 - mse: 25.6306 - val_loss: 0.0267 - val_mse: 23.2257\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0226 - mse: 19.7071 - val_loss: 0.0216 - val_mse: 18.8433\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0208 - mse: 18.1445 - val_loss: 0.0223 - val_mse: 19.4456\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0215 - mse: 18.7214 - val_loss: 0.0293 - val_mse: 25.5180\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0328 - mse: 28.5405 - val_loss: 0.0292 - val_mse: 25.4537\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0246 - mse: 21.4318 - val_loss: 0.0234 - val_mse: 20.3566\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0215 - mse: 18.7307 - val_loss: 0.0182 - val_mse: 15.8080\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0161 - mse: 14.0191 - val_loss: 0.0187 - val_mse: 16.3163\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0175 - mse: 15.2093 - val_loss: 0.0224 - val_mse: 19.4638\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0245 - mse: 21.3328 - val_loss: 0.0268 - val_mse: 23.3291\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0256 - mse: 22.2732 - val_loss: 0.0253 - val_mse: 22.0025\n",
      "Epoch 73/1000\n",
      "\n",
      "Epoch 00072: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0240 - mse: 20.9118 - val_loss: 0.0204 - val_mse: 17.7411\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0177 - mse: 15.4239 - val_loss: 0.0181 - val_mse: 15.7805\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0198 - mse: 17.2373 - val_loss: 0.0177 - val_mse: 15.4142\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0171 - mse: 14.9034 - val_loss: 0.0194 - val_mse: 16.8711\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0189 - mse: 16.4841 - val_loss: 0.0214 - val_mse: 18.5970\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0193 - mse: 16.8100 - val_loss: 0.0219 - val_mse: 19.0484\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0170 - mse: 14.8154 - val_loss: 0.0211 - val_mse: 18.3733\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0190 - mse: 16.5531 - val_loss: 0.0198 - val_mse: 17.2402\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0217 - mse: 18.9148 - val_loss: 0.0178 - val_mse: 15.4602\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0156 - mse: 13.5628 - val_loss: 0.0174 - val_mse: 15.1212\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0193 - mse: 16.7809 - val_loss: 0.0178 - val_mse: 15.4640\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0160 - mse: 13.9278 - val_loss: 0.0190 - val_mse: 16.5056\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0209 - mse: 18.1806 - val_loss: 0.0191 - val_mse: 16.6482\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0218 - mse: 18.9802 - val_loss: 0.0180 - val_mse: 15.7133\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0175 - mse: 15.2123 - val_loss: 0.0172 - val_mse: 14.9917\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0164 - mse: 14.2464 - val_loss: 0.0174 - val_mse: 15.1913\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0153 - mse: 13.3325 - val_loss: 0.0187 - val_mse: 16.3008\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0201 - mse: 17.5256 - val_loss: 0.0186 - val_mse: 16.1842\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0174 - mse: 15.1404 - val_loss: 0.0176 - val_mse: 15.3698\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0176 - mse: 15.3628 - val_loss: 0.0163 - val_mse: 14.1973\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0164 - mse: 14.2512 - val_loss: 0.0160 - val_mse: 13.9369\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0151 - mse: 13.1535 - val_loss: 0.0164 - val_mse: 14.3180\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0173 - mse: 15.0249 - val_loss: 0.0180 - val_mse: 15.6536\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0218 - mse: 18.9671 - val_loss: 0.0178 - val_mse: 15.4672\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0166 - mse: 14.4448 - val_loss: 0.0176 - val_mse: 15.3342\n",
      "Epoch 98/1000\n",
      "\n",
      "Epoch 00097: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0148 - mse: 12.8950 - val_loss: 0.0170 - val_mse: 14.7678\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0183 - mse: 15.9333 - val_loss: 0.0161 - val_mse: 13.9841\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0170 - mse: 14.7751 - val_loss: 0.0157 - val_mse: 13.6576\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0147 - mse: 12.7764 - val_loss: 0.0155 - val_mse: 13.5147\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0165 - mse: 14.3363 - val_loss: 0.0156 - val_mse: 13.5439\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0145 - mse: 12.6671 - val_loss: 0.0160 - val_mse: 13.9555\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0211 - mse: 18.3824 - val_loss: 0.0159 - val_mse: 13.8473\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0168 - mse: 14.6471 - val_loss: 0.0153 - val_mse: 13.3529\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0134 - mse: 11.6496 - val_loss: 0.0150 - val_mse: 13.0950\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0163 - mse: 14.2341 - val_loss: 0.0151 - val_mse: 13.1367\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0141 - mse: 12.3191 - val_loss: 0.0152 - val_mse: 13.2091\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0149 - mse: 12.9442 - val_loss: 0.0151 - val_mse: 13.1417\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0155 - mse: 13.4802 - val_loss: 0.0150 - val_mse: 13.0301\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0163 - mse: 14.1560 - val_loss: 0.0149 - val_mse: 13.0037\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0140 - mse: 12.1837 - val_loss: 0.0150 - val_mse: 13.0388\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0145 - mse: 12.5905 - val_loss: 0.0150 - val_mse: 13.1032\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0147 - mse: 12.8345 - val_loss: 0.0153 - val_mse: 13.3543\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0156 - mse: 13.6093 - val_loss: 0.0160 - val_mse: 13.9171\n",
      "Epoch 116/1000\n",
      "\n",
      "Epoch 00115: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0151 - mse: 13.1279 - val_loss: 0.0164 - val_mse: 14.3094\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0133 - mse: 11.5934 - val_loss: 0.0165 - val_mse: 14.3778\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0146 - mse: 12.6885 - val_loss: 0.0164 - val_mse: 14.2537\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0138 - mse: 11.9906 - val_loss: 0.0161 - val_mse: 14.0086\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0160 - mse: 13.9139 - val_loss: 0.0154 - val_mse: 13.4164\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0137 - mse: 11.9088 - val_loss: 0.0148 - val_mse: 12.9302\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0137 - mse: 11.9298 - val_loss: 0.0144 - val_mse: 12.5563\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0148 - mse: 12.8753 - val_loss: 0.0141 - val_mse: 12.3044\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0128 - mse: 11.1668 - val_loss: 0.0142 - val_mse: 12.3423\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0144 - mse: 12.5222 - val_loss: 0.0142 - val_mse: 12.3884\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0152 - mse: 13.2599 - val_loss: 0.0141 - val_mse: 12.2924\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0139 - mse: 12.1244 - val_loss: 0.0140 - val_mse: 12.1858\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0152 - mse: 13.2130 - val_loss: 0.0139 - val_mse: 12.0668\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0130 - mse: 11.3597 - val_loss: 0.0139 - val_mse: 12.0964\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0141 - mse: 12.2511 - val_loss: 0.0141 - val_mse: 12.3044\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0155 - mse: 13.5237 - val_loss: 0.0143 - val_mse: 12.4250\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0178 - mse: 15.5210 - val_loss: 0.0143 - val_mse: 12.4514\n",
      "Epoch 133/1000\n",
      "\n",
      "Epoch 00132: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0149 - mse: 12.9521 - val_loss: 0.0143 - val_mse: 12.4758\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0158 - mse: 13.7478 - val_loss: 0.0144 - val_mse: 12.5185\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0148 - mse: 12.8559 - val_loss: 0.0145 - val_mse: 12.6348\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0165 - mse: 14.3778 - val_loss: 0.0148 - val_mse: 12.8908\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0127 - mse: 11.0657 - val_loss: 0.0151 - val_mse: 13.1617\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0182 - mse: 15.8680 - val_loss: 0.0150 - val_mse: 13.0642\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0145 - mse: 12.6040 - val_loss: 0.0147 - val_mse: 12.7652\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0160 - mse: 13.9720 - val_loss: 0.0142 - val_mse: 12.3873\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0150 - mse: 13.0530 - val_loss: 0.0139 - val_mse: 12.1024\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0127 - mse: 11.0316 - val_loss: 0.0138 - val_mse: 12.0460\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0135 - mse: 11.7482 - val_loss: 0.0139 - val_mse: 12.1417\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0144 - mse: 12.5451 - val_loss: 0.0141 - val_mse: 12.2617\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0181 - mse: 15.7414 - val_loss: 0.0140 - val_mse: 12.2347\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0142 - mse: 12.3655 - val_loss: 0.0139 - val_mse: 12.1403\n",
      "Epoch 147/1000\n",
      "\n",
      "Epoch 00146: reducing learning rate to 0.000823543069419.\n",
      "1s - loss: 0.0145 - mse: 12.6336 - val_loss: 0.0138 - val_mse: 12.0195\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0191 - mse: 16.6431 - val_loss: 0.0137 - val_mse: 11.9658\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0139 - mse: 12.0723 - val_loss: 0.0137 - val_mse: 11.9631\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0148 - mse: 12.8777 - val_loss: 0.0137 - val_mse: 11.9659\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0125 - mse: 10.8962 - val_loss: 0.0138 - val_mse: 12.0037\n",
      "Epoch 152/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0151 - mse: 13.1258 - val_loss: 0.0137 - val_mse: 11.9700\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0144 - mse: 12.5596 - val_loss: 0.0138 - val_mse: 12.0140\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0173 - mse: 15.0239 - val_loss: 0.0139 - val_mse: 12.0955\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0162 - mse: 14.0749 - val_loss: 0.0139 - val_mse: 12.0627\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0153 - mse: 13.3171 - val_loss: 0.0137 - val_mse: 11.9607\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0128 - mse: 11.1609 - val_loss: 0.0137 - val_mse: 11.9310\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0156 - mse: 13.6070 - val_loss: 0.0136 - val_mse: 11.8816\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0154 - mse: 13.4217 - val_loss: 0.0137 - val_mse: 11.9403\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0133 - mse: 11.6021 - val_loss: 0.0137 - val_mse: 11.9740\n",
      "Epoch 161/1000\n",
      "\n",
      "Epoch 00160: reducing learning rate to 0.000576480140444.\n",
      "1s - loss: 0.0239 - mse: 20.7887 - val_loss: 0.0138 - val_mse: 11.9805\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0164 - mse: 14.3232 - val_loss: 0.0137 - val_mse: 11.9615\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0131 - mse: 11.4019 - val_loss: 0.0137 - val_mse: 11.8955\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0142 - mse: 12.3662 - val_loss: 0.0136 - val_mse: 11.8182\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0128 - mse: 11.1467 - val_loss: 0.0135 - val_mse: 11.7397\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0123 - mse: 10.7224 - val_loss: 0.0134 - val_mse: 11.6699\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0146 - mse: 12.7011 - val_loss: 0.0134 - val_mse: 11.6275\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0116 - mse: 10.1102 - val_loss: 0.0133 - val_mse: 11.6019\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0126 - mse: 10.9574 - val_loss: 0.0133 - val_mse: 11.5933\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0123 - mse: 10.7112 - val_loss: 0.0133 - val_mse: 11.5784\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0124 - mse: 10.7855 - val_loss: 0.0133 - val_mse: 11.5735\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0142 - mse: 12.3851 - val_loss: 0.0133 - val_mse: 11.5688\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0152 - mse: 13.2414 - val_loss: 0.0132 - val_mse: 11.5377\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0169 - mse: 14.6929 - val_loss: 0.0132 - val_mse: 11.4827\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0122 - mse: 10.6019 - val_loss: 0.0131 - val_mse: 11.4485\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0138 - mse: 12.0307 - val_loss: 0.0131 - val_mse: 11.4328\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0135 - mse: 11.7633 - val_loss: 0.0131 - val_mse: 11.4320\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0142 - mse: 12.3961 - val_loss: 0.0132 - val_mse: 11.4551\n",
      "Epoch 179/1000\n",
      "\n",
      "Epoch 00178: reducing learning rate to 0.000403536082013.\n",
      "1s - loss: 0.0116 - mse: 10.0788 - val_loss: 0.0132 - val_mse: 11.4850\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0119 - mse: 10.3647 - val_loss: 0.0132 - val_mse: 11.5129\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0118 - mse: 10.2456 - val_loss: 0.0132 - val_mse: 11.5383\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0132 - mse: 11.4611 - val_loss: 0.0133 - val_mse: 11.5496\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0121 - mse: 10.5448 - val_loss: 0.0133 - val_mse: 11.5668\n",
      "Epoch 184/1000\n",
      "1s - loss: 0.0122 - mse: 10.6335 - val_loss: 0.0133 - val_mse: 11.5974\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0123 - mse: 10.7331 - val_loss: 0.0133 - val_mse: 11.6177\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.0129 - mse: 11.2004 - val_loss: 0.0134 - val_mse: 11.6401\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0157 - mse: 13.6334 - val_loss: 0.0133 - val_mse: 11.6224\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0143 - mse: 12.4288 - val_loss: 0.0133 - val_mse: 11.5826\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0119 - mse: 10.3691 - val_loss: 0.0133 - val_mse: 11.5580\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0120 - mse: 10.4115 - val_loss: 0.0132 - val_mse: 11.5213\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0131 - mse: 11.4330 - val_loss: 0.0132 - val_mse: 11.4724\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0125 - mse: 10.9201 - val_loss: 0.0131 - val_mse: 11.4251\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0152 - mse: 13.2478 - val_loss: 0.0131 - val_mse: 11.3948\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0124 - mse: 10.7738 - val_loss: 0.0131 - val_mse: 11.3722\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0132 - mse: 11.4871 - val_loss: 0.0130 - val_mse: 11.3621\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0127 - mse: 11.0806 - val_loss: 0.0131 - val_mse: 11.3646\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0128 - mse: 11.1156 - val_loss: 0.0131 - val_mse: 11.3677\n",
      "Epoch 198/1000\n",
      "\n",
      "Epoch 00197: reducing learning rate to 0.000282475253334.\n",
      "1s - loss: 0.0127 - mse: 11.0253 - val_loss: 0.0130 - val_mse: 11.3610\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0130 - mse: 11.3057 - val_loss: 0.0130 - val_mse: 11.3511\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0131 - mse: 11.4089 - val_loss: 0.0130 - val_mse: 11.3402\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0126 - mse: 10.9478 - val_loss: 0.0130 - val_mse: 11.3221\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0132 - mse: 11.5014 - val_loss: 0.0130 - val_mse: 11.3156\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0121 - mse: 10.5708 - val_loss: 0.0130 - val_mse: 11.3085\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0126 - mse: 10.9308 - val_loss: 0.0130 - val_mse: 11.3023\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0122 - mse: 10.5913 - val_loss: 0.0130 - val_mse: 11.2932\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0123 - mse: 10.7255 - val_loss: 0.0130 - val_mse: 11.2897\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0121 - mse: 10.5355 - val_loss: 0.0130 - val_mse: 11.2845\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0151 - mse: 13.1480 - val_loss: 0.0129 - val_mse: 11.2729\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0126 - mse: 10.9933 - val_loss: 0.0129 - val_mse: 11.2658\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0120 - mse: 10.4510 - val_loss: 0.0129 - val_mse: 11.2564\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0121 - mse: 10.5065 - val_loss: 0.0129 - val_mse: 11.2426\n",
      "Epoch 212/1000\n",
      "\n",
      "Epoch 00211: reducing learning rate to 0.000197732681409.\n",
      "1s - loss: 0.0116 - mse: 10.1358 - val_loss: 0.0129 - val_mse: 11.2314\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0116 - mse: 10.1184 - val_loss: 0.0129 - val_mse: 11.2236\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0119 - mse: 10.3427 - val_loss: 0.0129 - val_mse: 11.2164\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0151 - mse: 13.1489 - val_loss: 0.0129 - val_mse: 11.2119\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0130 - mse: 11.3259 - val_loss: 0.0129 - val_mse: 11.2051\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0125 - mse: 10.9198 - val_loss: 0.0129 - val_mse: 11.1965\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0131 - mse: 11.4121 - val_loss: 0.0129 - val_mse: 11.1907\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0127 - mse: 11.0803 - val_loss: 0.0128 - val_mse: 11.1888\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0142 - mse: 12.3816 - val_loss: 0.0128 - val_mse: 11.1875\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0132 - mse: 11.5347 - val_loss: 0.0128 - val_mse: 11.1845\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0135 - mse: 11.7236 - val_loss: 0.0128 - val_mse: 11.1802\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0122 - mse: 10.6032 - val_loss: 0.0128 - val_mse: 11.1785\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0126 - mse: 10.9989 - val_loss: 0.0128 - val_mse: 11.1781\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0125 - mse: 10.9127 - val_loss: 0.0128 - val_mse: 11.1764\n",
      "Epoch 226/1000\n",
      "\n",
      "Epoch 00225: reducing learning rate to 0.000138412872911.\n",
      "1s - loss: 0.0134 - mse: 11.7078 - val_loss: 0.0128 - val_mse: 11.1752\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0124 - mse: 10.7871 - val_loss: 0.0128 - val_mse: 11.1747\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0136 - mse: 11.8852 - val_loss: 0.0128 - val_mse: 11.1762\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0131 - mse: 11.3753 - val_loss: 0.0128 - val_mse: 11.1782\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0116 - mse: 10.0758 - val_loss: 0.0128 - val_mse: 11.1806\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0130 - mse: 11.2845 - val_loss: 0.0128 - val_mse: 11.1794\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0124 - mse: 10.7986 - val_loss: 0.0128 - val_mse: 11.1755\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0132 - mse: 11.4640 - val_loss: 0.0128 - val_mse: 11.1741\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0116 - mse: 10.1286 - val_loss: 0.0128 - val_mse: 11.1733\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0126 - mse: 10.9982 - val_loss: 0.0128 - val_mse: 11.1731\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0138 - mse: 12.0435 - val_loss: 0.0128 - val_mse: 11.1722\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0124 - mse: 10.8331 - val_loss: 0.0128 - val_mse: 11.1731\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0150 - mse: 13.0381 - val_loss: 0.0128 - val_mse: 11.1753\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0131 - mse: 11.3798 - val_loss: 0.0128 - val_mse: 11.1756\n",
      "Epoch 240/1000\n",
      "\n",
      "Epoch 00239: reducing learning rate to 9.68890090007e-05.\n",
      "1s - loss: 0.0120 - mse: 10.4470 - val_loss: 0.0128 - val_mse: 11.1753\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0133 - mse: 11.5933 - val_loss: 0.0128 - val_mse: 11.1747\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0124 - mse: 10.7625 - val_loss: 0.0128 - val_mse: 11.1733\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.0123 - mse: 10.6990 - val_loss: 0.0128 - val_mse: 11.1704\n",
      "Epoch 244/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0167 - mse: 14.5079 - val_loss: 0.0128 - val_mse: 11.1672\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0120 - mse: 10.4205 - val_loss: 0.0128 - val_mse: 11.1644\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0120 - mse: 10.4649 - val_loss: 0.0128 - val_mse: 11.1618\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0113 - mse: 9.8535 - val_loss: 0.0128 - val_mse: 11.1593\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0114 - mse: 9.9379 - val_loss: 0.0128 - val_mse: 11.1568\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0164 - mse: 14.2639 - val_loss: 0.0128 - val_mse: 11.1550\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.0117 - mse: 10.1922 - val_loss: 0.0128 - val_mse: 11.1532\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0120 - mse: 10.4864 - val_loss: 0.0128 - val_mse: 11.1537\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0133 - mse: 11.5729 - val_loss: 0.0128 - val_mse: 11.1541\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0120 - mse: 10.4645 - val_loss: 0.0128 - val_mse: 11.1536\n",
      "Epoch 254/1000\n",
      "\n",
      "Epoch 00253: reducing learning rate to 6.78223063005e-05.\n",
      "1s - loss: 0.0117 - mse: 10.1870 - val_loss: 0.0128 - val_mse: 11.1537\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0121 - mse: 10.5683 - val_loss: 0.0128 - val_mse: 11.1530\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0130 - mse: 11.3140 - val_loss: 0.0128 - val_mse: 11.1534\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.0126 - mse: 10.9448 - val_loss: 0.0128 - val_mse: 11.1554\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0132 - mse: 11.4696 - val_loss: 0.0128 - val_mse: 11.1560\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.0121 - mse: 10.5054 - val_loss: 0.0128 - val_mse: 11.1556\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0145 - mse: 12.5844 - val_loss: 0.0128 - val_mse: 11.1544\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.0127 - mse: 11.0203 - val_loss: 0.0128 - val_mse: 11.1538\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.0118 - mse: 10.2841 - val_loss: 0.0128 - val_mse: 11.1541\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0118 - mse: 10.3161 - val_loss: 0.0128 - val_mse: 11.1539\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.0184 - mse: 16.0311 - val_loss: 0.0128 - val_mse: 11.1537\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0122 - mse: 10.6278 - val_loss: 0.0128 - val_mse: 11.1531\n",
      "Epoch 266/1000\n",
      "1s - loss: 0.0123 - mse: 10.6784 - val_loss: 0.0128 - val_mse: 11.1535\n",
      "[{'loss': 0.0070067006163299084, 'width0': 135, 'width1': 150, 'val_mse': 9.5660200119018555, 'mse': 6.101707935333252, 'val_loss': 0.010984830558300018, 'dropout1': 0.0, 'dropout0': 0}, {'loss': 0.012262199074029922, 'width0': 135, 'width1': 150, 'val_mse': 11.153453826904297, 'mse': 10.678401947021484, 'val_loss': 0.012807708233594894, 'dropout1': 0.10000000000000001, 'dropout0': 0}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_34 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 0.0144 - mse: 12.5246 - val_loss: 1.0793 - val_mse: 939.9023\n",
      "Epoch 2/1000\n",
      "1s - loss: 1.0006 - mse: 871.3946 - val_loss: 0.3035 - val_mse: 264.3270\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.3426 - mse: 298.3321 - val_loss: 0.1632 - val_mse: 142.1244\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.1977 - mse: 172.1987 - val_loss: 0.1186 - val_mse: 103.2402\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.0925 - mse: 80.5232 - val_loss: 0.1061 - val_mse: 92.3955\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.0909 - mse: 79.1767 - val_loss: 0.0880 - val_mse: 76.6512\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.0881 - mse: 76.6901 - val_loss: 0.1146 - val_mse: 99.8258\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.0860 - mse: 74.8730 - val_loss: 0.0916 - val_mse: 79.7539\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.0731 - mse: 63.6198 - val_loss: 0.1000 - val_mse: 87.0613\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.1203 - mse: 104.8007 - val_loss: 0.0706 - val_mse: 61.4846\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.0631 - mse: 54.9430 - val_loss: 0.1017 - val_mse: 88.5589\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.1080 - mse: 94.0670 - val_loss: 0.0818 - val_mse: 71.1994\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0840 - mse: 73.1072 - val_loss: 0.0592 - val_mse: 51.5861\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0592 - mse: 51.5310 - val_loss: 0.0814 - val_mse: 70.8858\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0864 - mse: 75.2651 - val_loss: 0.0781 - val_mse: 68.0169\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0750 - mse: 65.3496 - val_loss: 0.0551 - val_mse: 47.9686\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0350 - mse: 30.5005 - val_loss: 0.0522 - val_mse: 45.4235\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0664 - mse: 57.8097 - val_loss: 0.0506 - val_mse: 44.0460\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0546 - mse: 47.5496 - val_loss: 0.0513 - val_mse: 44.6783\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0525 - mse: 45.7034 - val_loss: 0.0447 - val_mse: 38.8908\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0495 - mse: 43.0665 - val_loss: 0.0351 - val_mse: 30.5257\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0380 - mse: 33.0908 - val_loss: 0.0477 - val_mse: 41.5531\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0493 - mse: 42.9265 - val_loss: 0.0658 - val_mse: 57.3161\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0488 - mse: 42.5110 - val_loss: 0.0616 - val_mse: 53.6815\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0796 - mse: 69.3373 - val_loss: 0.0384 - val_mse: 33.4398\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 00025: reducing learning rate to 0.00699999984354.\n",
      "2s - loss: 0.0449 - mse: 39.1166 - val_loss: 0.0486 - val_mse: 42.2951\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0414 - mse: 36.0118 - val_loss: 0.0603 - val_mse: 52.5320\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.1012 - mse: 88.1185 - val_loss: 0.0574 - val_mse: 49.9594\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0616 - mse: 53.6148 - val_loss: 0.0515 - val_mse: 44.8184\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0694 - mse: 60.4051 - val_loss: 0.0517 - val_mse: 45.0329\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0571 - mse: 49.7483 - val_loss: 0.0618 - val_mse: 53.7873\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0515 - mse: 44.8357 - val_loss: 0.0705 - val_mse: 61.4315\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0398 - mse: 34.6993 - val_loss: 0.0707 - val_mse: 61.5708\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0744 - mse: 64.7708 - val_loss: 0.0541 - val_mse: 47.0951\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0475 - mse: 41.3549 - val_loss: 0.0413 - val_mse: 35.9421\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0551 - mse: 47.9822 - val_loss: 0.0410 - val_mse: 35.6683\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0375 - mse: 32.6156 - val_loss: 0.0525 - val_mse: 45.7366\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0280 - mse: 24.3603 - val_loss: 0.0672 - val_mse: 58.5368\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0578 - mse: 50.3528 - val_loss: 0.0689 - val_mse: 60.0106\n",
      "Epoch 40/1000\n",
      "\n",
      "Epoch 00039: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0571 - mse: 49.7076 - val_loss: 0.0570 - val_mse: 49.6425\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0525 - mse: 45.7309 - val_loss: 0.0453 - val_mse: 39.4186\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0350 - mse: 30.4482 - val_loss: 0.0341 - val_mse: 29.6929\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0378 - mse: 32.9278 - val_loss: 0.0274 - val_mse: 23.8477\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0400 - mse: 34.8430 - val_loss: 0.0279 - val_mse: 24.2727\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0397 - mse: 34.5393 - val_loss: 0.0332 - val_mse: 28.9071\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0303 - mse: 26.3580 - val_loss: 0.0451 - val_mse: 39.2554\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0348 - mse: 30.2898 - val_loss: 0.0526 - val_mse: 45.7920\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0605 - mse: 52.7216 - val_loss: 0.0490 - val_mse: 42.6304\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0727 - mse: 63.3239 - val_loss: 0.0353 - val_mse: 30.7346\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0226 - mse: 19.6468 - val_loss: 0.0272 - val_mse: 23.6710\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0331 - mse: 28.8482 - val_loss: 0.0239 - val_mse: 20.7937\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0288 - mse: 25.0825 - val_loss: 0.0264 - val_mse: 22.9643\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0239 - mse: 20.7697 - val_loss: 0.0323 - val_mse: 28.1629\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0394 - mse: 34.3199 - val_loss: 0.0364 - val_mse: 31.6980\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0257 - mse: 22.4172 - val_loss: 0.0386 - val_mse: 33.6506\n",
      "Epoch 56/1000\n",
      "\n",
      "Epoch 00055: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0360 - mse: 31.3720 - val_loss: 0.0368 - val_mse: 32.0354\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0371 - mse: 32.2906 - val_loss: 0.0334 - val_mse: 29.0642\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0254 - mse: 22.1567 - val_loss: 0.0297 - val_mse: 25.8897\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0276 - mse: 24.0626 - val_loss: 0.0259 - val_mse: 22.5807\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0208 - mse: 18.0932 - val_loss: 0.0233 - val_mse: 20.3061\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0270 - mse: 23.5155 - val_loss: 0.0217 - val_mse: 18.8891\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0201 - mse: 17.4971 - val_loss: 0.0216 - val_mse: 18.7969\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0221 - mse: 19.2375 - val_loss: 0.0226 - val_mse: 19.6703\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0242 - mse: 21.0603 - val_loss: 0.0241 - val_mse: 20.9925\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0282 - mse: 24.5539 - val_loss: 0.0259 - val_mse: 22.5775\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0230 - mse: 20.0683 - val_loss: 0.0292 - val_mse: 25.4181\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0282 - mse: 24.5328 - val_loss: 0.0312 - val_mse: 27.1657\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0449 - mse: 39.1367 - val_loss: 0.0299 - val_mse: 26.0358\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0367 - mse: 31.9237 - val_loss: 0.0270 - val_mse: 23.5167\n",
      "Epoch 70/1000\n",
      "\n",
      "Epoch 00069: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0229 - mse: 19.9791 - val_loss: 0.0242 - val_mse: 21.0866\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0295 - mse: 25.7177 - val_loss: 0.0223 - val_mse: 19.4003\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0252 - mse: 21.9682 - val_loss: 0.0210 - val_mse: 18.2539\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0214 - mse: 18.6406 - val_loss: 0.0208 - val_mse: 18.0932\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0200 - mse: 17.4101 - val_loss: 0.0215 - val_mse: 18.7021\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0415 - mse: 36.1585 - val_loss: 0.0219 - val_mse: 19.0715\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0243 - mse: 21.1432 - val_loss: 0.0220 - val_mse: 19.1546\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0229 - mse: 19.9651 - val_loss: 0.0218 - val_mse: 18.9510\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0238 - mse: 20.7535 - val_loss: 0.0216 - val_mse: 18.7871\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0186 - mse: 16.2037 - val_loss: 0.0213 - val_mse: 18.5337\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0180 - mse: 15.6346 - val_loss: 0.0210 - val_mse: 18.2717\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0259 - mse: 22.5160 - val_loss: 0.0204 - val_mse: 17.7372\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0253 - mse: 21.9959 - val_loss: 0.0198 - val_mse: 17.2399\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0207 - mse: 18.0074 - val_loss: 0.0196 - val_mse: 17.0768\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0276 - mse: 24.0746 - val_loss: 0.0193 - val_mse: 16.7871\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0185 - mse: 16.1449 - val_loss: 0.0191 - val_mse: 16.6430\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0205 - mse: 17.8477 - val_loss: 0.0190 - val_mse: 16.5345\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0217 - mse: 18.8579 - val_loss: 0.0189 - val_mse: 16.4449\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0482 - mse: 41.9784 - val_loss: 0.0188 - val_mse: 16.3551\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0175 - mse: 15.2647 - val_loss: 0.0189 - val_mse: 16.4448\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0206 - mse: 17.9538 - val_loss: 0.0192 - val_mse: 16.6821\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0197 - mse: 17.1690 - val_loss: 0.0193 - val_mse: 16.8031\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0194 - mse: 16.8618 - val_loss: 0.0195 - val_mse: 16.9728\n",
      "Epoch 93/1000\n",
      "\n",
      "Epoch 00092: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0223 - mse: 19.4207 - val_loss: 0.0194 - val_mse: 16.9017\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0247 - mse: 21.4728 - val_loss: 0.0194 - val_mse: 16.8923\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0174 - mse: 15.1130 - val_loss: 0.0193 - val_mse: 16.7975\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0191 - mse: 16.6006 - val_loss: 0.0191 - val_mse: 16.6242\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0182 - mse: 15.8506 - val_loss: 0.0188 - val_mse: 16.3897\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0207 - mse: 18.0128 - val_loss: 0.0185 - val_mse: 16.1374\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0290 - mse: 25.2710 - val_loss: 0.0184 - val_mse: 16.0271\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0361 - mse: 31.4096 - val_loss: 0.0180 - val_mse: 15.6868\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0198 - mse: 17.2034 - val_loss: 0.0177 - val_mse: 15.4432\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0212 - mse: 18.4459 - val_loss: 0.0176 - val_mse: 15.3295\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0199 - mse: 17.3020 - val_loss: 0.0178 - val_mse: 15.5052\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0159 - mse: 13.8361 - val_loss: 0.0183 - val_mse: 15.9667\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0178 - mse: 15.5133 - val_loss: 0.0189 - val_mse: 16.4652\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0336 - mse: 29.2307 - val_loss: 0.0192 - val_mse: 16.7407\n",
      "Epoch 107/1000\n",
      "\n",
      "Epoch 00106: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0575 - mse: 50.0808 - val_loss: 0.0188 - val_mse: 16.3578\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0350 - mse: 30.4653 - val_loss: 0.0182 - val_mse: 15.8663\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0184 - mse: 16.0653 - val_loss: 0.0177 - val_mse: 15.4247\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0183 - mse: 15.9481 - val_loss: 0.0174 - val_mse: 15.1290\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0221 - mse: 19.2239 - val_loss: 0.0171 - val_mse: 14.8923\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0191 - mse: 16.6464 - val_loss: 0.0170 - val_mse: 14.7998\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0152 - mse: 13.2626 - val_loss: 0.0170 - val_mse: 14.7758\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0202 - mse: 17.5899 - val_loss: 0.0170 - val_mse: 14.7868\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0195 - mse: 16.9383 - val_loss: 0.0170 - val_mse: 14.8377\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0197 - mse: 17.1817 - val_loss: 0.0171 - val_mse: 14.8607\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0181 - mse: 15.7958 - val_loss: 0.0171 - val_mse: 14.9130\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0213 - mse: 18.5907 - val_loss: 0.0173 - val_mse: 15.0253\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0269 - mse: 23.4335 - val_loss: 0.0173 - val_mse: 15.0427\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0191 - mse: 16.6091 - val_loss: 0.0172 - val_mse: 14.9921\n",
      "Epoch 121/1000\n",
      "\n",
      "Epoch 00120: reducing learning rate to 0.000823543069419.\n",
      "1s - loss: 0.0192 - mse: 16.7038 - val_loss: 0.0171 - val_mse: 14.9067\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0175 - mse: 15.2232 - val_loss: 0.0171 - val_mse: 14.8660\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0287 - mse: 25.0085 - val_loss: 0.0170 - val_mse: 14.7995\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0221 - mse: 19.2574 - val_loss: 0.0169 - val_mse: 14.7540\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0162 - mse: 14.1121 - val_loss: 0.0169 - val_mse: 14.7259\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0170 - mse: 14.8049 - val_loss: 0.0169 - val_mse: 14.7111\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0178 - mse: 15.4861 - val_loss: 0.0169 - val_mse: 14.6888\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0159 - mse: 13.8871 - val_loss: 0.0168 - val_mse: 14.6626\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0217 - mse: 18.8996 - val_loss: 0.0168 - val_mse: 14.6184\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0177 - mse: 15.4120 - val_loss: 0.0168 - val_mse: 14.5992\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0143 - mse: 12.4948 - val_loss: 0.0167 - val_mse: 14.5845\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0162 - mse: 14.1260 - val_loss: 0.0167 - val_mse: 14.5650\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0163 - mse: 14.1894 - val_loss: 0.0167 - val_mse: 14.5634\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0207 - mse: 18.0400 - val_loss: 0.0167 - val_mse: 14.5402\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0165 - mse: 14.4064 - val_loss: 0.0167 - val_mse: 14.5247\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0152 - mse: 13.2513 - val_loss: 0.0167 - val_mse: 14.5222\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0185 - mse: 16.1275 - val_loss: 0.0167 - val_mse: 14.5188\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0203 - mse: 17.7032 - val_loss: 0.0167 - val_mse: 14.5228\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0166 - mse: 14.4137 - val_loss: 0.0167 - val_mse: 14.5135\n",
      "Epoch 140/1000\n",
      "\n",
      "Epoch 00139: reducing learning rate to 0.000576480140444.\n",
      "1s - loss: 0.0202 - mse: 17.6069 - val_loss: 0.0167 - val_mse: 14.5320\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0156 - mse: 13.6010 - val_loss: 0.0167 - val_mse: 14.5343\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0170 - mse: 14.8478 - val_loss: 0.0167 - val_mse: 14.5418\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0171 - mse: 14.9332 - val_loss: 0.0167 - val_mse: 14.5671\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0173 - mse: 15.0906 - val_loss: 0.0167 - val_mse: 14.5790\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0162 - mse: 14.1079 - val_loss: 0.0168 - val_mse: 14.5907\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0192 - mse: 16.7433 - val_loss: 0.0168 - val_mse: 14.5886\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0166 - mse: 14.4650 - val_loss: 0.0168 - val_mse: 14.5908\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0166 - mse: 14.4780 - val_loss: 0.0168 - val_mse: 14.5950\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0159 - mse: 13.8735 - val_loss: 0.0168 - val_mse: 14.5894\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0166 - mse: 14.4555 - val_loss: 0.0168 - val_mse: 14.5966\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0152 - mse: 13.2386 - val_loss: 0.0168 - val_mse: 14.5877\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0182 - mse: 15.8853 - val_loss: 0.0167 - val_mse: 14.5519\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0174 - mse: 15.1827 - val_loss: 0.0167 - val_mse: 14.5128\n",
      "Epoch 154/1000\n",
      "\n",
      "Epoch 00153: reducing learning rate to 0.000403536082013.\n",
      "1s - loss: 0.0210 - mse: 18.2834 - val_loss: 0.0166 - val_mse: 14.4549\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0170 - mse: 14.8200 - val_loss: 0.0166 - val_mse: 14.4167\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0176 - mse: 15.3185 - val_loss: 0.0165 - val_mse: 14.3757\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0165 - mse: 14.4085 - val_loss: 0.0165 - val_mse: 14.3400\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0202 - mse: 17.5508 - val_loss: 0.0164 - val_mse: 14.3239\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0142 - mse: 12.3304 - val_loss: 0.0164 - val_mse: 14.3139\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0177 - mse: 15.3722 - val_loss: 0.0164 - val_mse: 14.3116\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0175 - mse: 15.2181 - val_loss: 0.0164 - val_mse: 14.3027\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0224 - mse: 19.5131 - val_loss: 0.0164 - val_mse: 14.2767\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0153 - mse: 13.3665 - val_loss: 0.0164 - val_mse: 14.2549\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0165 - mse: 14.3963 - val_loss: 0.0163 - val_mse: 14.2335\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0240 - mse: 20.8899 - val_loss: 0.0163 - val_mse: 14.1991\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0156 - mse: 13.5445 - val_loss: 0.0163 - val_mse: 14.1656\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0175 - mse: 15.2729 - val_loss: 0.0162 - val_mse: 14.1354\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0185 - mse: 16.1012 - val_loss: 0.0162 - val_mse: 14.1114\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0154 - mse: 13.4355 - val_loss: 0.0162 - val_mse: 14.0932\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0214 - mse: 18.6669 - val_loss: 0.0162 - val_mse: 14.0779\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0210 - mse: 18.3182 - val_loss: 0.0162 - val_mse: 14.0657\n",
      "Epoch 172/1000\n",
      "\n",
      "Epoch 00171: reducing learning rate to 0.000282475253334.\n",
      "1s - loss: 0.0233 - mse: 20.2562 - val_loss: 0.0161 - val_mse: 14.0620\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0232 - mse: 20.1701 - val_loss: 0.0162 - val_mse: 14.0681\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0170 - mse: 14.7961 - val_loss: 0.0162 - val_mse: 14.0750\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0174 - mse: 15.1823 - val_loss: 0.0162 - val_mse: 14.0797\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0145 - mse: 12.6124 - val_loss: 0.0162 - val_mse: 14.0859\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0173 - mse: 15.0879 - val_loss: 0.0162 - val_mse: 14.0906\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0190 - mse: 16.5242 - val_loss: 0.0162 - val_mse: 14.0993\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0175 - mse: 15.2332 - val_loss: 0.0162 - val_mse: 14.1094\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0147 - mse: 12.7659 - val_loss: 0.0162 - val_mse: 14.1155\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0166 - mse: 14.4846 - val_loss: 0.0162 - val_mse: 14.1263\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0207 - mse: 18.0396 - val_loss: 0.0162 - val_mse: 14.1247\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0145 - mse: 12.6503 - val_loss: 0.0162 - val_mse: 14.1226\n",
      "Epoch 184/1000\n",
      "1s - loss: 0.0231 - mse: 20.0819 - val_loss: 0.0162 - val_mse: 14.1071\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0156 - mse: 13.5953 - val_loss: 0.0162 - val_mse: 14.0867\n",
      "Epoch 186/1000\n",
      "\n",
      "Epoch 00185: reducing learning rate to 0.000197732681409.\n",
      "1s - loss: 0.0162 - mse: 14.1086 - val_loss: 0.0162 - val_mse: 14.0693\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0164 - mse: 14.2609 - val_loss: 0.0161 - val_mse: 14.0621\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0156 - mse: 13.6231 - val_loss: 0.0161 - val_mse: 14.0555\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0161 - mse: 14.0474 - val_loss: 0.0161 - val_mse: 14.0504\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0150 - mse: 13.0490 - val_loss: 0.0161 - val_mse: 14.0462\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0168 - mse: 14.5960 - val_loss: 0.0161 - val_mse: 14.0366\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0174 - mse: 15.1295 - val_loss: 0.0161 - val_mse: 14.0251\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0154 - mse: 13.4266 - val_loss: 0.0161 - val_mse: 14.0165\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0148 - mse: 12.8464 - val_loss: 0.0161 - val_mse: 14.0075\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0138 - mse: 12.0157 - val_loss: 0.0161 - val_mse: 14.0009\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0178 - mse: 15.4924 - val_loss: 0.0161 - val_mse: 13.9894\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0365 - mse: 31.7775 - val_loss: 0.0161 - val_mse: 13.9814\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0193 - mse: 16.8115 - val_loss: 0.0160 - val_mse: 13.9674\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0189 - mse: 16.4555 - val_loss: 0.0160 - val_mse: 13.9582\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.0175 - mse: 15.2601 - val_loss: 0.0160 - val_mse: 13.9496\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.0174 - mse: 15.1219 - val_loss: 0.0160 - val_mse: 13.9421\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0263 - mse: 22.8782 - val_loss: 0.0160 - val_mse: 13.9360\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0167 - mse: 14.5582 - val_loss: 0.0160 - val_mse: 13.9273\n",
      "Epoch 204/1000\n",
      "\n",
      "Epoch 00203: reducing learning rate to 0.000138412872911.\n",
      "1s - loss: 0.0195 - mse: 16.9577 - val_loss: 0.0160 - val_mse: 13.9205\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0153 - mse: 13.3198 - val_loss: 0.0160 - val_mse: 13.9162\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0326 - mse: 28.3758 - val_loss: 0.0160 - val_mse: 13.9145\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0176 - mse: 15.3682 - val_loss: 0.0160 - val_mse: 13.9135\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0197 - mse: 17.1529 - val_loss: 0.0160 - val_mse: 13.9096\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0160 - mse: 13.9574 - val_loss: 0.0160 - val_mse: 13.9051\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0160 - mse: 13.9414 - val_loss: 0.0160 - val_mse: 13.9024\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0172 - mse: 14.9726 - val_loss: 0.0160 - val_mse: 13.8982\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0157 - mse: 13.6691 - val_loss: 0.0160 - val_mse: 13.8923\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0173 - mse: 15.0351 - val_loss: 0.0159 - val_mse: 13.8883\n",
      "Epoch 214/1000\n",
      "1s - loss: 0.0150 - mse: 13.0405 - val_loss: 0.0159 - val_mse: 13.8837\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0147 - mse: 12.7762 - val_loss: 0.0159 - val_mse: 13.8802\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0215 - mse: 18.7487 - val_loss: 0.0159 - val_mse: 13.8734\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0151 - mse: 13.1131 - val_loss: 0.0159 - val_mse: 13.8681\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0141 - mse: 12.2422 - val_loss: 0.0159 - val_mse: 13.8640\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0158 - mse: 13.7556 - val_loss: 0.0159 - val_mse: 13.8591\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0165 - mse: 14.4011 - val_loss: 0.0159 - val_mse: 13.8536\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0170 - mse: 14.7836 - val_loss: 0.0159 - val_mse: 13.8483\n",
      "Epoch 222/1000\n",
      "\n",
      "Epoch 00221: reducing learning rate to 9.68890090007e-05.\n",
      "1s - loss: 0.0338 - mse: 29.3978 - val_loss: 0.0159 - val_mse: 13.8444\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0263 - mse: 22.9106 - val_loss: 0.0159 - val_mse: 13.8393\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0162 - mse: 14.0810 - val_loss: 0.0159 - val_mse: 13.8332\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0158 - mse: 13.7696 - val_loss: 0.0159 - val_mse: 13.8288\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0148 - mse: 12.8590 - val_loss: 0.0159 - val_mse: 13.8243\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0185 - mse: 16.1507 - val_loss: 0.0159 - val_mse: 13.8215\n",
      "Epoch 228/1000\n",
      "1s - loss: 0.0141 - mse: 12.2907 - val_loss: 0.0159 - val_mse: 13.8187\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0147 - mse: 12.8011 - val_loss: 0.0159 - val_mse: 13.8166\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0181 - mse: 15.7723 - val_loss: 0.0159 - val_mse: 13.8160\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0161 - mse: 14.0205 - val_loss: 0.0159 - val_mse: 13.8157\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0148 - mse: 12.9093 - val_loss: 0.0159 - val_mse: 13.8155\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0232 - mse: 20.2003 - val_loss: 0.0159 - val_mse: 13.8137\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0158 - mse: 13.7986 - val_loss: 0.0159 - val_mse: 13.8124\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0189 - mse: 16.4876 - val_loss: 0.0159 - val_mse: 13.8119\n",
      "Epoch 236/1000\n",
      "\n",
      "Epoch 00235: reducing learning rate to 6.78223063005e-05.\n",
      "1s - loss: 0.0181 - mse: 15.7218 - val_loss: 0.0159 - val_mse: 13.8107\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0138 - mse: 12.0528 - val_loss: 0.0159 - val_mse: 13.8096\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0149 - mse: 12.9915 - val_loss: 0.0159 - val_mse: 13.8086\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0168 - mse: 14.6084 - val_loss: 0.0159 - val_mse: 13.8074\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0155 - mse: 13.4632 - val_loss: 0.0159 - val_mse: 13.8059\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0173 - mse: 15.1080 - val_loss: 0.0159 - val_mse: 13.8045\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.0148 - mse: 12.8908 - val_loss: 0.0159 - val_mse: 13.8030\n",
      "Epoch 243/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0212 - mse: 18.4348 - val_loss: 0.0158 - val_mse: 13.8017\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.0253 - mse: 22.0110 - val_loss: 0.0158 - val_mse: 13.7991\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.0168 - mse: 14.6235 - val_loss: 0.0158 - val_mse: 13.7962\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.0169 - mse: 14.7052 - val_loss: 0.0158 - val_mse: 13.7938\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.0164 - mse: 14.3237 - val_loss: 0.0158 - val_mse: 13.7913\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.0177 - mse: 15.4003 - val_loss: 0.0158 - val_mse: 13.7894\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.0148 - mse: 12.8788 - val_loss: 0.0158 - val_mse: 13.7880\n",
      "Epoch 250/1000\n",
      "\n",
      "Epoch 00249: reducing learning rate to 4.7475615429e-05.\n",
      "1s - loss: 0.0183 - mse: 15.9043 - val_loss: 0.0158 - val_mse: 13.7860\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.0143 - mse: 12.4646 - val_loss: 0.0158 - val_mse: 13.7846\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.0143 - mse: 12.4469 - val_loss: 0.0158 - val_mse: 13.7832\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.0151 - mse: 13.1923 - val_loss: 0.0158 - val_mse: 13.7823\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.0158 - mse: 13.7919 - val_loss: 0.0158 - val_mse: 13.7813\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.0146 - mse: 12.7420 - val_loss: 0.0158 - val_mse: 13.7802\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.0153 - mse: 13.3376 - val_loss: 0.0158 - val_mse: 13.7791\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.0191 - mse: 16.6539 - val_loss: 0.0158 - val_mse: 13.7776\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.0143 - mse: 12.4945 - val_loss: 0.0158 - val_mse: 13.7762\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.0137 - mse: 11.9012 - val_loss: 0.0158 - val_mse: 13.7748\n",
      "Epoch 260/1000\n",
      "1s - loss: 0.0184 - mse: 16.0078 - val_loss: 0.0158 - val_mse: 13.7738\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.0172 - mse: 14.9575 - val_loss: 0.0158 - val_mse: 13.7729\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.0285 - mse: 24.8448 - val_loss: 0.0158 - val_mse: 13.7724\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.0164 - mse: 14.2788 - val_loss: 0.0158 - val_mse: 13.7724\n",
      "Epoch 264/1000\n",
      "\n",
      "Epoch 00263: reducing learning rate to 3.32329313096e-05.\n",
      "1s - loss: 0.0138 - mse: 11.9909 - val_loss: 0.0158 - val_mse: 13.7722\n",
      "Epoch 265/1000\n",
      "1s - loss: 0.0198 - mse: 17.2121 - val_loss: 0.0158 - val_mse: 13.7724\n",
      "Epoch 266/1000\n",
      "1s - loss: 0.0169 - mse: 14.6805 - val_loss: 0.0158 - val_mse: 13.7725\n",
      "Epoch 267/1000\n",
      "1s - loss: 0.0213 - mse: 18.5228 - val_loss: 0.0158 - val_mse: 13.7727\n",
      "Epoch 268/1000\n",
      "1s - loss: 0.0181 - mse: 15.7270 - val_loss: 0.0158 - val_mse: 13.7731\n",
      "Epoch 269/1000\n",
      "1s - loss: 0.0160 - mse: 13.8928 - val_loss: 0.0158 - val_mse: 13.7734\n",
      "Epoch 270/1000\n",
      "1s - loss: 0.0181 - mse: 15.7735 - val_loss: 0.0158 - val_mse: 13.7737\n",
      "Epoch 271/1000\n",
      "1s - loss: 0.0154 - mse: 13.3899 - val_loss: 0.0158 - val_mse: 13.7735\n",
      "Epoch 272/1000\n",
      "1s - loss: 0.0148 - mse: 12.8579 - val_loss: 0.0158 - val_mse: 13.7735\n",
      "Epoch 273/1000\n",
      "1s - loss: 0.0265 - mse: 23.1114 - val_loss: 0.0158 - val_mse: 13.7737\n",
      "Epoch 274/1000\n",
      "1s - loss: 0.0147 - mse: 12.8077 - val_loss: 0.0158 - val_mse: 13.7739\n",
      "Epoch 275/1000\n",
      "1s - loss: 0.0157 - mse: 13.6387 - val_loss: 0.0158 - val_mse: 13.7741\n",
      "Epoch 276/1000\n",
      "1s - loss: 0.0237 - mse: 20.6524 - val_loss: 0.0158 - val_mse: 13.7745\n",
      "Epoch 277/1000\n",
      "1s - loss: 0.0160 - mse: 13.9592 - val_loss: 0.0158 - val_mse: 13.7747\n",
      "Epoch 278/1000\n",
      "\n",
      "Epoch 00277: reducing learning rate to 2.32630529354e-05.\n",
      "1s - loss: 0.0241 - mse: 21.0138 - val_loss: 0.0158 - val_mse: 13.7748\n",
      "Epoch 279/1000\n",
      "1s - loss: 0.0144 - mse: 12.5412 - val_loss: 0.0158 - val_mse: 13.7747\n",
      "Epoch 280/1000\n",
      "1s - loss: 0.0188 - mse: 16.3998 - val_loss: 0.0158 - val_mse: 13.7750\n",
      "Epoch 281/1000\n",
      "1s - loss: 0.0188 - mse: 16.3380 - val_loss: 0.0158 - val_mse: 13.7750\n",
      "Epoch 282/1000\n",
      "1s - loss: 0.0357 - mse: 31.0736 - val_loss: 0.0158 - val_mse: 13.7753\n",
      "Epoch 283/1000\n",
      "1s - loss: 0.0197 - mse: 17.1909 - val_loss: 0.0158 - val_mse: 13.7759\n",
      "Epoch 284/1000\n",
      "1s - loss: 0.0174 - mse: 15.1584 - val_loss: 0.0158 - val_mse: 13.7766\n",
      "Epoch 285/1000\n",
      "1s - loss: 0.0235 - mse: 20.4271 - val_loss: 0.0158 - val_mse: 13.7776\n",
      "Epoch 286/1000\n",
      "1s - loss: 0.0165 - mse: 14.3744 - val_loss: 0.0158 - val_mse: 13.7783\n",
      "Epoch 287/1000\n",
      "1s - loss: 0.0187 - mse: 16.3079 - val_loss: 0.0158 - val_mse: 13.7793\n",
      "Epoch 288/1000\n",
      "1s - loss: 0.0220 - mse: 19.1821 - val_loss: 0.0158 - val_mse: 13.7798\n",
      "Epoch 289/1000\n",
      "1s - loss: 0.0149 - mse: 12.9643 - val_loss: 0.0158 - val_mse: 13.7802\n",
      "Epoch 290/1000\n",
      "1s - loss: 0.0156 - mse: 13.5800 - val_loss: 0.0158 - val_mse: 13.7802\n",
      "Epoch 291/1000\n",
      "1s - loss: 0.0295 - mse: 25.6809 - val_loss: 0.0158 - val_mse: 13.7806\n",
      "Epoch 292/1000\n",
      "\n",
      "Epoch 00291: reducing learning rate to 1.62841375641e-05.\n",
      "1s - loss: 0.0237 - mse: 20.6306 - val_loss: 0.0158 - val_mse: 13.7814\n",
      "Epoch 293/1000\n",
      "1s - loss: 0.0141 - mse: 12.2624 - val_loss: 0.0158 - val_mse: 13.7818\n",
      "Epoch 294/1000\n",
      "1s - loss: 0.0140 - mse: 12.1694 - val_loss: 0.0158 - val_mse: 13.7821\n",
      "Epoch 295/1000\n",
      "1s - loss: 0.0166 - mse: 14.4415 - val_loss: 0.0158 - val_mse: 13.7825\n",
      "Epoch 296/1000\n",
      "1s - loss: 0.0292 - mse: 25.4377 - val_loss: 0.0158 - val_mse: 13.7832\n",
      "Epoch 297/1000\n",
      "1s - loss: 0.0144 - mse: 12.5354 - val_loss: 0.0158 - val_mse: 13.7838\n",
      "Epoch 298/1000\n",
      "1s - loss: 0.0249 - mse: 21.7126 - val_loss: 0.0158 - val_mse: 13.7840\n",
      "Epoch 299/1000\n",
      "1s - loss: 0.0151 - mse: 13.1591 - val_loss: 0.0158 - val_mse: 13.7841\n",
      "Epoch 300/1000\n",
      "1s - loss: 0.0232 - mse: 20.2110 - val_loss: 0.0158 - val_mse: 13.7839\n",
      "Epoch 301/1000\n",
      "1s - loss: 0.0149 - mse: 12.9969 - val_loss: 0.0158 - val_mse: 13.7836\n",
      "Epoch 302/1000\n",
      "1s - loss: 0.0145 - mse: 12.6671 - val_loss: 0.0158 - val_mse: 13.7832\n",
      "Epoch 303/1000\n",
      "1s - loss: 0.0156 - mse: 13.6166 - val_loss: 0.0158 - val_mse: 13.7829\n",
      "Epoch 304/1000\n",
      "1s - loss: 0.0191 - mse: 16.6536 - val_loss: 0.0158 - val_mse: 13.7825\n",
      "Epoch 305/1000\n",
      "1s - loss: 0.0195 - mse: 17.0051 - val_loss: 0.0158 - val_mse: 13.7818\n",
      "Epoch 306/1000\n",
      "\n",
      "Epoch 00305: reducing learning rate to 1.13988959129e-05.\n",
      "1s - loss: 0.0169 - mse: 14.6926 - val_loss: 0.0158 - val_mse: 13.7810\n",
      "[{'loss': 0.0070067006163299084, 'width0': 135, 'width1': 150, 'val_mse': 9.5660200119018555, 'mse': 6.101707935333252, 'val_loss': 0.010984830558300018, 'dropout1': 0.0, 'dropout0': 0}, {'loss': 0.012262199074029922, 'width0': 135, 'width1': 150, 'val_mse': 11.153453826904297, 'mse': 10.678401947021484, 'val_loss': 0.012807708233594894, 'dropout1': 0.10000000000000001, 'dropout0': 0}, {'loss': 0.016871752217411995, 'width0': 135, 'width1': 150, 'val_mse': 13.781032562255859, 'mse': 14.692578315734863, 'val_loss': 0.015825005248188972, 'dropout1': 0.20000000000000001, 'dropout0': 0}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_36 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 0.0313 - mse: 27.2898 - val_loss: 0.3248 - val_mse: 282.8837\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.3484 - mse: 303.4375 - val_loss: 0.1350 - val_mse: 117.5637\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.1258 - mse: 109.5938 - val_loss: 0.0886 - val_mse: 77.1267\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.0803 - mse: 69.9170 - val_loss: 0.1150 - val_mse: 100.1477\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.1201 - mse: 104.5503 - val_loss: 0.0616 - val_mse: 53.6254\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.0643 - mse: 55.9942 - val_loss: 0.0415 - val_mse: 36.1484\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.0422 - mse: 36.7796 - val_loss: 0.0602 - val_mse: 52.3864\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.0543 - mse: 47.3100 - val_loss: 0.0533 - val_mse: 46.3835\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.1014 - mse: 88.2741 - val_loss: 0.0349 - val_mse: 30.3774\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.0382 - mse: 33.2985 - val_loss: 0.0553 - val_mse: 48.1535\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.0784 - mse: 68.3045 - val_loss: 0.0517 - val_mse: 45.0036\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.0665 - mse: 57.8798 - val_loss: 0.0286 - val_mse: 24.8712\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0363 - mse: 31.6018 - val_loss: 0.0260 - val_mse: 22.6553\n",
      "Epoch 14/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0224 - mse: 19.5400 - val_loss: 0.0396 - val_mse: 34.4931\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0319 - mse: 27.8006 - val_loss: 0.0427 - val_mse: 37.1672\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0285 - mse: 24.7758 - val_loss: 0.0439 - val_mse: 38.2638\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0657 - mse: 57.2058 - val_loss: 0.0277 - val_mse: 24.1645\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0284 - mse: 24.7449 - val_loss: 0.0214 - val_mse: 18.6380\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0225 - mse: 19.6289 - val_loss: 0.0237 - val_mse: 20.6728\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0410 - mse: 35.7435 - val_loss: 0.0244 - val_mse: 21.2051\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0282 - mse: 24.5384 - val_loss: 0.0291 - val_mse: 25.3246\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0370 - mse: 32.1901 - val_loss: 0.0281 - val_mse: 24.5063\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00022: reducing learning rate to 0.00699999984354.\n",
      "3s - loss: 0.0278 - mse: 24.1676 - val_loss: 0.0263 - val_mse: 22.9400\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0491 - mse: 42.7247 - val_loss: 0.0261 - val_mse: 22.7434\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0284 - mse: 24.7182 - val_loss: 0.0263 - val_mse: 22.9047\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0266 - mse: 23.1908 - val_loss: 0.0254 - val_mse: 22.1276\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0326 - mse: 28.4048 - val_loss: 0.0225 - val_mse: 19.6294\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0287 - mse: 24.9961 - val_loss: 0.0236 - val_mse: 20.5762\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.1168 - mse: 101.7123 - val_loss: 0.0311 - val_mse: 27.1083\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0263 - mse: 22.9038 - val_loss: 0.0458 - val_mse: 39.9015\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0439 - mse: 38.2523 - val_loss: 0.0483 - val_mse: 42.0449\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0237 - mse: 20.6003 - val_loss: 0.0443 - val_mse: 38.5621\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0448 - mse: 38.9871 - val_loss: 0.0297 - val_mse: 25.8808\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0264 - mse: 22.9699 - val_loss: 0.0199 - val_mse: 17.3445\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0176 - mse: 15.3350 - val_loss: 0.0186 - val_mse: 16.2276\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0162 - mse: 14.1465 - val_loss: 0.0210 - val_mse: 18.3280\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0303 - mse: 26.3568 - val_loss: 0.0213 - val_mse: 18.5907\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0180 - mse: 15.6856 - val_loss: 0.0205 - val_mse: 17.8840\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0195 - mse: 17.0183 - val_loss: 0.0195 - val_mse: 17.0136\n",
      "Epoch 40/1000\n",
      "\n",
      "Epoch 00039: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0203 - mse: 17.6844 - val_loss: 0.0188 - val_mse: 16.3664\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0201 - mse: 17.5113 - val_loss: 0.0185 - val_mse: 16.1532\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0193 - mse: 16.7829 - val_loss: 0.0191 - val_mse: 16.6673\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0215 - mse: 18.7005 - val_loss: 0.0195 - val_mse: 16.9635\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0215 - mse: 18.7367 - val_loss: 0.0180 - val_mse: 15.6333\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0266 - mse: 23.1929 - val_loss: 0.0151 - val_mse: 13.1736\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0169 - mse: 14.7496 - val_loss: 0.0142 - val_mse: 12.3775\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0186 - mse: 16.1971 - val_loss: 0.0154 - val_mse: 13.4051\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0127 - mse: 11.0794 - val_loss: 0.0185 - val_mse: 16.1222\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0222 - mse: 19.3200 - val_loss: 0.0203 - val_mse: 17.6968\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0232 - mse: 20.2356 - val_loss: 0.0202 - val_mse: 17.5765\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0284 - mse: 24.7645 - val_loss: 0.0181 - val_mse: 15.7747\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0190 - mse: 16.5035 - val_loss: 0.0166 - val_mse: 14.4525\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0215 - mse: 18.7490 - val_loss: 0.0156 - val_mse: 13.5549\n",
      "Epoch 54/1000\n",
      "\n",
      "Epoch 00053: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0183 - mse: 15.8967 - val_loss: 0.0165 - val_mse: 14.3419\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0185 - mse: 16.1490 - val_loss: 0.0177 - val_mse: 15.4347\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0292 - mse: 25.4270 - val_loss: 0.0169 - val_mse: 14.7243\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0144 - mse: 12.5013 - val_loss: 0.0159 - val_mse: 13.8604\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0158 - mse: 13.7955 - val_loss: 0.0144 - val_mse: 12.5717\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0349 - mse: 30.3538 - val_loss: 0.0123 - val_mse: 10.7155\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0188 - mse: 16.3928 - val_loss: 0.0124 - val_mse: 10.7707\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0166 - mse: 14.4213 - val_loss: 0.0138 - val_mse: 11.9813\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0368 - mse: 32.0277 - val_loss: 0.0138 - val_mse: 12.0470\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0105 - mse: 9.1633 - val_loss: 0.0143 - val_mse: 12.4242\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0123 - mse: 10.6775 - val_loss: 0.0146 - val_mse: 12.7168\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0258 - mse: 22.4337 - val_loss: 0.0139 - val_mse: 12.1385\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0262 - mse: 22.8166 - val_loss: 0.0128 - val_mse: 11.1748\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0180 - mse: 15.6514 - val_loss: 0.0122 - val_mse: 10.5985\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0117 - mse: 10.1485 - val_loss: 0.0121 - val_mse: 10.5588\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0131 - mse: 11.3782 - val_loss: 0.0124 - val_mse: 10.7980\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0144 - mse: 12.5419 - val_loss: 0.0125 - val_mse: 10.8852\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0197 - mse: 17.1384 - val_loss: 0.0123 - val_mse: 10.6826\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0160 - mse: 13.9028 - val_loss: 0.0118 - val_mse: 10.2403\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0118 - mse: 10.2927 - val_loss: 0.0114 - val_mse: 9.8914\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0150 - mse: 13.0864 - val_loss: 0.0114 - val_mse: 9.9551\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0114 - mse: 9.9182 - val_loss: 0.0119 - val_mse: 10.3863\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0123 - mse: 10.6799 - val_loss: 0.0127 - val_mse: 11.0363\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0134 - mse: 11.6397 - val_loss: 0.0129 - val_mse: 11.2056\n",
      "Epoch 78/1000\n",
      "\n",
      "Epoch 00077: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0157 - mse: 13.7099 - val_loss: 0.0125 - val_mse: 10.8558\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0125 - mse: 10.9041 - val_loss: 0.0121 - val_mse: 10.4993\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0167 - mse: 14.5307 - val_loss: 0.0119 - val_mse: 10.3828\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0222 - mse: 19.2962 - val_loss: 0.0114 - val_mse: 9.9554\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0183 - mse: 15.9731 - val_loss: 0.0108 - val_mse: 9.3683\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0094 - mse: 8.1774 - val_loss: 0.0105 - val_mse: 9.1325\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0143 - mse: 12.4486 - val_loss: 0.0103 - val_mse: 8.9741\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0304 - mse: 26.4547 - val_loss: 0.0105 - val_mse: 9.1509\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0151 - mse: 13.1112 - val_loss: 0.0108 - val_mse: 9.3775\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0170 - mse: 14.8150 - val_loss: 0.0112 - val_mse: 9.7622\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0154 - mse: 13.4298 - val_loss: 0.0122 - val_mse: 10.6013\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0125 - mse: 10.9212 - val_loss: 0.0128 - val_mse: 11.1177\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0149 - mse: 12.9530 - val_loss: 0.0130 - val_mse: 11.3561\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0097 - mse: 8.4333 - val_loss: 0.0132 - val_mse: 11.5031\n",
      "Epoch 92/1000\n",
      "\n",
      "Epoch 00091: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0108 - mse: 9.4151 - val_loss: 0.0135 - val_mse: 11.7248\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0137 - mse: 11.9549 - val_loss: 0.0133 - val_mse: 11.5583\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0170 - mse: 14.8422 - val_loss: 0.0126 - val_mse: 10.9344\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0105 - mse: 9.1277 - val_loss: 0.0122 - val_mse: 10.6134\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0118 - mse: 10.2738 - val_loss: 0.0120 - val_mse: 10.4368\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0120 - mse: 10.4159 - val_loss: 0.0118 - val_mse: 10.2974\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0118 - mse: 10.2660 - val_loss: 0.0115 - val_mse: 10.0133\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0079 - mse: 6.8985 - val_loss: 0.0113 - val_mse: 9.8219\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0132 - mse: 11.5378 - val_loss: 0.0108 - val_mse: 9.4482\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0101 - mse: 8.7544 - val_loss: 0.0105 - val_mse: 9.1075\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0152 - mse: 13.2724 - val_loss: 0.0101 - val_mse: 8.7657\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0158 - mse: 13.7221 - val_loss: 0.0098 - val_mse: 8.5160\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0107 - mse: 9.3415 - val_loss: 0.0098 - val_mse: 8.5061\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0126 - mse: 10.9989 - val_loss: 0.0101 - val_mse: 8.7613\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0087 - mse: 7.5792 - val_loss: 0.0107 - val_mse: 9.2845\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0083 - mse: 7.2129 - val_loss: 0.0113 - val_mse: 9.8764\n",
      "Epoch 108/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00107: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0129 - mse: 11.2024 - val_loss: 0.0124 - val_mse: 10.8048\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0101 - mse: 8.8000 - val_loss: 0.0130 - val_mse: 11.3466\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0088 - mse: 7.6826 - val_loss: 0.0135 - val_mse: 11.7688\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0239 - mse: 20.7758 - val_loss: 0.0135 - val_mse: 11.7367\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0139 - mse: 12.0674 - val_loss: 0.0132 - val_mse: 11.5131\n",
      "Epoch 113/1000\n",
      "2s - loss: 0.0091 - mse: 7.9033 - val_loss: 0.0130 - val_mse: 11.3313\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0140 - mse: 12.1734 - val_loss: 0.0126 - val_mse: 10.9788\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0110 - mse: 9.6159 - val_loss: 0.0121 - val_mse: 10.5362\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0098 - mse: 8.5213 - val_loss: 0.0116 - val_mse: 10.1082\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0147 - mse: 12.8303 - val_loss: 0.0112 - val_mse: 9.7637\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0123 - mse: 10.6811 - val_loss: 0.0107 - val_mse: 9.3461\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0244 - mse: 21.2739 - val_loss: 0.0101 - val_mse: 8.7826\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0097 - mse: 8.4722 - val_loss: 0.0095 - val_mse: 8.2919\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0094 - mse: 8.2035 - val_loss: 0.0091 - val_mse: 7.9422\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0098 - mse: 8.5299 - val_loss: 0.0089 - val_mse: 7.7262\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0105 - mse: 9.1404 - val_loss: 0.0087 - val_mse: 7.5603\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0210 - mse: 18.2982 - val_loss: 0.0086 - val_mse: 7.5101\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0085 - mse: 7.4221 - val_loss: 0.0087 - val_mse: 7.6075\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0107 - mse: 9.3337 - val_loss: 0.0090 - val_mse: 7.8539\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0131 - mse: 11.3689 - val_loss: 0.0093 - val_mse: 8.0945\n",
      "Epoch 128/1000\n",
      "\n",
      "Epoch 00127: reducing learning rate to 0.000823543069419.\n",
      "1s - loss: 0.0098 - mse: 8.5738 - val_loss: 0.0096 - val_mse: 8.3496\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0083 - mse: 7.1898 - val_loss: 0.0098 - val_mse: 8.5074\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0107 - mse: 9.2879 - val_loss: 0.0099 - val_mse: 8.6635\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0193 - mse: 16.7759 - val_loss: 0.0099 - val_mse: 8.6259\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0078 - mse: 6.7579 - val_loss: 0.0098 - val_mse: 8.5676\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0140 - mse: 12.2064 - val_loss: 0.0096 - val_mse: 8.4010\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0122 - mse: 10.6450 - val_loss: 0.0094 - val_mse: 8.1812\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0103 - mse: 8.9674 - val_loss: 0.0092 - val_mse: 8.0016\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0114 - mse: 9.9012 - val_loss: 0.0089 - val_mse: 7.7917\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0077 - mse: 6.7238 - val_loss: 0.0088 - val_mse: 7.6477\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0090 - mse: 7.8705 - val_loss: 0.0086 - val_mse: 7.5174\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0080 - mse: 6.9451 - val_loss: 0.0085 - val_mse: 7.4355\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0087 - mse: 7.5768 - val_loss: 0.0085 - val_mse: 7.3951\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0087 - mse: 7.6114 - val_loss: 0.0085 - val_mse: 7.3979\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0093 - mse: 8.0919 - val_loss: 0.0085 - val_mse: 7.3867\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0123 - mse: 10.6726 - val_loss: 0.0085 - val_mse: 7.3782\n",
      "Epoch 144/1000\n",
      "\n",
      "Epoch 00143: reducing learning rate to 0.000576480140444.\n",
      "1s - loss: 0.0121 - mse: 10.5586 - val_loss: 0.0085 - val_mse: 7.3650\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0094 - mse: 8.1818 - val_loss: 0.0084 - val_mse: 7.3377\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0146 - mse: 12.6761 - val_loss: 0.0084 - val_mse: 7.3363\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0103 - mse: 8.9504 - val_loss: 0.0084 - val_mse: 7.3435\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0109 - mse: 9.4744 - val_loss: 0.0084 - val_mse: 7.3256\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0077 - mse: 6.7408 - val_loss: 0.0084 - val_mse: 7.3200\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0067 - mse: 5.8021 - val_loss: 0.0084 - val_mse: 7.3083\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0085 - mse: 7.3684 - val_loss: 0.0084 - val_mse: 7.2960\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0107 - mse: 9.2906 - val_loss: 0.0084 - val_mse: 7.2874\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0106 - mse: 9.2385 - val_loss: 0.0084 - val_mse: 7.3065\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0090 - mse: 7.7943 - val_loss: 0.0084 - val_mse: 7.3054\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0116 - mse: 10.1410 - val_loss: 0.0084 - val_mse: 7.2884\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0193 - mse: 16.8166 - val_loss: 0.0083 - val_mse: 7.2562\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0077 - mse: 6.7054 - val_loss: 0.0083 - val_mse: 7.2190\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0104 - mse: 9.0837 - val_loss: 0.0083 - val_mse: 7.2067\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0116 - mse: 10.1318 - val_loss: 0.0082 - val_mse: 7.1838\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0074 - mse: 6.4141 - val_loss: 0.0082 - val_mse: 7.1753\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0099 - mse: 8.6511 - val_loss: 0.0082 - val_mse: 7.1513\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0107 - mse: 9.2890 - val_loss: 0.0082 - val_mse: 7.1142\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0080 - mse: 6.9506 - val_loss: 0.0081 - val_mse: 7.0921\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0084 - mse: 7.2944 - val_loss: 0.0081 - val_mse: 7.0727\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0086 - mse: 7.5252 - val_loss: 0.0081 - val_mse: 7.0564\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0082 - mse: 7.1607 - val_loss: 0.0081 - val_mse: 7.0431\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0113 - mse: 9.8552 - val_loss: 0.0081 - val_mse: 7.0130\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0080 - mse: 6.9832 - val_loss: 0.0080 - val_mse: 6.9870\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0096 - mse: 8.3190 - val_loss: 0.0080 - val_mse: 6.9670\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0105 - mse: 9.1027 - val_loss: 0.0080 - val_mse: 6.9534\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0100 - mse: 8.7086 - val_loss: 0.0080 - val_mse: 6.9484\n",
      "Epoch 172/1000\n",
      "\n",
      "Epoch 00171: reducing learning rate to 0.000403536082013.\n",
      "1s - loss: 0.0110 - mse: 9.6045 - val_loss: 0.0080 - val_mse: 6.9503\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.0076 - mse: 6.5829 - val_loss: 0.0080 - val_mse: 6.9552\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0117 - mse: 10.1892 - val_loss: 0.0080 - val_mse: 6.9518\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0111 - mse: 9.6268 - val_loss: 0.0080 - val_mse: 6.9429\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0083 - mse: 7.2274 - val_loss: 0.0080 - val_mse: 6.9344\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0159 - mse: 13.8184 - val_loss: 0.0079 - val_mse: 6.9179\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.0071 - mse: 6.2201 - val_loss: 0.0079 - val_mse: 6.9056\n",
      "Epoch 179/1000\n",
      "1s - loss: 0.0079 - mse: 6.9212 - val_loss: 0.0079 - val_mse: 6.8924\n",
      "Epoch 180/1000\n",
      "1s - loss: 0.0098 - mse: 8.5259 - val_loss: 0.0079 - val_mse: 6.8785\n",
      "Epoch 181/1000\n",
      "1s - loss: 0.0100 - mse: 8.6707 - val_loss: 0.0079 - val_mse: 6.8666\n",
      "Epoch 182/1000\n",
      "1s - loss: 0.0082 - mse: 7.1572 - val_loss: 0.0079 - val_mse: 6.8575\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.0084 - mse: 7.2947 - val_loss: 0.0079 - val_mse: 6.8539\n",
      "Epoch 184/1000\n",
      "1s - loss: 0.0121 - mse: 10.5237 - val_loss: 0.0079 - val_mse: 6.8524\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.0092 - mse: 7.9955 - val_loss: 0.0079 - val_mse: 6.8486\n",
      "Epoch 186/1000\n",
      "\n",
      "Epoch 00185: reducing learning rate to 0.000282475253334.\n",
      "1s - loss: 0.0077 - mse: 6.6856 - val_loss: 0.0079 - val_mse: 6.8460\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.0077 - mse: 6.7486 - val_loss: 0.0079 - val_mse: 6.8478\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.0100 - mse: 8.6994 - val_loss: 0.0079 - val_mse: 6.8453\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.0166 - mse: 14.4233 - val_loss: 0.0079 - val_mse: 6.8366\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.0141 - mse: 12.2492 - val_loss: 0.0078 - val_mse: 6.8254\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.0090 - mse: 7.8474 - val_loss: 0.0078 - val_mse: 6.8173\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.0067 - mse: 5.8466 - val_loss: 0.0078 - val_mse: 6.8106\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.0084 - mse: 7.2762 - val_loss: 0.0078 - val_mse: 6.8099\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.0069 - mse: 6.0179 - val_loss: 0.0078 - val_mse: 6.8104\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.0081 - mse: 7.0792 - val_loss: 0.0078 - val_mse: 6.8135\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.0130 - mse: 11.3545 - val_loss: 0.0078 - val_mse: 6.8110\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.0115 - mse: 10.0147 - val_loss: 0.0078 - val_mse: 6.8122\n",
      "Epoch 198/1000\n",
      "1s - loss: 0.0090 - mse: 7.8421 - val_loss: 0.0078 - val_mse: 6.8146\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.0078 - mse: 6.7938 - val_loss: 0.0078 - val_mse: 6.8142\n",
      "Epoch 200/1000\n",
      "\n",
      "Epoch 00199: reducing learning rate to 0.000197732681409.\n",
      "1s - loss: 0.0064 - mse: 5.5858 - val_loss: 0.0078 - val_mse: 6.8162\n",
      "Epoch 201/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0077 - mse: 6.6725 - val_loss: 0.0078 - val_mse: 6.8189\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.0092 - mse: 8.0158 - val_loss: 0.0078 - val_mse: 6.8185\n",
      "Epoch 203/1000\n",
      "1s - loss: 0.0076 - mse: 6.6221 - val_loss: 0.0078 - val_mse: 6.8211\n",
      "Epoch 204/1000\n",
      "1s - loss: 0.0097 - mse: 8.4686 - val_loss: 0.0078 - val_mse: 6.8196\n",
      "Epoch 205/1000\n",
      "1s - loss: 0.0083 - mse: 7.2489 - val_loss: 0.0078 - val_mse: 6.8174\n",
      "Epoch 206/1000\n",
      "1s - loss: 0.0075 - mse: 6.5733 - val_loss: 0.0078 - val_mse: 6.8132\n",
      "Epoch 207/1000\n",
      "1s - loss: 0.0089 - mse: 7.7938 - val_loss: 0.0078 - val_mse: 6.8102\n",
      "Epoch 208/1000\n",
      "1s - loss: 0.0092 - mse: 7.9942 - val_loss: 0.0078 - val_mse: 6.8061\n",
      "Epoch 209/1000\n",
      "1s - loss: 0.0097 - mse: 8.4698 - val_loss: 0.0078 - val_mse: 6.8069\n",
      "Epoch 210/1000\n",
      "1s - loss: 0.0089 - mse: 7.7169 - val_loss: 0.0078 - val_mse: 6.8061\n",
      "Epoch 211/1000\n",
      "1s - loss: 0.0068 - mse: 5.9218 - val_loss: 0.0078 - val_mse: 6.8075\n",
      "Epoch 212/1000\n",
      "1s - loss: 0.0099 - mse: 8.6459 - val_loss: 0.0078 - val_mse: 6.8078\n",
      "Epoch 213/1000\n",
      "1s - loss: 0.0078 - mse: 6.7774 - val_loss: 0.0078 - val_mse: 6.8056\n",
      "Epoch 214/1000\n",
      "\n",
      "Epoch 00213: reducing learning rate to 0.000138412872911.\n",
      "1s - loss: 0.0082 - mse: 7.1666 - val_loss: 0.0078 - val_mse: 6.8038\n",
      "Epoch 215/1000\n",
      "1s - loss: 0.0092 - mse: 7.9696 - val_loss: 0.0078 - val_mse: 6.8045\n",
      "Epoch 216/1000\n",
      "1s - loss: 0.0085 - mse: 7.3915 - val_loss: 0.0078 - val_mse: 6.8050\n",
      "Epoch 217/1000\n",
      "1s - loss: 0.0085 - mse: 7.3744 - val_loss: 0.0078 - val_mse: 6.8055\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.0085 - mse: 7.3679 - val_loss: 0.0078 - val_mse: 6.8071\n",
      "Epoch 219/1000\n",
      "1s - loss: 0.0077 - mse: 6.7414 - val_loss: 0.0078 - val_mse: 6.8071\n",
      "Epoch 220/1000\n",
      "1s - loss: 0.0080 - mse: 7.0035 - val_loss: 0.0078 - val_mse: 6.8063\n",
      "Epoch 221/1000\n",
      "1s - loss: 0.0101 - mse: 8.8351 - val_loss: 0.0078 - val_mse: 6.8061\n",
      "Epoch 222/1000\n",
      "1s - loss: 0.0093 - mse: 8.1415 - val_loss: 0.0078 - val_mse: 6.8067\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.0064 - mse: 5.5453 - val_loss: 0.0078 - val_mse: 6.8083\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.0078 - mse: 6.7558 - val_loss: 0.0078 - val_mse: 6.8075\n",
      "Epoch 225/1000\n",
      "1s - loss: 0.0073 - mse: 6.3437 - val_loss: 0.0078 - val_mse: 6.8071\n",
      "Epoch 226/1000\n",
      "1s - loss: 0.0190 - mse: 16.5466 - val_loss: 0.0078 - val_mse: 6.8085\n",
      "Epoch 227/1000\n",
      "1s - loss: 0.0080 - mse: 6.9404 - val_loss: 0.0078 - val_mse: 6.8076\n",
      "Epoch 228/1000\n",
      "\n",
      "Epoch 00227: reducing learning rate to 9.68890090007e-05.\n",
      "1s - loss: 0.0117 - mse: 10.1490 - val_loss: 0.0078 - val_mse: 6.8073\n",
      "Epoch 229/1000\n",
      "1s - loss: 0.0079 - mse: 6.8591 - val_loss: 0.0078 - val_mse: 6.8084\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.0086 - mse: 7.5304 - val_loss: 0.0078 - val_mse: 6.8083\n",
      "Epoch 231/1000\n",
      "1s - loss: 0.0085 - mse: 7.4052 - val_loss: 0.0078 - val_mse: 6.8090\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.0071 - mse: 6.2143 - val_loss: 0.0078 - val_mse: 6.8110\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.0098 - mse: 8.5242 - val_loss: 0.0078 - val_mse: 6.8103\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.0085 - mse: 7.3716 - val_loss: 0.0078 - val_mse: 6.8079\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.0087 - mse: 7.5436 - val_loss: 0.0078 - val_mse: 6.8045\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.0092 - mse: 8.0499 - val_loss: 0.0078 - val_mse: 6.8029\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.0128 - mse: 11.1351 - val_loss: 0.0078 - val_mse: 6.8018\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.0080 - mse: 6.9504 - val_loss: 0.0078 - val_mse: 6.8008\n",
      "Epoch 239/1000\n",
      "1s - loss: 0.0077 - mse: 6.6824 - val_loss: 0.0078 - val_mse: 6.7991\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.0083 - mse: 7.2584 - val_loss: 0.0078 - val_mse: 6.7961\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.0079 - mse: 6.8809 - val_loss: 0.0078 - val_mse: 6.7947\n",
      "[{'loss': 0.0070067006163299084, 'width0': 135, 'width1': 150, 'val_mse': 9.5660200119018555, 'mse': 6.101707935333252, 'val_loss': 0.010984830558300018, 'dropout1': 0.0, 'dropout0': 0}, {'loss': 0.012262199074029922, 'width0': 135, 'width1': 150, 'val_mse': 11.153453826904297, 'mse': 10.678401947021484, 'val_loss': 0.012807708233594894, 'dropout1': 0.10000000000000001, 'dropout0': 0}, {'loss': 0.016871752217411995, 'width0': 135, 'width1': 150, 'val_mse': 13.781032562255859, 'mse': 14.692578315734863, 'val_loss': 0.015825005248188972, 'dropout1': 0.20000000000000001, 'dropout0': 0}, {'loss': 0.0079014608636498451, 'width0': 135, 'width1': 150, 'val_mse': 6.7946681976318359, 'mse': 6.8809013366699219, 'val_loss': 0.0078024403192102909, 'dropout1': 0.30000000000000004, 'dropout0': 0}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_38 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 0.0104 - mse: 9.0223 - val_loss: 0.1895 - val_mse: 165.0675\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.1872 - mse: 163.0241 - val_loss: 0.0584 - val_mse: 50.8445\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.0365 - mse: 31.7444 - val_loss: 0.0583 - val_mse: 50.7584\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.0364 - mse: 31.6704 - val_loss: 0.1174 - val_mse: 102.2213\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.1486 - mse: 129.4168 - val_loss: 0.0447 - val_mse: 38.8894\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.0266 - mse: 23.1433 - val_loss: 0.0603 - val_mse: 52.5117\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.0620 - mse: 53.9719 - val_loss: 0.0360 - val_mse: 31.3112\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.0394 - mse: 34.3101 - val_loss: 0.0241 - val_mse: 20.9844\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.0386 - mse: 33.6049 - val_loss: 0.0257 - val_mse: 22.3895\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.0356 - mse: 30.9971 - val_loss: 0.0291 - val_mse: 25.3402\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.0450 - mse: 39.1530 - val_loss: 0.0372 - val_mse: 32.4369\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.0233 - mse: 20.3069 - val_loss: 0.0411 - val_mse: 35.8103\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.00699999984354.\n",
      "3s - loss: 0.0564 - mse: 49.1385 - val_loss: 0.0321 - val_mse: 27.9194\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0444 - mse: 38.6253 - val_loss: 0.0192 - val_mse: 16.7586\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0130 - mse: 11.3135 - val_loss: 0.0176 - val_mse: 15.3039\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0252 - mse: 21.9193 - val_loss: 0.0197 - val_mse: 17.1394\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0327 - mse: 28.4576 - val_loss: 0.0199 - val_mse: 17.3552\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0237 - mse: 20.6136 - val_loss: 0.0165 - val_mse: 14.3452\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0305 - mse: 26.5344 - val_loss: 0.0133 - val_mse: 11.5537\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0394 - mse: 34.2767 - val_loss: 0.0143 - val_mse: 12.4251\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0246 - mse: 21.3826 - val_loss: 0.0200 - val_mse: 17.4322\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0231 - mse: 20.1036 - val_loss: 0.0222 - val_mse: 19.3712\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0157 - mse: 13.7086 - val_loss: 0.0233 - val_mse: 20.3055\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0250 - mse: 21.7739 - val_loss: 0.0209 - val_mse: 18.1716\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0265 - mse: 23.0497 - val_loss: 0.0250 - val_mse: 21.7521\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0375 - mse: 32.6792 - val_loss: 0.0242 - val_mse: 21.1011\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 00026: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0452 - mse: 39.3447 - val_loss: 0.0203 - val_mse: 17.7000\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0200 - mse: 17.4494 - val_loss: 0.0196 - val_mse: 17.0777\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0141 - mse: 12.3115 - val_loss: 0.0209 - val_mse: 18.1667\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0307 - mse: 26.7456 - val_loss: 0.0209 - val_mse: 18.2094\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0333 - mse: 28.9769 - val_loss: 0.0186 - val_mse: 16.2175\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0354 - mse: 30.7999 - val_loss: 0.0141 - val_mse: 12.3130\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0210 - mse: 18.2805 - val_loss: 0.0108 - val_mse: 9.3748\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0091 - mse: 7.8956 - val_loss: 0.0100 - val_mse: 8.6898\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0163 - mse: 14.2340 - val_loss: 0.0114 - val_mse: 9.9362\n",
      "Epoch 36/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0214 - mse: 18.6165 - val_loss: 0.0132 - val_mse: 11.4864\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0183 - mse: 15.9712 - val_loss: 0.0133 - val_mse: 11.6016\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0130 - mse: 11.2956 - val_loss: 0.0124 - val_mse: 10.8130\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0132 - mse: 11.5137 - val_loss: 0.0111 - val_mse: 9.6787\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0180 - mse: 15.6484 - val_loss: 0.0104 - val_mse: 9.0295\n",
      "Epoch 41/1000\n",
      "\n",
      "Epoch 00040: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0077 - mse: 6.7184 - val_loss: 0.0104 - val_mse: 9.0457\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0091 - mse: 7.9280 - val_loss: 0.0103 - val_mse: 8.9894\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0128 - mse: 11.1699 - val_loss: 0.0099 - val_mse: 8.6513\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0212 - mse: 18.4535 - val_loss: 0.0098 - val_mse: 8.5735\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0132 - mse: 11.4914 - val_loss: 0.0096 - val_mse: 8.3646\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0174 - mse: 15.1774 - val_loss: 0.0090 - val_mse: 7.8250\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0115 - mse: 9.9810 - val_loss: 0.0084 - val_mse: 7.3458\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0200 - mse: 17.4156 - val_loss: 0.0080 - val_mse: 6.9421\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0090 - mse: 7.8015 - val_loss: 0.0082 - val_mse: 7.1668\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0136 - mse: 11.8796 - val_loss: 0.0085 - val_mse: 7.4157\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0089 - mse: 7.7226 - val_loss: 0.0088 - val_mse: 7.6521\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0113 - mse: 9.8571 - val_loss: 0.0086 - val_mse: 7.4893\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0107 - mse: 9.3384 - val_loss: 0.0086 - val_mse: 7.4932\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0292 - mse: 25.3944 - val_loss: 0.0080 - val_mse: 6.9468\n",
      "Epoch 55/1000\n",
      "\n",
      "Epoch 00054: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0201 - mse: 17.5087 - val_loss: 0.0087 - val_mse: 7.5977\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0121 - mse: 10.5458 - val_loss: 0.0092 - val_mse: 7.9899\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0076 - mse: 6.5860 - val_loss: 0.0094 - val_mse: 8.2138\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0122 - mse: 10.6012 - val_loss: 0.0092 - val_mse: 8.0409\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0172 - mse: 14.9986 - val_loss: 0.0086 - val_mse: 7.5187\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0168 - mse: 14.6313 - val_loss: 0.0077 - val_mse: 6.7288\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0127 - mse: 11.0951 - val_loss: 0.0073 - val_mse: 6.3622\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0162 - mse: 14.1354 - val_loss: 0.0071 - val_mse: 6.2241\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0066 - mse: 5.7794 - val_loss: 0.0072 - val_mse: 6.2940\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0076 - mse: 6.5931 - val_loss: 0.0072 - val_mse: 6.3117\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0079 - mse: 6.8686 - val_loss: 0.0071 - val_mse: 6.2260\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0103 - mse: 8.9584 - val_loss: 0.0073 - val_mse: 6.3614\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0124 - mse: 10.8279 - val_loss: 0.0074 - val_mse: 6.4705\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0246 - mse: 21.4102 - val_loss: 0.0070 - val_mse: 6.1225\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0058 - mse: 5.0829 - val_loss: 0.0067 - val_mse: 5.8562\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0086 - mse: 7.4747 - val_loss: 0.0066 - val_mse: 5.7120\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0152 - mse: 13.2357 - val_loss: 0.0067 - val_mse: 5.8486\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0108 - mse: 9.4052 - val_loss: 0.0074 - val_mse: 6.4847\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0066 - mse: 5.7057 - val_loss: 0.0089 - val_mse: 7.7586\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0080 - mse: 7.0038 - val_loss: 0.0106 - val_mse: 9.2446\n",
      "Epoch 75/1000\n",
      "\n",
      "Epoch 00074: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0307 - mse: 26.7502 - val_loss: 0.0110 - val_mse: 9.6207\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0251 - mse: 21.8804 - val_loss: 0.0105 - val_mse: 9.1534\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0112 - mse: 9.7309 - val_loss: 0.0096 - val_mse: 8.3691\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0187 - mse: 16.3282 - val_loss: 0.0084 - val_mse: 7.2919\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0055 - mse: 4.7755 - val_loss: 0.0075 - val_mse: 6.5437\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0148 - mse: 12.9234 - val_loss: 0.0065 - val_mse: 5.6990\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0111 - mse: 9.6768 - val_loss: 0.0060 - val_mse: 5.2631\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0060 - mse: 5.2134 - val_loss: 0.0057 - val_mse: 4.9746\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0142 - mse: 12.3430 - val_loss: 0.0055 - val_mse: 4.8027\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0080 - mse: 7.0102 - val_loss: 0.0055 - val_mse: 4.7736\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0080 - mse: 6.9286 - val_loss: 0.0055 - val_mse: 4.7906\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0070 - mse: 6.1355 - val_loss: 0.0056 - val_mse: 4.9013\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0098 - mse: 8.5536 - val_loss: 0.0057 - val_mse: 4.9831\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0103 - mse: 8.9640 - val_loss: 0.0057 - val_mse: 4.9601\n",
      "Epoch 89/1000\n",
      "\n",
      "Epoch 00088: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0051 - mse: 4.4679 - val_loss: 0.0057 - val_mse: 4.9203\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0114 - mse: 9.9544 - val_loss: 0.0056 - val_mse: 4.8760\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0118 - mse: 10.3086 - val_loss: 0.0055 - val_mse: 4.8009\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0079 - mse: 6.9125 - val_loss: 0.0054 - val_mse: 4.7189\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0125 - mse: 10.9282 - val_loss: 0.0053 - val_mse: 4.6297\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0049 - mse: 4.2794 - val_loss: 0.0053 - val_mse: 4.5962\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0150 - mse: 13.0681 - val_loss: 0.0054 - val_mse: 4.6592\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0045 - mse: 3.9615 - val_loss: 0.0055 - val_mse: 4.7534\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0075 - mse: 6.5584 - val_loss: 0.0056 - val_mse: 4.8749\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0065 - mse: 5.6289 - val_loss: 0.0058 - val_mse: 5.0498\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0048 - mse: 4.1764 - val_loss: 0.0060 - val_mse: 5.2073\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0082 - mse: 7.1661 - val_loss: 0.0060 - val_mse: 5.2515\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0086 - mse: 7.5029 - val_loss: 0.0060 - val_mse: 5.1917\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0132 - mse: 11.5295 - val_loss: 0.0059 - val_mse: 5.1003\n",
      "Epoch 103/1000\n",
      "\n",
      "Epoch 00102: reducing learning rate to 0.000823543069419.\n",
      "1s - loss: 0.0056 - mse: 4.9108 - val_loss: 0.0057 - val_mse: 5.0016\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.0099 - mse: 8.6135 - val_loss: 0.0057 - val_mse: 4.9359\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0081 - mse: 7.0908 - val_loss: 0.0056 - val_mse: 4.8489\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0072 - mse: 6.2522 - val_loss: 0.0055 - val_mse: 4.7828\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0088 - mse: 7.6929 - val_loss: 0.0054 - val_mse: 4.7041\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0120 - mse: 10.4419 - val_loss: 0.0053 - val_mse: 4.6364\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0049 - mse: 4.2265 - val_loss: 0.0053 - val_mse: 4.6082\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0050 - mse: 4.3943 - val_loss: 0.0053 - val_mse: 4.5979\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0048 - mse: 4.1560 - val_loss: 0.0053 - val_mse: 4.6080\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0067 - mse: 5.8203 - val_loss: 0.0053 - val_mse: 4.6350\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0056 - mse: 4.8476 - val_loss: 0.0053 - val_mse: 4.6294\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0102 - mse: 8.8404 - val_loss: 0.0053 - val_mse: 4.5963\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0068 - mse: 5.9081 - val_loss: 0.0053 - val_mse: 4.6002\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0126 - mse: 10.9715 - val_loss: 0.0053 - val_mse: 4.5737\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0160 - mse: 13.9344 - val_loss: 0.0052 - val_mse: 4.5115\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0066 - mse: 5.7315 - val_loss: 0.0051 - val_mse: 4.4505\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0087 - mse: 7.5457 - val_loss: 0.0050 - val_mse: 4.3779\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0048 - mse: 4.2108 - val_loss: 0.0050 - val_mse: 4.3424\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0055 - mse: 4.7746 - val_loss: 0.0050 - val_mse: 4.3147\n",
      "Epoch 122/1000\n",
      "1s - loss: 0.0105 - mse: 9.1737 - val_loss: 0.0049 - val_mse: 4.2689\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0050 - mse: 4.3783 - val_loss: 0.0049 - val_mse: 4.2291\n",
      "Epoch 124/1000\n",
      "1s - loss: 0.0066 - mse: 5.7592 - val_loss: 0.0048 - val_mse: 4.2143\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0087 - mse: 7.5567 - val_loss: 0.0048 - val_mse: 4.1905\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0132 - mse: 11.4689 - val_loss: 0.0048 - val_mse: 4.1744\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0107 - mse: 9.2862 - val_loss: 0.0048 - val_mse: 4.1692\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0103 - mse: 8.9388 - val_loss: 0.0048 - val_mse: 4.1621\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0087 - mse: 7.5644 - val_loss: 0.0048 - val_mse: 4.1541\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0094 - mse: 8.1512 - val_loss: 0.0048 - val_mse: 4.1584\n",
      "Epoch 131/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00130: reducing learning rate to 0.000576480140444.\n",
      "1s - loss: 0.0051 - mse: 4.4146 - val_loss: 0.0048 - val_mse: 4.1742\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0057 - mse: 4.9593 - val_loss: 0.0048 - val_mse: 4.1940\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0050 - mse: 4.3660 - val_loss: 0.0048 - val_mse: 4.2081\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0068 - mse: 5.9402 - val_loss: 0.0048 - val_mse: 4.2215\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0092 - mse: 8.0345 - val_loss: 0.0048 - val_mse: 4.2168\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0080 - mse: 6.9850 - val_loss: 0.0049 - val_mse: 4.2253\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0084 - mse: 7.3156 - val_loss: 0.0049 - val_mse: 4.2311\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.0058 - mse: 5.0749 - val_loss: 0.0049 - val_mse: 4.2460\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0083 - mse: 7.2162 - val_loss: 0.0049 - val_mse: 4.2526\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0083 - mse: 7.2402 - val_loss: 0.0049 - val_mse: 4.2588\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0095 - mse: 8.2777 - val_loss: 0.0049 - val_mse: 4.2670\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0062 - mse: 5.4115 - val_loss: 0.0049 - val_mse: 4.2716\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0057 - mse: 4.9978 - val_loss: 0.0049 - val_mse: 4.2616\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0075 - mse: 6.5528 - val_loss: 0.0049 - val_mse: 4.2399\n",
      "Epoch 145/1000\n",
      "\n",
      "Epoch 00144: reducing learning rate to 0.000403536082013.\n",
      "1s - loss: 0.0046 - mse: 3.9885 - val_loss: 0.0048 - val_mse: 4.2193\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0146 - mse: 12.6960 - val_loss: 0.0048 - val_mse: 4.2041\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0149 - mse: 13.0030 - val_loss: 0.0048 - val_mse: 4.1953\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0092 - mse: 8.0454 - val_loss: 0.0048 - val_mse: 4.1983\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0118 - mse: 10.2820 - val_loss: 0.0048 - val_mse: 4.2108\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0080 - mse: 6.9426 - val_loss: 0.0049 - val_mse: 4.2318\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0066 - mse: 5.7622 - val_loss: 0.0049 - val_mse: 4.2594\n",
      "Epoch 152/1000\n",
      "1s - loss: 0.0109 - mse: 9.4549 - val_loss: 0.0049 - val_mse: 4.2806\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0085 - mse: 7.4338 - val_loss: 0.0050 - val_mse: 4.3137\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0133 - mse: 11.5643 - val_loss: 0.0050 - val_mse: 4.3356\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0052 - mse: 4.5545 - val_loss: 0.0050 - val_mse: 4.3670\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0057 - mse: 4.9862 - val_loss: 0.0050 - val_mse: 4.3948\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0057 - mse: 4.9378 - val_loss: 0.0051 - val_mse: 4.4306\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0143 - mse: 12.4214 - val_loss: 0.0051 - val_mse: 4.4664\n",
      "Epoch 159/1000\n",
      "\n",
      "Epoch 00158: reducing learning rate to 0.000282475253334.\n",
      "1s - loss: 0.0058 - mse: 5.0100 - val_loss: 0.0052 - val_mse: 4.5103\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0130 - mse: 11.2917 - val_loss: 0.0052 - val_mse: 4.5435\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0050 - mse: 4.3182 - val_loss: 0.0053 - val_mse: 4.5749\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0062 - mse: 5.4187 - val_loss: 0.0053 - val_mse: 4.5981\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0071 - mse: 6.1529 - val_loss: 0.0053 - val_mse: 4.6299\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0048 - mse: 4.1839 - val_loss: 0.0053 - val_mse: 4.6529\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0067 - mse: 5.8187 - val_loss: 0.0054 - val_mse: 4.6614\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.0056 - mse: 4.8855 - val_loss: 0.0053 - val_mse: 4.6539\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0098 - mse: 8.5067 - val_loss: 0.0053 - val_mse: 4.6187\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0038 - mse: 3.3458 - val_loss: 0.0053 - val_mse: 4.5919\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0122 - mse: 10.6482 - val_loss: 0.0053 - val_mse: 4.5867\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0049 - mse: 4.2523 - val_loss: 0.0053 - val_mse: 4.5835\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.0074 - mse: 6.4240 - val_loss: 0.0053 - val_mse: 4.5725\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.0077 - mse: 6.7397 - val_loss: 0.0052 - val_mse: 4.5615\n",
      "Epoch 173/1000\n",
      "\n",
      "Epoch 00172: reducing learning rate to 0.000197732681409.\n",
      "1s - loss: 0.0086 - mse: 7.4583 - val_loss: 0.0052 - val_mse: 4.5286\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.0102 - mse: 8.9184 - val_loss: 0.0052 - val_mse: 4.4972\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.0088 - mse: 7.6878 - val_loss: 0.0051 - val_mse: 4.4557\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.0125 - mse: 10.8976 - val_loss: 0.0051 - val_mse: 4.4087\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.0078 - mse: 6.7832 - val_loss: 0.0050 - val_mse: 4.3584\n",
      "[{'loss': 0.0070067006163299084, 'width0': 135, 'width1': 150, 'val_mse': 9.5660200119018555, 'mse': 6.101707935333252, 'val_loss': 0.010984830558300018, 'dropout1': 0.0, 'dropout0': 0}, {'loss': 0.012262199074029922, 'width0': 135, 'width1': 150, 'val_mse': 11.153453826904297, 'mse': 10.678401947021484, 'val_loss': 0.012807708233594894, 'dropout1': 0.10000000000000001, 'dropout0': 0}, {'loss': 0.016871752217411995, 'width0': 135, 'width1': 150, 'val_mse': 13.781032562255859, 'mse': 14.692578315734863, 'val_loss': 0.015825005248188972, 'dropout1': 0.20000000000000001, 'dropout0': 0}, {'loss': 0.0079014608636498451, 'width0': 135, 'width1': 150, 'val_mse': 6.7946681976318359, 'mse': 6.8809013366699219, 'val_loss': 0.0078024403192102909, 'dropout1': 0.30000000000000004, 'dropout0': 0}, {'loss': 0.0077892974950373173, 'width0': 135, 'width1': 150, 'val_mse': 4.3584160804748535, 'mse': 6.7832231521606445, 'val_loss': 0.0050048483535647392, 'dropout1': 0.40000000000000002, 'dropout0': 0}]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_40 (LSTM)               (None, None, 135)         74520     \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, None, 150)         171600    \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, None, 1)           151       \n",
      "=================================================================\n",
      "Total params: 246,271\n",
      "Trainable params: 246,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 0.0104 - mse: 9.0597 - val_loss: 0.2979 - val_mse: 259.4381\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.3539 - mse: 308.2012 - val_loss: 0.0630 - val_mse: 54.8601\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.0638 - mse: 55.5423 - val_loss: 0.0634 - val_mse: 55.2524\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.0499 - mse: 43.4372 - val_loss: 0.1100 - val_mse: 95.7903\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.0839 - mse: 73.0413 - val_loss: 0.0859 - val_mse: 74.7839\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.0726 - mse: 63.1871 - val_loss: 0.0446 - val_mse: 38.8184\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.0991 - mse: 86.2778 - val_loss: 0.0469 - val_mse: 40.8733\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.1460 - mse: 127.1755 - val_loss: 0.0299 - val_mse: 26.0714\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.0260 - mse: 22.6031 - val_loss: 0.0256 - val_mse: 22.3159\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.0218 - mse: 18.9519 - val_loss: 0.0356 - val_mse: 31.0283\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.0484 - mse: 42.1291 - val_loss: 0.0238 - val_mse: 20.7241\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.0194 - mse: 16.8813 - val_loss: 0.0252 - val_mse: 21.9118\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0187 - mse: 16.2669 - val_loss: 0.0271 - val_mse: 23.5656\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0198 - mse: 17.2446 - val_loss: 0.0261 - val_mse: 22.7511\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0323 - mse: 28.1092 - val_loss: 0.0218 - val_mse: 18.9587\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0478 - mse: 41.6465 - val_loss: 0.0239 - val_mse: 20.7929\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0252 - mse: 21.9472 - val_loss: 0.0315 - val_mse: 27.4376\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0442 - mse: 38.4643 - val_loss: 0.0245 - val_mse: 21.3437\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0277 - mse: 24.0896 - val_loss: 0.0246 - val_mse: 21.4519\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.00699999984354.\n",
      "3s - loss: 0.0251 - mse: 21.8595 - val_loss: 0.0323 - val_mse: 28.1328\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0309 - mse: 26.9472 - val_loss: 0.0361 - val_mse: 31.4091\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0381 - mse: 33.1732 - val_loss: 0.0306 - val_mse: 26.6765\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0192 - mse: 16.7380 - val_loss: 0.0221 - val_mse: 19.2023\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0217 - mse: 18.9299 - val_loss: 0.0148 - val_mse: 12.8662\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0170 - mse: 14.8079 - val_loss: 0.0148 - val_mse: 12.8939\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0136 - mse: 11.8598 - val_loss: 0.0241 - val_mse: 21.0264\n",
      "Epoch 27/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0230 - mse: 20.0259 - val_loss: 0.0285 - val_mse: 24.7944\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0395 - mse: 34.4102 - val_loss: 0.0245 - val_mse: 21.3770\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0224 - mse: 19.4996 - val_loss: 0.0176 - val_mse: 15.3327\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0471 - mse: 40.9988 - val_loss: 0.0159 - val_mse: 13.8649\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0415 - mse: 36.1646 - val_loss: 0.0134 - val_mse: 11.7091\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0330 - mse: 28.7197 - val_loss: 0.0169 - val_mse: 14.6984\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0250 - mse: 21.7506 - val_loss: 0.0199 - val_mse: 17.3207\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0318 - mse: 27.7016 - val_loss: 0.0186 - val_mse: 16.2405\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0166 - mse: 14.4512 - val_loss: 0.0196 - val_mse: 17.0674\n",
      "Epoch 36/1000\n",
      "\n",
      "Epoch 00035: reducing learning rate to 0.00489999982528.\n",
      "1s - loss: 0.0309 - mse: 26.9096 - val_loss: 0.0168 - val_mse: 14.6408\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0210 - mse: 18.2606 - val_loss: 0.0145 - val_mse: 12.6204\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0161 - mse: 14.0313 - val_loss: 0.0129 - val_mse: 11.2299\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0176 - mse: 15.2862 - val_loss: 0.0115 - val_mse: 10.0302\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0194 - mse: 16.8835 - val_loss: 0.0110 - val_mse: 9.6150\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0223 - mse: 19.4144 - val_loss: 0.0137 - val_mse: 11.8916\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0259 - mse: 22.5163 - val_loss: 0.0153 - val_mse: 13.3187\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0171 - mse: 14.8861 - val_loss: 0.0156 - val_mse: 13.6248\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0188 - mse: 16.3583 - val_loss: 0.0142 - val_mse: 12.3376\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0156 - mse: 13.5826 - val_loss: 0.0120 - val_mse: 10.4151\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0198 - mse: 17.2433 - val_loss: 0.0101 - val_mse: 8.7945\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0214 - mse: 18.6699 - val_loss: 0.0090 - val_mse: 7.8200\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0112 - mse: 9.7570 - val_loss: 0.0095 - val_mse: 8.2779\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0186 - mse: 16.1920 - val_loss: 0.0109 - val_mse: 9.5245\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0147 - mse: 12.8088 - val_loss: 0.0121 - val_mse: 10.5382\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0156 - mse: 13.5641 - val_loss: 0.0122 - val_mse: 10.6189\n",
      "Epoch 52/1000\n",
      "\n",
      "Epoch 00051: reducing learning rate to 0.00343000004068.\n",
      "1s - loss: 0.0151 - mse: 13.1204 - val_loss: 0.0115 - val_mse: 10.0219\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0108 - mse: 9.4226 - val_loss: 0.0106 - val_mse: 9.2646\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0146 - mse: 12.7480 - val_loss: 0.0097 - val_mse: 8.4667\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0142 - mse: 12.4041 - val_loss: 0.0088 - val_mse: 7.7025\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0111 - mse: 9.7047 - val_loss: 0.0083 - val_mse: 7.2533\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0096 - mse: 8.3618 - val_loss: 0.0082 - val_mse: 7.1325\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0113 - mse: 9.8295 - val_loss: 0.0082 - val_mse: 7.1840\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0098 - mse: 8.5071 - val_loss: 0.0087 - val_mse: 7.5882\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0179 - mse: 15.5527 - val_loss: 0.0102 - val_mse: 8.9231\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0108 - mse: 9.4167 - val_loss: 0.0117 - val_mse: 10.1930\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0255 - mse: 22.2141 - val_loss: 0.0117 - val_mse: 10.1738\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0219 - mse: 19.1077 - val_loss: 0.0110 - val_mse: 9.6210\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0098 - mse: 8.5565 - val_loss: 0.0103 - val_mse: 8.9597\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0178 - mse: 15.4676 - val_loss: 0.0090 - val_mse: 7.8339\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0114 - mse: 9.9099 - val_loss: 0.0078 - val_mse: 6.7652\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0090 - mse: 7.8593 - val_loss: 0.0072 - val_mse: 6.2388\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0088 - mse: 7.6216 - val_loss: 0.0069 - val_mse: 6.0142\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0110 - mse: 9.5910 - val_loss: 0.0072 - val_mse: 6.3129\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0075 - mse: 6.4983 - val_loss: 0.0075 - val_mse: 6.5395\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0114 - mse: 9.9142 - val_loss: 0.0083 - val_mse: 7.1979\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0224 - mse: 19.4813 - val_loss: 0.0095 - val_mse: 8.2665\n",
      "Epoch 73/1000\n",
      "\n",
      "Epoch 00072: reducing learning rate to 0.00240100002848.\n",
      "1s - loss: 0.0103 - mse: 8.9466 - val_loss: 0.0102 - val_mse: 8.9090\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0182 - mse: 15.8649 - val_loss: 0.0099 - val_mse: 8.6511\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0086 - mse: 7.5025 - val_loss: 0.0094 - val_mse: 8.2036\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0172 - mse: 14.9649 - val_loss: 0.0096 - val_mse: 8.3954\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0174 - mse: 15.1602 - val_loss: 0.0091 - val_mse: 7.9024\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0113 - mse: 9.8112 - val_loss: 0.0083 - val_mse: 7.2636\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0092 - mse: 8.0151 - val_loss: 0.0077 - val_mse: 6.7340\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0104 - mse: 9.0232 - val_loss: 0.0075 - val_mse: 6.5691\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0119 - mse: 10.3360 - val_loss: 0.0073 - val_mse: 6.3588\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0140 - mse: 12.1717 - val_loss: 0.0069 - val_mse: 6.0189\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0089 - mse: 7.7494 - val_loss: 0.0067 - val_mse: 5.8076\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0111 - mse: 9.6692 - val_loss: 0.0064 - val_mse: 5.5565\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0087 - mse: 7.6091 - val_loss: 0.0063 - val_mse: 5.4446\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0092 - mse: 8.0469 - val_loss: 0.0063 - val_mse: 5.4676\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0068 - mse: 5.9592 - val_loss: 0.0066 - val_mse: 5.7339\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0108 - mse: 9.4382 - val_loss: 0.0071 - val_mse: 6.1455\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0155 - mse: 13.5162 - val_loss: 0.0074 - val_mse: 6.4578\n",
      "Epoch 90/1000\n",
      "\n",
      "Epoch 00089: reducing learning rate to 0.00168070008513.\n",
      "1s - loss: 0.0088 - mse: 7.6953 - val_loss: 0.0076 - val_mse: 6.6429\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0101 - mse: 8.7795 - val_loss: 0.0076 - val_mse: 6.5942\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0220 - mse: 19.1478 - val_loss: 0.0073 - val_mse: 6.3654\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0092 - mse: 8.0069 - val_loss: 0.0070 - val_mse: 6.1271\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0145 - mse: 12.6292 - val_loss: 0.0068 - val_mse: 5.9431\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0099 - mse: 8.6367 - val_loss: 0.0066 - val_mse: 5.7306\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0070 - mse: 6.1210 - val_loss: 0.0064 - val_mse: 5.5930\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0157 - mse: 13.6447 - val_loss: 0.0063 - val_mse: 5.5270\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0071 - mse: 6.1568 - val_loss: 0.0064 - val_mse: 5.5459\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0221 - mse: 19.2199 - val_loss: 0.0063 - val_mse: 5.5247\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0128 - mse: 11.1145 - val_loss: 0.0065 - val_mse: 5.6428\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0062 - mse: 5.3931 - val_loss: 0.0067 - val_mse: 5.8579\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.0092 - mse: 8.0371 - val_loss: 0.0070 - val_mse: 6.0794\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.0120 - mse: 10.4932 - val_loss: 0.0074 - val_mse: 6.4263\n",
      "Epoch 104/1000\n",
      "\n",
      "Epoch 00103: reducing learning rate to 0.00117649007589.\n",
      "1s - loss: 0.0147 - mse: 12.8392 - val_loss: 0.0076 - val_mse: 6.6403\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.0135 - mse: 11.7851 - val_loss: 0.0075 - val_mse: 6.5403\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.0087 - mse: 7.5917 - val_loss: 0.0073 - val_mse: 6.3416\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.0054 - mse: 4.7288 - val_loss: 0.0070 - val_mse: 6.1123\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.0047 - mse: 4.1229 - val_loss: 0.0067 - val_mse: 5.8776\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.0074 - mse: 6.4810 - val_loss: 0.0066 - val_mse: 5.7134\n",
      "Epoch 110/1000\n",
      "1s - loss: 0.0101 - mse: 8.8282 - val_loss: 0.0064 - val_mse: 5.5359\n",
      "Epoch 111/1000\n",
      "1s - loss: 0.0086 - mse: 7.4550 - val_loss: 0.0061 - val_mse: 5.3099\n",
      "Epoch 112/1000\n",
      "1s - loss: 0.0105 - mse: 9.1645 - val_loss: 0.0058 - val_mse: 5.0542\n",
      "Epoch 113/1000\n",
      "1s - loss: 0.0135 - mse: 11.7918 - val_loss: 0.0055 - val_mse: 4.7952\n",
      "Epoch 114/1000\n",
      "1s - loss: 0.0098 - mse: 8.5760 - val_loss: 0.0053 - val_mse: 4.6113\n",
      "Epoch 115/1000\n",
      "1s - loss: 0.0056 - mse: 4.8509 - val_loss: 0.0052 - val_mse: 4.5181\n",
      "Epoch 116/1000\n",
      "1s - loss: 0.0070 - mse: 6.0984 - val_loss: 0.0051 - val_mse: 4.4226\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.0095 - mse: 8.2452 - val_loss: 0.0049 - val_mse: 4.3026\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.0194 - mse: 16.8735 - val_loss: 0.0048 - val_mse: 4.1788\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.0076 - mse: 6.6052 - val_loss: 0.0047 - val_mse: 4.0868\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.0065 - mse: 5.6523 - val_loss: 0.0046 - val_mse: 4.0347\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.0069 - mse: 6.0249 - val_loss: 0.0046 - val_mse: 4.0036\n",
      "Epoch 122/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.0085 - mse: 7.3826 - val_loss: 0.0046 - val_mse: 4.0034\n",
      "Epoch 123/1000\n",
      "1s - loss: 0.0101 - mse: 8.8049 - val_loss: 0.0046 - val_mse: 4.0189\n",
      "Epoch 124/1000\n",
      "\n",
      "Epoch 00123: reducing learning rate to 0.000823543069419.\n",
      "1s - loss: 0.0068 - mse: 5.9617 - val_loss: 0.0046 - val_mse: 4.0428\n",
      "Epoch 125/1000\n",
      "1s - loss: 0.0095 - mse: 8.2919 - val_loss: 0.0047 - val_mse: 4.0698\n",
      "Epoch 126/1000\n",
      "1s - loss: 0.0050 - mse: 4.3931 - val_loss: 0.0047 - val_mse: 4.1030\n",
      "Epoch 127/1000\n",
      "1s - loss: 0.0115 - mse: 10.0344 - val_loss: 0.0047 - val_mse: 4.1150\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.0063 - mse: 5.4656 - val_loss: 0.0048 - val_mse: 4.1442\n",
      "Epoch 129/1000\n",
      "1s - loss: 0.0142 - mse: 12.3470 - val_loss: 0.0047 - val_mse: 4.1284\n",
      "Epoch 130/1000\n",
      "1s - loss: 0.0094 - mse: 8.1556 - val_loss: 0.0047 - val_mse: 4.1108\n",
      "Epoch 131/1000\n",
      "1s - loss: 0.0090 - mse: 7.8049 - val_loss: 0.0047 - val_mse: 4.1057\n",
      "Epoch 132/1000\n",
      "1s - loss: 0.0056 - mse: 4.9026 - val_loss: 0.0047 - val_mse: 4.1048\n",
      "Epoch 133/1000\n",
      "1s - loss: 0.0184 - mse: 16.0500 - val_loss: 0.0047 - val_mse: 4.0997\n",
      "Epoch 134/1000\n",
      "1s - loss: 0.0115 - mse: 9.9964 - val_loss: 0.0047 - val_mse: 4.1207\n",
      "Epoch 135/1000\n",
      "1s - loss: 0.0127 - mse: 11.0984 - val_loss: 0.0047 - val_mse: 4.1353\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.0245 - mse: 21.3637 - val_loss: 0.0048 - val_mse: 4.1365\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.0048 - mse: 4.1953 - val_loss: 0.0048 - val_mse: 4.1437\n",
      "Epoch 138/1000\n",
      "\n",
      "Epoch 00137: reducing learning rate to 0.000576480140444.\n",
      "1s - loss: 0.0115 - mse: 9.9785 - val_loss: 0.0048 - val_mse: 4.1780\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.0091 - mse: 7.8980 - val_loss: 0.0048 - val_mse: 4.2089\n",
      "Epoch 140/1000\n",
      "1s - loss: 0.0104 - mse: 9.0328 - val_loss: 0.0048 - val_mse: 4.2198\n",
      "Epoch 141/1000\n",
      "1s - loss: 0.0058 - mse: 5.0448 - val_loss: 0.0049 - val_mse: 4.2372\n",
      "Epoch 142/1000\n",
      "1s - loss: 0.0062 - mse: 5.3758 - val_loss: 0.0049 - val_mse: 4.2451\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.0073 - mse: 6.3627 - val_loss: 0.0049 - val_mse: 4.2395\n",
      "Epoch 144/1000\n",
      "1s - loss: 0.0043 - mse: 3.7712 - val_loss: 0.0049 - val_mse: 4.2363\n",
      "Epoch 145/1000\n",
      "1s - loss: 0.0239 - mse: 20.8517 - val_loss: 0.0049 - val_mse: 4.2572\n",
      "Epoch 146/1000\n",
      "1s - loss: 0.0132 - mse: 11.5047 - val_loss: 0.0049 - val_mse: 4.2987\n",
      "Epoch 147/1000\n",
      "1s - loss: 0.0105 - mse: 9.1486 - val_loss: 0.0050 - val_mse: 4.3294\n",
      "Epoch 148/1000\n",
      "1s - loss: 0.0121 - mse: 10.5611 - val_loss: 0.0050 - val_mse: 4.3365\n",
      "Epoch 149/1000\n",
      "1s - loss: 0.0051 - mse: 4.4083 - val_loss: 0.0050 - val_mse: 4.3454\n",
      "Epoch 150/1000\n",
      "1s - loss: 0.0110 - mse: 9.5965 - val_loss: 0.0050 - val_mse: 4.3679\n",
      "Epoch 151/1000\n",
      "1s - loss: 0.0102 - mse: 8.8633 - val_loss: 0.0050 - val_mse: 4.3794\n",
      "Epoch 152/1000\n",
      "\n",
      "Epoch 00151: reducing learning rate to 0.000403536082013.\n",
      "1s - loss: 0.0061 - mse: 5.2904 - val_loss: 0.0051 - val_mse: 4.3986\n",
      "Epoch 153/1000\n",
      "1s - loss: 0.0140 - mse: 12.1538 - val_loss: 0.0050 - val_mse: 4.3936\n",
      "Epoch 154/1000\n",
      "1s - loss: 0.0090 - mse: 7.8745 - val_loss: 0.0051 - val_mse: 4.4112\n",
      "Epoch 155/1000\n",
      "1s - loss: 0.0103 - mse: 8.9266 - val_loss: 0.0051 - val_mse: 4.4211\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.0119 - mse: 10.3766 - val_loss: 0.0051 - val_mse: 4.4319\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.0088 - mse: 7.6797 - val_loss: 0.0051 - val_mse: 4.4396\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.0069 - mse: 6.0294 - val_loss: 0.0051 - val_mse: 4.4328\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.0084 - mse: 7.3045 - val_loss: 0.0051 - val_mse: 4.4231\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.0056 - mse: 4.9117 - val_loss: 0.0051 - val_mse: 4.4047\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.0088 - mse: 7.6552 - val_loss: 0.0051 - val_mse: 4.4038\n",
      "Epoch 162/1000\n",
      "1s - loss: 0.0177 - mse: 15.4302 - val_loss: 0.0051 - val_mse: 4.4238\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.0139 - mse: 12.1216 - val_loss: 0.0051 - val_mse: 4.4242\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.0060 - mse: 5.1960 - val_loss: 0.0051 - val_mse: 4.4413\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.0162 - mse: 14.1468 - val_loss: 0.0051 - val_mse: 4.4194\n",
      "Epoch 166/1000\n",
      "\n",
      "Epoch 00165: reducing learning rate to 0.000282475253334.\n",
      "1s - loss: 0.0051 - mse: 4.4375 - val_loss: 0.0051 - val_mse: 4.4081\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.0073 - mse: 6.3922 - val_loss: 0.0050 - val_mse: 4.3899\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.0072 - mse: 6.2866 - val_loss: 0.0050 - val_mse: 4.3722\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.0103 - mse: 8.9291 - val_loss: 0.0050 - val_mse: 4.3637\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.0063 - mse: 5.4721 - val_loss: 0.0050 - val_mse: 4.3569\n",
      "[{'loss': 0.0070067006163299084, 'width0': 135, 'width1': 150, 'val_mse': 9.5660200119018555, 'mse': 6.101707935333252, 'val_loss': 0.010984830558300018, 'dropout1': 0.0, 'dropout0': 0}, {'loss': 0.012262199074029922, 'width0': 135, 'width1': 150, 'val_mse': 11.153453826904297, 'mse': 10.678401947021484, 'val_loss': 0.012807708233594894, 'dropout1': 0.10000000000000001, 'dropout0': 0}, {'loss': 0.016871752217411995, 'width0': 135, 'width1': 150, 'val_mse': 13.781032562255859, 'mse': 14.692578315734863, 'val_loss': 0.015825005248188972, 'dropout1': 0.20000000000000001, 'dropout0': 0}, {'loss': 0.0079014608636498451, 'width0': 135, 'width1': 150, 'val_mse': 6.7946681976318359, 'mse': 6.8809013366699219, 'val_loss': 0.0078024403192102909, 'dropout1': 0.30000000000000004, 'dropout0': 0}, {'loss': 0.0077892974950373173, 'width0': 135, 'width1': 150, 'val_mse': 4.3584160804748535, 'mse': 6.7832231521606445, 'val_loss': 0.0050048483535647392, 'dropout1': 0.40000000000000002, 'dropout0': 0}, {'loss': 0.006283740047365427, 'width0': 135, 'width1': 150, 'val_mse': 4.3569273948669434, 'mse': 5.4721245765686035, 'val_loss': 0.0050031379796564579, 'dropout1': 0.5, 'dropout0': 0}]\n"
     ]
    }
   ],
   "source": [
    "dropout_patience = 2\n",
    "width1 = 135\n",
    "width2 = 150\n",
    "dropout_perf = []\n",
    "prev_weights = None\n",
    "for dropout2 in np.linspace(0.0, 0.5, num=6):\n",
    "    fit, results, prev_weights = run_depthn([{'width':width1, 'dropout':0},\n",
    "                                             {'width':width2, 'dropout':dropout2}\n",
    "                                            ], learningrates2, prev_weights, 'dropout2nd')\n",
    "    dropout_perf.append(results)\n",
    "    print dropout_perf\n",
    "    if len(dropout_perf)>dropout_patience and \\\n",
    "            dropout_perf[-1]['val_mse']>np.min(x['val_mse'] for x in dropout_perf[-dropout_patience:-1]):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x160b10a10>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAENCAYAAAAFcn7UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkgIvhBAuSgq8EEK4KCnwQgjhov4fzvKcYz6k5jIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x160b24690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(dropout_perf).loc[:,['dropout1','mse','val_mse']].set_index('dropout1').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no improvement...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Model tuning\n",
    "With the evaluation functions defined, we can start to create a predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First naive model\n",
    "Here we create a first model, a simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_regression = Pipeline([\n",
    "        (\"lm\", linear_model.LinearRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The quality of our first naive model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, predicting y with the original features (thus only just the soi):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"num features:{}\".format(X_orig.shape[1])\n",
    "print \"Scores (higher is better);\"\n",
    "print \"cv train:\\t{}\".format(np.mean(cross_val_train(linear_regression, X_orig.loc[X_train.index,:], y_train, cv_count=10)))\n",
    "print \"cv test:\\t{}\".format(np.mean(cross_val(linear_regression, X_orig.loc[X_train.index,:], y_train, cv_count=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably you do much better with your new set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"num features:{}\".format(X.shape[1])\n",
    "print \"Scores (higher is better);\"\n",
    "print \"cv train:\\t{}\".format(np.mean(cross_val_train(linear_regression, X, y, cv_count=10)))\n",
    "print \"cv test:\\t{}\".format(np.mean(cross_val(linear_regression, X, y, cv_count=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a different model\n",
    "Feel free to select a different model, and see if you can increase the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alternative = Pipeline([\n",
    "        (\"ridge\", Ridge(alpha=1.0))\n",
    "    ])\n",
    "print \"num features:\\t{}\".format(X.shape[1])\n",
    "print \"Scores (higher is better);\"\n",
    "print \"cv train:\\t{}\".format(np.mean(cross_val_train(alternative, X_train, y_train, cv_count=10)))\n",
    "print \"cv test:\\t{}\".format(np.mean(cross_val(alternative, X_train, y_train, cv_count=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "### Recommended solution for model tuning: grid search cv on meta params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models need some meta parameter tuning, so let us also do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid={'alpha':np.power(1.5, range(-8,4))}\n",
    "display(param_grid)\n",
    "ridge_cv = GridSearchCV(estimator = Ridge(),\n",
    "                       param_grid=param_grid,\n",
    "                       scoring=model_scorer,\n",
    "                       n_jobs=1,\n",
    "                       cv=GrowingWindow(4), # Usually useful to select a slightly different cv set\n",
    "                       verbose=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulted in the following best meta parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"params:\\t\\t{}\".format(ridge_cv.best_params_)\n",
    "print \"num features:\\t{}\".format(X.shape[1])\n",
    "print \"Scores (higher is better);\"\n",
    "print \"cv train:\\t{}\".format(np.mean(cross_val_train(ridge_cv.best_estimator_, X_train, y_train, cv_count=10)))\n",
    "print \"cv test:\\t{}\".format(np.mean(cross_val(ridge_cv.best_estimator_, X_train, y_train, cv_count=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_best = Ridge(**(ridge_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 - Selecting features\n",
    "Not all features are actually equally usefull. With too many available features, most models will have problems distinguishing noise from signal. One way is to use regularization (as used above), the other is to actually remove useless features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (partial) Autocorrelation\n",
    "One classic way to determine usefull features is by looking at the autocorrelation, and partial autocorrelation.  \n",
    "The regular autocorrelation shows the correlation of the target variable `recruit` with the lagged versions. Thus this is done by correlating the `recruit` with **just** the 1 lagged version of `recruit`, and then repeated for different lags.  \n",
    "However, if lag 1 correlates, lag 2 will also correlate, because lag 2 is the 1 lagged version of lag 1.  \n",
    "Therefore the partial autocorrelation plot is more usefull, as it shows the correlation, corrected for the correlation with the other lags. It can be made by applying a linear regression of the target `recruit` with **all** lagged versions in 1 model, such that the individual coefficients indicate the individual marginal effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.plot_acf(y_orig.squeeze(), lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.plot_pacf(y_orig.squeeze(), lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that mainly the first 2 lags of `recruit` are useful, the others are just noise. (The first one at `x==0` reflects the correlation of `recruit` with itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended solution for feature selection: Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended solution is to iteratively remove features, a few at a time, and take the set which works best on a validation set.\n",
    "\n",
    "First we show how `RFE` in general works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfe = RFECV(model_best, step=5, scoring = model_scorer, cv=GrowingWindow(6))\n",
    "print \"Scores (higher is better) (not this selects features per fold);\"\n",
    "print \"cv train:\\t{}\".format(np.mean(cross_val_train(rfe, X_train, y_train, cv_count=10)))\n",
    "print \"cv test:\\t{}\".format(np.mean(cross_val(rfe, X_train, y_train, cv_count=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we determine a fit, to get the order in which features are removed.  \n",
    "In the current setup, the model is first fitted using all features. The 5 least important features are removed, and the model is refit. This is repeated until only 10 features remain. This gives an ordering of all features (the order they were removed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfe_fit = rfe.fit(X_train, y_train)\n",
    "print rfe_fit.n_features_\n",
    "print \"As an example, the last remaining features were:\"\n",
    "X_train.loc[:, rfe_fit.ranking_<=1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, per step, we determine the cross val score using the features that were still remaining at that step. This gives a good evaluation of those features, of which we can then select the best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfe_all = [np.mean(cross_val(\n",
    "                                model_best, \n",
    "                                X_train.loc[:, rfe_fit.ranking_<=i], \n",
    "                                y_train, \n",
    "                                cv_count=3))\n",
    "     for i in range(1, max(rfe_fit.ranking_))]\n",
    "best_index = np.array(rfe_all).argsort()[::-1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define two variables to indicate the selected features:\n",
    " * `column_mask` : an indexer to be used on the pandas dataframe to select columns\n",
    " * `X_sub_train` : The full train dataset (`X_train`), but only the selected features\n",
    " * `X_sub_test` : The train dataset (`X_test`), but only the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_mask = rfe_fit.ranking_<=(best_index+1)\n",
    "X_sub_train = X_train.loc[:, column_mask]\n",
    "X_sub_test = X_test.loc[:, column_mask]\n",
    "print 'Best index          = {}'.format(best_index)\n",
    "print 'Best nr of features = {}'.format(sum(column_mask))\n",
    "print 'Which gives score   = {}'.format(rfe_all[best_index])\n",
    "\n",
    "print 'Column names        = {}'.format(X_train.columns[column_mask].values)\n",
    "print \"Scores (higher is better);\"\n",
    "print \"cv train:\\t{}\".format(np.mean(cross_val_train(model_best, X_sub_train, y_train, cv_count=10)))\n",
    "print \"cv test:\\t{}\".format(np.mean(cross_val(model_best, X_sub_train, y_train, cv_count=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 - Final score on the test set\n",
    "When you are done tuning and selection, it is time to evaluate the performance on a hold out set.\n",
    "\n",
    "To easily print the same measurements for different configurations, we define a helper function here:\n",
    "> Here we multiply the `model_score` by -1 to get the score comparable to the previous cross validations  \n",
    "> Note that the holdout test score will very likely be worse than the cv test score. One reason is that all meta params were selected to optimize that test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_describe(model, train, test):\n",
    "    \"\"\"Evaluates a model on the final test set. For comparison, also (cv) measurements are given about the train set.\n",
    "    model is the pandas pipeline that is evaluated\n",
    "    train is the train dataset (with more/less features)\n",
    "    test is the test dataset (with the same features as x_train)\n",
    "    \"\"\"\n",
    "    # first some cross validation measurements\n",
    "    print \"cross validated (within train set)\"\n",
    "    print \"cv train:\\t{}\".format(np.mean(cross_val_train(model, train, y_train, cv_count=10)))\n",
    "    print \"cv test:\\t{}\".format(np.mean(cross_val(model, train, y_train, cv_count=10)))\n",
    "    # first fit the model on the FULL train set\n",
    "    fit = model.fit(train, y_train)\n",
    "    test_predictions = fit.predict(test)\n",
    "    print 'full train:\\t{}'.format(-model_score(y_train, fit.predict(train)))\n",
    "    print \"After fitting on full train set, evaluating on holdout set:\"\n",
    "    print 'Holdout test:\\t{}'.format(-model_score(y_test, test_predictions))\n",
    "    print 'Holdout r2:\\t{}'.format(r2_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_describe(model_best, X_sub_train, X_sub_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = model_best.fit(X_sub_train, y_train)\n",
    "test_predictions = fit.predict(X_sub_test)\n",
    "result = pd.DataFrame({\"y_pred\":test_predictions, \"y_real\":y_test})\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_describe(model_best, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative model on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_describe(alternative, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial (linear) model on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_describe(linear_regression, X_train, X_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bdranalytics]",
   "language": "python",
   "name": "conda-env-bdranalytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
